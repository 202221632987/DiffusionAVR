func NewSTM(c *v3.Client, apply func(STM) error, so ...stmOption) (*v3.TxnResponse, error) {
	opts := &stmOptions{ctx: c.Ctx()}
	for _, f := range so {
		f(opts)
	}
	if len(opts.prefetch) != 0 {
		f := apply
		apply = func(s STM) error {
			s.Get(opts.prefetch...)
			return f(s)
		}
	}
	return runSTM(mkSTM(c, opts), apply)
}
func (rs readSet) first() int64 {
	ret := int64(math.MaxInt64 - 1)
	for _, resp := range rs {
		if rev := resp.Header.Revision; rev < ret {
			ret = rev
		}
	}
	return ret
}
func (ws writeSet) cmps(rev int64) []v3.Cmp {
	cmps := make([]v3.Cmp, 0, len(ws))
	for key := range ws {
		cmps = append(cmps, v3.Compare(v3.ModRevision(key), "<", rev))
	}
	return cmps
}
func NewSTMRepeatable(ctx context.Context, c *v3.Client, apply func(STM) error) (*v3.TxnResponse, error) {
	return NewSTM(c, apply, WithAbortContext(ctx), WithIsolation(RepeatableReads))
}
func NewSTMSerializable(ctx context.Context, c *v3.Client, apply func(STM) error) (*v3.TxnResponse, error) {
	return NewSTM(c, apply, WithAbortContext(ctx), WithIsolation(Serializable))
}
func NewSTMReadCommitted(ctx context.Context, c *v3.Client, apply func(STM) error) (*v3.TxnResponse, error) {
	return NewSTM(c, apply, WithAbortContext(ctx), WithIsolation(ReadCommitted))
}
func NewCertPool(CAFiles []string) (*x509.CertPool, error) {
	certPool := x509.NewCertPool()

	for _, CAFile := range CAFiles {
		pemByte, err := ioutil.ReadFile(CAFile)
		if err != nil {
			return nil, err
		}

		for {
			var block *pem.Block
			block, pemByte = pem.Decode(pemByte)
			if block == nil {
				break
			}
			cert, err := x509.ParseCertificate(block.Bytes)
			if err != nil {
				return nil, err
			}

			certPool.AddCert(cert)
		}
	}

	return certPool, nil
}
func NewCert(certfile, keyfile string, parseFunc func([]byte, []byte) (tls.Certificate, error)) (*tls.Certificate, error) {
	cert, err := ioutil.ReadFile(certfile)
	if err != nil {
		return nil, err
	}

	key, err := ioutil.ReadFile(keyfile)
	if err != nil {
		return nil, err
	}

	if parseFunc == nil {
		parseFunc = tls.X509KeyPair
	}

	tlsCert, err := parseFunc(cert, key)
	if err != nil {
		return nil, err
	}
	return &tlsCert, nil
}
func (p *peer) Pause() {
	p.mu.Lock()
	defer p.mu.Unlock()
	p.paused = true
	p.msgAppReader.pause()
	p.msgAppV2Reader.pause()
}
func (p *peer) Resume() {
	p.mu.Lock()
	defer p.mu.Unlock()
	p.paused = false
	p.msgAppReader.resume()
	p.msgAppV2Reader.resume()
}
func (p *peer) pick(m raftpb.Message) (writec chan<- raftpb.Message, picked string) {
	var ok bool
	// Considering MsgSnap may have a big size, e.g., 1G, and will block
	// stream for a long time, only use one of the N pipelines to send MsgSnap.
	if isMsgSnap(m) {
		return p.pipeline.msgc, pipelineMsg
	} else if writec, ok = p.msgAppV2Writer.writec(); ok && isMsgApp(m) {
		return writec, streamAppV2
	} else if writec, ok = p.writer.writec(); ok {
		return writec, streamMsg
	}
	return p.pipeline.msgc, pipelineMsg
}
func (s *snapshotSender) post(req *http.Request) (err error) {
	ctx, cancel := context.WithCancel(context.Background())
	req = req.WithContext(ctx)
	defer cancel()

	type responseAndError struct {
		resp *http.Response
		body []byte
		err  error
	}
	result := make(chan responseAndError, 1)

	go func() {
		resp, err := s.tr.pipelineRt.RoundTrip(req)
		if err != nil {
			result <- responseAndError{resp, nil, err}
			return
		}

		// close the response body when timeouts.
		// prevents from reading the body forever when the other side dies right after
		// successfully receives the request body.
		time.AfterFunc(snapResponseReadTimeout, func() { httputil.GracefulClose(resp) })
		body, err := ioutil.ReadAll(resp.Body)
		result <- responseAndError{resp, body, err}
	}()

	select {
	case <-s.stopc:
		return errStopped
	case r := <-result:
		if r.err != nil {
			return r.err
		}
		return checkPostResponse(r.resp, r.body, req, s.to)
	}
}
func newTxnResp(rt *pb.TxnRequest, txnPath []bool) (txnResp *pb.TxnResponse, txnCount int) {
	reqs := rt.Success
	if !txnPath[0] {
		reqs = rt.Failure
	}
	resps := make([]*pb.ResponseOp, len(reqs))
	txnResp = &pb.TxnResponse{
		Responses: resps,
		Succeeded: txnPath[0],
		Header:    &pb.ResponseHeader{},
	}
	for i, req := range reqs {
		switch tv := req.Request.(type) {
		case *pb.RequestOp_RequestRange:
			resps[i] = &pb.ResponseOp{Response: &pb.ResponseOp_ResponseRange{}}
		case *pb.RequestOp_RequestPut:
			resps[i] = &pb.ResponseOp{Response: &pb.ResponseOp_ResponsePut{}}
		case *pb.RequestOp_RequestDeleteRange:
			resps[i] = &pb.ResponseOp{Response: &pb.ResponseOp_ResponseDeleteRange{}}
		case *pb.RequestOp_RequestTxn:
			resp, txns := newTxnResp(tv.RequestTxn, txnPath[1:])
			resps[i] = &pb.ResponseOp{Response: &pb.ResponseOp_ResponseTxn{ResponseTxn: resp}}
			txnPath = txnPath[1+txns:]
			txnCount += txns + 1
		default:
		}
	}
	return txnResp, txnCount
}
func applyCompare(rv mvcc.ReadView, c *pb.Compare) bool {
	// TODO: possible optimizations
	// * chunk reads for large ranges to conserve memory
	// * rewrite rules for common patterns:
	//	ex. "[a, b) createrev > 0" => "limit 1 /\ kvs > 0"
	// * caching
	rr, err := rv.Range(c.Key, mkGteRange(c.RangeEnd), mvcc.RangeOptions{})
	if err != nil {
		return false
	}
	if len(rr.KVs) == 0 {
		if c.Target == pb.Compare_VALUE {
			// Always fail if comparing a value on a key/keys that doesn't exist;
			// nil == empty string in grpc; no way to represent missing value
			return false
		}
		return compareKV(c, mvccpb.KeyValue{})
	}
	for _, kv := range rr.KVs {
		if !compareKV(c, kv) {
			return false
		}
	}
	return true
}
func OpCompact(rev int64, opts ...CompactOption) CompactOp {
	ret := CompactOp{revision: rev}
	ret.applyCompactOpts(opts)
	return ret
}
func NewPriorityQueue(client *v3.Client, key string) *PriorityQueue {
	return &PriorityQueue{client, context.TODO(), key + "/"}
}
func (q *PriorityQueue) Enqueue(val string, pr uint16) error {
	prefix := fmt.Sprintf("%s%05d", q.key, pr)
	_, err := newSequentialKV(q.client, prefix, val)
	return err
}
func NewLeaderStats(id string) *LeaderStats {
	return &LeaderStats{
		leaderStats: leaderStats{
			Leader:    id,
			Followers: make(map[string]*FollowerStats),
		},
	}
}
func (fs *FollowerStats) Succ(d time.Duration) {
	fs.Lock()
	defer fs.Unlock()

	total := float64(fs.Counts.Success) * fs.Latency.Average
	totalSquare := float64(fs.Counts.Success) * fs.Latency.averageSquare

	fs.Counts.Success++

	fs.Latency.Current = float64(d) / (1000000.0)

	if fs.Latency.Current > fs.Latency.Maximum {
		fs.Latency.Maximum = fs.Latency.Current
	}

	if fs.Latency.Current < fs.Latency.Minimum {
		fs.Latency.Minimum = fs.Latency.Current
	}

	fs.Latency.Average = (total + fs.Latency.Current) / float64(fs.Counts.Success)
	fs.Latency.averageSquare = (totalSquare + fs.Latency.Current*fs.Latency.Current) / float64(fs.Counts.Success)

	// sdv = sqrt(avg(x^2) - avg(x)^2)
	fs.Latency.StandardDeviation = math.Sqrt(fs.Latency.averageSquare - fs.Latency.Average*fs.Latency.Average)
}
func (fs *FollowerStats) Fail() {
	fs.Lock()
	defer fs.Unlock()
	fs.Counts.Fail++
}
func (wbs *watchBroadcasts) delete(w *watcher) int {
	wbs.mu.Lock()
	defer wbs.mu.Unlock()

	wb, ok := wbs.watchers[w]
	if !ok {
		panic("deleting missing watcher from broadcasts")
	}
	delete(wbs.watchers, w)
	wb.delete(w)
	if wb.empty() {
		delete(wbs.bcasts, wb)
		wb.stop()
	}
	return len(wbs.bcasts)
}
func startStreamWriter(lg *zap.Logger, local, id types.ID, status *peerStatus, fs *stats.FollowerStats, r Raft) *streamWriter {
	w := &streamWriter{
		lg: lg,

		localID: local,
		peerID:  id,

		status: status,
		fs:     fs,
		r:      r,
		msgc:   make(chan raftpb.Message, streamBufSize),
		connc:  make(chan *outgoingConn),
		stopc:  make(chan struct{}),
		done:   make(chan struct{}),
	}
	go w.run()
	return w
}
func checkStreamSupport(v *semver.Version, t streamType) bool {
	nv := &semver.Version{Major: v.Major, Minor: v.Minor}
	for _, s := range supportedStream[nv.String()] {
		if s == t {
			return true
		}
	}
	return false
}
func (pr *Progress) maybeUpdate(n uint64) bool {
	var updated bool
	if pr.Match < n {
		pr.Match = n
		updated = true
		pr.resume()
	}
	if pr.Next < n+1 {
		pr.Next = n + 1
	}
	return updated
}
func (pr *Progress) IsPaused() bool {
	switch pr.State {
	case ProgressStateProbe:
		return pr.Paused
	case ProgressStateReplicate:
		return pr.ins.full()
	case ProgressStateSnapshot:
		return true
	default:
		panic("unexpected state")
	}
}
func (pr *Progress) needSnapshotAbort() bool {
	return pr.State == ProgressStateSnapshot && pr.Match >= pr.PendingSnapshot
}
func (in *inflights) add(inflight uint64) {
	if in.full() {
		panic("cannot add into a full inflights")
	}
	next := in.start + in.count
	size := in.size
	if next >= size {
		next -= size
	}
	if next >= len(in.buffer) {
		in.growBuf()
	}
	in.buffer[next] = inflight
	in.count++
}
func (in *inflights) growBuf() {
	newSize := len(in.buffer) * 2
	if newSize == 0 {
		newSize = 1
	} else if newSize > in.size {
		newSize = in.size
	}
	newBuffer := make([]uint64, newSize)
	copy(newBuffer, in.buffer)
	in.buffer = newBuffer
}
func (in *inflights) freeTo(to uint64) {
	if in.count == 0 || to < in.buffer[in.start] {
		// out of the left side of the window
		return
	}

	idx := in.start
	var i int
	for i = 0; i < in.count; i++ {
		if to < in.buffer[idx] { // found the first large inflight
			break
		}

		// increase index and maybe rotate
		size := in.size
		if idx++; idx >= size {
			idx -= size
		}
	}
	// free i inflights and set new start index
	in.count -= i
	in.start = idx
	if in.count == 0 {
		// inflights is empty, reset the start index so that we don't grow the
		// buffer unnecessarily.
		in.start = 0
	}
}
func (s *Snapshotter) SaveDBFrom(r io.Reader, id uint64) (int64, error) {
	start := time.Now()

	f, err := ioutil.TempFile(s.dir, "tmp")
	if err != nil {
		return 0, err
	}
	var n int64
	n, err = io.Copy(f, r)
	if err == nil {
		fsyncStart := time.Now()
		err = fileutil.Fsync(f)
		snapDBFsyncSec.Observe(time.Since(fsyncStart).Seconds())
	}
	f.Close()
	if err != nil {
		os.Remove(f.Name())
		return n, err
	}
	fn := s.dbFilePath(id)
	if fileutil.Exist(fn) {
		os.Remove(f.Name())
		return n, nil
	}
	err = os.Rename(f.Name(), fn)
	if err != nil {
		os.Remove(f.Name())
		return n, err
	}

	if s.lg != nil {
		s.lg.Info(
			"saved database snapshot to disk",
			zap.String("path", fn),
			zap.Int64("bytes", n),
			zap.String("size", humanize.Bytes(uint64(n))),
		)
	} else {
		plog.Infof("saved database snapshot to disk [total bytes: %d]", n)
	}

	snapDBSaveSec.Observe(time.Since(start).Seconds())
	return n, nil
}
func (s *Snapshotter) DBFilePath(id uint64) (string, error) {
	if _, err := fileutil.ReadDir(s.dir); err != nil {
		return "", err
	}
	fn := s.dbFilePath(id)
	if fileutil.Exist(fn) {
		return fn, nil
	}
	if s.lg != nil {
		s.lg.Warn(
			"failed to find [SNAPSHOT-INDEX].snap.db",
			zap.Uint64("snapshot-index", id),
			zap.String("snapshot-file-path", fn),
			zap.Error(ErrNoDBSnapshot),
		)
	}
	return "", ErrNoDBSnapshot
}
func (us *UniqueStringsValue) Set(s string) error {
	us.Values = make(map[string]struct{})
	for _, v := range strings.Split(s, ",") {
		us.Values[v] = struct{}{}
	}
	return nil
}
func NewUniqueStringsValue(s string) (us *UniqueStringsValue) {
	us = &UniqueStringsValue{Values: make(map[string]struct{})}
	if s == "" {
		return us
	}
	if err := us.Set(s); err != nil {
		plog.Panicf("new UniqueStringsValue should never fail: %v", err)
	}
	return us
}
func UniqueStringsFromFlag(fs *flag.FlagSet, flagName string) []string {
	return (*fs.Lookup(flagName).Value.(*UniqueStringsValue)).stringSlice()
}
func UniqueStringsMapFromFlag(fs *flag.FlagSet, flagName string) map[string]struct{} {
	return (*fs.Lookup(flagName).Value.(*UniqueStringsValue)).Values
}
func Percentiles(nums []float64) (pcs []float64, data []float64) {
	return pctls, percentiles(nums)
}
func (c *ServerConfig) VerifyBootstrap() error {
	if err := c.hasLocalMember(); err != nil {
		return err
	}
	if err := c.advertiseMatchesCluster(); err != nil {
		return err
	}
	if checkDuplicateURL(c.InitialPeerURLsMap) {
		return fmt.Errorf("initial cluster %s has duplicate url", c.InitialPeerURLsMap)
	}
	if c.InitialPeerURLsMap.String() == "" && c.DiscoveryURL == "" {
		return fmt.Errorf("initial cluster unset and no discovery URL found")
	}
	return nil
}
func (c *ServerConfig) VerifyJoinExisting() error {
	// The member has announced its peer urls to the cluster before starting; no need to
	// set the configuration again.
	if err := c.hasLocalMember(); err != nil {
		return err
	}
	if checkDuplicateURL(c.InitialPeerURLsMap) {
		return fmt.Errorf("initial cluster %s has duplicate url", c.InitialPeerURLsMap)
	}
	if c.DiscoveryURL != "" {
		return fmt.Errorf("discovery URL should not be set when joining existing initial cluster")
	}
	return nil
}
func (c *ServerConfig) hasLocalMember() error {
	if urls := c.InitialPeerURLsMap[c.Name]; urls == nil {
		return fmt.Errorf("couldn't find local name %q in the initial cluster configuration", c.Name)
	}
	return nil
}
func (c *ServerConfig) advertiseMatchesCluster() error {
	urls, apurls := c.InitialPeerURLsMap[c.Name], c.PeerURLs.StringSlice()
	urls.Sort()
	sort.Strings(apurls)
	ctx, cancel := context.WithTimeout(context.TODO(), 30*time.Second)
	defer cancel()
	ok, err := netutil.URLStringsEqual(ctx, c.Logger, apurls, urls.StringSlice())
	if ok {
		return nil
	}

	initMap, apMap := make(map[string]struct{}), make(map[string]struct{})
	for _, url := range c.PeerURLs {
		apMap[url.String()] = struct{}{}
	}
	for _, url := range c.InitialPeerURLsMap[c.Name] {
		initMap[url.String()] = struct{}{}
	}

	missing := []string{}
	for url := range initMap {
		if _, ok := apMap[url]; !ok {
			missing = append(missing, url)
		}
	}
	if len(missing) > 0 {
		for i := range missing {
			missing[i] = c.Name + "=" + missing[i]
		}
		mstr := strings.Join(missing, ",")
		apStr := strings.Join(apurls, ",")
		return fmt.Errorf("--initial-cluster has %s but missing from --initial-advertise-peer-urls=%s (%v)", mstr, apStr, err)
	}

	for url := range apMap {
		if _, ok := initMap[url]; !ok {
			missing = append(missing, url)
		}
	}
	if len(missing) > 0 {
		mstr := strings.Join(missing, ",")
		umap := types.URLsMap(map[string]types.URLs{c.Name: c.PeerURLs})
		return fmt.Errorf("--initial-advertise-peer-urls has %s but missing from --initial-cluster=%s", mstr, umap.String())
	}

	// resolved URLs from "--initial-advertise-peer-urls" and "--initial-cluster" did not match or failed
	apStr := strings.Join(apurls, ",")
	umap := types.URLsMap(map[string]types.URLs{c.Name: c.PeerURLs})
	return fmt.Errorf("failed to resolve %s to match --initial-cluster=%s (%v)", apStr, umap.String(), err)
}
func (c *ServerConfig) ReqTimeout() time.Duration {
	// 5s for queue waiting, computation and disk IO delay
	// + 2 * election timeout for possible leader election
	return 5*time.Second + 2*time.Duration(c.ElectionTicks*int(c.TickMs))*time.Millisecond
}
func getStatus(r *raft) Status {
	s := getStatusWithoutProgress(r)
	if s.RaftState == StateLeader {
		s.Progress = getProgressCopy(r)
	}
	return s
}
func GetDefaultHost() (string, error) {
	rmsgs, rerr := getDefaultRoutes()
	if rerr != nil {
		return "", rerr
	}

	// prioritize IPv4
	if rmsg, ok := rmsgs[syscall.AF_INET]; ok {
		if host, err := chooseHost(syscall.AF_INET, rmsg); host != "" || err != nil {
			return host, err
		}
		delete(rmsgs, syscall.AF_INET)
	}

	// sort so choice is deterministic
	var families []int
	for family := range rmsgs {
		families = append(families, int(family))
	}
	sort.Ints(families)

	for _, f := range families {
		family := uint8(f)
		if host, err := chooseHost(family, rmsgs[family]); host != "" || err != nil {
			return host, err
		}
	}

	return "", errNoDefaultHost
}
func getIfaceAddr(idx uint32, family uint8) (*syscall.NetlinkMessage, error) {
	dat, err := syscall.NetlinkRIB(syscall.RTM_GETADDR, int(family))
	if err != nil {
		return nil, err
	}

	msgs, msgErr := syscall.ParseNetlinkMessage(dat)
	if msgErr != nil {
		return nil, msgErr
	}

	ifaddrmsg := syscall.IfAddrmsg{}
	for _, m := range msgs {
		if m.Header.Type != syscall.RTM_NEWADDR {
			continue
		}
		buf := bytes.NewBuffer(m.Data[:syscall.SizeofIfAddrmsg])
		if rerr := binary.Read(buf, cpuutil.ByteOrder(), &ifaddrmsg); rerr != nil {
			continue
		}
		if ifaddrmsg.Index == idx {
			return &m, nil
		}
	}

	return nil, fmt.Errorf("could not find address for interface index %v", idx)

}
func getIfaceLink(idx uint32) (*syscall.NetlinkMessage, error) {
	dat, err := syscall.NetlinkRIB(syscall.RTM_GETLINK, syscall.AF_UNSPEC)
	if err != nil {
		return nil, err
	}

	msgs, msgErr := syscall.ParseNetlinkMessage(dat)
	if msgErr != nil {
		return nil, msgErr
	}

	ifinfomsg := syscall.IfInfomsg{}
	for _, m := range msgs {
		if m.Header.Type != syscall.RTM_NEWLINK {
			continue
		}
		buf := bytes.NewBuffer(m.Data[:syscall.SizeofIfInfomsg])
		if rerr := binary.Read(buf, cpuutil.ByteOrder(), &ifinfomsg); rerr != nil {
			continue
		}
		if ifinfomsg.Index == int32(idx) {
			return &m, nil
		}
	}

	return nil, fmt.Errorf("could not find link for interface index %v", idx)
}
func lsCommandFunc(c *cli.Context, ki client.KeysAPI) {
	key := "/"
	if len(c.Args()) != 0 {
		key = c.Args()[0]
	}

	sort := c.Bool("sort")
	recursive := c.Bool("recursive")
	quorum := c.Bool("quorum")

	ctx, cancel := contextWithTotalTimeout(c)
	resp, err := ki.Get(ctx, key, &client.GetOptions{Sort: sort, Recursive: recursive, Quorum: quorum})
	cancel()
	if err != nil {
		handleError(c, ExitServerError, err)
	}

	printLs(c, resp)
}
func printLs(c *cli.Context, resp *client.Response) {
	if c.GlobalString("output") == "simple" {
		if !resp.Node.Dir {
			fmt.Println(resp.Node.Key)
		}
		for _, node := range resp.Node.Nodes {
			rPrint(c, node)
		}
	} else {
		// user wants JSON or extended output
		printResponseKey(resp, c.GlobalString("output"))
	}
}
func rPrint(c *cli.Context, n *client.Node) {
	if n.Dir && c.Bool("p") {
		fmt.Println(fmt.Sprintf("%v/", n.Key))
	} else {
		fmt.Println(n.Key)
	}

	for _, node := range n.Nodes {
		rPrint(c, node)
	}
}
func NewLeaseRenewerCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "lease-renewer",
		Short: "Performs lease renew operation",
		Run:   runLeaseRenewerFunc,
	}
	cmd.Flags().Int64Var(&leaseTTL, "ttl", 5, "lease's ttl")
	return cmd
}
func Read(lg *zap.Logger, snapname string) (*raftpb.Snapshot, error) {
	b, err := ioutil.ReadFile(snapname)
	if err != nil {
		if lg != nil {
			lg.Warn("failed to read a snap file", zap.String("path", snapname), zap.Error(err))
		} else {
			plog.Errorf("cannot read file %v: %v", snapname, err)
		}
		return nil, err
	}

	if len(b) == 0 {
		if lg != nil {
			lg.Warn("failed to read empty snapshot file", zap.String("path", snapname))
		} else {
			plog.Errorf("unexpected empty snapshot")
		}
		return nil, ErrEmptySnapshot
	}

	var serializedSnap snappb.Snapshot
	if err = serializedSnap.Unmarshal(b); err != nil {
		if lg != nil {
			lg.Warn("failed to unmarshal snappb.Snapshot", zap.String("path", snapname), zap.Error(err))
		} else {
			plog.Errorf("corrupted snapshot file %v: %v", snapname, err)
		}
		return nil, err
	}

	if len(serializedSnap.Data) == 0 || serializedSnap.Crc == 0 {
		if lg != nil {
			lg.Warn("failed to read empty snapshot data", zap.String("path", snapname))
		} else {
			plog.Errorf("unexpected empty snapshot")
		}
		return nil, ErrEmptySnapshot
	}

	crc := crc32.Update(0, crcTable, serializedSnap.Data)
	if crc != serializedSnap.Crc {
		if lg != nil {
			lg.Warn("snap file is corrupt",
				zap.String("path", snapname),
				zap.Uint32("prev-crc", serializedSnap.Crc),
				zap.Uint32("new-crc", crc),
			)
		} else {
			plog.Errorf("corrupted snapshot file %v: crc mismatch", snapname)
		}
		return nil, ErrCRCMismatch
	}

	var snap raftpb.Snapshot
	if err = snap.Unmarshal(serializedSnap.Data); err != nil {
		if lg != nil {
			lg.Warn("failed to unmarshal raftpb.Snapshot", zap.String("path", snapname), zap.Error(err))
		} else {
			plog.Errorf("corrupted snapshot file %v: %v", snapname, err)
		}
		return nil, err
	}
	return &snap, nil
}
func GetCipherSuite(s string) (uint16, bool) {
	v, ok := cipherSuites[s]
	return v, ok
}
func (p *pipeline) post(data []byte) (err error) {
	u := p.picker.pick()
	req := createPostRequest(u, RaftPrefix, bytes.NewBuffer(data), "application/protobuf", p.tr.URLs, p.tr.ID, p.tr.ClusterID)

	done := make(chan struct{}, 1)
	ctx, cancel := context.WithCancel(context.Background())
	req = req.WithContext(ctx)
	go func() {
		select {
		case <-done:
		case <-p.stopc:
			waitSchedule()
			cancel()
		}
	}()

	resp, err := p.tr.pipelineRt.RoundTrip(req)
	done <- struct{}{}
	if err != nil {
		p.picker.unreachable(u)
		return err
	}
	defer resp.Body.Close()
	b, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		p.picker.unreachable(u)
		return err
	}

	err = checkPostResponse(resp, b, req, p.peerID)
	if err != nil {
		p.picker.unreachable(u)
		// errMemberRemoved is a critical error since a removed member should
		// always be stopped. So we use reportCriticalError to report it to errorc.
		if err == errMemberRemoved {
			reportCriticalError(err, p.errorc)
		}
		return err
	}

	return nil
}
func (r *raft) send(m pb.Message) {
	m.From = r.id
	if m.Type == pb.MsgVote || m.Type == pb.MsgVoteResp || m.Type == pb.MsgPreVote || m.Type == pb.MsgPreVoteResp {
		if m.Term == 0 {
			// All {pre-,}campaign messages need to have the term set when
			// sending.
			// - MsgVote: m.Term is the term the node is campaigning for,
			//   non-zero as we increment the term when campaigning.
			// - MsgVoteResp: m.Term is the new r.Term if the MsgVote was
			//   granted, non-zero for the same reason MsgVote is
			// - MsgPreVote: m.Term is the term the node will campaign,
			//   non-zero as we use m.Term to indicate the next term we'll be
			//   campaigning for
			// - MsgPreVoteResp: m.Term is the term received in the original
			//   MsgPreVote if the pre-vote was granted, non-zero for the
			//   same reasons MsgPreVote is
			panic(fmt.Sprintf("term should be set when sending %s", m.Type))
		}
	} else {
		if m.Term != 0 {
			panic(fmt.Sprintf("term should not be set when sending %s (was %d)", m.Type, m.Term))
		}
		// do not attach term to MsgProp, MsgReadIndex
		// proposals are a way to forward to the leader and
		// should be treated as local message.
		// MsgReadIndex is also forwarded to leader.
		if m.Type != pb.MsgProp && m.Type != pb.MsgReadIndex {
			m.Term = r.Term
		}
	}
	r.msgs = append(r.msgs, m)
}
func (r *raft) sendHeartbeat(to uint64, ctx []byte) {
	// Attach the commit as min(to.matched, r.committed).
	// When the leader sends out heartbeat message,
	// the receiver(follower) might not be matched with the leader
	// or it might not have all the committed entries.
	// The leader MUST NOT forward the follower's commit to
	// an unmatched index.
	commit := min(r.getProgress(to).Match, r.raftLog.committed)
	m := pb.Message{
		To:      to,
		Type:    pb.MsgHeartbeat,
		Commit:  commit,
		Context: ctx,
	}

	r.send(m)
}
func (r *raft) bcastAppend() {
	r.forEachProgress(func(id uint64, _ *Progress) {
		if id == r.id {
			return
		}

		r.sendAppend(id)
	})
}
func (r *raft) bcastHeartbeat() {
	lastCtx := r.readOnly.lastPendingRequestCtx()
	if len(lastCtx) == 0 {
		r.bcastHeartbeatWithCtx(nil)
	} else {
		r.bcastHeartbeatWithCtx([]byte(lastCtx))
	}
}
func (r *raft) tickElection() {
	r.electionElapsed++

	if r.promotable() && r.pastElectionTimeout() {
		r.electionElapsed = 0
		r.Step(pb.Message{From: r.id, Type: pb.MsgHup})
	}
}
func (r *raft) tickHeartbeat() {
	r.heartbeatElapsed++
	r.electionElapsed++

	if r.electionElapsed >= r.electionTimeout {
		r.electionElapsed = 0
		if r.checkQuorum {
			r.Step(pb.Message{From: r.id, Type: pb.MsgCheckQuorum})
		}
		// If current leader cannot transfer leadership in electionTimeout, it becomes leader again.
		if r.state == StateLeader && r.leadTransferee != None {
			r.abortLeaderTransfer()
		}
	}

	if r.state != StateLeader {
		return
	}

	if r.heartbeatElapsed >= r.heartbeatTimeout {
		r.heartbeatElapsed = 0
		r.Step(pb.Message{From: r.id, Type: pb.MsgBeat})
	}
}
func stepCandidate(r *raft, m pb.Message) error {
	// Only handle vote responses corresponding to our candidacy (while in
	// StateCandidate, we may get stale MsgPreVoteResp messages in this term from
	// our pre-candidate state).
	var myVoteRespType pb.MessageType
	if r.state == StatePreCandidate {
		myVoteRespType = pb.MsgPreVoteResp
	} else {
		myVoteRespType = pb.MsgVoteResp
	}
	switch m.Type {
	case pb.MsgProp:
		r.logger.Infof("%x no leader at term %d; dropping proposal", r.id, r.Term)
		return ErrProposalDropped
	case pb.MsgApp:
		r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
		r.handleAppendEntries(m)
	case pb.MsgHeartbeat:
		r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
		r.handleHeartbeat(m)
	case pb.MsgSnap:
		r.becomeFollower(m.Term, m.From) // always m.Term == r.Term
		r.handleSnapshot(m)
	case myVoteRespType:
		gr := r.poll(m.From, m.Type, !m.Reject)
		r.logger.Infof("%x [quorum:%d] has received %d %s votes and %d vote rejections", r.id, r.quorum(), gr, m.Type, len(r.votes)-gr)
		switch r.quorum() {
		case gr:
			if r.state == StatePreCandidate {
				r.campaign(campaignElection)
			} else {
				r.becomeLeader()
				r.bcastAppend()
			}
		case len(r.votes) - gr:
			// pb.MsgPreVoteResp contains future term of pre-candidate
			// m.Term > r.Term; reuse r.Term
			r.becomeFollower(r.Term, None)
		}
	case pb.MsgTimeoutNow:
		r.logger.Debugf("%x [term %d state %v] ignored MsgTimeoutNow from %x", r.id, r.Term, r.state, m.From)
	}
	return nil
}
func (r *raft) restore(s pb.Snapshot) bool {
	if s.Metadata.Index <= r.raftLog.committed {
		return false
	}
	if r.raftLog.matchTerm(s.Metadata.Index, s.Metadata.Term) {
		r.logger.Infof("%x [commit: %d, lastindex: %d, lastterm: %d] fast-forwarded commit to snapshot [index: %d, term: %d]",
			r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term)
		r.raftLog.commitTo(s.Metadata.Index)
		return false
	}

	// The normal peer can't become learner.
	if !r.isLearner {
		for _, id := range s.Metadata.ConfState.Learners {
			if id == r.id {
				r.logger.Errorf("%x can't become learner when restores snapshot [index: %d, term: %d]", r.id, s.Metadata.Index, s.Metadata.Term)
				return false
			}
		}
	}

	r.logger.Infof("%x [commit: %d, lastindex: %d, lastterm: %d] starts to restore snapshot [index: %d, term: %d]",
		r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term)

	r.raftLog.restore(s)
	r.prs = make(map[uint64]*Progress)
	r.learnerPrs = make(map[uint64]*Progress)
	r.restoreNode(s.Metadata.ConfState.Nodes, false)
	r.restoreNode(s.Metadata.ConfState.Learners, true)
	return true
}
func (r *raft) promotable() bool {
	_, ok := r.prs[r.id]
	return ok
}
func (r *raft) checkQuorumActive() bool {
	var act int

	r.forEachProgress(func(id uint64, pr *Progress) {
		if id == r.id { // self is always active
			act++
			return
		}

		if pr.RecentActive && !pr.IsLearner {
			act++
		}

		pr.RecentActive = false
	})

	return act >= r.quorum()
}
func (r *raft) increaseUncommittedSize(ents []pb.Entry) bool {
	var s uint64
	for _, e := range ents {
		s += uint64(PayloadSize(e))
	}

	if r.uncommittedSize > 0 && r.uncommittedSize+s > r.maxUncommittedSize {
		// If the uncommitted tail of the Raft log is empty, allow any size
		// proposal. Otherwise, limit the size of the uncommitted tail of the
		// log and drop any proposal that would push the size over the limit.
		return false
	}
	r.uncommittedSize += s
	return true
}
func (r *raft) reduceUncommittedSize(ents []pb.Entry) {
	if r.uncommittedSize == 0 {
		// Fast-path for followers, who do not track or enforce the limit.
		return
	}

	var s uint64
	for _, e := range ents {
		s += uint64(PayloadSize(e))
	}
	if s > r.uncommittedSize {
		// uncommittedSize may underestimate the size of the uncommitted Raft
		// log tail but will never overestimate it. Saturate at 0 instead of
		// allowing overflow.
		r.uncommittedSize = 0
	} else {
		r.uncommittedSize -= s
	}
}
func newPeriodic(lg *zap.Logger, clock clockwork.Clock, h time.Duration, rg RevGetter, c Compactable) *Periodic {
	pc := &Periodic{
		lg:     lg,
		clock:  clock,
		period: h,
		rg:     rg,
		c:      c,
		revs:   make([]int64, 0),
	}
	pc.ctx, pc.cancel = context.WithCancel(context.Background())
	return pc
}
func (pc *Periodic) Pause() {
	pc.mu.Lock()
	pc.paused = true
	pc.mu.Unlock()
}
func (pc *Periodic) Resume() {
	pc.mu.Lock()
	pc.paused = false
	pc.mu.Unlock()
}
func (m *Mutex) Lock(ctx context.Context) error {
	s := m.s
	client := m.s.Client()

	m.myKey = fmt.Sprintf("%s%x", m.pfx, s.Lease())
	cmp := v3.Compare(v3.CreateRevision(m.myKey), "=", 0)
	// put self in lock waiters via myKey; oldest waiter holds lock
	put := v3.OpPut(m.myKey, "", v3.WithLease(s.Lease()))
	// reuse key in case this session already holds the lock
	get := v3.OpGet(m.myKey)
	// fetch current holder to complete uncontended path with only one RPC
	getOwner := v3.OpGet(m.pfx, v3.WithFirstCreate()...)
	resp, err := client.Txn(ctx).If(cmp).Then(put, getOwner).Else(get, getOwner).Commit()
	if err != nil {
		return err
	}
	m.myRev = resp.Header.Revision
	if !resp.Succeeded {
		m.myRev = resp.Responses[0].GetResponseRange().Kvs[0].CreateRevision
	}
	// if no key on prefix / the minimum rev is key, already hold the lock
	ownerKey := resp.Responses[1].GetResponseRange().Kvs
	if len(ownerKey) == 0 || ownerKey[0].CreateRevision == m.myRev {
		m.hdr = resp.Header
		return nil
	}

	// wait for deletion revisions prior to myKey
	hdr, werr := waitDeletes(ctx, client, m.pfx, m.myRev-1)
	// release lock key if wait failed
	if werr != nil {
		m.Unlock(client.Ctx())
	} else {
		m.hdr = hdr
	}
	return werr
}
func NewLocker(s *Session, pfx string) sync.Locker {
	return &lockerMutex{NewMutex(s, pfx)}
}
func NewFIFOScheduler() Scheduler {
	f := &fifo{
		resume: make(chan struct{}, 1),
		donec:  make(chan struct{}, 1),
	}
	f.finishCond = sync.NewCond(&f.mu)
	f.ctx, f.cancel = context.WithCancel(context.Background())
	go f.run()
	return f
}
func (f *fifo) Schedule(j Job) {
	f.mu.Lock()
	defer f.mu.Unlock()

	if f.cancel == nil {
		panic("schedule: schedule to stopped scheduler")
	}

	if len(f.pendings) == 0 {
		select {
		case f.resume <- struct{}{}:
		default:
		}
	}
	f.pendings = append(f.pendings, j)
}
func (f *fifo) Stop() {
	f.mu.Lock()
	f.cancel()
	f.cancel = nil
	f.mu.Unlock()
	<-f.donec
}
func NewServer(
	lg *zap.Logger,
	network string,
	address string,
) *Server {
	return &Server{
		lg:                         lg,
		network:                    network,
		address:                    address,
		last:                       rpcpb.Operation_NOT_STARTED,
		advertiseClientPortToProxy: make(map[int]proxy.Server),
		advertisePeerPortToProxy:   make(map[int]proxy.Server),
	}
}
func (srv *Server) StartServe() error {
	var err error
	srv.ln, err = net.Listen(srv.network, srv.address)
	if err != nil {
		return err
	}

	var opts []grpc.ServerOption
	opts = append(opts, grpc.MaxRecvMsgSize(int(maxRequestBytes+grpcOverheadBytes)))
	opts = append(opts, grpc.MaxSendMsgSize(maxSendBytes))
	opts = append(opts, grpc.MaxConcurrentStreams(maxStreams))
	srv.grpcServer = grpc.NewServer(opts...)

	rpcpb.RegisterTransportServer(srv.grpcServer, srv)

	srv.lg.Info(
		"gRPC server started",
		zap.String("address", srv.address),
		zap.String("listener-address", srv.ln.Addr().String()),
	)
	err = srv.grpcServer.Serve(srv.ln)
	if err != nil && strings.Contains(err.Error(), "use of closed network connection") {
		srv.lg.Info(
			"gRPC server is shut down",
			zap.String("address", srv.address),
			zap.Error(err),
		)
	} else {
		srv.lg.Warn(
			"gRPC server returned with error",
			zap.String("address", srv.address),
			zap.Error(err),
		)
	}
	return err
}
func (srv *Server) Stop() {
	srv.lg.Info("gRPC server stopping", zap.String("address", srv.address))
	srv.grpcServer.Stop()
	srv.lg.Info("gRPC server stopped", zap.String("address", srv.address))
}
func (srv *Server) Transport(stream rpcpb.Transport_TransportServer) (err error) {
	errc := make(chan error)
	go func() {
		for {
			var req *rpcpb.Request
			req, err = stream.Recv()
			if err != nil {
				errc <- err
				// TODO: handle error and retry
				return
			}
			if req.Member != nil {
				srv.Member = req.Member
			}
			if req.Tester != nil {
				srv.Tester = req.Tester
			}

			var resp *rpcpb.Response
			resp, err = srv.handleTesterRequest(req)
			if err != nil {
				errc <- err
				// TODO: handle error and retry
				return
			}

			if err = stream.Send(resp); err != nil {
				errc <- err
				// TODO: handle error and retry
				return
			}
		}
	}()

	select {
	case err = <-errc:
	case <-stream.Context().Done():
		err = stream.Context().Err()
	}
	return err
}
func RegisterInterruptHandler(h InterruptHandler) {
	interruptRegisterMu.Lock()
	defer interruptRegisterMu.Unlock()
	interruptHandlers = append(interruptHandlers, h)
}
func HandleInterrupts(lg *zap.Logger) {
	notifier := make(chan os.Signal, 1)
	signal.Notify(notifier, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		sig := <-notifier

		interruptRegisterMu.Lock()
		ihs := make([]InterruptHandler, len(interruptHandlers))
		copy(ihs, interruptHandlers)
		interruptRegisterMu.Unlock()

		interruptExitMu.Lock()

		if lg != nil {
			lg.Info("received signal; shutting down", zap.String("signal", sig.String()))
		} else {
			plog.Noticef("received %v signal, shutting down...", sig)
		}

		for _, h := range ihs {
			h()
		}
		signal.Stop(notifier)
		pid := syscall.Getpid()
		// exit directly if it is the "init" process, since the kernel will not help to kill pid 1.
		if pid == 1 {
			os.Exit(0)
		}
		setDflSignal(sig.(syscall.Signal))
		syscall.Kill(pid, sig.(syscall.Signal))
	}()
}
func OpGet(key string, opts ...OpOption) Op {
	// WithPrefix and WithFromKey are not supported together
	if isWithPrefix(opts) && isWithFromKey(opts) {
		panic("`WithPrefix` and `WithFromKey` cannot be set at the same time, choose one")
	}
	ret := Op{t: tRange, key: []byte(key)}
	ret.applyOpts(opts)
	return ret
}
func OpDelete(key string, opts ...OpOption) Op {
	// WithPrefix and WithFromKey are not supported together
	if isWithPrefix(opts) && isWithFromKey(opts) {
		panic("`WithPrefix` and `WithFromKey` cannot be set at the same time, choose one")
	}
	ret := Op{t: tDeleteRange, key: []byte(key)}
	ret.applyOpts(opts)
	switch {
	case ret.leaseID != 0:
		panic("unexpected lease in delete")
	case ret.limit != 0:
		panic("unexpected limit in delete")
	case ret.rev != 0:
		panic("unexpected revision in delete")
	case ret.sort != nil:
		panic("unexpected sort in delete")
	case ret.serializable:
		panic("unexpected serializable in delete")
	case ret.countOnly:
		panic("unexpected countOnly in delete")
	case ret.minModRev != 0, ret.maxModRev != 0:
		panic("unexpected mod revision filter in delete")
	case ret.minCreateRev != 0, ret.maxCreateRev != 0:
		panic("unexpected create revision filter in delete")
	case ret.filterDelete, ret.filterPut:
		panic("unexpected filter in delete")
	case ret.createdNotify:
		panic("unexpected createdNotify in delete")
	}
	return ret
}
func OpPut(key, val string, opts ...OpOption) Op {
	ret := Op{t: tPut, key: []byte(key), val: []byte(val)}
	ret.applyOpts(opts)
	switch {
	case ret.end != nil:
		panic("unexpected range in put")
	case ret.limit != 0:
		panic("unexpected limit in put")
	case ret.rev != 0:
		panic("unexpected revision in put")
	case ret.sort != nil:
		panic("unexpected sort in put")
	case ret.serializable:
		panic("unexpected serializable in put")
	case ret.countOnly:
		panic("unexpected countOnly in put")
	case ret.minModRev != 0, ret.maxModRev != 0:
		panic("unexpected mod revision filter in put")
	case ret.minCreateRev != 0, ret.maxCreateRev != 0:
		panic("unexpected create revision filter in put")
	case ret.filterDelete, ret.filterPut:
		panic("unexpected filter in put")
	case ret.createdNotify:
		panic("unexpected createdNotify in put")
	}
	return ret
}
func OpTxn(cmps []Cmp, thenOps []Op, elseOps []Op) Op {
	return Op{t: tTxn, cmps: cmps, thenOps: thenOps, elseOps: elseOps}
}
func WithFromKey() OpOption {
	return func(op *Op) {
		if len(op.key) == 0 {
			op.key = []byte{0}
		}
		op.end = []byte("\x00")
	}
}
func withTop(target SortTarget, order SortOrder) []OpOption {
	return []OpOption{WithPrefix(), WithSort(target, order), WithLimit(1)}
}
func Exist(dir string) bool {
	names, err := fileutil.ReadDir(dir, fileutil.WithExt(".wal"))
	if err != nil {
		return false
	}
	return len(names) != 0
}
func searchIndex(lg *zap.Logger, names []string, index uint64) (int, bool) {
	for i := len(names) - 1; i >= 0; i-- {
		name := names[i]
		_, curIndex, err := parseWALName(name)
		if err != nil {
			if lg != nil {
				lg.Panic("failed to parse WAL file name", zap.String("path", name), zap.Error(err))
			} else {
				plog.Panicf("parse correct name should never fail: %v", err)
			}
		}
		if index >= curIndex {
			return i, true
		}
	}
	return -1, false
}
func isValidSeq(lg *zap.Logger, names []string) bool {
	var lastSeq uint64
	for _, name := range names {
		curSeq, _, err := parseWALName(name)
		if err != nil {
			if lg != nil {
				lg.Panic("failed to parse WAL file name", zap.String("path", name), zap.Error(err))
			} else {
				plog.Panicf("parse correct name should never fail: %v", err)
			}
		}
		if lastSeq != 0 && lastSeq != curSeq-1 {
			return false
		}
		lastSeq = curSeq
	}
	return true
}
func NewListener(addr, scheme string, tlsinfo *TLSInfo) (l net.Listener, err error) {
	if l, err = newListener(addr, scheme); err != nil {
		return nil, err
	}
	return wrapTLS(scheme, tlsinfo, l)
}
func (info TLSInfo) cafiles() []string {
	cs := make([]string, 0)
	if info.TrustedCAFile != "" {
		cs = append(cs, info.TrustedCAFile)
	}
	return cs
}
func (info TLSInfo) ServerConfig() (*tls.Config, error) {
	cfg, err := info.baseConfig()
	if err != nil {
		return nil, err
	}

	cfg.ClientAuth = tls.NoClientCert
	if info.TrustedCAFile != "" || info.ClientCertAuth {
		cfg.ClientAuth = tls.RequireAndVerifyClientCert
	}

	cs := info.cafiles()
	if len(cs) > 0 {
		cp, err := tlsutil.NewCertPool(cs)
		if err != nil {
			return nil, err
		}
		cfg.ClientCAs = cp
	}

	// "h2" NextProtos is necessary for enabling HTTP2 for go's HTTP server
	cfg.NextProtos = []string{"h2"}

	return cfg, nil
}
func (info TLSInfo) ClientConfig() (*tls.Config, error) {
	var cfg *tls.Config
	var err error

	if !info.Empty() {
		cfg, err = info.baseConfig()
		if err != nil {
			return nil, err
		}
	} else {
		cfg = &tls.Config{ServerName: info.ServerName}
	}
	cfg.InsecureSkipVerify = info.InsecureSkipVerify

	cs := info.cafiles()
	if len(cs) > 0 {
		cfg.RootCAs, err = tlsutil.NewCertPool(cs)
		if err != nil {
			return nil, err
		}
	}

	if info.selfCert {
		cfg.InsecureSkipVerify = true
	}

	if info.EmptyCN {
		hasNonEmptyCN := false
		cn := ""
		tlsutil.NewCert(info.CertFile, info.KeyFile, func(certPEMBlock []byte, keyPEMBlock []byte) (tls.Certificate, error) {
			var block *pem.Block
			block, _ = pem.Decode(certPEMBlock)
			cert, err := x509.ParseCertificate(block.Bytes)
			if err != nil {
				return tls.Certificate{}, err
			}
			if len(cert.Subject.CommonName) != 0 {
				hasNonEmptyCN = true
				cn = cert.Subject.CommonName
			}
			return tls.X509KeyPair(certPEMBlock, keyPEMBlock)
		})
		if hasNonEmptyCN {
			return nil, fmt.Errorf("cert has non empty Common Name (%s)", cn)
		}
	}

	return cfg, nil
}
func newTLSKeepaliveListener(inner net.Listener, config *tls.Config) net.Listener {
	l := &tlsKeepaliveListener{}
	l.Listener = inner
	l.config = config
	return l
}
func (s *EtcdServer) applyV2Request(r *RequestV2) Response {
	defer warnOfExpensiveRequest(s.getLogger(), time.Now(), r, nil, nil)

	switch r.Method {
	case "POST":
		return s.applyV2.Post(r)
	case "PUT":
		return s.applyV2.Put(r)
	case "DELETE":
		return s.applyV2.Delete(r)
	case "QGET":
		return s.applyV2.QGet(r)
	case "SYNC":
		return s.applyV2.Sync(r)
	default:
		// This should never be reached, but just in case:
		return Response{Err: ErrUnknownMethod}
	}
}
func NewRoleCommand() *cobra.Command {
	ac := &cobra.Command{
		Use:   "role <subcommand>",
		Short: "Role related commands",
	}

	ac.AddCommand(newRoleAddCommand())
	ac.AddCommand(newRoleDeleteCommand())
	ac.AddCommand(newRoleGetCommand())
	ac.AddCommand(newRoleListCommand())
	ac.AddCommand(newRoleGrantPermissionCommand())
	ac.AddCommand(newRoleRevokePermissionCommand())

	return ac
}
func roleAddCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("role add command requires role name as its argument"))
	}

	resp, err := mustClientFromCmd(cmd).Auth.RoleAdd(context.TODO(), args[0])
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.RoleAdd(args[0], *resp)
}
func roleGetCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("role get command requires role name as its argument"))
	}

	name := args[0]
	resp, err := mustClientFromCmd(cmd).Auth.RoleGet(context.TODO(), name)
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.RoleGet(name, *resp)
}
func roleGrantPermissionCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) < 3 {
		ExitWithError(ExitBadArgs, fmt.Errorf("role grant command requires role name, permission type, and key [endkey] as its argument"))
	}

	perm, err := clientv3.StrToPermissionType(args[1])
	if err != nil {
		ExitWithError(ExitBadArgs, err)
	}

	key, rangeEnd := permRange(args[2:])
	resp, err := mustClientFromCmd(cmd).Auth.RoleGrantPermission(context.TODO(), args[0], key, rangeEnd, perm)
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.RoleGrantPermission(args[0], *resp)
}
func roleRevokePermissionCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) < 2 {
		ExitWithError(ExitBadArgs, fmt.Errorf("role revoke-permission command requires role name and key [endkey] as its argument"))
	}

	key, rangeEnd := permRange(args[1:])
	resp, err := mustClientFromCmd(cmd).Auth.RoleRevokePermission(context.TODO(), args[0], key, rangeEnd)
	if err != nil {
		ExitWithError(ExitError, err)
	}
	display.RoleRevokePermission(args[0], args[1], rangeEnd, *resp)
}
func NewCluster(t testing.TB, size int) *cluster {
	return newCluster(t, &ClusterConfig{Size: size})
}
func NewClusterByConfig(t testing.TB, cfg *ClusterConfig) *cluster {
	return newCluster(t, cfg)
}
func (c *cluster) HTTPMembers() []client.Member {
	ms := []client.Member{}
	for _, m := range c.Members {
		pScheme := schemeFromTLSInfo(m.PeerTLSInfo)
		cScheme := schemeFromTLSInfo(m.ClientTLSInfo)
		cm := client.Member{Name: m.Name}
		for _, ln := range m.PeerListeners {
			cm.PeerURLs = append(cm.PeerURLs, pScheme+"://"+ln.Addr().String())
		}
		for _, ln := range m.ClientListeners {
			cm.ClientURLs = append(cm.ClientURLs, cScheme+"://"+ln.Addr().String())
		}
		ms = append(ms, cm)
	}
	return ms
}
func (c *cluster) waitLeader(t testing.TB, membs []*member) int {
	possibleLead := make(map[uint64]bool)
	var lead uint64
	for _, m := range membs {
		possibleLead[uint64(m.s.ID())] = true
	}
	cc := MustNewHTTPClient(t, getMembersURLs(membs), nil)
	kapi := client.NewKeysAPI(cc)

	// ensure leader is up via linearizable get
	for {
		ctx, cancel := context.WithTimeout(context.Background(), 10*tickDuration+time.Second)
		_, err := kapi.Get(ctx, "0", &client.GetOptions{Quorum: true})
		cancel()
		if err == nil || strings.Contains(err.Error(), "Key not found") {
			break
		}
	}

	for lead == 0 || !possibleLead[lead] {
		lead = 0
		for _, m := range membs {
			select {
			case <-m.s.StopNotify():
				continue
			default:
			}
			if lead != 0 && lead != m.s.Lead() {
				lead = 0
				time.Sleep(10 * tickDuration)
				break
			}
			lead = m.s.Lead()
		}
	}

	for i, m := range membs {
		if uint64(m.s.ID()) == lead {
			return i
		}
	}

	return -1
}
func (c *cluster) waitNoLeader(membs []*member) {
	noLeader := false
	for !noLeader {
		noLeader = true
		for _, m := range membs {
			select {
			case <-m.s.StopNotify():
				continue
			default:
			}
			if m.s.Lead() != 0 {
				noLeader = false
				time.Sleep(10 * tickDuration)
				break
			}
		}
	}
}
func isMembersEqual(membs []client.Member, wmembs []client.Member) bool {
	sort.Sort(SortableMemberSliceByPeerURLs(membs))
	sort.Sort(SortableMemberSliceByPeerURLs(wmembs))
	for i := range membs {
		membs[i].ID = ""
	}
	return reflect.DeepEqual(membs, wmembs)
}
func (m *member) listenGRPC() error {
	// prefix with localhost so cert has right domain
	m.grpcAddr = "localhost:" + m.Name
	if m.useIP { // for IP-only TLS certs
		m.grpcAddr = "127.0.0.1:" + m.Name
	}
	l, err := transport.NewUnixListener(m.grpcAddr)
	if err != nil {
		return fmt.Errorf("listen failed on grpc socket %s (%v)", m.grpcAddr, err)
	}
	m.grpcBridge, err = newBridge(m.grpcAddr)
	if err != nil {
		l.Close()
		return err
	}
	m.grpcAddr = schemeFromTLSInfo(m.ClientTLSInfo) + "://" + m.grpcBridge.inaddr
	m.grpcListener = l
	return nil
}
func NewClientV3(m *member) (*clientv3.Client, error) {
	if m.grpcAddr == "" {
		return nil, fmt.Errorf("member not configured for grpc")
	}

	cfg := clientv3.Config{
		Endpoints:          []string{m.grpcAddr},
		DialTimeout:        5 * time.Second,
		DialOptions:        []grpc.DialOption{grpc.WithBlock()},
		MaxCallSendMsgSize: m.clientMaxCallSendMsgSize,
		MaxCallRecvMsgSize: m.clientMaxCallRecvMsgSize,
	}

	if m.ClientTLSInfo != nil {
		tls, err := m.ClientTLSInfo.ClientConfig()
		if err != nil {
			return nil, err
		}
		cfg.TLS = tls
	}
	if m.DialOptions != nil {
		cfg.DialOptions = append(cfg.DialOptions, m.DialOptions...)
	}
	return newClientV3(cfg)
}
func (m *member) Clone(t testing.TB) *member {
	mm := &member{}
	mm.ServerConfig = m.ServerConfig

	var err error
	clientURLStrs := m.ClientURLs.StringSlice()
	mm.ClientURLs, err = types.NewURLs(clientURLStrs)
	if err != nil {
		// this should never fail
		panic(err)
	}
	peerURLStrs := m.PeerURLs.StringSlice()
	mm.PeerURLs, err = types.NewURLs(peerURLStrs)
	if err != nil {
		// this should never fail
		panic(err)
	}
	clusterStr := m.InitialPeerURLsMap.String()
	mm.InitialPeerURLsMap, err = types.NewURLsMap(clusterStr)
	if err != nil {
		// this should never fail
		panic(err)
	}
	mm.InitialClusterToken = m.InitialClusterToken
	mm.ElectionTicks = m.ElectionTicks
	mm.PeerTLSInfo = m.PeerTLSInfo
	mm.ClientTLSInfo = m.ClientTLSInfo
	return mm
}
func (m *member) Close() {
	if m.grpcBridge != nil {
		m.grpcBridge.Close()
		m.grpcBridge = nil
	}
	if m.serverClient != nil {
		m.serverClient.Close()
		m.serverClient = nil
	}
	if m.grpcServer != nil {
		m.grpcServer.Stop()
		m.grpcServer.GracefulStop()
		m.grpcServer = nil
		m.grpcServerPeer.Stop()
		m.grpcServerPeer.GracefulStop()
		m.grpcServerPeer = nil
	}
	m.s.HardStop()
	for _, f := range m.serverClosers {
		f()
	}
}
func (m *member) Stop(t testing.TB) {
	lg.Info(
		"stopping a member",
		zap.String("name", m.Name),
		zap.Strings("advertise-peer-urls", m.PeerURLs.StringSlice()),
		zap.Strings("listen-client-urls", m.ClientURLs.StringSlice()),
		zap.String("grpc-address", m.grpcAddr),
	)
	m.Close()
	m.serverClosers = nil
	lg.Info(
		"stopped a member",
		zap.String("name", m.Name),
		zap.Strings("advertise-peer-urls", m.PeerURLs.StringSlice()),
		zap.Strings("listen-client-urls", m.ClientURLs.StringSlice()),
		zap.String("grpc-address", m.grpcAddr),
	)
}
func checkLeaderTransition(m *member, oldLead uint64) uint64 {
	interval := time.Duration(m.s.Cfg.TickMs) * time.Millisecond
	for m.s.Lead() == 0 || (m.s.Lead() == oldLead) {
		time.Sleep(interval)
	}
	return m.s.Lead()
}
func (m *member) Restart(t testing.TB) error {
	lg.Info(
		"restarting a member",
		zap.String("name", m.Name),
		zap.Strings("advertise-peer-urls", m.PeerURLs.StringSlice()),
		zap.Strings("listen-client-urls", m.ClientURLs.StringSlice()),
		zap.String("grpc-address", m.grpcAddr),
	)
	newPeerListeners := make([]net.Listener, 0)
	for _, ln := range m.PeerListeners {
		newPeerListeners = append(newPeerListeners, NewListenerWithAddr(t, ln.Addr().String()))
	}
	m.PeerListeners = newPeerListeners
	newClientListeners := make([]net.Listener, 0)
	for _, ln := range m.ClientListeners {
		newClientListeners = append(newClientListeners, NewListenerWithAddr(t, ln.Addr().String()))
	}
	m.ClientListeners = newClientListeners

	if m.grpcListener != nil {
		if err := m.listenGRPC(); err != nil {
			t.Fatal(err)
		}
	}

	err := m.Launch()
	lg.Info(
		"restarted a member",
		zap.String("name", m.Name),
		zap.Strings("advertise-peer-urls", m.PeerURLs.StringSlice()),
		zap.Strings("listen-client-urls", m.ClientURLs.StringSlice()),
		zap.String("grpc-address", m.grpcAddr),
		zap.Error(err),
	)
	return err
}
func (m *member) Terminate(t testing.TB) {
	lg.Info(
		"terminating a member",
		zap.String("name", m.Name),
		zap.Strings("advertise-peer-urls", m.PeerURLs.StringSlice()),
		zap.Strings("listen-client-urls", m.ClientURLs.StringSlice()),
		zap.String("grpc-address", m.grpcAddr),
	)
	m.Close()
	if !m.keepDataDirTerminate {
		if err := os.RemoveAll(m.ServerConfig.DataDir); err != nil {
			t.Fatal(err)
		}
	}
	lg.Info(
		"terminated a member",
		zap.String("name", m.Name),
		zap.Strings("advertise-peer-urls", m.PeerURLs.StringSlice()),
		zap.Strings("listen-client-urls", m.ClientURLs.StringSlice()),
		zap.String("grpc-address", m.grpcAddr),
	)
}
func (m *member) Metric(metricName string) (string, error) {
	cfgtls := transport.TLSInfo{}
	tr, err := transport.NewTimeoutTransport(cfgtls, time.Second, time.Second, time.Second)
	if err != nil {
		return "", err
	}
	cli := &http.Client{Transport: tr}
	resp, err := cli.Get(m.ClientURLs[0].String() + "/metrics")
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()
	b, rerr := ioutil.ReadAll(resp.Body)
	if rerr != nil {
		return "", rerr
	}
	lines := strings.Split(string(b), "\n")
	for _, l := range lines {
		if strings.HasPrefix(l, metricName) {
			return strings.Split(l, " ")[1], nil
		}
	}
	return "", nil
}
func (m *member) InjectPartition(t testing.TB, others ...*member) {
	for _, other := range others {
		m.s.CutPeer(other.s.ID())
		other.s.CutPeer(m.s.ID())
	}
}
func (m *member) RecoverPartition(t testing.TB, others ...*member) {
	for _, other := range others {
		m.s.MendPeer(other.s.ID())
		other.s.MendPeer(m.s.ID())
	}
}
func NewClusterV3(t testing.TB, cfg *ClusterConfig) *ClusterV3 {
	cfg.UseGRPC = true
	if os.Getenv("CLIENT_DEBUG") != "" {
		clientv3.SetLogger(grpclog.NewLoggerV2WithVerbosity(os.Stderr, os.Stderr, os.Stderr, 4))
	}
	clus := &ClusterV3{
		cluster: NewClusterByConfig(t, cfg),
	}
	clus.Launch(t)

	if !cfg.SkipCreatingClient {
		for _, m := range clus.Members {
			client, err := NewClientV3(m)
			if err != nil {
				t.Fatalf("cannot create client: %v", err)
			}
			clus.clients = append(clus.clients, client)
		}
	}

	return clus
}
func (opts *jwtOptions) ParseWithDefaults(optMap map[string]string) error {
	if opts.TTL == 0 && optMap[optTTL] == "" {
		opts.TTL = DefaultTTL
	}

	return opts.Parse(optMap)
}
func (opts *jwtOptions) Parse(optMap map[string]string) error {
	var err error
	if ttl := optMap[optTTL]; ttl != "" {
		opts.TTL, err = time.ParseDuration(ttl)
		if err != nil {
			return err
		}
	}

	if file := optMap[optPublicKey]; file != "" {
		opts.PublicKey, err = ioutil.ReadFile(file)
		if err != nil {
			return err
		}
	}

	if file := optMap[optPrivateKey]; file != "" {
		opts.PrivateKey, err = ioutil.ReadFile(file)
		if err != nil {
			return err
		}
	}

	// signing method is a required field
	method := optMap[optSignMethod]
	opts.SignMethod = jwt.GetSigningMethod(method)
	if opts.SignMethod == nil {
		return ErrInvalidAuthMethod
	}

	return nil
}
func (opts *jwtOptions) Key() (interface{}, error) {
	switch opts.SignMethod.(type) {
	case *jwt.SigningMethodRSA, *jwt.SigningMethodRSAPSS:
		return opts.rsaKey()
	case *jwt.SigningMethodECDSA:
		return opts.ecKey()
	case *jwt.SigningMethodHMAC:
		return opts.hmacKey()
	default:
		return nil, fmt.Errorf("unsupported signing method: %T", opts.SignMethod)
	}
}
func (h *header) fill(rh *pb.ResponseHeader) {
	if rh == nil {
		plog.Panic("unexpected nil resp.Header")
	}
	rh.ClusterId = uint64(h.clusterID)
	rh.MemberId = uint64(h.memberID)
	rh.RaftTerm = h.sg.Term()
	if rh.Revision == 0 {
		rh.Revision = h.rev()
	}
}
func (wb *watchBroadcast) add(w *watcher) bool {
	wb.mu.Lock()
	defer wb.mu.Unlock()
	if wb.nextrev > w.nextrev || (wb.nextrev == 0 && w.nextrev != 0) {
		// wb is too far ahead, w will miss events
		// or wb is being established with a current watcher
		return false
	}
	if wb.responses == 0 {
		// Newly created; create event will be sent by etcd.
		wb.receivers[w] = struct{}{}
		return true
	}
	// already sent by etcd; emulate create event
	ok := w.post(&pb.WatchResponse{
		Header: &pb.ResponseHeader{
			// todo: fill in ClusterId
			// todo: fill in MemberId:
			Revision: w.nextrev,
			// todo: fill in RaftTerm:
		},
		WatchId: w.id,
		Created: true,
	})
	if !ok {
		return false
	}
	wb.receivers[w] = struct{}{}
	watchersCoalescing.Inc()

	return true
}
func (ws *watchStream) Watch(id WatchID, key, end []byte, startRev int64, fcs ...FilterFunc) (WatchID, error) {
	// prevent wrong range where key >= end lexicographically
	// watch request with 'WithFromKey' has empty-byte range end
	if len(end) != 0 && bytes.Compare(key, end) != -1 {
		return -1, ErrEmptyWatcherRange
	}

	ws.mu.Lock()
	defer ws.mu.Unlock()
	if ws.closed {
		return -1, ErrEmptyWatcherRange
	}

	if id == AutoWatchID {
		for ws.watchers[ws.nextID] != nil {
			ws.nextID++
		}
		id = ws.nextID
		ws.nextID++
	} else if _, ok := ws.watchers[id]; ok {
		return -1, ErrWatcherDuplicateID
	}

	w, c := ws.watchable.watch(key, end, startRev, id, ws.ch, fcs...)

	ws.cancels[id] = c
	ws.watchers[id] = w
	return id, nil
}
func newFileEncoder(f *os.File, prevCrc uint32) (*encoder, error) {
	offset, err := f.Seek(0, io.SeekCurrent)
	if err != nil {
		return nil, err
	}
	return newEncoder(f, prevCrc, int(offset)), nil
}
func purgeFile(lg *zap.Logger, dirname string, suffix string, max uint, interval time.Duration, stop <-chan struct{}, purgec chan<- string) <-chan error {
	errC := make(chan error, 1)
	go func() {
		for {
			fnames, err := ReadDir(dirname)
			if err != nil {
				errC <- err
				return
			}
			newfnames := make([]string, 0)
			for _, fname := range fnames {
				if strings.HasSuffix(fname, suffix) {
					newfnames = append(newfnames, fname)
				}
			}
			sort.Strings(newfnames)
			fnames = newfnames
			for len(newfnames) > int(max) {
				f := filepath.Join(dirname, newfnames[0])
				l, err := TryLockFile(f, os.O_WRONLY, PrivateFileMode)
				if err != nil {
					break
				}
				if err = os.Remove(f); err != nil {
					errC <- err
					return
				}
				if err = l.Close(); err != nil {
					if lg != nil {
						lg.Warn("failed to unlock/close", zap.String("path", l.Name()), zap.Error(err))
					} else {
						plog.Errorf("error unlocking %s when purging file (%v)", l.Name(), err)
					}
					errC <- err
					return
				}
				if lg != nil {
					lg.Info("purged", zap.String("path", f))
				} else {
					plog.Infof("purged file %s successfully", f)
				}
				newfnames = newfnames[1:]
			}
			if purgec != nil {
				for i := 0; i < len(fnames)-len(newfnames); i++ {
					purgec <- fnames[i]
				}
			}
			select {
			case <-time.After(interval):
			case <-stop:
				return
			}
		}
	}()
	return errC
}
func (ss *StringsValue) Set(s string) error {
	*ss = strings.Split(s, ",")
	return nil
}
func NewStringsValue(s string) (ss *StringsValue) {
	if s == "" {
		return &StringsValue{}
	}
	ss = new(StringsValue)
	if err := ss.Set(s); err != nil {
		plog.Panicf("new StringsValue should never fail: %v", err)
	}
	return ss
}
func StringsFromFlag(fs *flag.FlagSet, flagName string) []string {
	return []string(*fs.Lookup(flagName).Value.(*StringsValue))
}
func Cluster(v string) string {
	vs := strings.Split(v, ".")
	if len(vs) <= 2 {
		return v
	}
	return fmt.Sprintf("%s.%s", vs[0], vs[1])
}
func NewPageWriter(w io.Writer, pageBytes, pageOffset int) *PageWriter {
	return &PageWriter{
		w:                 w,
		pageOffset:        pageOffset,
		pageBytes:         pageBytes,
		buf:               make([]byte, defaultBufferBytes+pageBytes),
		bufWatermarkBytes: defaultBufferBytes,
	}
}
func (wh *watcherHub) watch(key string, recursive, stream bool, index, storeIndex uint64) (Watcher, *v2error.Error) {
	reportWatchRequest()
	event, err := wh.EventHistory.scan(key, recursive, index)

	if err != nil {
		err.Index = storeIndex
		return nil, err
	}

	w := &watcher{
		eventChan:  make(chan *Event, 100), // use a buffered channel
		recursive:  recursive,
		stream:     stream,
		sinceIndex: index,
		startIndex: storeIndex,
		hub:        wh,
	}

	wh.mutex.Lock()
	defer wh.mutex.Unlock()
	// If the event exists in the known history, append the EtcdIndex and return immediately
	if event != nil {
		ne := event.Clone()
		ne.EtcdIndex = storeIndex
		w.eventChan <- ne
		return w, nil
	}

	l, ok := wh.watchers[key]

	var elem *list.Element

	if ok { // add the new watcher to the back of the list
		elem = l.PushBack(w)
	} else { // create a new list and add the new watcher
		l = list.New()
		elem = l.PushBack(w)
		wh.watchers[key] = l
	}

	w.remove = func() {
		if w.removed { // avoid removing it twice
			return
		}
		w.removed = true
		l.Remove(elem)
		atomic.AddInt64(&wh.count, -1)
		reportWatcherRemoved()
		if l.Len() == 0 {
			delete(wh.watchers, key)
		}
	}

	atomic.AddInt64(&wh.count, 1)
	reportWatcherAdded()

	return w, nil
}
func (wh *watcherHub) notify(e *Event) {
	e = wh.EventHistory.addEvent(e) // add event into the eventHistory

	segments := strings.Split(e.Node.Key, "/")

	currPath := "/"

	// walk through all the segments of the path and notify the watchers
	// if the path is "/foo/bar", it will notify watchers with path "/",
	// "/foo" and "/foo/bar"

	for _, segment := range segments {
		currPath = path.Join(currPath, segment)
		// notify the watchers who interests in the changes of current path
		wh.notifyWatchers(e, currPath, false)
	}
}
func (wh *watcherHub) clone() *watcherHub {
	clonedHistory := wh.EventHistory.clone()

	return &watcherHub{
		EventHistory: clonedHistory,
	}
}
func isHidden(watchPath, keyPath string) bool {
	// When deleting a directory, watchPath might be deeper than the actual keyPath
	// For example, when deleting /foo we also need to notify watchers on /foo/bar.
	if len(watchPath) > len(keyPath) {
		return false
	}
	// if watch path is just a "/", after path will start without "/"
	// add a "/" to deal with the special case when watchPath is "/"
	afterPath := path.Clean("/" + keyPath[len(watchPath):])
	return strings.Contains(afterPath, "/_")
}
func (srv *Server) createEtcdLogFile() error {
	var err error
	srv.etcdLogFile, err = os.Create(srv.Member.Etcd.LogOutputs[0])
	if err != nil {
		return err
	}
	srv.lg.Info("created etcd log file", zap.String("path", srv.Member.Etcd.LogOutputs[0]))
	return nil
}
func (srv *Server) runEtcd() error {
	errc := make(chan error)
	go func() {
		time.Sleep(5 * time.Second)
		// server advertise client/peer listener had to start first
		// before setting up proxy listener
		errc <- srv.startProxy()
	}()

	if srv.etcdCmd != nil {
		srv.lg.Info(
			"starting etcd command",
			zap.String("command-path", srv.etcdCmd.Path),
		)
		err := srv.etcdCmd.Start()
		perr := <-errc
		srv.lg.Info(
			"started etcd command",
			zap.String("command-path", srv.etcdCmd.Path),
			zap.Errors("errors", []error{err, perr}),
		)
		if err != nil {
			return err
		}
		return perr
	}

	select {
	case <-srv.etcdServer.Server.ReadyNotify():
		srv.lg.Info("embedded etcd is ready")
	case <-time.After(time.Minute):
		srv.etcdServer.Close()
		return fmt.Errorf("took too long to start %v", <-srv.etcdServer.Err())
	}
	return <-errc
}
func (srv *Server) stopEtcd(sig os.Signal) error {
	srv.stopProxy()

	if srv.etcdCmd != nil {
		srv.lg.Info(
			"stopping etcd command",
			zap.String("command-path", srv.etcdCmd.Path),
			zap.String("signal", sig.String()),
		)

		err := srv.etcdCmd.Process.Signal(sig)
		if err != nil {
			return err
		}

		errc := make(chan error)
		go func() {
			_, ew := srv.etcdCmd.Process.Wait()
			errc <- ew
			close(errc)
		}()

		select {
		case <-time.After(5 * time.Second):
			srv.etcdCmd.Process.Kill()
		case e := <-errc:
			return e
		}

		err = <-errc

		srv.lg.Info(
			"stopped etcd command",
			zap.String("command-path", srv.etcdCmd.Path),
			zap.String("signal", sig.String()),
			zap.Error(err),
		)
		return err
	}

	srv.lg.Info("stopping embedded etcd")
	srv.etcdServer.Server.HardStop()
	srv.etcdServer.Close()
	srv.lg.Info("stopped embedded etcd")
	return nil
}
func (srv *Server) handle_SIGQUIT_ETCD_AND_REMOVE_DATA_AND_STOP_AGENT() (*rpcpb.Response, error) {
	err := srv.stopEtcd(syscall.SIGQUIT)
	if err != nil {
		return nil, err
	}

	if srv.etcdServer != nil {
		srv.etcdServer.GetLogger().Sync()
	} else {
		srv.etcdLogFile.Sync()
		srv.etcdLogFile.Close()
	}

	err = os.RemoveAll(srv.Member.BaseDir)
	if err != nil {
		return nil, err
	}
	srv.lg.Info("removed base directory", zap.String("dir", srv.Member.BaseDir))

	// stop agent server
	srv.Stop()

	return &rpcpb.Response{
		Success: true,
		Status:  "destroyed etcd and agent",
	}, nil
}
func LimitListener(l net.Listener, n int) net.Listener {
	return &limitListener{l, make(chan struct{}, n)}
}
func allowMethod(w http.ResponseWriter, m string, ms ...string) bool {
	for _, meth := range ms {
		if m == meth {
			return true
		}
	}
	w.Header().Set("Allow", strings.Join(ms, ","))
	http.Error(w, "Method Not Allowed", http.StatusMethodNotAllowed)
	return false
}
func NewWatchServer(s *etcdserver.EtcdServer) pb.WatchServer {
	return &watchServer{
		lg: s.Cfg.Logger,

		clusterID: int64(s.Cluster().ID()),
		memberID:  int64(s.ID()),

		maxRequestBytes: int(s.Cfg.MaxRequestBytes + grpcOverheadBytes),

		sg:        s,
		watchable: s.Watchable(),
		ag:        s,
	}
}
func FiltersFromRequest(creq *pb.WatchCreateRequest) []mvcc.FilterFunc {
	filters := make([]mvcc.FilterFunc, 0, len(creq.Filters))
	for _, ft := range creq.Filters {
		switch ft {
		case pb.WatchCreateRequest_NOPUT:
			filters = append(filters, filterNoPut)
		case pb.WatchCreateRequest_NODELETE:
			filters = append(filters, filterNoDelete)
		default:
		}
	}
	return filters
}
func newPipelineHandler(t *Transport, r Raft, cid types.ID) http.Handler {
	return &pipelineHandler{
		lg:      t.Logger,
		localID: t.ID,
		tr:      t,
		r:       r,
		cid:     cid,
	}
}
func checkClusterCompatibilityFromHeader(lg *zap.Logger, localID types.ID, header http.Header, cid types.ID) error {
	remoteName := header.Get("X-Server-From")

	remoteServer := serverVersion(header)
	remoteVs := ""
	if remoteServer != nil {
		remoteVs = remoteServer.String()
	}

	remoteMinClusterVer := minClusterVersion(header)
	remoteMinClusterVs := ""
	if remoteMinClusterVer != nil {
		remoteMinClusterVs = remoteMinClusterVer.String()
	}

	localServer, localMinCluster, err := checkVersionCompatibility(remoteName, remoteServer, remoteMinClusterVer)

	localVs := ""
	if localServer != nil {
		localVs = localServer.String()
	}
	localMinClusterVs := ""
	if localMinCluster != nil {
		localMinClusterVs = localMinCluster.String()
	}

	if err != nil {
		if lg != nil {
			lg.Warn(
				"failed to check version compatibility",
				zap.String("local-member-id", localID.String()),
				zap.String("local-member-cluster-id", cid.String()),
				zap.String("local-member-server-version", localVs),
				zap.String("local-member-server-minimum-cluster-version", localMinClusterVs),
				zap.String("remote-peer-server-name", remoteName),
				zap.String("remote-peer-server-version", remoteVs),
				zap.String("remote-peer-server-minimum-cluster-version", remoteMinClusterVs),
				zap.Error(err),
			)
		} else {
			plog.Errorf("request version incompatibility (%v)", err)
		}
		return errIncompatibleVersion
	}
	if gcid := header.Get("X-Etcd-Cluster-ID"); gcid != cid.String() {
		if lg != nil {
			lg.Warn(
				"request cluster ID mismatch",
				zap.String("local-member-id", localID.String()),
				zap.String("local-member-cluster-id", cid.String()),
				zap.String("local-member-server-version", localVs),
				zap.String("local-member-server-minimum-cluster-version", localMinClusterVs),
				zap.String("remote-peer-server-name", remoteName),
				zap.String("remote-peer-server-version", remoteVs),
				zap.String("remote-peer-server-minimum-cluster-version", remoteMinClusterVs),
				zap.String("remote-peer-cluster-id", gcid),
			)
		} else {
			plog.Errorf("request cluster ID mismatch (got %s want %s)", gcid, cid)
		}
		return errClusterIDMismatch
	}
	return nil
}
func KeyExists(key string) clientv3.Cmp {
	return clientv3.Compare(clientv3.Version(key), ">", 0)
}
func KeyMissing(key string) clientv3.Cmp {
	return clientv3.Compare(clientv3.Version(key), "=", 0)
}
func ValidateSecureEndpoints(tlsInfo TLSInfo, eps []string) ([]string, error) {
	t, err := NewTransport(tlsInfo, 5*time.Second)
	if err != nil {
		return nil, err
	}
	var errs []string
	var endpoints []string
	for _, ep := range eps {
		if !strings.HasPrefix(ep, "https://") {
			errs = append(errs, fmt.Sprintf("%q is insecure", ep))
			continue
		}
		conn, cerr := t.Dial("tcp", ep[len("https://"):])
		if cerr != nil {
			errs = append(errs, fmt.Sprintf("%q failed to dial (%v)", ep, cerr))
			continue
		}
		conn.Close()
		endpoints = append(endpoints, ep)
	}
	if len(errs) != 0 {
		err = fmt.Errorf("%s", strings.Join(errs, ","))
	}
	return endpoints, err
}
func putNewKV(kv v3.KV, key, val string, leaseID v3.LeaseID) (int64, error) {
	cmp := v3.Compare(v3.Version(key), "=", 0)
	req := v3.OpPut(key, val, v3.WithLease(leaseID))
	txnresp, err := kv.Txn(context.TODO()).If(cmp).Then(req).Commit()
	if err != nil {
		return 0, err
	}
	if !txnresp.Succeeded {
		return 0, ErrKeyExists
	}
	return txnresp.Header.Revision, nil
}
func newUniqueEphemeralKey(s *concurrency.Session, prefix string) (*EphemeralKV, error) {
	return newUniqueEphemeralKV(s, prefix, "")
}
func NewUpdateDirCommand() cli.Command {
	return cli.Command{
		Name:      "updatedir",
		Usage:     "update an existing directory",
		ArgsUsage: "<key> <value>",
		Flags: []cli.Flag{
			cli.IntFlag{Name: "ttl", Value: 0, Usage: "key time-to-live in seconds"},
		},
		Action: func(c *cli.Context) error {
			updatedirCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func updatedirCommandFunc(c *cli.Context, ki client.KeysAPI) {
	if len(c.Args()) == 0 {
		handleError(c, ExitBadArgs, errors.New("key required"))
	}
	key := c.Args()[0]
	ttl := c.Int("ttl")
	ctx, cancel := contextWithTotalTimeout(c)
	resp, err := ki.Set(ctx, key, "", &client.SetOptions{TTL: time.Duration(ttl) * time.Second, Dir: true, PrevExist: client.PrevExist})
	cancel()
	if err != nil {
		handleError(c, ExitServerError, err)
	}
	if c.GlobalString("output") != "simple" {
		printResponseKey(resp, c.GlobalString("output"))
	}
}
func handleBackup(c *cli.Context) error {
	var srcWAL string
	var destWAL string

	withV3 := c.Bool("with-v3")
	srcSnap := filepath.Join(c.String("data-dir"), "member", "snap")
	destSnap := filepath.Join(c.String("backup-dir"), "member", "snap")

	if c.String("wal-dir") != "" {
		srcWAL = c.String("wal-dir")
	} else {
		srcWAL = filepath.Join(c.String("data-dir"), "member", "wal")
	}

	if c.String("backup-wal-dir") != "" {
		destWAL = c.String("backup-wal-dir")
	} else {
		destWAL = filepath.Join(c.String("backup-dir"), "member", "wal")
	}

	if err := fileutil.CreateDirAll(destSnap); err != nil {
		log.Fatalf("failed creating backup snapshot dir %v: %v", destSnap, err)
	}

	walsnap := saveSnap(destSnap, srcSnap)
	metadata, state, ents := loadWAL(srcWAL, walsnap, withV3)
	saveDB(filepath.Join(destSnap, "db"), filepath.Join(srcSnap, "db"), state.Commit, withV3)

	idgen := idutil.NewGenerator(0, time.Now())
	metadata.NodeID = idgen.Next()
	metadata.ClusterID = idgen.Next()

	neww, err := wal.Create(zap.NewExample(), destWAL, pbutil.MustMarshal(&metadata))
	if err != nil {
		log.Fatal(err)
	}
	defer neww.Close()
	if err := neww.Save(state, ents); err != nil {
		log.Fatal(err)
	}
	if err := neww.SaveSnapshot(walsnap); err != nil {
		log.Fatal(err)
	}

	return nil
}
func saveDB(destDB, srcDB string, idx uint64, v3 bool) {
	// open src db to safely copy db state
	if v3 {
		var src *bolt.DB
		ch := make(chan *bolt.DB, 1)
		go func() {
			db, err := bolt.Open(srcDB, 0444, &bolt.Options{ReadOnly: true})
			if err != nil {
				log.Fatal(err)
			}
			ch <- db
		}()
		select {
		case src = <-ch:
		case <-time.After(time.Second):
			log.Println("waiting to acquire lock on", srcDB)
			src = <-ch
		}
		defer src.Close()

		tx, err := src.Begin(false)
		if err != nil {
			log.Fatal(err)
		}

		// copy srcDB to destDB
		dest, err := os.Create(destDB)
		if err != nil {
			log.Fatal(err)
		}
		if _, err := tx.WriteTo(dest); err != nil {
			log.Fatal(err)
		}
		dest.Close()
		if err := tx.Rollback(); err != nil {
			log.Fatal(err)
		}
	}

	db, err := bolt.Open(destDB, 0644, &bolt.Options{})
	if err != nil {
		log.Fatal(err)
	}
	tx, err := db.Begin(true)
	if err != nil {
		log.Fatal(err)
	}

	// remove membership information; should be clobbered by --force-new-cluster
	for _, bucket := range []string{"members", "members_removed", "cluster"} {
		tx.DeleteBucket([]byte(bucket))
	}

	// update consistent index to match hard state
	if !v3 {
		idxBytes := make([]byte, 8)
		binary.BigEndian.PutUint64(idxBytes, idx)
		b, err := tx.CreateBucketIfNotExists([]byte("meta"))
		if err != nil {
			log.Fatal(err)
		}
		b.Put([]byte("consistent_index"), idxBytes)
	}

	if err := tx.Commit(); err != nil {
		log.Fatal(err)
	}
	if err := db.Close(); err != nil {
		log.Fatal(err)
	}
}
func NewWatchCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "watcher",
		Short: "Performs watch operation",
		Run:   runWatcherFunc,
	}
	cmd.Flags().DurationVar(&runningTime, "running-time", 60, "number of seconds to run")
	cmd.Flags().StringVar(&watchPrefix, "prefix", "", "the prefix to append on all keys")
	cmd.Flags().IntVar(&noOfPrefixes, "total-prefixes", 10, "total no of prefixes to use")
	cmd.Flags().IntVar(&watchPerPrefix, "watch-per-prefix", 10, "number of watchers per prefix")
	cmd.Flags().IntVar(&totalKeys, "total-keys", 1000, "total number of keys to watch")

	return cmd
}
func NewV3(lg *zap.Logger) Manager {
	if lg == nil {
		lg = zap.NewExample()
	}
	return &v3Manager{lg: lg}
}
func (s *v3Manager) Save(ctx context.Context, cfg clientv3.Config, dbPath string) error {
	if len(cfg.Endpoints) != 1 {
		return fmt.Errorf("snapshot must be requested to one selected node, not multiple %v", cfg.Endpoints)
	}
	cli, err := clientv3.New(cfg)
	if err != nil {
		return err
	}
	defer cli.Close()

	partpath := dbPath + ".part"
	defer os.RemoveAll(partpath)

	var f *os.File
	f, err = os.OpenFile(partpath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, fileutil.PrivateFileMode)
	if err != nil {
		return fmt.Errorf("could not open %s (%v)", partpath, err)
	}
	s.lg.Info(
		"created temporary db file",
		zap.String("path", partpath),
	)

	now := time.Now()
	var rd io.ReadCloser
	rd, err = cli.Snapshot(ctx)
	if err != nil {
		return err
	}
	s.lg.Info(
		"fetching snapshot",
		zap.String("endpoint", cfg.Endpoints[0]),
	)
	if _, err = io.Copy(f, rd); err != nil {
		return err
	}
	if err = fileutil.Fsync(f); err != nil {
		return err
	}
	if err = f.Close(); err != nil {
		return err
	}
	s.lg.Info(
		"fetched snapshot",
		zap.String("endpoint", cfg.Endpoints[0]),
		zap.Duration("took", time.Since(now)),
	)

	if err = os.Rename(partpath, dbPath); err != nil {
		return fmt.Errorf("could not rename %s to %s (%v)", partpath, dbPath, err)
	}
	s.lg.Info("saved", zap.String("path", dbPath))
	return nil
}
func (s *v3Manager) Status(dbPath string) (ds Status, err error) {
	if _, err = os.Stat(dbPath); err != nil {
		return ds, err
	}

	db, err := bolt.Open(dbPath, 0400, &bolt.Options{ReadOnly: true})
	if err != nil {
		return ds, err
	}
	defer db.Close()

	h := crc32.New(crc32.MakeTable(crc32.Castagnoli))

	if err = db.View(func(tx *bolt.Tx) error {
		// check snapshot file integrity first
		var dbErrStrings []string
		for dbErr := range tx.Check() {
			dbErrStrings = append(dbErrStrings, dbErr.Error())
		}
		if len(dbErrStrings) > 0 {
			return fmt.Errorf("snapshot file integrity check failed. %d errors found.\n"+strings.Join(dbErrStrings, "\n"), len(dbErrStrings))
		}
		ds.TotalSize = tx.Size()
		c := tx.Cursor()
		for next, _ := c.First(); next != nil; next, _ = c.Next() {
			b := tx.Bucket(next)
			if b == nil {
				return fmt.Errorf("cannot get hash of bucket %s", string(next))
			}
			h.Write(next)
			iskeyb := (string(next) == "key")
			b.ForEach(func(k, v []byte) error {
				h.Write(k)
				h.Write(v)
				if iskeyb {
					rev := bytesToRev(k)
					ds.Revision = rev.main
				}
				ds.TotalKey++
				return nil
			})
		}
		return nil
	}); err != nil {
		return ds, err
	}

	ds.Hash = h.Sum32()
	return ds, nil
}
func (s *v3Manager) Restore(cfg RestoreConfig) error {
	pURLs, err := types.NewURLs(cfg.PeerURLs)
	if err != nil {
		return err
	}
	var ics types.URLsMap
	ics, err = types.NewURLsMap(cfg.InitialCluster)
	if err != nil {
		return err
	}

	srv := etcdserver.ServerConfig{
		Logger:              s.lg,
		Name:                cfg.Name,
		PeerURLs:            pURLs,
		InitialPeerURLsMap:  ics,
		InitialClusterToken: cfg.InitialClusterToken,
	}
	if err = srv.VerifyBootstrap(); err != nil {
		return err
	}

	s.cl, err = membership.NewClusterFromURLsMap(s.lg, cfg.InitialClusterToken, ics)
	if err != nil {
		return err
	}

	dataDir := cfg.OutputDataDir
	if dataDir == "" {
		dataDir = cfg.Name + ".etcd"
	}
	if fileutil.Exist(dataDir) {
		return fmt.Errorf("data-dir %q exists", dataDir)
	}

	walDir := cfg.OutputWALDir
	if walDir == "" {
		walDir = filepath.Join(dataDir, "member", "wal")
	} else if fileutil.Exist(walDir) {
		return fmt.Errorf("wal-dir %q exists", walDir)
	}

	s.name = cfg.Name
	s.dbPath = cfg.SnapshotPath
	s.walDir = walDir
	s.snapDir = filepath.Join(dataDir, "member", "snap")
	s.skipHashCheck = cfg.SkipHashCheck

	s.lg.Info(
		"restoring snapshot",
		zap.String("path", s.dbPath),
		zap.String("wal-dir", s.walDir),
		zap.String("data-dir", dataDir),
		zap.String("snap-dir", s.snapDir),
	)
	if err = s.saveDB(); err != nil {
		return err
	}
	if err = s.saveWALAndSnap(); err != nil {
		return err
	}
	s.lg.Info(
		"restored snapshot",
		zap.String("path", s.dbPath),
		zap.String("wal-dir", s.walDir),
		zap.String("data-dir", dataDir),
		zap.String("snap-dir", s.snapDir),
	)

	return nil
}
func NewAuthStore(lg *zap.Logger, be backend.Backend, tp TokenProvider, bcryptCost int) *authStore {
	if bcryptCost < bcrypt.MinCost || bcryptCost > bcrypt.MaxCost {
		if lg != nil {
			lg.Warn(
				"use default bcrypt cost instead of the invalid given cost",
				zap.Int("min-cost", bcrypt.MinCost),
				zap.Int("max-cost", bcrypt.MaxCost),
				zap.Int("default-cost", bcrypt.DefaultCost),
				zap.Int("given-cost", bcryptCost))
		} else {
			plog.Warningf("Use default bcrypt-cost %d instead of the invalid value %d",
				bcrypt.DefaultCost, bcryptCost)
		}

		bcryptCost = bcrypt.DefaultCost
	}

	tx := be.BatchTx()
	tx.Lock()

	tx.UnsafeCreateBucket(authBucketName)
	tx.UnsafeCreateBucket(authUsersBucketName)
	tx.UnsafeCreateBucket(authRolesBucketName)

	enabled := false
	_, vs := tx.UnsafeRange(authBucketName, enableFlagKey, nil, 0)
	if len(vs) == 1 {
		if bytes.Equal(vs[0], authEnabled) {
			enabled = true
		}
	}

	as := &authStore{
		revision:       getRevision(tx),
		lg:             lg,
		be:             be,
		enabled:        enabled,
		rangePermCache: make(map[string]*unifiedRangePermissions),
		tokenProvider:  tp,
		bcryptCost:     bcryptCost,
	}

	if enabled {
		as.tokenProvider.enable()
	}

	if as.Revision() == 0 {
		as.commitRevision(tx)
	}

	tx.Unlock()
	be.ForceCommit()

	return as
}
func NewTokenProvider(
	lg *zap.Logger,
	tokenOpts string,
	indexWaiter func(uint64) <-chan struct{}) (TokenProvider, error) {
	tokenType, typeSpecificOpts, err := decomposeOpts(lg, tokenOpts)
	if err != nil {
		return nil, ErrInvalidAuthOpts
	}

	switch tokenType {
	case tokenTypeSimple:
		if lg != nil {
			lg.Warn("simple token is not cryptographically signed")
		} else {
			plog.Warningf("simple token is not cryptographically signed")
		}
		return newTokenProviderSimple(lg, indexWaiter), nil

	case tokenTypeJWT:
		return newTokenProviderJWT(lg, typeSpecificOpts)

	case "":
		return newTokenProviderNop()

	default:
		if lg != nil {
			lg.Warn(
				"unknown token type",
				zap.String("type", tokenType),
				zap.Error(ErrInvalidAuthOpts),
			)
		} else {
			plog.Errorf("unknown token type: %s", tokenType)
		}
		return nil, ErrInvalidAuthOpts
	}
}
func (t *Transport) MendPeer(id types.ID) {
	t.mu.RLock()
	p, pok := t.peers[id]
	g, gok := t.remotes[id]
	t.mu.RUnlock()

	if pok {
		p.(Pausable).Resume()
	}
	if gok {
		g.Resume()
	}
}
func (t *Transport) removePeer(id types.ID) {
	if peer, ok := t.peers[id]; ok {
		peer.stop()
	} else {
		if t.Logger != nil {
			t.Logger.Panic("unexpected removal of unknown remote peer", zap.String("remote-peer-id", id.String()))
		} else {
			plog.Panicf("unexpected removal of unknown peer '%d'", id)
		}
	}
	delete(t.peers, id)
	delete(t.LeaderStats.Followers, id.String())
	t.pipelineProber.Remove(id.String())
	t.streamProber.Remove(id.String())

	if t.Logger != nil {
		t.Logger.Info(
			"removed remote peer",
			zap.String("local-member-id", t.ID.String()),
			zap.String("removed-remote-peer-id", id.String()),
		)
	} else {
		plog.Infof("removed peer %s", id)
	}
}
func (t *Transport) ActivePeers() (cnt int) {
	t.mu.RLock()
	defer t.mu.RUnlock()
	for _, p := range t.peers {
		if !p.activeSince().IsZero() {
			cnt++
		}
	}
	return cnt
}
func resolveTCPAddrDefault(ctx context.Context, addr string) (*net.TCPAddr, error) {
	host, port, serr := net.SplitHostPort(addr)
	if serr != nil {
		return nil, serr
	}
	portnum, perr := net.DefaultResolver.LookupPort(ctx, "tcp", port)
	if perr != nil {
		return nil, perr
	}

	var ips []net.IPAddr
	if ip := net.ParseIP(host); ip != nil {
		ips = []net.IPAddr{{IP: ip}}
	} else {
		// Try as a DNS name.
		ipss, err := net.DefaultResolver.LookupIPAddr(ctx, host)
		if err != nil {
			return nil, err
		}
		ips = ipss
	}
	// randomize?
	ip := ips[0]
	return &net.TCPAddr{IP: ip.IP, Port: portnum, Zone: ip.Zone}, nil
}
func resolveTCPAddrs(ctx context.Context, lg *zap.Logger, urls [][]url.URL) ([][]url.URL, error) {
	newurls := make([][]url.URL, 0)
	for _, us := range urls {
		nus := make([]url.URL, len(us))
		for i, u := range us {
			nu, err := url.Parse(u.String())
			if err != nil {
				return nil, fmt.Errorf("failed to parse %q (%v)", u.String(), err)
			}
			nus[i] = *nu
		}
		for i, u := range nus {
			h, err := resolveURL(ctx, lg, u)
			if err != nil {
				return nil, fmt.Errorf("failed to resolve %q (%v)", u.String(), err)
			}
			if h != "" {
				nus[i].Host = h
			}
		}
		newurls = append(newurls, nus)
	}
	return newurls, nil
}
func urlsEqual(ctx context.Context, lg *zap.Logger, a []url.URL, b []url.URL) (bool, error) {
	if len(a) != len(b) {
		return false, fmt.Errorf("len(%q) != len(%q)", urlsToStrings(a), urlsToStrings(b))
	}
	urls, err := resolveTCPAddrs(ctx, lg, [][]url.URL{a, b})
	if err != nil {
		return false, err
	}
	preva, prevb := a, b
	a, b = urls[0], urls[1]
	sort.Sort(types.URLs(a))
	sort.Sort(types.URLs(b))
	for i := range a {
		if !reflect.DeepEqual(a[i], b[i]) {
			return false, fmt.Errorf("%q(resolved from %q) != %q(resolved from %q)",
				a[i].String(), preva[i].String(),
				b[i].String(), prevb[i].String(),
			)
		}
	}
	return true, nil
}
func URLStringsEqual(ctx context.Context, lg *zap.Logger, a []string, b []string) (bool, error) {
	if len(a) != len(b) {
		return false, fmt.Errorf("len(%q) != len(%q)", a, b)
	}
	urlsA := make([]url.URL, 0)
	for _, str := range a {
		u, err := url.Parse(str)
		if err != nil {
			return false, fmt.Errorf("failed to parse %q", str)
		}
		urlsA = append(urlsA, *u)
	}
	urlsB := make([]url.URL, 0)
	for _, str := range b {
		u, err := url.Parse(str)
		if err != nil {
			return false, fmt.Errorf("failed to parse %q", str)
		}
		urlsB = append(urlsB, *u)
	}
	if lg == nil {
		lg, _ = zap.NewProduction()
		if lg == nil {
			lg = zap.NewExample()
		}
	}
	return urlsEqual(ctx, lg, urlsA, urlsB)
}
func NewLeaseCommand() *cobra.Command {
	lc := &cobra.Command{
		Use:   "lease <subcommand>",
		Short: "Lease related commands",
	}

	lc.AddCommand(NewLeaseGrantCommand())
	lc.AddCommand(NewLeaseRevokeCommand())
	lc.AddCommand(NewLeaseTimeToLiveCommand())
	lc.AddCommand(NewLeaseListCommand())
	lc.AddCommand(NewLeaseKeepAliveCommand())

	return lc
}
func NewLeaseGrantCommand() *cobra.Command {
	lc := &cobra.Command{
		Use:   "grant <ttl>",
		Short: "Creates leases",

		Run: leaseGrantCommandFunc,
	}

	return lc
}
func leaseGrantCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("lease grant command needs TTL argument"))
	}

	ttl, err := strconv.ParseInt(args[0], 10, 64)
	if err != nil {
		ExitWithError(ExitBadArgs, fmt.Errorf("bad TTL (%v)", err))
	}

	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).Grant(ctx, ttl)
	cancel()
	if err != nil {
		ExitWithError(ExitError, fmt.Errorf("failed to grant lease (%v)", err))
	}
	display.Grant(*resp)
}
func NewLeaseRevokeCommand() *cobra.Command {
	lc := &cobra.Command{
		Use:   "revoke <leaseID>",
		Short: "Revokes leases",

		Run: leaseRevokeCommandFunc,
	}

	return lc
}
func leaseRevokeCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("lease revoke command needs 1 argument"))
	}

	id := leaseFromArgs(args[0])
	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).Revoke(ctx, id)
	cancel()
	if err != nil {
		ExitWithError(ExitError, fmt.Errorf("failed to revoke lease (%v)", err))
	}
	display.Revoke(id, *resp)
}
func NewLeaseTimeToLiveCommand() *cobra.Command {
	lc := &cobra.Command{
		Use:   "timetolive <leaseID> [options]",
		Short: "Get lease information",

		Run: leaseTimeToLiveCommandFunc,
	}
	lc.Flags().BoolVar(&timeToLiveKeys, "keys", false, "Get keys attached to this lease")

	return lc
}
func leaseTimeToLiveCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("lease timetolive command needs lease ID as argument"))
	}
	var opts []v3.LeaseOption
	if timeToLiveKeys {
		opts = append(opts, v3.WithAttachedKeys())
	}
	resp, rerr := mustClientFromCmd(cmd).TimeToLive(context.TODO(), leaseFromArgs(args[0]), opts...)
	if rerr != nil {
		ExitWithError(ExitBadConnection, rerr)
	}
	display.TimeToLive(*resp, timeToLiveKeys)
}
func NewLeaseListCommand() *cobra.Command {
	lc := &cobra.Command{
		Use:   "list",
		Short: "List all active leases",
		Run:   leaseListCommandFunc,
	}
	return lc
}
func leaseListCommandFunc(cmd *cobra.Command, args []string) {
	resp, rerr := mustClientFromCmd(cmd).Leases(context.TODO())
	if rerr != nil {
		ExitWithError(ExitBadConnection, rerr)
	}
	display.Leases(*resp)
}
func NewLeaseKeepAliveCommand() *cobra.Command {
	lc := &cobra.Command{
		Use:   "keep-alive [options] <leaseID>",
		Short: "Keeps leases alive (renew)",

		Run: leaseKeepAliveCommandFunc,
	}

	lc.Flags().BoolVar(&leaseKeepAliveOnce, "once", false, "Resets the keep-alive time to its original value and exits immediately")

	return lc
}
func leaseKeepAliveCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("lease keep-alive command needs lease ID as argument"))
	}

	id := leaseFromArgs(args[0])

	if leaseKeepAliveOnce {
		respc, kerr := mustClientFromCmd(cmd).KeepAliveOnce(context.TODO(), id)
		if kerr != nil {
			ExitWithError(ExitBadConnection, kerr)
		}
		display.KeepAlive(*respc)
		return
	}

	respc, kerr := mustClientFromCmd(cmd).KeepAlive(context.TODO(), id)
	if kerr != nil {
		ExitWithError(ExitBadConnection, kerr)
	}
	for resp := range respc {
		display.KeepAlive(*resp)
	}

	if _, ok := (display).(*simplePrinter); ok {
		fmt.Printf("lease %016x expired or revoked.\n", id)
	}
}
func NewAlarmCommand() *cobra.Command {
	ac := &cobra.Command{
		Use:   "alarm <subcommand>",
		Short: "Alarm related commands",
	}

	ac.AddCommand(NewAlarmDisarmCommand())
	ac.AddCommand(NewAlarmListCommand())

	return ac
}
func alarmDisarmCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 0 {
		ExitWithError(ExitBadArgs, fmt.Errorf("alarm disarm command accepts no arguments"))
	}
	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).AlarmDisarm(ctx, &v3.AlarmMember{})
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	display.Alarm(*resp)
}
func alarmListCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 0 {
		ExitWithError(ExitBadArgs, fmt.Errorf("alarm list command accepts no arguments"))
	}
	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).AlarmList(ctx)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	display.Alarm(*resp)
}
func (e *Etcd) Flags() (fs []string) {
	tp := reflect.TypeOf(*e)
	vo := reflect.ValueOf(*e)
	for _, name := range etcdFields {
		field, ok := tp.FieldByName(name)
		if !ok {
			panic(fmt.Errorf("field %q not found", name))
		}
		fv := reflect.Indirect(vo).FieldByName(name)
		var sv string
		switch fv.Type().Kind() {
		case reflect.String:
			sv = fv.String()
		case reflect.Slice:
			n := fv.Len()
			sl := make([]string, n)
			for i := 0; i < n; i++ {
				sl[i] = fv.Index(i).String()
			}
			sv = strings.Join(sl, ",")
		case reflect.Int64:
			sv = fmt.Sprintf("%d", fv.Int())
		case reflect.Bool:
			sv = fmt.Sprintf("%v", fv.Bool())
		default:
			panic(fmt.Errorf("field %q (%v) cannot be parsed", name, fv.Type().Kind()))
		}

		fname := field.Tag.Get("yaml")

		// TODO: remove this
		if fname == "initial-corrupt-check" {
			fname = "experimental-" + fname
		}

		if sv != "" {
			fs = append(fs, fmt.Sprintf("--%s=%s", fname, sv))
		}
	}
	return fs
}
func (e *Etcd) EmbedConfig() (cfg *embed.Config, err error) {
	var lcURLs types.URLs
	lcURLs, err = types.NewURLs(e.ListenClientURLs)
	if err != nil {
		return nil, err
	}
	var acURLs types.URLs
	acURLs, err = types.NewURLs(e.AdvertiseClientURLs)
	if err != nil {
		return nil, err
	}
	var lpURLs types.URLs
	lpURLs, err = types.NewURLs(e.ListenPeerURLs)
	if err != nil {
		return nil, err
	}
	var apURLs types.URLs
	apURLs, err = types.NewURLs(e.AdvertisePeerURLs)
	if err != nil {
		return nil, err
	}

	cfg = embed.NewConfig()
	cfg.Name = e.Name
	cfg.Dir = e.DataDir
	cfg.WalDir = e.WALDir
	cfg.TickMs = uint(e.HeartbeatIntervalMs)
	cfg.ElectionMs = uint(e.ElectionTimeoutMs)

	cfg.LCUrls = lcURLs
	cfg.ACUrls = acURLs
	cfg.ClientAutoTLS = e.ClientAutoTLS
	cfg.ClientTLSInfo = transport.TLSInfo{
		ClientCertAuth: e.ClientCertAuth,
		CertFile:       e.ClientCertFile,
		KeyFile:        e.ClientKeyFile,
		TrustedCAFile:  e.ClientTrustedCAFile,
	}

	cfg.LPUrls = lpURLs
	cfg.APUrls = apURLs
	cfg.PeerAutoTLS = e.PeerAutoTLS
	cfg.PeerTLSInfo = transport.TLSInfo{
		ClientCertAuth: e.PeerClientCertAuth,
		CertFile:       e.PeerCertFile,
		KeyFile:        e.PeerKeyFile,
		TrustedCAFile:  e.PeerTrustedCAFile,
	}

	cfg.InitialCluster = e.InitialCluster
	cfg.ClusterState = e.InitialClusterState
	cfg.InitialClusterToken = e.InitialClusterToken

	cfg.SnapshotCount = uint64(e.SnapshotCount)
	cfg.QuotaBackendBytes = e.QuotaBackendBytes

	cfg.PreVote = e.PreVote
	cfg.ExperimentalInitialCorruptCheck = e.InitialCorruptCheck

	cfg.Logger = e.Logger
	cfg.LogOutputs = e.LogOutputs
	cfg.Debug = e.Debug

	return cfg, nil
}
func PProfHandlers() map[string]http.Handler {
	// set only when there's no existing setting
	if runtime.SetMutexProfileFraction(-1) == 0 {
		// 1 out of 5 mutex events are reported, on average
		runtime.SetMutexProfileFraction(5)
	}

	m := make(map[string]http.Handler)

	m[HTTPPrefixPProf+"/"] = http.HandlerFunc(pprof.Index)
	m[HTTPPrefixPProf+"/profile"] = http.HandlerFunc(pprof.Profile)
	m[HTTPPrefixPProf+"/symbol"] = http.HandlerFunc(pprof.Symbol)
	m[HTTPPrefixPProf+"/cmdline"] = http.HandlerFunc(pprof.Cmdline)
	m[HTTPPrefixPProf+"/trace "] = http.HandlerFunc(pprof.Trace)
	m[HTTPPrefixPProf+"/heap"] = pprof.Handler("heap")
	m[HTTPPrefixPProf+"/goroutine"] = pprof.Handler("goroutine")
	m[HTTPPrefixPProf+"/threadcreate"] = pprof.Handler("threadcreate")
	m[HTTPPrefixPProf+"/block"] = pprof.Handler("block")
	m[HTTPPrefixPProf+"/mutex"] = pprof.Handler("mutex")

	return m
}
func NewBackendQuota(s *EtcdServer, name string) Quota {
	lg := s.getLogger()
	quotaBackendBytes.Set(float64(s.Cfg.QuotaBackendBytes))

	if s.Cfg.QuotaBackendBytes < 0 {
		// disable quotas if negative
		quotaLogOnce.Do(func() {
			if lg != nil {
				lg.Info(
					"disabled backend quota",
					zap.String("quota-name", name),
					zap.Int64("quota-size-bytes", s.Cfg.QuotaBackendBytes),
				)
			} else {
				plog.Warningf("disabling backend quota")
			}
		})
		return &passthroughQuota{}
	}

	if s.Cfg.QuotaBackendBytes == 0 {
		// use default size if no quota size given
		quotaLogOnce.Do(func() {
			if lg != nil {
				lg.Info(
					"enabled backend quota with default value",
					zap.String("quota-name", name),
					zap.Int64("quota-size-bytes", DefaultQuotaBytes),
					zap.String("quota-size", DefaultQuotaSize),
				)
			}
		})
		quotaBackendBytes.Set(float64(DefaultQuotaBytes))
		return &backendQuota{s, DefaultQuotaBytes}
	}

	quotaLogOnce.Do(func() {
		if s.Cfg.QuotaBackendBytes > MaxQuotaBytes {
			if lg != nil {
				lg.Warn(
					"quota exceeds the maximum value",
					zap.String("quota-name", name),
					zap.Int64("quota-size-bytes", s.Cfg.QuotaBackendBytes),
					zap.String("quota-size", humanize.Bytes(uint64(s.Cfg.QuotaBackendBytes))),
					zap.Int64("quota-maximum-size-bytes", MaxQuotaBytes),
					zap.String("quota-maximum-size", maxQuotaSize),
				)
			} else {
				plog.Warningf("backend quota %v exceeds maximum recommended quota %v", s.Cfg.QuotaBackendBytes, MaxQuotaBytes)
			}
		}
		if lg != nil {
			lg.Info(
				"enabled backend quota",
				zap.String("quota-name", name),
				zap.Int64("quota-size-bytes", s.Cfg.QuotaBackendBytes),
				zap.String("quota-size", humanize.Bytes(uint64(s.Cfg.QuotaBackendBytes))),
			)
		}
	})
	return &backendQuota{s, s.Cfg.QuotaBackendBytes}
}
func NewClusterProxy(c *clientv3.Client, advaddr string, prefix string) (pb.ClusterServer, <-chan struct{}) {
	cp := &clusterProxy{
		clus: c.Cluster,
		ctx:  c.Ctx(),
		gr:   &naming.GRPCResolver{Client: c},

		advaddr: advaddr,
		prefix:  prefix,
		umap:    make(map[string]gnaming.Update),
	}

	donec := make(chan struct{})
	if advaddr != "" && prefix != "" {
		go func() {
			defer close(donec)
			cp.resolve(prefix)
		}()
		return cp, donec
	}

	close(donec)
	return cp, donec
}
func NewHandler(l lease.Lessor, waitch func() <-chan struct{}) http.Handler {
	return &leaseHandler{l, waitch}
}
func TimeToLiveHTTP(ctx context.Context, id lease.LeaseID, keys bool, url string, rt http.RoundTripper) (*leasepb.LeaseInternalResponse, error) {
	// will post lreq protobuf to leader
	lreq, err := (&leasepb.LeaseInternalRequest{
		LeaseTimeToLiveRequest: &pb.LeaseTimeToLiveRequest{
			ID:   int64(id),
			Keys: keys,
		},
	}).Marshal()
	if err != nil {
		return nil, err
	}

	req, err := http.NewRequest("POST", url, bytes.NewReader(lreq))
	if err != nil {
		return nil, err
	}
	req.Header.Set("Content-Type", "application/protobuf")

	req = req.WithContext(ctx)

	cc := &http.Client{Transport: rt}
	var b []byte
	// buffer errc channel so that errc don't block inside the go routinue
	resp, err := cc.Do(req)
	if err != nil {
		return nil, err
	}
	b, err = readResponse(resp)
	if err != nil {
		return nil, err
	}
	if resp.StatusCode == http.StatusRequestTimeout {
		return nil, ErrLeaseHTTPTimeout
	}
	if resp.StatusCode == http.StatusNotFound {
		return nil, lease.ErrLeaseNotFound
	}
	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("lease: unknown error(%s)", string(b))
	}

	lresp := &leasepb.LeaseInternalResponse{}
	if err := lresp.Unmarshal(b); err != nil {
		return nil, fmt.Errorf(`lease: %v. data = "%s"`, err, string(b))
	}
	if lresp.LeaseTimeToLiveResponse.ID != int64(id) {
		return nil, fmt.Errorf("lease: renew id mismatch")
	}
	return lresp, nil
}
func newWatcherBatch(wg *watcherGroup, evs []mvccpb.Event) watcherBatch {
	if len(wg.watchers) == 0 {
		return nil
	}

	wb := make(watcherBatch)
	for _, ev := range evs {
		for w := range wg.watcherSetByKey(string(ev.Kv.Key)) {
			if ev.Kv.ModRevision >= w.minRev {
				// don't double notify
				wb.add(w, ev)
			}
		}
	}
	return wb
}
func (wg *watcherGroup) add(wa *watcher) {
	wg.watchers.add(wa)
	if wa.end == nil {
		wg.keyWatchers.add(wa)
		return
	}

	// interval already registered?
	ivl := adt.NewStringAffineInterval(string(wa.key), string(wa.end))
	if iv := wg.ranges.Find(ivl); iv != nil {
		iv.Val.(watcherSet).add(wa)
		return
	}

	// not registered, put in interval tree
	ws := make(watcherSet)
	ws.add(wa)
	wg.ranges.Insert(ivl, ws)
}
func (wg *watcherGroup) contains(key string) bool {
	_, ok := wg.keyWatchers[key]
	return ok || wg.ranges.Intersects(adt.NewStringAffinePoint(key))
}
func (wg *watcherGroup) delete(wa *watcher) bool {
	if _, ok := wg.watchers[wa]; !ok {
		return false
	}
	wg.watchers.delete(wa)
	if wa.end == nil {
		wg.keyWatchers.delete(wa)
		return true
	}

	ivl := adt.NewStringAffineInterval(string(wa.key), string(wa.end))
	iv := wg.ranges.Find(ivl)
	if iv == nil {
		return false
	}

	ws := iv.Val.(watcherSet)
	delete(ws, wa)
	if len(ws) == 0 {
		// remove interval missing watchers
		if ok := wg.ranges.Delete(ivl); !ok {
			panic("could not remove watcher from interval tree")
		}
	}

	return true
}
func (wg *watcherGroup) choose(maxWatchers int, curRev, compactRev int64) (*watcherGroup, int64) {
	if len(wg.watchers) < maxWatchers {
		return wg, wg.chooseAll(curRev, compactRev)
	}
	ret := newWatcherGroup()
	for w := range wg.watchers {
		if maxWatchers <= 0 {
			break
		}
		maxWatchers--
		ret.add(w)
	}
	return &ret, ret.chooseAll(curRev, compactRev)
}
func (wg *watcherGroup) watcherSetByKey(key string) watcherSet {
	wkeys := wg.keyWatchers[key]
	wranges := wg.ranges.Stab(adt.NewStringAffinePoint(key))

	// zero-copy cases
	switch {
	case len(wranges) == 0:
		// no need to merge ranges or copy; reuse single-key set
		return wkeys
	case len(wranges) == 0 && len(wkeys) == 0:
		return nil
	case len(wranges) == 1 && len(wkeys) == 0:
		return wranges[0].Val.(watcherSet)
	}

	// copy case
	ret := make(watcherSet)
	ret.union(wg.keyWatchers[key])
	for _, item := range wranges {
		ret.union(item.Val.(watcherSet))
	}
	return ret
}
func (ivl *Interval) Compare(c Comparable) int {
	ivl2 := c.(*Interval)
	ivbCmpBegin := ivl.Begin.Compare(ivl2.Begin)
	ivbCmpEnd := ivl.Begin.Compare(ivl2.End)
	iveCmpBegin := ivl.End.Compare(ivl2.Begin)

	// ivl is left of ivl2
	if ivbCmpBegin < 0 && iveCmpBegin <= 0 {
		return -1
	}

	// iv is right of iv2
	if ivbCmpEnd >= 0 {
		return 1
	}

	return 0
}
func (x *intervalNode) successor() *intervalNode {
	if x.right != nil {
		return x.right.min()
	}
	y := x.parent
	for y != nil && x == y.right {
		x = y
		y = y.parent
	}
	return y
}
func (x *intervalNode) updateMax() {
	for x != nil {
		oldmax := x.max
		max := x.iv.Ivl.End
		if x.left != nil && x.left.max.Compare(max) > 0 {
			max = x.left.max
		}
		if x.right != nil && x.right.max.Compare(max) > 0 {
			max = x.right.max
		}
		if oldmax.Compare(max) == 0 {
			break
		}
		x.max = max
		x = x.parent
	}
}
func (x *intervalNode) visit(iv *Interval, nv nodeVisitor) bool {
	if x == nil {
		return true
	}
	v := iv.Compare(&x.iv.Ivl)
	switch {
	case v < 0:
		if !x.left.visit(iv, nv) {
			return false
		}
	case v > 0:
		maxiv := Interval{x.iv.Ivl.Begin, x.max}
		if maxiv.Compare(iv) == 0 {
			if !x.left.visit(iv, nv) || !x.right.visit(iv, nv) {
				return false
			}
		}
	default:
		if !x.left.visit(iv, nv) || !nv(x) || !x.right.visit(iv, nv) {
			return false
		}
	}
	return true
}
func (ivt *IntervalTree) Delete(ivl Interval) bool {
	z := ivt.find(ivl)
	if z == nil {
		return false
	}

	y := z
	if z.left != nil && z.right != nil {
		y = z.successor()
	}

	x := y.left
	if x == nil {
		x = y.right
	}
	if x != nil {
		x.parent = y.parent
	}

	if y.parent == nil {
		ivt.root = x
	} else {
		if y == y.parent.left {
			y.parent.left = x
		} else {
			y.parent.right = x
		}
		y.parent.updateMax()
	}
	if y != z {
		z.iv = y.iv
		z.updateMax()
	}

	if y.color() == black && x != nil {
		ivt.deleteFixup(x)
	}

	ivt.count--
	return true
}
func (ivt *IntervalTree) Insert(ivl Interval, val interface{}) {
	var y *intervalNode
	z := &intervalNode{iv: IntervalValue{ivl, val}, max: ivl.End, c: red}
	x := ivt.root
	for x != nil {
		y = x
		if z.iv.Ivl.Begin.Compare(x.iv.Ivl.Begin) < 0 {
			x = x.left
		} else {
			x = x.right
		}
	}

	z.parent = y
	if y == nil {
		ivt.root = z
	} else {
		if z.iv.Ivl.Begin.Compare(y.iv.Ivl.Begin) < 0 {
			y.left = z
		} else {
			y.right = z
		}
		y.updateMax()
	}
	z.c = red
	ivt.insertFixup(z)
	ivt.count++
}
func (ivt *IntervalTree) rotateLeft(x *intervalNode) {
	y := x.right
	x.right = y.left
	if y.left != nil {
		y.left.parent = x
	}
	x.updateMax()
	ivt.replaceParent(x, y)
	y.left = x
	y.updateMax()
}
func (ivt *IntervalTree) replaceParent(x *intervalNode, y *intervalNode) {
	y.parent = x.parent
	if x.parent == nil {
		ivt.root = y
	} else {
		if x == x.parent.left {
			x.parent.left = y
		} else {
			x.parent.right = y
		}
		x.parent.updateMax()
	}
	x.parent = y
}
func (ivt *IntervalTree) MaxHeight() int {
	return int((2 * math.Log2(float64(ivt.Len()+1))) + 0.5)
}
func (ivt *IntervalTree) Visit(ivl Interval, ivv IntervalVisitor) {
	ivt.root.visit(&ivl, func(n *intervalNode) bool { return ivv(&n.iv) })
}
func (ivt *IntervalTree) find(ivl Interval) (ret *intervalNode) {
	f := func(n *intervalNode) bool {
		if n.iv.Ivl != ivl {
			return true
		}
		ret = n
		return false
	}
	ivt.root.visit(&ivl, f)
	return ret
}
func (ivt *IntervalTree) Find(ivl Interval) (ret *IntervalValue) {
	n := ivt.find(ivl)
	if n == nil {
		return nil
	}
	return &n.iv
}
func (ivt *IntervalTree) Intersects(iv Interval) bool {
	x := ivt.root
	for x != nil && iv.Compare(&x.iv.Ivl) != 0 {
		if x.left != nil && x.left.max.Compare(iv.Begin) > 0 {
			x = x.left
		} else {
			x = x.right
		}
	}
	return x != nil
}
func (ivt *IntervalTree) Contains(ivl Interval) bool {
	var maxEnd, minBegin Comparable

	isContiguous := true
	ivt.Visit(ivl, func(n *IntervalValue) bool {
		if minBegin == nil {
			minBegin = n.Ivl.Begin
			maxEnd = n.Ivl.End
			return true
		}
		if maxEnd.Compare(n.Ivl.Begin) < 0 {
			isContiguous = false
			return false
		}
		if n.Ivl.End.Compare(maxEnd) > 0 {
			maxEnd = n.Ivl.End
		}
		return true
	})

	return isContiguous && minBegin != nil && maxEnd.Compare(ivl.End) >= 0 && minBegin.Compare(ivl.Begin) <= 0
}
func (ivt *IntervalTree) Stab(iv Interval) (ivs []*IntervalValue) {
	if ivt.count == 0 {
		return nil
	}
	f := func(n *IntervalValue) bool { ivs = append(ivs, n); return true }
	ivt.Visit(iv, f)
	return ivs
}
func (ivt *IntervalTree) Union(inIvt IntervalTree, ivl Interval) {
	f := func(n *IntervalValue) bool {
		ivt.Insert(n.Ivl, n.Val)
		return true
	}
	inIvt.Visit(ivl, f)
}
func NewExactReadCloser(rc io.ReadCloser, totalBytes int64) io.ReadCloser {
	return &exactReadCloser{rc: rc, totalBytes: totalBytes}
}
func NewElection(s *Session, pfx string) *Election {
	return &Election{session: s, keyPrefix: pfx + "/"}
}
func ResumeElection(s *Session, pfx string, leaderKey string, leaderRev int64) *Election {
	return &Election{
		keyPrefix:     pfx,
		session:       s,
		leaderKey:     leaderKey,
		leaderRev:     leaderRev,
		leaderSession: s,
	}
}
func (e *Election) Proclaim(ctx context.Context, val string) error {
	if e.leaderSession == nil {
		return ErrElectionNotLeader
	}
	client := e.session.Client()
	cmp := v3.Compare(v3.CreateRevision(e.leaderKey), "=", e.leaderRev)
	txn := client.Txn(ctx).If(cmp)
	txn = txn.Then(v3.OpPut(e.leaderKey, val, v3.WithLease(e.leaderSession.Lease())))
	tresp, terr := txn.Commit()
	if terr != nil {
		return terr
	}
	if !tresp.Succeeded {
		e.leaderKey = ""
		return ErrElectionNotLeader
	}

	e.hdr = tresp.Header
	return nil
}
func (e *Election) Resign(ctx context.Context) (err error) {
	if e.leaderSession == nil {
		return nil
	}
	client := e.session.Client()
	cmp := v3.Compare(v3.CreateRevision(e.leaderKey), "=", e.leaderRev)
	resp, err := client.Txn(ctx).If(cmp).Then(v3.OpDelete(e.leaderKey)).Commit()
	if err == nil {
		e.hdr = resp.Header
	}
	e.leaderKey = ""
	e.leaderSession = nil
	return err
}
func (e *Election) Leader(ctx context.Context) (*v3.GetResponse, error) {
	client := e.session.Client()
	resp, err := client.Get(ctx, e.keyPrefix, v3.WithFirstCreate()...)
	if err != nil {
		return nil, err
	} else if len(resp.Kvs) == 0 {
		// no leader currently elected
		return nil, ErrElectionNoLeader
	}
	return resp, nil
}
func (e *Election) Observe(ctx context.Context) <-chan v3.GetResponse {
	retc := make(chan v3.GetResponse)
	go e.observe(ctx, retc)
	return retc
}
func (qa *quotaAlarmer) check(ctx context.Context, r interface{}) error {
	if qa.q.Available(r) {
		return nil
	}
	req := &pb.AlarmRequest{
		MemberID: uint64(qa.id),
		Action:   pb.AlarmRequest_ACTIVATE,
		Alarm:    pb.AlarmType_NOSPACE,
	}
	qa.a.Alarm(ctx, req)
	return rpctypes.ErrGRPCNoSpace
}
func NewExecWatchCommand() cli.Command {
	return cli.Command{
		Name:      "exec-watch",
		Usage:     "watch a key for changes and exec an executable",
		ArgsUsage: "<key> <command> [args...]",
		Flags: []cli.Flag{
			cli.IntFlag{Name: "after-index", Value: 0, Usage: "watch after the given index"},
			cli.BoolFlag{Name: "recursive, r", Usage: "watch all values for key and child keys"},
		},
		Action: func(c *cli.Context) error {
			execWatchCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func execWatchCommandFunc(c *cli.Context, ki client.KeysAPI) {
	args := c.Args()
	argslen := len(args)

	if argslen < 2 {
		handleError(c, ExitBadArgs, errors.New("key and command to exec required"))
	}

	var (
		key     string
		cmdArgs []string
	)

	foundSep := false
	for i := range args {
		if args[i] == "--" && i != 0 {
			foundSep = true
			break
		}
	}

	if foundSep {
		key = args[0]
		cmdArgs = args[2:]
	} else {
		// If no flag is parsed, the order of key and cmdArgs will be switched and
		// args will not contain `--`.
		key = args[argslen-1]
		cmdArgs = args[:argslen-1]
	}

	index := 0
	if c.Int("after-index") != 0 {
		index = c.Int("after-index")
	}

	recursive := c.Bool("recursive")

	sigch := make(chan os.Signal, 1)
	signal.Notify(sigch, os.Interrupt)

	go func() {
		<-sigch
		os.Exit(0)
	}()

	w := ki.Watcher(key, &client.WatcherOptions{AfterIndex: uint64(index), Recursive: recursive})

	for {
		resp, err := w.Next(context.TODO())
		if err != nil {
			handleError(c, ExitServerError, err)
		}
		if resp.Node.Dir {
			fmt.Fprintf(os.Stderr, "Ignored dir %s change\n", resp.Node.Key)
			continue
		}

		cmd := exec.Command(cmdArgs[0], cmdArgs[1:]...)
		cmd.Env = environResponse(resp, os.Environ())

		cmd.Stdout = os.Stdout
		cmd.Stderr = os.Stderr

		go func() {
			err := cmd.Start()
			if err != nil {
				fmt.Fprintf(os.Stderr, err.Error())
				os.Exit(1)
			}
			cmd.Wait()
		}()
	}
}
func NewListener(u url.URL, tlsinfo *transport.TLSInfo) (net.Listener, error) {
	return transport.NewTimeoutListener(u.Host, u.Scheme, tlsinfo, ConnReadTimeout, ConnWriteTimeout)
}
func NewRoundTripper(tlsInfo transport.TLSInfo, dialTimeout time.Duration) (http.RoundTripper, error) {
	// It uses timeout transport to pair with remote timeout listeners.
	// It sets no read/write timeout, because message in requests may
	// take long time to write out before reading out the response.
	return transport.NewTimeoutTransport(tlsInfo, dialTimeout, 0, 0)
}
func createPostRequest(u url.URL, path string, body io.Reader, ct string, urls types.URLs, from, cid types.ID) *http.Request {
	uu := u
	uu.Path = path
	req, err := http.NewRequest("POST", uu.String(), body)
	if err != nil {
		plog.Panicf("unexpected new request error (%v)", err)
	}
	req.Header.Set("Content-Type", ct)
	req.Header.Set("X-Server-From", from.String())
	req.Header.Set("X-Server-Version", version.Version)
	req.Header.Set("X-Min-Cluster-Version", version.MinClusterVersion)
	req.Header.Set("X-Etcd-Cluster-ID", cid.String())
	setPeerURLsHeader(req, urls)

	return req
}
func checkPostResponse(resp *http.Response, body []byte, req *http.Request, to types.ID) error {
	switch resp.StatusCode {
	case http.StatusPreconditionFailed:
		switch strings.TrimSuffix(string(body), "\n") {
		case errIncompatibleVersion.Error():
			plog.Errorf("request sent was ignored by peer %s (server version incompatible)", to)
			return errIncompatibleVersion
		case errClusterIDMismatch.Error():
			plog.Errorf("request sent was ignored (cluster ID mismatch: remote[%s]=%s, local=%s)",
				to, resp.Header.Get("X-Etcd-Cluster-ID"), req.Header.Get("X-Etcd-Cluster-ID"))
			return errClusterIDMismatch
		default:
			return fmt.Errorf("unhandled error %q when precondition failed", string(body))
		}
	case http.StatusForbidden:
		return errMemberRemoved
	case http.StatusNoContent:
		return nil
	default:
		return fmt.Errorf("unexpected http status %s while posting to %q", http.StatusText(resp.StatusCode), req.URL.String())
	}
}
func serverVersion(h http.Header) *semver.Version {
	verStr := h.Get("X-Server-Version")
	// backward compatibility with etcd 2.0
	if verStr == "" {
		verStr = "2.0.0"
	}
	return semver.Must(semver.NewVersion(verStr))
}
func checkVersionCompatibility(name string, server, minCluster *semver.Version) (
	localServer *semver.Version,
	localMinCluster *semver.Version,
	err error) {
	localServer = semver.Must(semver.NewVersion(version.Version))
	localMinCluster = semver.Must(semver.NewVersion(version.MinClusterVersion))
	if compareMajorMinorVersion(server, localMinCluster) == -1 {
		return localServer, localMinCluster, fmt.Errorf("remote version is too low: remote[%s]=%s, local=%s", name, server, localServer)
	}
	if compareMajorMinorVersion(minCluster, localServer) == 1 {
		return localServer, localMinCluster, fmt.Errorf("local version is too low: remote[%s]=%s, local=%s", name, server, localServer)
	}
	return localServer, localMinCluster, nil
}
func setPeerURLsHeader(req *http.Request, urls types.URLs) {
	if urls == nil {
		// often not set in unit tests
		return
	}
	peerURLs := make([]string, urls.Len())
	for i := range urls {
		peerURLs[i] = urls[i].String()
	}
	req.Header.Set("X-PeerURLs", strings.Join(peerURLs, ","))
}
func addRemoteFromRequest(tr Transporter, r *http.Request) {
	if from, err := types.IDFromString(r.Header.Get("X-Server-From")); err == nil {
		if urls := r.Header.Get("X-PeerURLs"); urls != "" {
			tr.AddRemote(from, strings.Split(urls, ","))
		}
	}
}
func NewKeysAPIWithPrefix(c Client, p string) KeysAPI {
	return &httpKeysAPI{
		client: c,
		prefix: p,
	}
}
func (n *Node) TTLDuration() time.Duration {
	return time.Duration(n.TTL) * time.Second
}
func SetPflagsFromEnv(prefix string, fs *pflag.FlagSet) error {
	var err error
	alreadySet := make(map[string]bool)
	usedEnvKey := make(map[string]bool)
	fs.VisitAll(func(f *pflag.Flag) {
		if f.Changed {
			alreadySet[FlagToEnv(prefix, f.Name)] = true
		}
		if serr := setFlagFromEnv(fs, prefix, f.Name, usedEnvKey, alreadySet, false); serr != nil {
			err = serr
		}
	})
	verifyEnv(prefix, usedEnvKey, alreadySet)
	return err
}
func FlagToEnv(prefix, name string) string {
	return prefix + "_" + strings.ToUpper(strings.Replace(name, "-", "_", -1))
}
func excerpt(str string, pre, suf int) string {
	if pre+suf > len(str) {
		return fmt.Sprintf("%q", str)
	}
	return fmt.Sprintf("%q...%q", str[:pre], str[len(str)-suf:])
}
func passConfChange(entry raftpb.Entry) (bool, string) {
	return entry.Type == raftpb.EntryConfChange, "ConfigChange"
}
func printInternalRaftRequest(entry raftpb.Entry) {
	var rr etcdserverpb.InternalRaftRequest
	if err := rr.Unmarshal(entry.Data); err == nil {
		fmt.Printf("%4d\t%10d\tnorm\t%s", entry.Term, entry.Index, rr.String())
	}
}
func listEntriesType(entrytype string, streamdecoder string, ents []raftpb.Entry) {
	entryFilters := evaluateEntrytypeFlag(entrytype)
	printerMap := map[string]EntryPrinter{"InternalRaftRequest": printInternalRaftRequest,
		"Request":       printRequest,
		"ConfigChange":  printConfChange,
		"UnknownNormal": printUnknownNormal}
	var stderr bytes.Buffer
	args := strings.Split(streamdecoder, " ")
	cmd := exec.Command(args[0], args[1:]...)
	stdin, err := cmd.StdinPipe()
	if err != nil {
		log.Panic(err)
	}
	stdout, err := cmd.StdoutPipe()
	if err != nil {
		log.Panic(err)
	}
	cmd.Stderr = &stderr
	if streamdecoder != "" {
		err = cmd.Start()
		if err != nil {
			log.Panic(err)
		}
	}

	cnt := 0

	for _, e := range ents {
		passed := false
		currtype := ""
		for _, filter := range entryFilters {
			passed, currtype = filter(e)
			if passed {
				cnt++
				break
			}
		}
		if passed {
			printer := printerMap[currtype]
			printer(e)
			if streamdecoder == "" {
				fmt.Println()
				continue
			}

			// if decoder is set, pass the e.Data to stdin and read the stdout from decoder
			io.WriteString(stdin, hex.EncodeToString(e.Data))
			io.WriteString(stdin, "\n")
			outputReader := bufio.NewReader(stdout)
			decoderoutput, currerr := outputReader.ReadString('\n')
			if currerr != nil {
				fmt.Println(currerr)
				return
			}

			decoder_status, decoded_data := parseDecoderOutput(decoderoutput)

			fmt.Printf("\t%s\t%s", decoder_status, decoded_data)
		}
	}

	stdin.Close()
	err = cmd.Wait()
	if streamdecoder != "" {
		if err != nil {
			log.Panic(err)
		}
		if stderr.String() != "" {
			os.Stderr.WriteString("decoder stderr: " + stderr.String())
		}
	}

	fmt.Printf("\nEntry types (%s) count is : %d", entrytype, cnt)
}
func newLog(storage Storage, logger Logger) *raftLog {
	return newLogWithSize(storage, logger, noLimit)
}
func newLogWithSize(storage Storage, logger Logger, maxNextEntsSize uint64) *raftLog {
	if storage == nil {
		log.Panic("storage must not be nil")
	}
	log := &raftLog{
		storage:         storage,
		logger:          logger,
		maxNextEntsSize: maxNextEntsSize,
	}
	firstIndex, err := storage.FirstIndex()
	if err != nil {
		panic(err) // TODO(bdarnell)
	}
	lastIndex, err := storage.LastIndex()
	if err != nil {
		panic(err) // TODO(bdarnell)
	}
	log.unstable.offset = lastIndex + 1
	log.unstable.logger = logger
	// Initialize our committed and applied pointers to the time of the last compaction.
	log.committed = firstIndex - 1
	log.applied = firstIndex - 1

	return log
}
func (l *raftLog) findConflict(ents []pb.Entry) uint64 {
	for _, ne := range ents {
		if !l.matchTerm(ne.Index, ne.Term) {
			if ne.Index <= l.lastIndex() {
				l.logger.Infof("found conflict at index %d [existing term: %d, conflicting term: %d]",
					ne.Index, l.zeroTermOnErrCompacted(l.term(ne.Index)), ne.Term)
			}
			return ne.Index
		}
	}
	return 0
}
func (l *raftLog) nextEnts() (ents []pb.Entry) {
	off := max(l.applied+1, l.firstIndex())
	if l.committed+1 > off {
		ents, err := l.slice(off, l.committed+1, l.maxNextEntsSize)
		if err != nil {
			l.logger.Panicf("unexpected error when getting unapplied entries (%v)", err)
		}
		return ents
	}
	return nil
}
func (l *raftLog) allEntries() []pb.Entry {
	ents, err := l.entries(l.firstIndex(), noLimit)
	if err == nil {
		return ents
	}
	if err == ErrCompacted { // try again if there was a racing compaction
		return l.allEntries()
	}
	// TODO (xiangli): handle error?
	panic(err)
}
func (l *raftLog) slice(lo, hi, maxSize uint64) ([]pb.Entry, error) {
	err := l.mustCheckOutOfBounds(lo, hi)
	if err != nil {
		return nil, err
	}
	if lo == hi {
		return nil, nil
	}
	var ents []pb.Entry
	if lo < l.unstable.offset {
		storedEnts, err := l.storage.Entries(lo, min(hi, l.unstable.offset), maxSize)
		if err == ErrCompacted {
			return nil, err
		} else if err == ErrUnavailable {
			l.logger.Panicf("entries[%d:%d) is unavailable from storage", lo, min(hi, l.unstable.offset))
		} else if err != nil {
			panic(err) // TODO(bdarnell)
		}

		// check if ents has reached the size limitation
		if uint64(len(storedEnts)) < min(hi, l.unstable.offset)-lo {
			return storedEnts, nil
		}

		ents = storedEnts
	}
	if hi > l.unstable.offset {
		unstable := l.unstable.slice(max(lo, l.unstable.offset), hi)
		if len(ents) > 0 {
			combined := make([]pb.Entry, len(ents)+len(unstable))
			n := copy(combined, ents)
			copy(combined[n:], unstable)
			ents = combined
		} else {
			ents = unstable
		}
	}
	return limitSize(ents, maxSize), nil
}
func NewSession(client *v3.Client, opts ...SessionOption) (*Session, error) {
	ops := &sessionOptions{ttl: defaultSessionTTL, ctx: client.Ctx()}
	for _, opt := range opts {
		opt(ops)
	}

	id := ops.leaseID
	if id == v3.NoLease {
		resp, err := client.Grant(ops.ctx, int64(ops.ttl))
		if err != nil {
			return nil, err
		}
		id = v3.LeaseID(resp.ID)
	}

	ctx, cancel := context.WithCancel(ops.ctx)
	keepAlive, err := client.KeepAlive(ctx, id)
	if err != nil || keepAlive == nil {
		cancel()
		return nil, err
	}

	donec := make(chan struct{})
	s := &Session{client: client, opts: ops, id: id, cancel: cancel, donec: donec}

	// keep the lease alive until client error or cancelled context
	go func() {
		defer close(donec)
		for range keepAlive {
			// eat messages until keep alive channel closes
		}
	}()

	return s, nil
}
func (s *Session) Close() error {
	s.Orphan()
	// if revoke takes longer than the ttl, lease is expired anyway
	ctx, cancel := context.WithTimeout(s.opts.ctx, time.Duration(s.opts.ttl)*time.Second)
	_, err := s.client.Revoke(ctx, s.id)
	cancel()
	return err
}
func WithTTL(ttl int) SessionOption {
	return func(so *sessionOptions) {
		if ttl > 0 {
			so.ttl = ttl
		}
	}
}
func WithLease(leaseID v3.LeaseID) SessionOption {
	return func(so *sessionOptions) {
		so.leaseID = leaseID
	}
}
func (ro *readOnly) addRequest(index uint64, m pb.Message) {
	ctx := string(m.Entries[0].Data)
	if _, ok := ro.pendingReadIndex[ctx]; ok {
		return
	}
	ro.pendingReadIndex[ctx] = &readIndexStatus{index: index, req: m, acks: make(map[uint64]struct{})}
	ro.readIndexQueue = append(ro.readIndexQueue, ctx)
}
func (ro *readOnly) recvAck(m pb.Message) int {
	rs, ok := ro.pendingReadIndex[string(m.Context)]
	if !ok {
		return 0
	}

	rs.acks[m.From] = struct{}{}
	// add one to include an ack from local node
	return len(rs.acks) + 1
}
func (ro *readOnly) advance(m pb.Message) []*readIndexStatus {
	var (
		i     int
		found bool
	)

	ctx := string(m.Context)
	rss := []*readIndexStatus{}

	for _, okctx := range ro.readIndexQueue {
		i++
		rs, ok := ro.pendingReadIndex[okctx]
		if !ok {
			panic("cannot find corresponding read state from pending map")
		}
		rss = append(rss, rs)
		if okctx == ctx {
			found = true
			break
		}
	}

	if found {
		ro.readIndexQueue = ro.readIndexQueue[i:]
		for _, rs := range rss {
			delete(ro.pendingReadIndex, string(rs.req.Entries[0].Data))
		}
		return rss
	}

	return nil
}
func (ro *readOnly) lastPendingRequestCtx() string {
	if len(ro.readIndexQueue) == 0 {
		return ""
	}
	return ro.readIndexQueue[len(ro.readIndexQueue)-1]
}
func (s *EtcdServer) Start() {
	s.start()
	s.goAttach(func() { s.adjustTicks() })
	s.goAttach(func() { s.publish(s.Cfg.ReqTimeout()) })
	s.goAttach(s.purgeFile)
	s.goAttach(func() { monitorFileDescriptor(s.getLogger(), s.stopping) })
	s.goAttach(s.monitorVersions)
	s.goAttach(s.linearizableReadLoop)
	s.goAttach(s.monitorKVHash)
}
func (s *EtcdServer) start() {
	lg := s.getLogger()

	if s.Cfg.SnapshotCount == 0 {
		if lg != nil {
			lg.Info(
				"updating snapshot-count to default",
				zap.Uint64("given-snapshot-count", s.Cfg.SnapshotCount),
				zap.Uint64("updated-snapshot-count", DefaultSnapshotCount),
			)
		} else {
			plog.Infof("set snapshot count to default %d", DefaultSnapshotCount)
		}
		s.Cfg.SnapshotCount = DefaultSnapshotCount
	}
	if s.Cfg.SnapshotCatchUpEntries == 0 {
		if lg != nil {
			lg.Info(
				"updating snapshot catch-up entries to default",
				zap.Uint64("given-snapshot-catchup-entries", s.Cfg.SnapshotCatchUpEntries),
				zap.Uint64("updated-snapshot-catchup-entries", DefaultSnapshotCatchUpEntries),
			)
		}
		s.Cfg.SnapshotCatchUpEntries = DefaultSnapshotCatchUpEntries
	}

	s.w = wait.New()
	s.applyWait = wait.NewTimeList()
	s.done = make(chan struct{})
	s.stop = make(chan struct{})
	s.stopping = make(chan struct{})
	s.ctx, s.cancel = context.WithCancel(context.Background())
	s.readwaitc = make(chan struct{}, 1)
	s.readNotifier = newNotifier()
	s.leaderChanged = make(chan struct{})
	if s.ClusterVersion() != nil {
		if lg != nil {
			lg.Info(
				"starting etcd server",
				zap.String("local-member-id", s.ID().String()),
				zap.String("local-server-version", version.Version),
				zap.String("cluster-id", s.Cluster().ID().String()),
				zap.String("cluster-version", version.Cluster(s.ClusterVersion().String())),
			)
		} else {
			plog.Infof("starting server... [version: %v, cluster version: %v]", version.Version, version.Cluster(s.ClusterVersion().String()))
		}
		membership.ClusterVersionMetrics.With(prometheus.Labels{"cluster_version": s.ClusterVersion().String()}).Set(1)
	} else {
		if lg != nil {
			lg.Info(
				"starting etcd server",
				zap.String("local-member-id", s.ID().String()),
				zap.String("local-server-version", version.Version),
				zap.String("cluster-version", "to_be_decided"),
			)
		} else {
			plog.Infof("starting server... [version: %v, cluster version: to_be_decided]", version.Version)
		}
	}

	// TODO: if this is an empty log, writes all peer infos
	// into the first entry
	go s.run()
}
func (s *EtcdServer) Process(ctx context.Context, m raftpb.Message) error {
	if s.cluster.IsIDRemoved(types.ID(m.From)) {
		if lg := s.getLogger(); lg != nil {
			lg.Warn(
				"rejected Raft message from removed member",
				zap.String("local-member-id", s.ID().String()),
				zap.String("removed-member-id", types.ID(m.From).String()),
			)
		} else {
			plog.Warningf("reject message from removed member %s", types.ID(m.From).String())
		}
		return httptypes.NewHTTPError(http.StatusForbidden, "cannot process message from removed member")
	}
	if m.Type == raftpb.MsgApp {
		s.stats.RecvAppendReq(types.ID(m.From).String(), m.Size())
	}
	return s.r.Step(ctx, m)
}
func (s *EtcdServer) ReportSnapshot(id uint64, status raft.SnapshotStatus) {
	s.r.ReportSnapshot(id, status)
}
func (s *EtcdServer) MoveLeader(ctx context.Context, lead, transferee uint64) error {
	now := time.Now()
	interval := time.Duration(s.Cfg.TickMs) * time.Millisecond

	if lg := s.getLogger(); lg != nil {
		lg.Info(
			"leadership transfer starting",
			zap.String("local-member-id", s.ID().String()),
			zap.String("current-leader-member-id", types.ID(lead).String()),
			zap.String("transferee-member-id", types.ID(transferee).String()),
		)
	} else {
		plog.Infof("%s starts leadership transfer from %s to %s", s.ID(), types.ID(lead), types.ID(transferee))
	}

	s.r.TransferLeadership(ctx, lead, transferee)
	for s.Lead() != transferee {
		select {
		case <-ctx.Done(): // time out
			return ErrTimeoutLeaderTransfer
		case <-time.After(interval):
		}
	}

	// TODO: drain all requests, or drop all messages to the old leader
	if lg := s.getLogger(); lg != nil {
		lg.Info(
			"leadership transfer finished",
			zap.String("local-member-id", s.ID().String()),
			zap.String("old-leader-member-id", types.ID(lead).String()),
			zap.String("new-leader-member-id", types.ID(transferee).String()),
			zap.Duration("took", time.Since(now)),
		)
	} else {
		plog.Infof("%s finished leadership transfer from %s to %s (took %v)", s.ID(), types.ID(lead), types.ID(transferee), time.Since(now))
	}
	return nil
}
func (s *EtcdServer) TransferLeadership() error {
	if !s.isLeader() {
		if lg := s.getLogger(); lg != nil {
			lg.Info(
				"skipped leadership transfer; local server is not leader",
				zap.String("local-member-id", s.ID().String()),
				zap.String("current-leader-member-id", types.ID(s.Lead()).String()),
			)
		} else {
			plog.Printf("skipped leadership transfer for stopping non-leader member")
		}
		return nil
	}

	if !s.isMultiNode() {
		if lg := s.getLogger(); lg != nil {
			lg.Info(
				"skipped leadership transfer; it's a single-node cluster",
				zap.String("local-member-id", s.ID().String()),
				zap.String("current-leader-member-id", types.ID(s.Lead()).String()),
			)
		} else {
			plog.Printf("skipped leadership transfer for single member cluster")
		}
		return nil
	}

	transferee, ok := longestConnected(s.r.transport, s.cluster.MemberIDs())
	if !ok {
		return ErrUnhealthy
	}

	tm := s.Cfg.ReqTimeout()
	ctx, cancel := context.WithTimeout(s.ctx, tm)
	err := s.MoveLeader(ctx, s.Lead(), uint64(transferee))
	cancel()
	return err
}
func (s *EtcdServer) configure(ctx context.Context, cc raftpb.ConfChange) ([]*membership.Member, error) {
	cc.ID = s.reqIDGen.Next()
	ch := s.w.Register(cc.ID)

	start := time.Now()
	if err := s.r.ProposeConfChange(ctx, cc); err != nil {
		s.w.Trigger(cc.ID, nil)
		return nil, err
	}

	select {
	case x := <-ch:
		if x == nil {
			if lg := s.getLogger(); lg != nil {
				lg.Panic("failed to configure")
			} else {
				plog.Panicf("configure trigger value should never be nil")
			}
		}
		resp := x.(*confChangeResponse)
		if lg := s.getLogger(); lg != nil {
			lg.Info(
				"applied a configuration change through raft",
				zap.String("local-member-id", s.ID().String()),
				zap.String("raft-conf-change", cc.Type.String()),
				zap.String("raft-conf-change-node-id", types.ID(cc.NodeID).String()),
			)
		}
		return resp.membs, resp.err

	case <-ctx.Done():
		s.w.Trigger(cc.ID, nil) // GC wait
		return nil, s.parseProposeCtxErr(ctx.Err(), start)

	case <-s.stopping:
		return nil, ErrStopped
	}
}
func (s *EtcdServer) sync(timeout time.Duration) {
	req := pb.Request{
		Method: "SYNC",
		ID:     s.reqIDGen.Next(),
		Time:   time.Now().UnixNano(),
	}
	data := pbutil.MustMarshal(&req)
	// There is no promise that node has leader when do SYNC request,
	// so it uses goroutine to propose.
	ctx, cancel := context.WithTimeout(s.ctx, timeout)
	s.goAttach(func() {
		s.r.Propose(ctx, data)
		cancel()
	})
}
func (s *EtcdServer) publish(timeout time.Duration) {
	b, err := json.Marshal(s.attributes)
	if err != nil {
		if lg := s.getLogger(); lg != nil {
			lg.Panic("failed to marshal JSON", zap.Error(err))
		} else {
			plog.Panicf("json marshal error: %v", err)
		}
		return
	}
	req := pb.Request{
		Method: "PUT",
		Path:   membership.MemberAttributesStorePath(s.id),
		Val:    string(b),
	}

	for {
		ctx, cancel := context.WithTimeout(s.ctx, timeout)
		_, err := s.Do(ctx, req)
		cancel()
		switch err {
		case nil:
			close(s.readych)
			if lg := s.getLogger(); lg != nil {
				lg.Info(
					"published local member to cluster through raft",
					zap.String("local-member-id", s.ID().String()),
					zap.String("local-member-attributes", fmt.Sprintf("%+v", s.attributes)),
					zap.String("request-path", req.Path),
					zap.String("cluster-id", s.cluster.ID().String()),
					zap.Duration("publish-timeout", timeout),
				)
			} else {
				plog.Infof("published %+v to cluster %s", s.attributes, s.cluster.ID())
			}
			return

		case ErrStopped:
			if lg := s.getLogger(); lg != nil {
				lg.Warn(
					"stopped publish because server is stopped",
					zap.String("local-member-id", s.ID().String()),
					zap.String("local-member-attributes", fmt.Sprintf("%+v", s.attributes)),
					zap.Duration("publish-timeout", timeout),
					zap.Error(err),
				)
			} else {
				plog.Infof("aborting publish because server is stopped")
			}
			return

		default:
			if lg := s.getLogger(); lg != nil {
				lg.Warn(
					"failed to publish local member to cluster through raft",
					zap.String("local-member-id", s.ID().String()),
					zap.String("local-member-attributes", fmt.Sprintf("%+v", s.attributes)),
					zap.String("request-path", req.Path),
					zap.Duration("publish-timeout", timeout),
					zap.Error(err),
				)
			} else {
				plog.Errorf("publish error: %v", err)
			}
		}
	}
}
func (s *EtcdServer) applyEntryNormal(e *raftpb.Entry) {
	shouldApplyV3 := false
	if e.Index > s.consistIndex.ConsistentIndex() {
		// set the consistent index of current executing entry
		s.consistIndex.setConsistentIndex(e.Index)
		shouldApplyV3 = true
	}

	// raft state machine may generate noop entry when leader confirmation.
	// skip it in advance to avoid some potential bug in the future
	if len(e.Data) == 0 {
		select {
		case s.forceVersionC <- struct{}{}:
		default:
		}
		// promote lessor when the local member is leader and finished
		// applying all entries from the last term.
		if s.isLeader() {
			s.lessor.Promote(s.Cfg.electionTimeout())
		}
		return
	}

	var raftReq pb.InternalRaftRequest
	if !pbutil.MaybeUnmarshal(&raftReq, e.Data) { // backward compatible
		var r pb.Request
		rp := &r
		pbutil.MustUnmarshal(rp, e.Data)
		s.w.Trigger(r.ID, s.applyV2Request((*RequestV2)(rp)))
		return
	}
	if raftReq.V2 != nil {
		req := (*RequestV2)(raftReq.V2)
		s.w.Trigger(req.ID, s.applyV2Request(req))
		return
	}

	// do not re-apply applied entries.
	if !shouldApplyV3 {
		return
	}

	id := raftReq.ID
	if id == 0 {
		id = raftReq.Header.ID
	}

	var ar *applyResult
	needResult := s.w.IsRegistered(id)
	if needResult || !noSideEffect(&raftReq) {
		if !needResult && raftReq.Txn != nil {
			removeNeedlessRangeReqs(raftReq.Txn)
		}
		ar = s.applyV3.Apply(&raftReq)
	}

	if ar == nil {
		return
	}

	if ar.err != ErrNoSpace || len(s.alarmStore.Get(pb.AlarmType_NOSPACE)) > 0 {
		s.w.Trigger(id, ar)
		return
	}

	if lg := s.getLogger(); lg != nil {
		lg.Warn(
			"message exceeded backend quota; raising alarm",
			zap.Int64("quota-size-bytes", s.Cfg.QuotaBackendBytes),
			zap.String("quota-size", humanize.Bytes(uint64(s.Cfg.QuotaBackendBytes))),
			zap.Error(ar.err),
		)
	} else {
		plog.Errorf("applying raft message exceeded backend quota")
	}

	s.goAttach(func() {
		a := &pb.AlarmRequest{
			MemberID: uint64(s.ID()),
			Action:   pb.AlarmRequest_ACTIVATE,
			Alarm:    pb.AlarmType_NOSPACE,
		}
		s.raftRequest(s.ctx, pb.InternalRaftRequest{Alarm: a})
		s.w.Trigger(id, ar)
	})
}
func (s *EtcdServer) applyConfChange(cc raftpb.ConfChange, confState *raftpb.ConfState) (bool, error) {
	if err := s.cluster.ValidateConfigurationChange(cc); err != nil {
		cc.NodeID = raft.None
		s.r.ApplyConfChange(cc)
		return false, err
	}

	lg := s.getLogger()
	*confState = *s.r.ApplyConfChange(cc)
	switch cc.Type {
	case raftpb.ConfChangeAddNode:
		m := new(membership.Member)
		if err := json.Unmarshal(cc.Context, m); err != nil {
			if lg != nil {
				lg.Panic("failed to unmarshal member", zap.Error(err))
			} else {
				plog.Panicf("unmarshal member should never fail: %v", err)
			}
		}
		if cc.NodeID != uint64(m.ID) {
			if lg != nil {
				lg.Panic(
					"got different member ID",
					zap.String("member-id-from-config-change-entry", types.ID(cc.NodeID).String()),
					zap.String("member-id-from-message", m.ID.String()),
				)
			} else {
				plog.Panicf("nodeID should always be equal to member ID")
			}
		}
		s.cluster.AddMember(m)
		if m.ID != s.id {
			s.r.transport.AddPeer(m.ID, m.PeerURLs)
		}

	case raftpb.ConfChangeRemoveNode:
		id := types.ID(cc.NodeID)
		s.cluster.RemoveMember(id)
		if id == s.id {
			return true, nil
		}
		s.r.transport.RemovePeer(id)

	case raftpb.ConfChangeUpdateNode:
		m := new(membership.Member)
		if err := json.Unmarshal(cc.Context, m); err != nil {
			if lg != nil {
				lg.Panic("failed to unmarshal member", zap.Error(err))
			} else {
				plog.Panicf("unmarshal member should never fail: %v", err)
			}
		}
		if cc.NodeID != uint64(m.ID) {
			if lg != nil {
				lg.Panic(
					"got different member ID",
					zap.String("member-id-from-config-change-entry", types.ID(cc.NodeID).String()),
					zap.String("member-id-from-message", m.ID.String()),
				)
			} else {
				plog.Panicf("nodeID should always be equal to member ID")
			}
		}
		s.cluster.UpdateRaftAttributes(m.ID, m.RaftAttributes)
		if m.ID != s.id {
			s.r.transport.UpdatePeer(m.ID, m.PeerURLs)
		}
	}
	return false, nil
}
func (s *EtcdServer) monitorVersions() {
	for {
		select {
		case <-s.forceVersionC:
		case <-time.After(monitorVersionInterval):
		case <-s.stopping:
			return
		}

		if s.Leader() != s.ID() {
			continue
		}

		v := decideClusterVersion(s.getLogger(), getVersions(s.getLogger(), s.cluster, s.id, s.peerRt))
		if v != nil {
			// only keep major.minor version for comparison
			v = &semver.Version{
				Major: v.Major,
				Minor: v.Minor,
			}
		}

		// if the current version is nil:
		// 1. use the decided version if possible
		// 2. or use the min cluster version
		if s.cluster.Version() == nil {
			verStr := version.MinClusterVersion
			if v != nil {
				verStr = v.String()
			}
			s.goAttach(func() { s.updateClusterVersion(verStr) })
			continue
		}

		// update cluster version only if the decided version is greater than
		// the current cluster version
		if v != nil && s.cluster.Version().LessThan(*v) {
			s.goAttach(func() { s.updateClusterVersion(v.String()) })
		}
	}
}
func (s *EtcdServer) goAttach(f func()) {
	s.wgMu.RLock() // this blocks with ongoing close(s.stopping)
	defer s.wgMu.RUnlock()
	select {
	case <-s.stopping:
		if lg := s.getLogger(); lg != nil {
			lg.Warn("server has stopped; skipping goAttach")
		} else {
			plog.Warning("server has stopped (skipping goAttach)")
		}
		return
	default:
	}

	// now safe to add since waitgroup wait has not started yet
	s.wg.Add(1)
	go func() {
		defer s.wg.Done()
		f()
	}()
}
func NewRoundrobinBalanced(
	lg *zap.Logger,
	scs []balancer.SubConn,
	addrToSc map[resolver.Address]balancer.SubConn,
	scToAddr map[balancer.SubConn]resolver.Address,
) Picker {
	return &rrBalanced{
		lg:       lg,
		scs:      scs,
		addrToSc: addrToSc,
		scToAddr: scToAddr,
	}
}
func (rb *rrBalanced) Pick(ctx context.Context, opts balancer.PickOptions) (balancer.SubConn, func(balancer.DoneInfo), error) {
	rb.mu.RLock()
	n := len(rb.scs)
	rb.mu.RUnlock()
	if n == 0 {
		return nil, nil, balancer.ErrNoSubConnAvailable
	}

	rb.mu.Lock()
	cur := rb.next
	sc := rb.scs[cur]
	picked := rb.scToAddr[sc].Addr
	rb.next = (rb.next + 1) % len(rb.scs)
	rb.mu.Unlock()

	rb.lg.Debug(
		"picked",
		zap.String("address", picked),
		zap.Int("subconn-index", cur),
		zap.Int("subconn-size", n),
	)

	doneFunc := func(info balancer.DoneInfo) {
		// TODO: error handling?
		fss := []zapcore.Field{
			zap.Error(info.Err),
			zap.String("address", picked),
			zap.Bool("success", info.Err == nil),
			zap.Bool("bytes-sent", info.BytesSent),
			zap.Bool("bytes-received", info.BytesReceived),
		}
		if info.Err == nil {
			rb.lg.Debug("balancer done", fss...)
		} else {
			rb.lg.Warn("balancer failed", fss...)
		}
	}
	return sc, doneFunc, nil
}
func NewTLSListener(l net.Listener, tlsinfo *TLSInfo) (net.Listener, error) {
	check := func(context.Context, *tls.Conn) error { return nil }
	return newTLSListener(l, tlsinfo, check)
}
func (l *tlsListener) acceptLoop() {
	var wg sync.WaitGroup
	var pendingMu sync.Mutex

	pending := make(map[net.Conn]struct{})
	ctx, cancel := context.WithCancel(context.Background())
	defer func() {
		cancel()
		pendingMu.Lock()
		for c := range pending {
			c.Close()
		}
		pendingMu.Unlock()
		wg.Wait()
		close(l.donec)
	}()

	for {
		conn, err := l.Listener.Accept()
		if err != nil {
			l.err = err
			return
		}

		pendingMu.Lock()
		pending[conn] = struct{}{}
		pendingMu.Unlock()

		wg.Add(1)
		go func() {
			defer func() {
				if conn != nil {
					conn.Close()
				}
				wg.Done()
			}()

			tlsConn := conn.(*tls.Conn)
			herr := tlsConn.Handshake()
			pendingMu.Lock()
			delete(pending, conn)
			pendingMu.Unlock()

			if herr != nil {
				l.handshakeFailure(tlsConn, herr)
				return
			}
			if err := l.check(ctx, tlsConn); err != nil {
				l.handshakeFailure(tlsConn, err)
				return
			}

			select {
			case l.connc <- tlsConn:
				conn = nil
			case <-ctx.Done():
			}
		}()
	}
}
func (e *ResolverGroup) SetEndpoints(endpoints []string) {
	addrs := epsToAddrs(endpoints...)
	e.mu.Lock()
	e.endpoints = endpoints
	for _, r := range e.resolvers {
		r.cc.NewAddress(addrs)
	}
	e.mu.Unlock()
}
func (e *ResolverGroup) Target(endpoint string) string {
	return Target(e.id, endpoint)
}
func Target(id, endpoint string) string {
	return fmt.Sprintf("%s://%s/%s", scheme, id, endpoint)
}
func (b *builder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOption) (resolver.Resolver, error) {
	if len(target.Authority) < 1 {
		return nil, fmt.Errorf("'etcd' target scheme requires non-empty authority identifying etcd cluster being routed to")
	}
	id := target.Authority
	es, err := b.getResolverGroup(id)
	if err != nil {
		return nil, fmt.Errorf("failed to build resolver: %v", err)
	}
	r := &Resolver{
		endpointID: id,
		cc:         cc,
	}
	es.addResolver(r)
	return r, nil
}
func (r *RequestV2) Handle(ctx context.Context, v2api RequestV2Handler) (Response, error) {
	if r.Method == "GET" && r.Quorum {
		r.Method = "QGET"
	}
	switch r.Method {
	case "POST":
		return v2api.Post(ctx, r)
	case "PUT":
		return v2api.Put(ctx, r)
	case "DELETE":
		return v2api.Delete(ctx, r)
	case "QGET":
		return v2api.QGet(ctx, r)
	case "GET":
		return v2api.Get(ctx, r)
	case "HEAD":
		return v2api.Head(ctx, r)
	}
	return Response{}, ErrUnknownMethod
}
func NewElectionCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "election [election name (defaults to 'elector')]",
		Short: "Performs election operation",
		Run:   runElectionFunc,
	}
	cmd.Flags().IntVar(&totalClientConnections, "total-client-connections", 10, "total number of client connections")
	return cmd
}
func nodeToMember(n *v2store.NodeExtern) (*Member, error) {
	m := &Member{ID: MustParseMemberIDFromKey(n.Key)}
	attrs := make(map[string][]byte)
	raftAttrKey := path.Join(n.Key, raftAttributesSuffix)
	attrKey := path.Join(n.Key, attributesSuffix)
	for _, nn := range n.Nodes {
		if nn.Key != raftAttrKey && nn.Key != attrKey {
			return nil, fmt.Errorf("unknown key %q", nn.Key)
		}
		attrs[nn.Key] = []byte(*nn.Value)
	}
	if data := attrs[raftAttrKey]; data != nil {
		if err := json.Unmarshal(data, &m.RaftAttributes); err != nil {
			return nil, fmt.Errorf("unmarshal raftAttributes error: %v", err)
		}
	} else {
		return nil, fmt.Errorf("raftAttributes key doesn't exist")
	}
	if data := attrs[attrKey]; data != nil {
		if err := json.Unmarshal(data, &m.Attributes); err != nil {
			return m, fmt.Errorf("unmarshal attributes error: %v", err)
		}
	}
	return m, nil
}
func NewTmpBackend(batchInterval time.Duration, batchLimit int) (*backend, string) {
	dir, err := ioutil.TempDir(os.TempDir(), "etcd_backend_test")
	if err != nil {
		panic(err)
	}
	tmpPath := filepath.Join(dir, "database")
	bcfg := DefaultBackendConfig()
	bcfg.Path, bcfg.BatchInterval, bcfg.BatchLimit = tmpPath, batchInterval, batchLimit
	return newBackend(bcfg), tmpPath
}
func newRevision(lg *zap.Logger, clock clockwork.Clock, retention int64, rg RevGetter, c Compactable) *Revision {
	rc := &Revision{
		lg:        lg,
		clock:     clock,
		retention: retention,
		rg:        rg,
		c:         c,
	}
	rc.ctx, rc.cancel = context.WithCancel(context.Background())
	return rc
}
func (rc *Revision) Run() {
	prev := int64(0)
	go func() {
		for {
			select {
			case <-rc.ctx.Done():
				return
			case <-rc.clock.After(revInterval):
				rc.mu.Lock()
				p := rc.paused
				rc.mu.Unlock()
				if p {
					continue
				}
			}

			rev := rc.rg.Rev() - rc.retention
			if rev <= 0 || rev == prev {
				continue
			}

			now := time.Now()
			if rc.lg != nil {
				rc.lg.Info(
					"starting auto revision compaction",
					zap.Int64("revision", rev),
					zap.Int64("revision-compaction-retention", rc.retention),
				)
			} else {
				plog.Noticef("Starting auto-compaction at revision %d (retention: %d revisions)", rev, rc.retention)
			}
			_, err := rc.c.Compact(rc.ctx, &pb.CompactionRequest{Revision: rev})
			if err == nil || err == mvcc.ErrCompacted {
				prev = rev
				if rc.lg != nil {
					rc.lg.Info(
						"completed auto revision compaction",
						zap.Int64("revision", rev),
						zap.Int64("revision-compaction-retention", rc.retention),
						zap.Duration("took", time.Since(now)),
					)
				} else {
					plog.Noticef("Finished auto-compaction at revision %d", rev)
				}
			} else {
				if rc.lg != nil {
					rc.lg.Warn(
						"failed auto revision compaction",
						zap.Int64("revision", rev),
						zap.Int64("revision-compaction-retention", rc.retention),
						zap.Duration("retry-interval", revInterval),
						zap.Error(err),
					)
				} else {
					plog.Noticef("Failed auto-compaction at revision %d (%v)", rev, err)
					plog.Noticef("Retry after %v", revInterval)
				}
			}
		}
	}()
}
func (rc *Revision) Pause() {
	rc.mu.Lock()
	rc.paused = true
	rc.mu.Unlock()
}
func (rc *Revision) Resume() {
	rc.mu.Lock()
	rc.paused = false
	rc.mu.Unlock()
}
func voteRespMsgType(msgt pb.MessageType) pb.MessageType {
	switch msgt {
	case pb.MsgVote:
		return pb.MsgVoteResp
	case pb.MsgPreVote:
		return pb.MsgPreVoteResp
	default:
		panic(fmt.Sprintf("not a vote message: %s", msgt))
	}
}
func DescribeMessage(m pb.Message, f EntryFormatter) string {
	var buf bytes.Buffer
	fmt.Fprintf(&buf, "%x->%x %v Term:%d Log:%d/%d", m.From, m.To, m.Type, m.Term, m.LogTerm, m.Index)
	if m.Reject {
		fmt.Fprintf(&buf, " Rejected (Hint: %d)", m.RejectHint)
	}
	if m.Commit != 0 {
		fmt.Fprintf(&buf, " Commit:%d", m.Commit)
	}
	if len(m.Entries) > 0 {
		fmt.Fprintf(&buf, " Entries:[")
		for i, e := range m.Entries {
			if i != 0 {
				buf.WriteString(", ")
			}
			buf.WriteString(DescribeEntry(e, f))
		}
		fmt.Fprintf(&buf, "]")
	}
	if !IsEmptySnap(m.Snapshot) {
		fmt.Fprintf(&buf, " Snapshot:%v", m.Snapshot)
	}
	return buf.String()
}
func DescribeEntry(e pb.Entry, f EntryFormatter) string {
	var formatted string
	if e.Type == pb.EntryNormal && f != nil {
		formatted = f(e.Data)
	} else {
		formatted = fmt.Sprintf("%q", e.Data)
	}
	return fmt.Sprintf("%d/%d %s %s", e.Term, e.Index, e.Type, formatted)
}
func DescribeEntries(ents []pb.Entry, f EntryFormatter) string {
	var buf bytes.Buffer
	for _, e := range ents {
		_, _ = buf.WriteString(DescribeEntry(e, f) + "\n")
	}
	return buf.String()
}
func SetLogger(l grpclog.LoggerV2) {
	lgMu.Lock()
	lg = logutil.NewLogger(l)
	// override grpclog so that any changes happen with locking
	grpclog.SetLoggerV2(lg)
	lgMu.Unlock()
}
func GetLogger() logutil.Logger {
	lgMu.RLock()
	l := lg
	lgMu.RUnlock()
	return l
}
func (u *unstable) maybeFirstIndex() (uint64, bool) {
	if u.snapshot != nil {
		return u.snapshot.Metadata.Index + 1, true
	}
	return 0, false
}
func (u *unstable) maybeLastIndex() (uint64, bool) {
	if l := len(u.entries); l != 0 {
		return u.offset + uint64(l) - 1, true
	}
	if u.snapshot != nil {
		return u.snapshot.Metadata.Index, true
	}
	return 0, false
}
func (u *unstable) maybeTerm(i uint64) (uint64, bool) {
	if i < u.offset {
		if u.snapshot == nil {
			return 0, false
		}
		if u.snapshot.Metadata.Index == i {
			return u.snapshot.Metadata.Term, true
		}
		return 0, false
	}

	last, ok := u.maybeLastIndex()
	if !ok {
		return 0, false
	}
	if i > last {
		return 0, false
	}
	return u.entries[i-u.offset].Term, true
}
func (u *unstable) shrinkEntriesArray() {
	// We replace the array if we're using less than half of the space in
	// it. This number is fairly arbitrary, chosen as an attempt to balance
	// memory usage vs number of allocations. It could probably be improved
	// with some focused tuning.
	const lenMultiple = 2
	if len(u.entries) == 0 {
		u.entries = nil
	} else if len(u.entries)*lenMultiple < cap(u.entries) {
		newEntries := make([]pb.Entry, len(u.entries))
		copy(newEntries, u.entries)
		u.entries = newEntries
	}
}
func (st *storage) SaveSnap(snap raftpb.Snapshot) error {
	walsnap := walpb.Snapshot{
		Index: snap.Metadata.Index,
		Term:  snap.Metadata.Term,
	}
	err := st.WAL.SaveSnapshot(walsnap)
	if err != nil {
		return err
	}
	err = st.Snapshotter.SaveSnap(snap)
	if err != nil {
		return err
	}
	return st.WAL.ReleaseLockTo(snap.Metadata.Index)
}
func New(cfg Config) (*Client, error) {
	if len(cfg.Endpoints) == 0 {
		return nil, ErrNoAvailableEndpoints
	}

	return newClient(&cfg)
}
func NewCtxClient(ctx context.Context) *Client {
	cctx, cancel := context.WithCancel(ctx)
	return &Client{ctx: cctx, cancel: cancel}
}
func NewFromURL(url string) (*Client, error) {
	return New(Config{Endpoints: []string{url}})
}
func (c *Client) Close() error {
	c.cancel()
	c.Watcher.Close()
	c.Lease.Close()
	if c.resolverGroup != nil {
		c.resolverGroup.Close()
	}
	if c.conn != nil {
		return toErr(c.ctx, c.conn.Close())
	}
	return c.ctx.Err()
}
func (c *Client) Endpoints() []string {
	// copy the slice; protect original endpoints from being changed
	c.mu.RLock()
	defer c.mu.RUnlock()
	eps := make([]string, len(c.cfg.Endpoints))
	copy(eps, c.cfg.Endpoints)
	return eps
}
func (c *Client) SetEndpoints(eps ...string) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.cfg.Endpoints = eps
	c.resolverGroup.SetEndpoints(eps)
}
func (c *Client) Sync(ctx context.Context) error {
	mresp, err := c.MemberList(ctx)
	if err != nil {
		return err
	}
	var eps []string
	for _, m := range mresp.Members {
		eps = append(eps, m.ClientURLs...)
	}
	c.SetEndpoints(eps...)
	return nil
}
func (c *Client) dialSetupOpts(creds *credentials.TransportCredentials, dopts ...grpc.DialOption) (opts []grpc.DialOption, err error) {
	if c.cfg.DialKeepAliveTime > 0 {
		params := keepalive.ClientParameters{
			Time:                c.cfg.DialKeepAliveTime,
			Timeout:             c.cfg.DialKeepAliveTimeout,
			PermitWithoutStream: c.cfg.PermitWithoutStream,
		}
		opts = append(opts, grpc.WithKeepaliveParams(params))
	}
	opts = append(opts, dopts...)

	// Provide a net dialer that supports cancelation and timeout.
	f := func(dialEp string, t time.Duration) (net.Conn, error) {
		proto, host, _ := endpoint.ParseEndpoint(dialEp)
		select {
		case <-c.ctx.Done():
			return nil, c.ctx.Err()
		default:
		}
		dialer := &net.Dialer{Timeout: t}
		return dialer.DialContext(c.ctx, proto, host)
	}
	opts = append(opts, grpc.WithDialer(f))

	if creds != nil {
		opts = append(opts, grpc.WithTransportCredentials(*creds))
	} else {
		opts = append(opts, grpc.WithInsecure())
	}

	// Interceptor retry and backoff.
	// TODO: Replace all of clientv3/retry.go with interceptor based retry, or with
	// https://github.com/grpc/proposal/blob/master/A6-client-retries.md#retry-policy
	// once it is available.
	rrBackoff := withBackoff(c.roundRobinQuorumBackoff(defaultBackoffWaitBetween, defaultBackoffJitterFraction))
	opts = append(opts,
		// Disable stream retry by default since go-grpc-middleware/retry does not support client streams.
		// Streams that are safe to retry are enabled individually.
		grpc.WithStreamInterceptor(c.streamClientInterceptor(c.lg, withMax(0), rrBackoff)),
		grpc.WithUnaryInterceptor(c.unaryClientInterceptor(c.lg, withMax(defaultUnaryMaxRetries), rrBackoff)),
	)

	return opts, nil
}
func (c *Client) Dial(ep string) (*grpc.ClientConn, error) {
	creds := c.directDialCreds(ep)
	// Use the grpc passthrough resolver to directly dial a single endpoint.
	// This resolver passes through the 'unix' and 'unixs' endpoints schemes used
	// by etcd without modification, allowing us to directly dial endpoints and
	// using the same dial functions that we use for load balancer dialing.
	return c.dial(fmt.Sprintf("passthrough:///%s", ep), creds)
}
func (c *Client) dialWithBalancer(ep string, dopts ...grpc.DialOption) (*grpc.ClientConn, error) {
	_, host, _ := endpoint.ParseEndpoint(ep)
	target := c.resolverGroup.Target(host)
	creds := c.dialWithBalancerCreds(ep)
	return c.dial(target, creds, dopts...)
}
func (c *Client) dial(target string, creds *credentials.TransportCredentials, dopts ...grpc.DialOption) (*grpc.ClientConn, error) {
	opts, err := c.dialSetupOpts(creds, dopts...)
	if err != nil {
		return nil, fmt.Errorf("failed to configure dialer: %v", err)
	}

	if c.Username != "" && c.Password != "" {
		c.tokenCred = &authTokenCredential{
			tokenMu: &sync.RWMutex{},
		}

		ctx, cancel := c.ctx, func() {}
		if c.cfg.DialTimeout > 0 {
			ctx, cancel = context.WithTimeout(ctx, c.cfg.DialTimeout)
		}

		err = c.getToken(ctx)
		if err != nil {
			if toErr(ctx, err) != rpctypes.ErrAuthNotEnabled {
				if err == ctx.Err() && ctx.Err() != c.ctx.Err() {
					err = context.DeadlineExceeded
				}
				cancel()
				return nil, err
			}
		} else {
			opts = append(opts, grpc.WithPerRPCCredentials(c.tokenCred))
		}
		cancel()
	}

	opts = append(opts, c.cfg.DialOptions...)

	dctx := c.ctx
	if c.cfg.DialTimeout > 0 {
		var cancel context.CancelFunc
		dctx, cancel = context.WithTimeout(c.ctx, c.cfg.DialTimeout)
		defer cancel() // TODO: Is this right for cases where grpc.WithBlock() is not set on the dial options?
	}

	conn, err := grpc.DialContext(dctx, target, opts...)
	if err != nil {
		return nil, err
	}
	return conn, nil
}
func WithRequireLeader(ctx context.Context) context.Context {
	md := metadata.Pairs(rpctypes.MetadataRequireLeaderKey, rpctypes.MetadataHasLeader)
	return metadata.NewOutgoingContext(ctx, md)
}
func (c *Client) roundRobinQuorumBackoff(waitBetween time.Duration, jitterFraction float64) backoffFunc {
	return func(attempt uint) time.Duration {
		// after each round robin across quorum, backoff for our wait between duration
		n := uint(len(c.Endpoints()))
		quorum := (n/2 + 1)
		if attempt%quorum == 0 {
			c.lg.Debug("backoff", zap.Uint("attempt", attempt), zap.Uint("quorum", quorum), zap.Duration("waitBetween", waitBetween), zap.Float64("jitterFraction", jitterFraction))
			return jitterUp(waitBetween, jitterFraction)
		}
		c.lg.Debug("backoff skipped", zap.Uint("attempt", attempt), zap.Uint("quorum", quorum))
		return 0
	}
}
func isHaltErr(ctx context.Context, err error) bool {
	if ctx != nil && ctx.Err() != nil {
		return true
	}
	if err == nil {
		return false
	}
	ev, _ := status.FromError(err)
	// Unavailable codes mean the system will be right back.
	// (e.g., can't connect, lost leader)
	// Treat Internal codes as if something failed, leaving the
	// system in an inconsistent state, but retrying could make progress.
	// (e.g., failed in middle of send, corrupted frame)
	// TODO: are permanent Internal errors possible from grpc?
	return ev.Code() != codes.Unavailable && ev.Code() != codes.Internal
}
func NewLease(l clientv3.Lease, prefix string) clientv3.Lease {
	return &leasePrefix{l, []byte(prefix)}
}
func (e *Event) IsCreate() bool {
	return e.Type == EventTypePut && e.Kv.CreateRevision == e.Kv.ModRevision
}
func (wr *WatchResponse) Err() error {
	switch {
	case wr.closeErr != nil:
		return v3rpc.Error(wr.closeErr)
	case wr.CompactRevision != 0:
		return v3rpc.ErrCompacted
	case wr.Canceled:
		if len(wr.cancelReason) != 0 {
			return v3rpc.Error(status.Error(codes.FailedPrecondition, wr.cancelReason))
		}
		return v3rpc.ErrFutureRev
	}
	return nil
}
func (wr *WatchResponse) IsProgressNotify() bool {
	return len(wr.Events) == 0 && !wr.Canceled && !wr.Created && wr.CompactRevision == 0 && wr.Header.Revision != 0
}
func (w *watcher) RequestProgress(ctx context.Context) (err error) {
	ctxKey := streamKeyFromCtx(ctx)

	w.mu.Lock()
	if w.streams == nil {
		return fmt.Errorf("no stream found for context")
	}
	wgs := w.streams[ctxKey]
	if wgs == nil {
		wgs = w.newWatcherGrpcStream(ctx)
		w.streams[ctxKey] = wgs
	}
	donec := wgs.donec
	reqc := wgs.reqc
	w.mu.Unlock()

	pr := &progressRequest{}

	select {
	case reqc <- pr:
		return nil
	case <-ctx.Done():
		if err == nil {
			return ctx.Err()
		}
		return err
	case <-donec:
		if wgs.closeErr != nil {
			return wgs.closeErr
		}
		// retry; may have dropped stream from no ctxs
		return w.RequestProgress(ctx)
	}
}
func (w *watchGrpcStream) nextResume() *watcherStream {
	for len(w.resuming) != 0 {
		if w.resuming[0] != nil {
			return w.resuming[0]
		}
		w.resuming = w.resuming[1:len(w.resuming)]
	}
	return nil
}
func (w *watchGrpcStream) dispatchEvent(pbresp *pb.WatchResponse) bool {
	events := make([]*Event, len(pbresp.Events))
	for i, ev := range pbresp.Events {
		events[i] = (*Event)(ev)
	}
	// TODO: return watch ID?
	wr := &WatchResponse{
		Header:          *pbresp.Header,
		Events:          events,
		CompactRevision: pbresp.CompactRevision,
		Created:         pbresp.Created,
		Canceled:        pbresp.Canceled,
		cancelReason:    pbresp.CancelReason,
	}

	// watch IDs are zero indexed, so request notify watch responses are assigned a watch ID of -1 to
	// indicate they should be broadcast.
	if wr.IsProgressNotify() && pbresp.WatchId == -1 {
		return w.broadcastResponse(wr)
	}

	return w.unicastResponse(wr, pbresp.WatchId)

}
func (w *watchGrpcStream) broadcastResponse(wr *WatchResponse) bool {
	for _, ws := range w.substreams {
		select {
		case ws.recvc <- wr:
		case <-ws.donec:
		}
	}
	return true
}
func (w *watchGrpcStream) unicastResponse(wr *WatchResponse, watchId int64) bool {
	ws, ok := w.substreams[watchId]
	if !ok {
		return false
	}
	select {
	case ws.recvc <- wr:
	case <-ws.donec:
		return false
	}
	return true
}
func (w *watchGrpcStream) joinSubstreams() {
	for _, ws := range w.substreams {
		<-ws.donec
	}
	for _, ws := range w.resuming {
		if ws != nil {
			<-ws.donec
		}
	}
}
func (wr *watchRequest) toPB() *pb.WatchRequest {
	req := &pb.WatchCreateRequest{
		StartRevision:  wr.rev,
		Key:            []byte(wr.key),
		RangeEnd:       []byte(wr.end),
		ProgressNotify: wr.progressNotify,
		Filters:        wr.filters,
		PrevKv:         wr.prevKV,
		Fragment:       wr.fragment,
	}
	cr := &pb.WatchRequest_CreateRequest{CreateRequest: req}
	return &pb.WatchRequest{RequestUnion: cr}
}
func (pr *progressRequest) toPB() *pb.WatchRequest {
	req := &pb.WatchProgressRequest{}
	cr := &pb.WatchRequest_ProgressRequest{ProgressRequest: req}
	return &pb.WatchRequest{RequestUnion: cr}
}
func (us *unsafeSet) Contains(value string) (exists bool) {
	_, exists = us.d[value]
	return exists
}
func (us *unsafeSet) ContainsAll(values []string) bool {
	for _, s := range values {
		if !us.Contains(s) {
			return false
		}
	}
	return true
}
func (us *unsafeSet) Equals(other Set) bool {
	v1 := sort.StringSlice(us.Values())
	v2 := sort.StringSlice(other.Values())
	v1.Sort()
	v2.Sort()
	return reflect.DeepEqual(v1, v2)
}
func (us *unsafeSet) Values() (values []string) {
	values = make([]string, 0)
	for val := range us.d {
		values = append(values, val)
	}
	return values
}
func (us *unsafeSet) Copy() Set {
	cp := NewUnsafeSet()
	for val := range us.d {
		cp.Add(val)
	}

	return cp
}
func (us *unsafeSet) Sub(other Set) Set {
	oValues := other.Values()
	result := us.Copy().(*unsafeSet)

	for _, val := range oValues {
		if _, ok := result.d[val]; !ok {
			continue
		}
		delete(result.d, val)
	}

	return result
}
func v2MembersURL(ep url.URL) *url.URL {
	ep.Path = path.Join(ep.Path, defaultV2MembersPrefix)
	return &ep
}
func NewMigrateCommand() *cobra.Command {
	mc := &cobra.Command{
		Use:   "migrate",
		Short: "Migrates keys in a v2 store to a mvcc store",
		Run:   migrateCommandFunc,
	}

	mc.Flags().BoolVar(&migrateExcludeTTLKey, "no-ttl", false, "Do not convert TTL keys")
	mc.Flags().StringVar(&migrateDatadir, "data-dir", "", "Path to the data directory")
	mc.Flags().StringVar(&migrateWALdir, "wal-dir", "", "Path to the WAL directory")
	mc.Flags().StringVar(&migrateTransformer, "transformer", "", "Path to the user-provided transformer program")
	return mc
}
func (rc *raftNode) publishEntries(ents []raftpb.Entry) bool {
	for i := range ents {
		switch ents[i].Type {
		case raftpb.EntryNormal:
			if len(ents[i].Data) == 0 {
				// ignore empty messages
				break
			}
			s := string(ents[i].Data)
			select {
			case rc.commitC <- &s:
			case <-rc.stopc:
				return false
			}

		case raftpb.EntryConfChange:
			var cc raftpb.ConfChange
			cc.Unmarshal(ents[i].Data)
			rc.confState = *rc.node.ApplyConfChange(cc)
			switch cc.Type {
			case raftpb.ConfChangeAddNode:
				if len(cc.Context) > 0 {
					rc.transport.AddPeer(types.ID(cc.NodeID), []string{string(cc.Context)})
				}
			case raftpb.ConfChangeRemoveNode:
				if cc.NodeID == uint64(rc.id) {
					log.Println("I've been removed from the cluster! Shutting down.")
					return false
				}
				rc.transport.RemovePeer(types.ID(cc.NodeID))
			}
		}

		// after commit, update appliedIndex
		rc.appliedIndex = ents[i].Index

		// special nil commit to signal replay has finished
		if ents[i].Index == rc.lastIndex {
			select {
			case rc.commitC <- nil:
			case <-rc.stopc:
				return false
			}
		}
	}
	return true
}
func (rc *raftNode) openWAL(snapshot *raftpb.Snapshot) *wal.WAL {
	if !wal.Exist(rc.waldir) {
		if err := os.Mkdir(rc.waldir, 0750); err != nil {
			log.Fatalf("raftexample: cannot create dir for wal (%v)", err)
		}

		w, err := wal.Create(zap.NewExample(), rc.waldir, nil)
		if err != nil {
			log.Fatalf("raftexample: create wal error (%v)", err)
		}
		w.Close()
	}

	walsnap := walpb.Snapshot{}
	if snapshot != nil {
		walsnap.Index, walsnap.Term = snapshot.Metadata.Index, snapshot.Metadata.Term
	}
	log.Printf("loading WAL at term %d and index %d", walsnap.Term, walsnap.Index)
	w, err := wal.Open(zap.NewExample(), rc.waldir, walsnap)
	if err != nil {
		log.Fatalf("raftexample: error loading wal (%v)", err)
	}

	return w
}
func (rc *raftNode) replayWAL() *wal.WAL {
	log.Printf("replaying WAL of member %d", rc.id)
	snapshot := rc.loadSnapshot()
	w := rc.openWAL(snapshot)
	_, st, ents, err := w.ReadAll()
	if err != nil {
		log.Fatalf("raftexample: failed to read WAL (%v)", err)
	}
	rc.raftStorage = raft.NewMemoryStorage()
	if snapshot != nil {
		rc.raftStorage.ApplySnapshot(*snapshot)
	}
	rc.raftStorage.SetHardState(st)

	// append to storage so raft starts at the right place in log
	rc.raftStorage.Append(ents)
	// send nil once lastIndex is published so client knows commit channel is current
	if len(ents) > 0 {
		rc.lastIndex = ents[len(ents)-1].Index
	} else {
		rc.commitC <- nil
	}
	return w
}
func (rc *raftNode) stop() {
	rc.stopHTTP()
	close(rc.commitC)
	close(rc.errorC)
	rc.node.Stop()
}
func NewWatchCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "watch [options] [key or prefix] [range_end] [--] [exec-command arg1 arg2 ...]",
		Short: "Watches events stream on keys or prefixes",
		Run:   watchCommandFunc,
	}

	cmd.Flags().BoolVarP(&watchInteractive, "interactive", "i", false, "Interactive mode")
	cmd.Flags().BoolVar(&watchPrefix, "prefix", false, "Watch on a prefix if prefix is set")
	cmd.Flags().Int64Var(&watchRev, "rev", 0, "Revision to start watching")
	cmd.Flags().BoolVar(&watchPrevKey, "prev-kv", false, "get the previous key-value pair before the event happens")

	return cmd
}
func (ms *MemoryStorage) InitialState() (pb.HardState, pb.ConfState, error) {
	return ms.hardState, ms.snapshot.Metadata.ConfState, nil
}
func (ms *MemoryStorage) SetHardState(st pb.HardState) error {
	ms.Lock()
	defer ms.Unlock()
	ms.hardState = st
	return nil
}
func (ms *MemoryStorage) Entries(lo, hi, maxSize uint64) ([]pb.Entry, error) {
	ms.Lock()
	defer ms.Unlock()
	offset := ms.ents[0].Index
	if lo <= offset {
		return nil, ErrCompacted
	}
	if hi > ms.lastIndex()+1 {
		raftLogger.Panicf("entries' hi(%d) is out of bound lastindex(%d)", hi, ms.lastIndex())
	}
	// only contains dummy entries.
	if len(ms.ents) == 1 {
		return nil, ErrUnavailable
	}

	ents := ms.ents[lo-offset : hi-offset]
	return limitSize(ents, maxSize), nil
}
func (ms *MemoryStorage) Term(i uint64) (uint64, error) {
	ms.Lock()
	defer ms.Unlock()
	offset := ms.ents[0].Index
	if i < offset {
		return 0, ErrCompacted
	}
	if int(i-offset) >= len(ms.ents) {
		return 0, ErrUnavailable
	}
	return ms.ents[i-offset].Term, nil
}
func (ms *MemoryStorage) LastIndex() (uint64, error) {
	ms.Lock()
	defer ms.Unlock()
	return ms.lastIndex(), nil
}
func (ms *MemoryStorage) FirstIndex() (uint64, error) {
	ms.Lock()
	defer ms.Unlock()
	return ms.firstIndex(), nil
}
func (ms *MemoryStorage) Snapshot() (pb.Snapshot, error) {
	ms.Lock()
	defer ms.Unlock()
	return ms.snapshot, nil
}
func (ms *MemoryStorage) ApplySnapshot(snap pb.Snapshot) error {
	ms.Lock()
	defer ms.Unlock()

	//handle check for old snapshot being applied
	msIndex := ms.snapshot.Metadata.Index
	snapIndex := snap.Metadata.Index
	if msIndex >= snapIndex {
		return ErrSnapOutOfDate
	}

	ms.snapshot = snap
	ms.ents = []pb.Entry{{Term: snap.Metadata.Term, Index: snap.Metadata.Index}}
	return nil
}
func (ms *MemoryStorage) Compact(compactIndex uint64) error {
	ms.Lock()
	defer ms.Unlock()
	offset := ms.ents[0].Index
	if compactIndex <= offset {
		return ErrCompacted
	}
	if compactIndex > ms.lastIndex() {
		raftLogger.Panicf("compact %d is out of bound lastindex(%d)", compactIndex, ms.lastIndex())
	}

	i := compactIndex - offset
	ents := make([]pb.Entry, 1, 1+uint64(len(ms.ents))-i)
	ents[0].Index = ms.ents[i].Index
	ents[0].Term = ms.ents[i].Term
	ents = append(ents, ms.ents[i+1:]...)
	ms.ents = ents
	return nil
}
func (p *urlPicker) unreachable(u url.URL) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if u == p.urls[p.picked] {
		p.picked = (p.picked + 1) % len(p.urls)
	}
}
func NewEndpointCommand() *cobra.Command {
	ec := &cobra.Command{
		Use:   "endpoint <subcommand>",
		Short: "Endpoint related commands",
	}

	ec.PersistentFlags().BoolVar(&epClusterEndpoints, "cluster", false, "use all endpoints from the cluster member list")
	ec.AddCommand(newEpHealthCommand())
	ec.AddCommand(newEpStatusCommand())
	ec.AddCommand(newEpHashKVCommand())

	return ec
}
func epHealthCommandFunc(cmd *cobra.Command, args []string) {
	flags.SetPflagsFromEnv("ETCDCTL", cmd.InheritedFlags())
	initDisplayFromCmd(cmd)

	sec := secureCfgFromCmd(cmd)
	dt := dialTimeoutFromCmd(cmd)
	ka := keepAliveTimeFromCmd(cmd)
	kat := keepAliveTimeoutFromCmd(cmd)
	auth := authCfgFromCmd(cmd)
	cfgs := []*v3.Config{}
	for _, ep := range endpointsFromCluster(cmd) {
		cfg, err := newClientCfg([]string{ep}, dt, ka, kat, sec, auth)
		if err != nil {
			ExitWithError(ExitBadArgs, err)
		}
		cfgs = append(cfgs, cfg)
	}

	var wg sync.WaitGroup
	hch := make(chan epHealth, len(cfgs))
	for _, cfg := range cfgs {
		wg.Add(1)
		go func(cfg *v3.Config) {
			defer wg.Done()
			ep := cfg.Endpoints[0]
			cli, err := v3.New(*cfg)
			if err != nil {
				hch <- epHealth{Ep: ep, Health: false, Error: err.Error()}
				return
			}
			st := time.Now()
			// get a random key. As long as we can get the response without an error, the
			// endpoint is health.
			ctx, cancel := commandCtx(cmd)
			_, err = cli.Get(ctx, "health")
			cancel()
			eh := epHealth{Ep: ep, Health: false, Took: time.Since(st).String()}
			// permission denied is OK since proposal goes through consensus to get it
			if err == nil || err == rpctypes.ErrPermissionDenied {
				eh.Health = true
			} else {
				eh.Error = err.Error()
			}
			hch <- eh
		}(cfg)
	}

	wg.Wait()
	close(hch)

	errs := false
	healthList := []epHealth{}
	for h := range hch {
		healthList = append(healthList, h)
		if h.Error != "" {
			errs = true
		}
	}
	display.EndpointHealth(healthList)
	if errs {
		ExitWithError(ExitError, fmt.Errorf("unhealthy cluster"))
	}
}
func NewElectCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "elect <election-name> [proposal]",
		Short: "Observes and participates in leader election",
		Run:   electCommandFunc,
	}
	cmd.Flags().BoolVarP(&electListen, "listen", "l", false, "observation mode")
	return cmd
}
func NewDefragCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "defrag",
		Short: "Defragments the storage of the etcd members with given endpoints",
		Run:   defragCommandFunc,
	}
	cmd.PersistentFlags().BoolVar(&epClusterEndpoints, "cluster", false, "use all endpoints from the cluster member list")
	cmd.Flags().StringVar(&defragDataDir, "data-dir", "", "Optional. If present, defragments a data directory not in use by etcd.")
	return cmd
}
func RegisterBuilder(cfg Config) {
	bb := &builder{cfg}
	balancer.Register(bb)

	bb.cfg.Logger.Debug(
		"registered balancer",
		zap.String("policy", bb.cfg.Policy.String()),
		zap.String("name", bb.cfg.Name),
	)
}
func (b *builder) Build(cc balancer.ClientConn, opt balancer.BuildOptions) balancer.Balancer {
	bb := &baseBalancer{
		id:     strconv.FormatInt(time.Now().UnixNano(), 36),
		policy: b.cfg.Policy,
		name:   b.cfg.Name,
		lg:     b.cfg.Logger,

		addrToSc: make(map[resolver.Address]balancer.SubConn),
		scToAddr: make(map[balancer.SubConn]resolver.Address),
		scToSt:   make(map[balancer.SubConn]connectivity.State),

		currentConn: nil,
		csEvltr:     &connectivityStateEvaluator{},

		// initialize picker always returns "ErrNoSubConnAvailable"
		Picker: picker.NewErr(balancer.ErrNoSubConnAvailable),
	}
	if bb.lg == nil {
		bb.lg = zap.NewNop()
	}

	// TODO: support multiple connections
	bb.mu.Lock()
	bb.currentConn = cc
	bb.mu.Unlock()

	bb.lg.Info(
		"built balancer",
		zap.String("balancer-id", bb.id),
		zap.String("policy", bb.policy.String()),
		zap.String("resolver-target", cc.Target()),
	)
	return bb
}
func (cse *connectivityStateEvaluator) recordTransition(oldState, newState connectivity.State) connectivity.State {
	// Update counters.
	for idx, state := range []connectivity.State{oldState, newState} {
		updateVal := 2*uint64(idx) - 1 // -1 for oldState and +1 for new.
		switch state {
		case connectivity.Ready:
			cse.numReady += updateVal
		case connectivity.Connecting:
			cse.numConnecting += updateVal
		case connectivity.TransientFailure:
			cse.numTransientFailure += updateVal
		}
	}

	// Evaluate.
	if cse.numReady > 0 {
		return connectivity.Ready
	}
	if cse.numConnecting > 0 {
		return connectivity.Connecting
	}
	return connectivity.TransientFailure
}
func (s *EtcdServer) doSerialize(ctx context.Context, chk func(*auth.AuthInfo) error, get func()) error {
	ai, err := s.AuthInfoFromCtx(ctx)
	if err != nil {
		return err
	}
	if ai == nil {
		// chk expects non-nil AuthInfo; use empty credentials
		ai = &auth.AuthInfo{}
	}
	if err = chk(ai); err != nil {
		return err
	}
	// fetch response for serialized request
	get()
	// check for stale token revision in case the auth store was updated while
	// the request has been handled.
	if ai.Revision != 0 && ai.Revision != s.authStore.Revision() {
		return auth.ErrAuthOldRevision
	}
	return nil
}
func (w *watcher) send(wr clientv3.WatchResponse) {
	if wr.IsProgressNotify() && !w.progress {
		return
	}
	if w.nextrev > wr.Header.Revision && len(wr.Events) > 0 {
		return
	}
	if w.nextrev == 0 {
		// current watch; expect updates following this revision
		w.nextrev = wr.Header.Revision + 1
	}

	events := make([]*mvccpb.Event, 0, len(wr.Events))

	var lastRev int64
	for i := range wr.Events {
		ev := (*mvccpb.Event)(wr.Events[i])
		if ev.Kv.ModRevision < w.nextrev {
			continue
		} else {
			// We cannot update w.rev here.
			// txn can have multiple events with the same rev.
			// If w.nextrev updates here, it would skip events in the same txn.
			lastRev = ev.Kv.ModRevision
		}

		filtered := false
		for _, filter := range w.filters {
			if filter(*ev) {
				filtered = true
				break
			}
		}
		if filtered {
			continue
		}

		if !w.prevKV {
			evCopy := *ev
			evCopy.PrevKv = nil
			ev = &evCopy
		}
		events = append(events, ev)
	}

	if lastRev >= w.nextrev {
		w.nextrev = lastRev + 1
	}

	// all events are filtered out?
	if !wr.IsProgressNotify() && !wr.Created && len(events) == 0 && wr.CompactRevision == 0 {
		return
	}

	w.lastHeader = wr.Header
	w.post(&pb.WatchResponse{
		Header:          &wr.Header,
		Created:         wr.Created,
		CompactRevision: wr.CompactRevision,
		Canceled:        wr.Canceled,
		WatchId:         w.id,
		Events:          events,
	})
}
func (w *watcher) post(wr *pb.WatchResponse) bool {
	select {
	case w.wps.watchCh <- wr:
	case <-time.After(50 * time.Millisecond):
		w.wps.cancel()
		return false
	}
	return true
}
func (ac *AccessController) OriginAllowed(origin string) bool {
	ac.corsMu.RLock()
	defer ac.corsMu.RUnlock()
	if len(ac.CORS) == 0 { // allow all
		return true
	}
	_, ok := ac.CORS["*"]
	if ok {
		return true
	}
	_, ok = ac.CORS[origin]
	return ok
}
func (ac *AccessController) IsHostWhitelisted(host string) bool {
	ac.hostWhitelistMu.RLock()
	defer ac.hostWhitelistMu.RUnlock()
	if len(ac.HostWhitelist) == 0 { // allow all
		return true
	}
	_, ok := ac.HostWhitelist["*"]
	if ok {
		return true
	}
	_, ok = ac.HostWhitelist[host]
	return ok
}
func (ss *SelectiveStringValue) Valids() []string {
	s := make([]string, 0, len(ss.valids))
	for k := range ss.valids {
		s = append(s, k)
	}
	sort.Strings(s)
	return s
}
func NewSelectiveStringsValue(valids ...string) *SelectiveStringsValue {
	vm := make(map[string]struct{})
	for _, v := range valids {
		vm[v] = struct{}{}
	}
	return &SelectiveStringsValue{valids: vm, vs: []string{}}
}
func NewKV(kv clientv3.KV, prefix string) clientv3.KV {
	return &kvPrefix{kv, prefix}
}
func NewURLsValue(s string) *URLsValue {
	if s == "" {
		return &URLsValue{}
	}
	v := &URLsValue{}
	if err := v.Set(s); err != nil {
		plog.Panicf("new URLsValue should never fail: %v", err)
	}
	return v
}
func URLsFromFlag(fs *flag.FlagSet, urlsFlagName string) []url.URL {
	return []url.URL(*fs.Lookup(urlsFlagName).Value.(*URLsValue))
}
func (e *Etcd) servePeers() (err error) {
	ph := etcdhttp.NewPeerHandler(e.GetLogger(), e.Server)
	var peerTLScfg *tls.Config
	if !e.cfg.PeerTLSInfo.Empty() {
		if peerTLScfg, err = e.cfg.PeerTLSInfo.ServerConfig(); err != nil {
			return err
		}
	}

	for _, p := range e.Peers {
		u := p.Listener.Addr().String()
		gs := v3rpc.Server(e.Server, peerTLScfg)
		m := cmux.New(p.Listener)
		go gs.Serve(m.Match(cmux.HTTP2()))
		srv := &http.Server{
			Handler:     grpcHandlerFunc(gs, ph),
			ReadTimeout: 5 * time.Minute,
			ErrorLog:    defaultLog.New(ioutil.Discard, "", 0), // do not log user error
		}
		go srv.Serve(m.Match(cmux.Any()))
		p.serve = func() error { return m.Serve() }
		p.close = func(ctx context.Context) error {
			// gracefully shutdown http.Server
			// close open listeners, idle connections
			// until context cancel or time-out
			if e.cfg.logger != nil {
				e.cfg.logger.Info(
					"stopping serving peer traffic",
					zap.String("address", u),
				)
			}
			stopServers(ctx, &servers{secure: peerTLScfg != nil, grpc: gs, http: srv})
			if e.cfg.logger != nil {
				e.cfg.logger.Info(
					"stopped serving peer traffic",
					zap.String("address", u),
				)
			}
			return nil
		}
	}

	// start peer servers in a goroutine
	for _, pl := range e.Peers {
		go func(l *peerListener) {
			u := l.Addr().String()
			if e.cfg.logger != nil {
				e.cfg.logger.Info(
					"serving peer traffic",
					zap.String("address", u),
				)
			} else {
				plog.Info("listening for peers on ", u)
			}
			e.errHandler(l.serve())
		}(pl)
	}
	return nil
}
func NewStore(lg *zap.Logger, b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) *store {
	s := &store{
		b:       b,
		ig:      ig,
		kvindex: newTreeIndex(lg),

		le: le,

		currentRev:     1,
		compactMainRev: -1,

		bytesBuf8: make([]byte, 8),
		fifoSched: schedule.NewFIFOScheduler(),

		stopc: make(chan struct{}),

		lg: lg,
	}
	s.ReadView = &readView{s}
	s.WriteView = &writeView{s}
	if s.le != nil {
		s.le.SetRangeDeleter(func() lease.TxnDelete { return s.Write() })
	}

	tx := s.b.BatchTx()
	tx.Lock()
	tx.UnsafeCreateBucket(keyBucketName)
	tx.UnsafeCreateBucket(metaBucketName)
	tx.Unlock()
	s.b.ForceCommit()

	s.mu.Lock()
	defer s.mu.Unlock()
	if err := s.restore(); err != nil {
		// TODO: return the error instead of panic here?
		panic("failed to recover store from backend")
	}

	return s
}
func appendMarkTombstone(lg *zap.Logger, b []byte) []byte {
	if len(b) != revBytesLen {
		if lg != nil {
			lg.Panic(
				"cannot append tombstone mark to non-normal revision bytes",
				zap.Int("expected-revision-bytes-size", revBytesLen),
				zap.Int("given-revision-bytes-size", len(b)),
			)
		} else {
			plog.Panicf("cannot append mark to non normal revision bytes")
		}
	}
	return append(b, markTombstone)
}
func IsDirWriteable(dir string) error {
	f := filepath.Join(dir, ".touch")
	if err := ioutil.WriteFile(f, []byte(""), PrivateFileMode); err != nil {
		return err
	}
	return os.Remove(f)
}
func TouchDirAll(dir string) error {
	// If path is already a directory, MkdirAll does nothing
	// and returns nil.
	err := os.MkdirAll(dir, PrivateDirMode)
	if err != nil {
		// if mkdirAll("a/text") and "text" is not
		// a directory, this will return syscall.ENOTDIR
		return err
	}
	return IsDirWriteable(dir)
}
func CreateDirAll(dir string) error {
	err := TouchDirAll(dir)
	if err == nil {
		var ns []string
		ns, err = ReadDir(dir)
		if err != nil {
			return err
		}
		if len(ns) != 0 {
			err = fmt.Errorf("expected %q to be empty, got %q", dir, ns)
		}
	}
	return err
}
func ZeroToEnd(f *os.File) error {
	// TODO: support FALLOC_FL_ZERO_RANGE
	off, err := f.Seek(0, io.SeekCurrent)
	if err != nil {
		return err
	}
	lenf, lerr := f.Seek(0, io.SeekEnd)
	if lerr != nil {
		return lerr
	}
	if err = f.Truncate(off); err != nil {
		return err
	}
	// make sure blocks remain allocated
	if err = Preallocate(f, lenf, true); err != nil {
		return err
	}
	_, err = f.Seek(off, io.SeekStart)
	return err
}
func (fp *filePipeline) Open() (f *fileutil.LockedFile, err error) {
	select {
	case f = <-fp.filec:
	case err = <-fp.errc:
	}
	return f, err
}
func NewRaftLoggerFromZapCore(cr zapcore.Core, syncer zapcore.WriteSyncer) raft.Logger {
	// "AddCallerSkip" to annotate caller outside of "logutil"
	lg := zap.New(cr, zap.AddCaller(), zap.AddCallerSkip(1), zap.ErrorOutput(syncer))
	return &zapRaftLogger{lg: lg, sugar: lg.Sugar()}
}
func NewConfig(fpath string) (*clientv3.Config, error) {
	b, err := ioutil.ReadFile(fpath)
	if err != nil {
		return nil, err
	}

	yc := &yamlConfig{}

	err = yaml.Unmarshal(b, yc)
	if err != nil {
		return nil, err
	}

	if yc.InsecureTransport {
		return &yc.Config, nil
	}

	var (
		cert *tls.Certificate
		cp   *x509.CertPool
	)

	if yc.Certfile != "" && yc.Keyfile != "" {
		cert, err = tlsutil.NewCert(yc.Certfile, yc.Keyfile, nil)
		if err != nil {
			return nil, err
		}
	}

	if yc.TrustedCAfile != "" {
		cp, err = tlsutil.NewCertPool([]string{yc.TrustedCAfile})
		if err != nil {
			return nil, err
		}
	}

	tlscfg := &tls.Config{
		MinVersion:         tls.VersionTLS12,
		InsecureSkipVerify: yc.InsecureSkipTLSVerify,
		RootCAs:            cp,
	}
	if cert != nil {
		tlscfg.Certificates = []tls.Certificate{*cert}
	}
	yc.Config.TLS = tlscfg

	return &yc.Config, nil
}
func RegisterElectionHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterElectionHandlerClient(ctx, mux, v3electionpb.NewElectionClient(conn))
}
func UpdateCapability(lg *zap.Logger, v *semver.Version) {
	if v == nil {
		// if recovered but version was never set by cluster
		return
	}
	enableMapMu.Lock()
	if curVersion != nil && !curVersion.LessThan(*v) {
		enableMapMu.Unlock()
		return
	}
	curVersion = v
	enabledMap = capabilityMaps[curVersion.String()]
	enableMapMu.Unlock()

	if lg != nil {
		lg.Info(
			"enabled capabilities for version",
			zap.String("cluster-version", version.Cluster(v.String())),
		)
	} else {
		plog.Infof("enabled capabilities for version %s", version.Cluster(v.String()))
	}
}
func NewLockCommand() *cobra.Command {
	c := &cobra.Command{
		Use:   "lock <lockname> [exec-command arg1 arg2 ...]",
		Short: "Acquires a named lock",
		Run:   lockCommandFunc,
	}
	c.Flags().IntVarP(&lockTTL, "ttl", "", lockTTL, "timeout for session")
	return c
}
func (r *raftNode) tick() {
	r.tickMu.Lock()
	r.Tick()
	r.tickMu.Unlock()
}
func (r *raftNode) advanceTicks(ticks int) {
	for i := 0; i < ticks; i++ {
		r.tick()
	}
}
func NewAuthCommand() *cobra.Command {
	ac := &cobra.Command{
		Use:   "auth <enable or disable>",
		Short: "Enable or disable authentication",
	}

	ac.AddCommand(newAuthEnableCommand())
	ac.AddCommand(newAuthDisableCommand())

	return ac
}
func authEnableCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 0 {
		ExitWithError(ExitBadArgs, fmt.Errorf("auth enable command does not accept any arguments"))
	}

	ctx, cancel := commandCtx(cmd)
	cli := mustClientFromCmd(cmd)
	var err error
	for err == nil {
		if _, err = cli.AuthEnable(ctx); err == nil {
			break
		}
		if err == rpctypes.ErrRootRoleNotExist {
			if _, err = cli.RoleAdd(ctx, "root"); err != nil {
				break
			}
			if _, err = cli.UserGrantRole(ctx, "root", "root"); err != nil {
				break
			}
		}
	}
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}

	fmt.Println("Authentication Enabled")
}
func authDisableCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 0 {
		ExitWithError(ExitBadArgs, fmt.Errorf("auth disable command does not accept any arguments"))
	}

	ctx, cancel := commandCtx(cmd)
	_, err := mustClientFromCmd(cmd).Auth.AuthDisable(ctx)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}

	fmt.Println("Authentication Disabled")
}
func RetryKVClient(c *Client) pb.KVClient {
	return &retryKVClient{
		kc: pb.NewKVClient(c.conn),
	}
}
func RetryLeaseClient(c *Client) pb.LeaseClient {
	return &retryLeaseClient{
		lc: pb.NewLeaseClient(c.conn),
	}
}
func RetryClusterClient(c *Client) pb.ClusterClient {
	return &retryClusterClient{
		cc: pb.NewClusterClient(c.conn),
	}
}
func RetryMaintenanceClient(c *Client, conn *grpc.ClientConn) pb.MaintenanceClient {
	return &retryMaintenanceClient{
		mc: pb.NewMaintenanceClient(conn),
	}
}
func RetryAuthClient(c *Client) pb.AuthClient {
	return &retryAuthClient{
		ac: pb.NewAuthClient(c.conn),
	}
}
func NewSetDirCommand() cli.Command {
	return cli.Command{
		Name:      "setdir",
		Usage:     "create a new directory or update an existing directory TTL",
		ArgsUsage: "<key>",
		Flags: []cli.Flag{
			cli.IntFlag{Name: "ttl", Value: 0, Usage: "key time-to-live in seconds"},
		},
		Action: func(c *cli.Context) error {
			mkdirCommandFunc(c, mustNewKeyAPI(c), client.PrevIgnore)
			return nil
		},
	}
}
func (b *DoubleBarrier) Enter() error {
	client := b.s.Client()
	ek, err := newUniqueEphemeralKey(b.s, b.key+"/waiters")
	if err != nil {
		return err
	}
	b.myKey = ek

	resp, err := client.Get(b.ctx, b.key+"/waiters", clientv3.WithPrefix())
	if err != nil {
		return err
	}

	if len(resp.Kvs) > b.count {
		return ErrTooManyClients
	}

	if len(resp.Kvs) == b.count {
		// unblock waiters
		_, err = client.Put(b.ctx, b.key+"/ready", "")
		return err
	}

	_, err = WaitEvents(
		client,
		b.key+"/ready",
		ek.Revision(),
		[]mvccpb.Event_EventType{mvccpb.PUT})
	return err
}
func (b *DoubleBarrier) Leave() error {
	client := b.s.Client()
	resp, err := client.Get(b.ctx, b.key+"/waiters", clientv3.WithPrefix())
	if err != nil {
		return err
	}
	if len(resp.Kvs) == 0 {
		return nil
	}

	lowest, highest := resp.Kvs[0], resp.Kvs[0]
	for _, k := range resp.Kvs {
		if k.ModRevision < lowest.ModRevision {
			lowest = k
		}
		if k.ModRevision > highest.ModRevision {
			highest = k
		}
	}
	isLowest := string(lowest.Key) == b.myKey.Key()

	if len(resp.Kvs) == 1 {
		// this is the only node in the barrier; finish up
		if _, err = client.Delete(b.ctx, b.key+"/ready"); err != nil {
			return err
		}
		return b.myKey.Delete()
	}

	// this ensures that if a process fails, the ephemeral lease will be
	// revoked, its barrier key is removed, and the barrier can resume

	// lowest process in node => wait on highest process
	if isLowest {
		_, err = WaitEvents(
			client,
			string(highest.Key),
			highest.ModRevision,
			[]mvccpb.Event_EventType{mvccpb.DELETE})
		if err != nil {
			return err
		}
		return b.Leave()
	}

	// delete self and wait on lowest process
	if err = b.myKey.Delete(); err != nil {
		return err
	}

	key := string(lowest.Key)
	_, err = WaitEvents(
		client,
		key,
		lowest.ModRevision,
		[]mvccpb.Event_EventType{mvccpb.DELETE})
	if err != nil {
		return err
	}
	return b.Leave()
}
func HandleBasic(mux *http.ServeMux, server etcdserver.ServerPeer) {
	mux.HandleFunc(varsPath, serveVars)

	// TODO: deprecate '/config/local/log' in v3.5
	mux.HandleFunc(configPath+"/local/log", logHandleFunc)

	HandleMetricsHealth(mux, server)
	mux.HandleFunc(versionPath, versionHandler(server.Cluster(), serveVersion))
}
func WriteError(lg *zap.Logger, w http.ResponseWriter, r *http.Request, err error) {
	if err == nil {
		return
	}
	switch e := err.(type) {
	case *v2error.Error:
		e.WriteTo(w)

	case *httptypes.HTTPError:
		if et := e.WriteTo(w); et != nil {
			if lg != nil {
				lg.Debug(
					"failed to write v2 HTTP error",
					zap.String("remote-addr", r.RemoteAddr),
					zap.String("internal-server-error", e.Error()),
					zap.Error(et),
				)
			} else {
				plog.Debugf("error writing HTTPError (%v) to %s", et, r.RemoteAddr)
			}
		}

	default:
		switch err {
		case etcdserver.ErrTimeoutDueToLeaderFail, etcdserver.ErrTimeoutDueToConnectionLost, etcdserver.ErrNotEnoughStartedMembers,
			etcdserver.ErrUnhealthy:
			if lg != nil {
				lg.Warn(
					"v2 response error",
					zap.String("remote-addr", r.RemoteAddr),
					zap.String("internal-server-error", err.Error()),
				)
			} else {
				mlog.MergeError(err)
			}

		default:
			if lg != nil {
				lg.Warn(
					"unexpected v2 response error",
					zap.String("remote-addr", r.RemoteAddr),
					zap.String("internal-server-error", err.Error()),
				)
			} else {
				mlog.MergeErrorf("got unexpected response error (%v)", err)
			}
		}

		herr := httptypes.NewHTTPError(http.StatusInternalServerError, "Internal Server Error")
		if et := herr.WriteTo(w); et != nil {
			if lg != nil {
				lg.Debug(
					"failed to write v2 HTTP error",
					zap.String("remote-addr", r.RemoteAddr),
					zap.String("internal-server-error", err.Error()),
					zap.Error(et),
				)
			} else {
				plog.Debugf("error writing HTTPError (%v) to %s", et, r.RemoteAddr)
			}
		}
	}
}
func (c *RaftCluster) MemberByName(name string) *Member {
	c.Lock()
	defer c.Unlock()
	var memb *Member
	for _, m := range c.members {
		if m.Name == name {
			if memb != nil {
				if c.lg != nil {
					c.lg.Panic("two member with same name found", zap.String("name", name))
				} else {
					plog.Panicf("two members with the given name %q exist", name)
				}
			}
			memb = m
		}
	}
	return memb.Clone()
}
func (c *RaftCluster) PeerURLs() []string {
	c.Lock()
	defer c.Unlock()
	urls := make([]string, 0)
	for _, p := range c.members {
		urls = append(urls, p.PeerURLs...)
	}
	sort.Strings(urls)
	return urls
}
func (c *RaftCluster) ValidateConfigurationChange(cc raftpb.ConfChange) error {
	members, removed := membersFromStore(c.lg, c.v2store)
	id := types.ID(cc.NodeID)
	if removed[id] {
		return ErrIDRemoved
	}
	switch cc.Type {
	case raftpb.ConfChangeAddNode:
		if members[id] != nil {
			return ErrIDExists
		}
		urls := make(map[string]bool)
		for _, m := range members {
			for _, u := range m.PeerURLs {
				urls[u] = true
			}
		}
		m := new(Member)
		if err := json.Unmarshal(cc.Context, m); err != nil {
			if c.lg != nil {
				c.lg.Panic("failed to unmarshal member", zap.Error(err))
			} else {
				plog.Panicf("unmarshal member should never fail: %v", err)
			}
		}
		for _, u := range m.PeerURLs {
			if urls[u] {
				return ErrPeerURLexists
			}
		}

	case raftpb.ConfChangeRemoveNode:
		if members[id] == nil {
			return ErrIDNotFound
		}

	case raftpb.ConfChangeUpdateNode:
		if members[id] == nil {
			return ErrIDNotFound
		}
		urls := make(map[string]bool)
		for _, m := range members {
			if m.ID == id {
				continue
			}
			for _, u := range m.PeerURLs {
				urls[u] = true
			}
		}
		m := new(Member)
		if err := json.Unmarshal(cc.Context, m); err != nil {
			if c.lg != nil {
				c.lg.Panic("failed to unmarshal member", zap.Error(err))
			} else {
				plog.Panicf("unmarshal member should never fail: %v", err)
			}
		}
		for _, u := range m.PeerURLs {
			if urls[u] {
				return ErrPeerURLexists
			}
		}

	default:
		if c.lg != nil {
			c.lg.Panic("unknown ConfChange type", zap.String("type", cc.Type.String()))
		} else {
			plog.Panicf("ConfChange type should be either AddNode, RemoveNode or UpdateNode")
		}
	}
	return nil
}
func (c *RaftCluster) AddMember(m *Member) {
	c.Lock()
	defer c.Unlock()
	if c.v2store != nil {
		mustSaveMemberToStore(c.v2store, m)
	}
	if c.be != nil {
		mustSaveMemberToBackend(c.be, m)
	}

	c.members[m.ID] = m

	if c.lg != nil {
		c.lg.Info(
			"added member",
			zap.String("cluster-id", c.cid.String()),
			zap.String("local-member-id", c.localID.String()),
			zap.String("added-peer-id", m.ID.String()),
			zap.Strings("added-peer-peer-urls", m.PeerURLs),
		)
	} else {
		plog.Infof("added member %s %v to cluster %s", m.ID, m.PeerURLs, c.cid)
	}
}
func (c *RaftCluster) RemoveMember(id types.ID) {
	c.Lock()
	defer c.Unlock()
	if c.v2store != nil {
		mustDeleteMemberFromStore(c.v2store, id)
	}
	if c.be != nil {
		mustDeleteMemberFromBackend(c.be, id)
	}

	m, ok := c.members[id]
	delete(c.members, id)
	c.removed[id] = true

	if c.lg != nil {
		if ok {
			c.lg.Info(
				"removed member",
				zap.String("cluster-id", c.cid.String()),
				zap.String("local-member-id", c.localID.String()),
				zap.String("removed-remote-peer-id", id.String()),
				zap.Strings("removed-remote-peer-urls", m.PeerURLs),
			)
		} else {
			c.lg.Warn(
				"skipped removing already removed member",
				zap.String("cluster-id", c.cid.String()),
				zap.String("local-member-id", c.localID.String()),
				zap.String("removed-remote-peer-id", id.String()),
			)
		}
	} else {
		plog.Infof("removed member %s from cluster %s", id, c.cid)
	}
}
func ValidateClusterAndAssignIDs(lg *zap.Logger, local *RaftCluster, existing *RaftCluster) error {
	ems := existing.Members()
	lms := local.Members()
	if len(ems) != len(lms) {
		return fmt.Errorf("member count is unequal")
	}
	sort.Sort(MembersByPeerURLs(ems))
	sort.Sort(MembersByPeerURLs(lms))

	ctx, cancel := context.WithTimeout(context.TODO(), 30*time.Second)
	defer cancel()
	for i := range ems {
		if ok, err := netutil.URLStringsEqual(ctx, lg, ems[i].PeerURLs, lms[i].PeerURLs); !ok {
			return fmt.Errorf("unmatched member while checking PeerURLs (%v)", err)
		}
		lms[i].ID = ems[i].ID
	}
	local.members = make(map[types.ID]*Member)
	for _, m := range lms {
		local.members[m.ID] = m
	}
	return nil
}
func (ti *treeIndex) Keep(rev int64) map[revision]struct{} {
	available := make(map[revision]struct{})
	ti.RLock()
	defer ti.RUnlock()
	ti.tree.Ascend(func(i btree.Item) bool {
		keyi := i.(*keyIndex)
		keyi.keep(rev, available)
		return true
	})
	return available
}
func (l *lessor) closeRequireLeader() {
	l.mu.Lock()
	defer l.mu.Unlock()
	for _, ka := range l.keepAlives {
		reqIdxs := 0
		// find all required leader channels, close, mark as nil
		for i, ctx := range ka.ctxs {
			md, ok := metadata.FromOutgoingContext(ctx)
			if !ok {
				continue
			}
			ks := md[rpctypes.MetadataRequireLeaderKey]
			if len(ks) < 1 || ks[0] != rpctypes.MetadataHasLeader {
				continue
			}
			close(ka.chs[i])
			ka.chs[i] = nil
			reqIdxs++
		}
		if reqIdxs == 0 {
			continue
		}
		// remove all channels that required a leader from keepalive
		newChs := make([]chan<- *LeaseKeepAliveResponse, len(ka.chs)-reqIdxs)
		newCtxs := make([]context.Context, len(newChs))
		newIdx := 0
		for i := range ka.chs {
			if ka.chs[i] == nil {
				continue
			}
			newChs[newIdx], newCtxs[newIdx] = ka.chs[i], ka.ctxs[newIdx]
			newIdx++
		}
		ka.chs, ka.ctxs = newChs, newCtxs
	}
}
func (l *lessor) resetRecv() (pb.Lease_LeaseKeepAliveClient, error) {
	sctx, cancel := context.WithCancel(l.stopCtx)
	stream, err := l.remote.LeaseKeepAlive(sctx, append(l.callOpts, withMax(0))...)
	if err != nil {
		cancel()
		return nil, err
	}

	l.mu.Lock()
	defer l.mu.Unlock()
	if l.stream != nil && l.streamCancel != nil {
		l.streamCancel()
	}

	l.streamCancel = cancel
	l.stream = stream

	go l.sendKeepAliveLoop(stream)
	return stream, nil
}
func (l *lessor) recvKeepAlive(resp *pb.LeaseKeepAliveResponse) {
	karesp := &LeaseKeepAliveResponse{
		ResponseHeader: resp.GetHeader(),
		ID:             LeaseID(resp.ID),
		TTL:            resp.TTL,
	}

	l.mu.Lock()
	defer l.mu.Unlock()

	ka, ok := l.keepAlives[karesp.ID]
	if !ok {
		return
	}

	if karesp.TTL <= 0 {
		// lease expired; close all keep alive channels
		delete(l.keepAlives, karesp.ID)
		ka.close()
		return
	}

	// send update to all channels
	nextKeepAlive := time.Now().Add((time.Duration(karesp.TTL) * time.Second) / 3.0)
	ka.deadline = time.Now().Add(time.Duration(karesp.TTL) * time.Second)
	for _, ch := range ka.chs {
		select {
		case ch <- karesp:
		default:
			if l.lg != nil {
				l.lg.Warn("lease keepalive response queue is full; dropping response send",
					zap.Int("queue-size", len(ch)),
					zap.Int("queue-capacity", cap(ch)),
				)
			}
		}
		// still advance in order to rate-limit keep-alive sends
		ka.nextKeepAlive = nextKeepAlive
	}
}
func (l *lessor) deadlineLoop() {
	for {
		select {
		case <-time.After(time.Second):
		case <-l.donec:
			return
		}
		now := time.Now()
		l.mu.Lock()
		for id, ka := range l.keepAlives {
			if ka.deadline.Before(now) {
				// waited too long for response; lease may be expired
				ka.close()
				delete(l.keepAlives, id)
			}
		}
		l.mu.Unlock()
	}
}
func (l *lessor) sendKeepAliveLoop(stream pb.Lease_LeaseKeepAliveClient) {
	for {
		var tosend []LeaseID

		now := time.Now()
		l.mu.Lock()
		for id, ka := range l.keepAlives {
			if ka.nextKeepAlive.Before(now) {
				tosend = append(tosend, id)
			}
		}
		l.mu.Unlock()

		for _, id := range tosend {
			r := &pb.LeaseKeepAliveRequest{ID: int64(id)}
			if err := stream.Send(r); err != nil {
				// TODO do something with this error?
				return
			}
		}

		select {
		case <-time.After(retryConnWait):
		case <-stream.Context().Done():
			return
		case <-l.donec:
			return
		case <-l.stopCtx.Done():
			return
		}
	}
}
func NewKV(cl *v3.Client, pfx string, opts ...concurrency.SessionOption) (v3.KV, func(), error) {
	cctx, cancel := context.WithCancel(cl.Ctx())
	lkv := &leasingKV{
		cl:          cl,
		kv:          cl.KV,
		pfx:         pfx,
		leases:      leaseCache{revokes: make(map[string]time.Time)},
		ctx:         cctx,
		cancel:      cancel,
		sessionOpts: opts,
		sessionc:    make(chan struct{}),
	}
	lkv.wg.Add(2)
	go func() {
		defer lkv.wg.Done()
		lkv.monitorSession()
	}()
	go func() {
		defer lkv.wg.Done()
		lkv.leases.clearOldRevokes(cctx)
	}()
	return lkv, lkv.Close, lkv.waitSession(cctx)
}
func (lkv *leasingKV) rescind(ctx context.Context, key string, rev int64) {
	if lkv.leases.Evict(key) > rev {
		return
	}
	cmp := v3.Compare(v3.CreateRevision(lkv.pfx+key), "<", rev)
	op := v3.OpDelete(lkv.pfx + key)
	for ctx.Err() == nil {
		if _, err := lkv.kv.Txn(ctx).If(cmp).Then(op).Commit(); err == nil {
			return
		}
	}
}
func LeaseValue(key string) Cmp {
	return Cmp{Key: []byte(key), Target: pb.Compare_LEASE}
}
func (cmp *Cmp) ValueBytes() []byte {
	if tu, ok := cmp.TargetUnion.(*pb.Compare_Value); ok {
		return tu.Value
	}
	return nil
}
func (cmp Cmp) WithRange(end string) Cmp {
	cmp.RangeEnd = []byte(end)
	return cmp
}
func (cmp Cmp) WithPrefix() Cmp {
	cmp.RangeEnd = getPrefix(cmp.Key)
	return cmp
}
func mustInt64(val interface{}) int64 {
	if v, ok := val.(int64); ok {
		return v
	}
	if v, ok := val.(int); ok {
		return int64(v)
	}
	panic("bad value")
}
func mustInt64orLeaseID(val interface{}) int64 {
	if v, ok := val.(LeaseID); ok {
		return int64(v)
	}
	return mustInt64(val)
}
func (gw *gRPCWatcher) Next() ([]*naming.Update, error) {
	if gw.wch == nil {
		// first Next() returns all addresses
		return gw.firstNext()
	}
	if gw.err != nil {
		return nil, gw.err
	}

	// process new events on target/*
	wr, ok := <-gw.wch
	if !ok {
		gw.err = status.Error(codes.Unavailable, ErrWatcherClosed.Error())
		return nil, gw.err
	}
	if gw.err = wr.Err(); gw.err != nil {
		return nil, gw.err
	}

	updates := make([]*naming.Update, 0, len(wr.Events))
	for _, e := range wr.Events {
		var jupdate naming.Update
		var err error
		switch e.Type {
		case etcd.EventTypePut:
			err = json.Unmarshal(e.Kv.Value, &jupdate)
			jupdate.Op = naming.Add
		case etcd.EventTypeDelete:
			err = json.Unmarshal(e.PrevKv.Value, &jupdate)
			jupdate.Op = naming.Delete
		default:
			continue
		}
		if err == nil {
			updates = append(updates, &jupdate)
		}
	}
	return updates, nil
}
func getJournalWriteSyncer() (zapcore.WriteSyncer, error) {
	jw, err := logutil.NewJournalWriter(os.Stderr)
	if err != nil {
		return nil, fmt.Errorf("can't find journal (%v)", err)
	}
	return zapcore.AddSync(jw), nil
}
func newKV(store *store, nodePath string, value string, createdIndex uint64, parent *node, expireTime time.Time) *node {
	return &node{
		Path:          nodePath,
		CreatedIndex:  createdIndex,
		ModifiedIndex: createdIndex,
		Parent:        parent,
		store:         store,
		ExpireTime:    expireTime,
		Value:         value,
	}
}
func newDir(store *store, nodePath string, createdIndex uint64, parent *node, expireTime time.Time) *node {
	return &node{
		Path:          nodePath,
		CreatedIndex:  createdIndex,
		ModifiedIndex: createdIndex,
		Parent:        parent,
		ExpireTime:    expireTime,
		Children:      make(map[string]*node),
		store:         store,
	}
}
func (n *node) Read() (string, *v2error.Error) {
	if n.IsDir() {
		return "", v2error.NewError(v2error.EcodeNotFile, "", n.store.CurrentIndex)
	}

	return n.Value, nil
}
func (n *node) Write(value string, index uint64) *v2error.Error {
	if n.IsDir() {
		return v2error.NewError(v2error.EcodeNotFile, "", n.store.CurrentIndex)
	}

	n.Value = value
	n.ModifiedIndex = index

	return nil
}
func (n *node) List() ([]*node, *v2error.Error) {
	if !n.IsDir() {
		return nil, v2error.NewError(v2error.EcodeNotDir, "", n.store.CurrentIndex)
	}

	nodes := make([]*node, len(n.Children))

	i := 0
	for _, node := range n.Children {
		nodes[i] = node
		i++
	}

	return nodes, nil
}
func (n *node) GetChild(name string) (*node, *v2error.Error) {
	if !n.IsDir() {
		return nil, v2error.NewError(v2error.EcodeNotDir, n.Path, n.store.CurrentIndex)
	}

	child, ok := n.Children[name]

	if ok {
		return child, nil
	}

	return nil, nil
}
func (n *node) Add(child *node) *v2error.Error {
	if !n.IsDir() {
		return v2error.NewError(v2error.EcodeNotDir, "", n.store.CurrentIndex)
	}

	_, name := path.Split(child.Path)

	if _, ok := n.Children[name]; ok {
		return v2error.NewError(v2error.EcodeNodeExist, "", n.store.CurrentIndex)
	}

	n.Children[name] = child

	return nil
}
func (n *node) Remove(dir, recursive bool, callback func(path string)) *v2error.Error {
	if !n.IsDir() { // key-value pair
		_, name := path.Split(n.Path)

		// find its parent and remove the node from the map
		if n.Parent != nil && n.Parent.Children[name] == n {
			delete(n.Parent.Children, name)
		}

		if callback != nil {
			callback(n.Path)
		}

		if !n.IsPermanent() {
			n.store.ttlKeyHeap.remove(n)
		}

		return nil
	}

	if !dir {
		// cannot delete a directory without dir set to true
		return v2error.NewError(v2error.EcodeNotFile, n.Path, n.store.CurrentIndex)
	}

	if len(n.Children) != 0 && !recursive {
		// cannot delete a directory if it is not empty and the operation
		// is not recursive
		return v2error.NewError(v2error.EcodeDirNotEmpty, n.Path, n.store.CurrentIndex)
	}

	for _, child := range n.Children { // delete all children
		child.Remove(true, true, callback)
	}

	// delete self
	_, name := path.Split(n.Path)
	if n.Parent != nil && n.Parent.Children[name] == n {
		delete(n.Parent.Children, name)

		if callback != nil {
			callback(n.Path)
		}

		if !n.IsPermanent() {
			n.store.ttlKeyHeap.remove(n)
		}
	}

	return nil
}
func (n *node) Compare(prevValue string, prevIndex uint64) (ok bool, which int) {
	indexMatch := prevIndex == 0 || n.ModifiedIndex == prevIndex
	valueMatch := prevValue == "" || n.Value == prevValue
	ok = valueMatch && indexMatch
	switch {
	case valueMatch && indexMatch:
		which = CompareMatch
	case indexMatch && !valueMatch:
		which = CompareValueNotMatch
	case valueMatch && !indexMatch:
		which = CompareIndexNotMatch
	default:
		which = CompareNotMatch
	}
	return ok, which
}
func (n *node) Clone() *node {
	if !n.IsDir() {
		newkv := newKV(n.store, n.Path, n.Value, n.CreatedIndex, n.Parent, n.ExpireTime)
		newkv.ModifiedIndex = n.ModifiedIndex
		return newkv
	}

	clone := newDir(n.store, n.Path, n.CreatedIndex, n.Parent, n.ExpireTime)
	clone.ModifiedIndex = n.ModifiedIndex

	for key, child := range n.Children {
		clone.Children[key] = child.Clone()
	}

	return clone
}
func isConnectedToQuorumSince(transport rafthttp.Transporter, since time.Time, self types.ID, members []*membership.Member) bool {
	return numConnectedSince(transport, since, self, members) >= (len(members)/2)+1
}
func isConnectedSince(transport rafthttp.Transporter, since time.Time, remote types.ID) bool {
	t := transport.ActiveSince(remote)
	return !t.IsZero() && t.Before(since)
}
func numConnectedSince(transport rafthttp.Transporter, since time.Time, self types.ID, members []*membership.Member) int {
	connectedNum := 0
	for _, m := range members {
		if m.ID == self || isConnectedSince(transport, since, m.ID) {
			connectedNum++
		}
	}
	return connectedNum
}
func longestConnected(tp rafthttp.Transporter, membs []types.ID) (types.ID, bool) {
	var longest types.ID
	var oldest time.Time
	for _, id := range membs {
		tm := tp.ActiveSince(id)
		if tm.IsZero() { // inactive
			continue
		}

		if oldest.IsZero() { // first longest candidate
			oldest = tm
			longest = id
		}

		if tm.Before(oldest) {
			oldest = tm
			longest = id
		}
	}
	if uint64(longest) == 0 {
		return longest, false
	}
	return longest, true
}
func (d *decoder) isTornEntry(data []byte) bool {
	if len(d.brs) != 1 {
		return false
	}

	fileOff := d.lastValidOff + frameSizeBytes
	curOff := 0
	chunks := [][]byte{}
	// split data on sector boundaries
	for curOff < len(data) {
		chunkLen := int(minSectorSize - (fileOff % minSectorSize))
		if chunkLen > len(data)-curOff {
			chunkLen = len(data) - curOff
		}
		chunks = append(chunks, data[curOff:curOff+chunkLen])
		fileOff += int64(chunkLen)
		curOff += chunkLen
	}

	// if any data for a sector chunk is all 0, it's a torn write
	for _, sect := range chunks {
		isZero := true
		for _, v := range sect {
			if v != 0 {
				isZero = false
				break
			}
		}
		if isZero {
			return true
		}
	}
	return false
}
func StartMockServersOnNetwork(count int, network string) (ms *MockServers, err error) {
	switch network {
	case "tcp":
		return startMockServersTcp(count)
	case "unix":
		return startMockServersUnix(count)
	default:
		return nil, fmt.Errorf("unsupported network type: %s", network)
	}
}
func (ms *MockServers) StartAt(idx int) (err error) {
	ms.mu.Lock()
	defer ms.mu.Unlock()

	if ms.Servers[idx].ln == nil {
		ms.Servers[idx].ln, err = net.Listen(ms.Servers[idx].Network, ms.Servers[idx].Address)
		if err != nil {
			return fmt.Errorf("failed to listen %v", err)
		}
	}

	svr := grpc.NewServer()
	pb.RegisterKVServer(svr, &mockKVServer{})
	ms.Servers[idx].GrpcServer = svr

	ms.wg.Add(1)
	go func(svr *grpc.Server, l net.Listener) {
		svr.Serve(l)
	}(ms.Servers[idx].GrpcServer, ms.Servers[idx].ln)
	return nil
}
func (ms *MockServers) StopAt(idx int) {
	ms.mu.Lock()
	defer ms.mu.Unlock()

	if ms.Servers[idx].ln == nil {
		return
	}

	ms.Servers[idx].GrpcServer.Stop()
	ms.Servers[idx].GrpcServer = nil
	ms.Servers[idx].ln = nil
	ms.wg.Done()
}
func (ms *MockServers) Stop() {
	for idx := range ms.Servers {
		ms.StopAt(idx)
	}
	ms.wg.Wait()
}
func NewCheckCommand() *cobra.Command {
	cc := &cobra.Command{
		Use:   "check <subcommand>",
		Short: "commands for checking properties of the etcd cluster",
	}

	cc.AddCommand(NewCheckPerfCommand())
	cc.AddCommand(NewCheckDatascaleCommand())

	return cc
}
func NewCheckPerfCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "perf [options]",
		Short: "Check the performance of the etcd cluster",
		Run:   newCheckPerfCommand,
	}

	// TODO: support customized configuration
	cmd.Flags().StringVar(&checkPerfLoad, "load", "s", "The performance check's workload model. Accepted workloads: s(small), m(medium), l(large), xl(xLarge)")
	cmd.Flags().StringVar(&checkPerfPrefix, "prefix", "/etcdctl-check-perf/", "The prefix for writing the performance check's keys.")
	cmd.Flags().BoolVar(&autoCompact, "auto-compact", false, "Compact storage with last revision after test is finished.")
	cmd.Flags().BoolVar(&autoDefrag, "auto-defrag", false, "Defragment storage after test is finished.")

	return cmd
}
func NewCheckDatascaleCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "datascale [options]",
		Short: "Check the memory usage of holding data for different workloads on a given server endpoint.",
		Long:  "If no endpoint is provided, localhost will be used. If multiple endpoints are provided, first endpoint will be used.",
		Run:   newCheckDatascaleCommand,
	}

	cmd.Flags().StringVar(&checkDatascaleLoad, "load", "s", "The datascale check's workload model. Accepted workloads: s(small), m(medium), l(large), xl(xLarge)")
	cmd.Flags().StringVar(&checkDatascalePrefix, "prefix", "/etcdctl-check-datascale/", "The prefix for writing the datascale check's keys.")
	cmd.Flags().BoolVar(&autoCompact, "auto-compact", false, "Compact storage with last revision after test is finished.")
	cmd.Flags().BoolVar(&autoDefrag, "auto-defrag", false, "Defragment storage after test is finished.")

	return cmd
}
func NewGetCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "get [options] <key> [range_end]",
		Short: "Gets the key or a range of keys",
		Run:   getCommandFunc,
	}

	cmd.Flags().StringVar(&getConsistency, "consistency", "l", "Linearizable(l) or Serializable(s)")
	cmd.Flags().StringVar(&getSortOrder, "order", "", "Order of results; ASCEND or DESCEND (ASCEND by default)")
	cmd.Flags().StringVar(&getSortTarget, "sort-by", "", "Sort target; CREATE, KEY, MODIFY, VALUE, or VERSION")
	cmd.Flags().Int64Var(&getLimit, "limit", 0, "Maximum number of results")
	cmd.Flags().BoolVar(&getPrefix, "prefix", false, "Get keys with matching prefix")
	cmd.Flags().BoolVar(&getFromKey, "from-key", false, "Get keys that are greater than or equal to the given key using byte compare")
	cmd.Flags().Int64Var(&getRev, "rev", 0, "Specify the kv revision")
	cmd.Flags().BoolVar(&getKeysOnly, "keys-only", false, "Get only the keys")
	cmd.Flags().BoolVar(&printValueOnly, "print-value-only", false, `Only write values when using the "simple" output format`)
	return cmd
}
func NewGetCommand() cli.Command {
	return cli.Command{
		Name:      "get",
		Usage:     "retrieve the value of a key",
		ArgsUsage: "<key>",
		Flags: []cli.Flag{
			cli.BoolFlag{Name: "sort", Usage: "returns result in sorted order"},
			cli.BoolFlag{Name: "quorum, q", Usage: "require quorum for get request"},
		},
		Action: func(c *cli.Context) error {
			getCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func (m *Member) PickPeerURL() string {
	if len(m.PeerURLs) == 0 {
		panic("member should always have some peer url")
	}
	return m.PeerURLs[rand.Intn(len(m.PeerURLs))]
}
func HandleMetricsHealth(mux *http.ServeMux, srv etcdserver.ServerV2) {
	mux.Handle(PathMetrics, promhttp.Handler())
	mux.Handle(PathHealth, NewHealthHandler(func() Health { return checkHealth(srv) }))
}
func NewRemoveCommand() cli.Command {
	return cli.Command{
		Name:      "rm",
		Usage:     "remove a key or a directory",
		ArgsUsage: "<key>",
		Flags: []cli.Flag{
			cli.BoolFlag{Name: "dir", Usage: "removes the key if it is an empty directory or a key-value pair"},
			cli.BoolFlag{Name: "recursive, r", Usage: "removes the key and all child keys(if it is a directory)"},
			cli.StringFlag{Name: "with-value", Value: "", Usage: "previous value"},
			cli.IntFlag{Name: "with-index", Value: 0, Usage: "previous index"},
		},
		Action: func(c *cli.Context) error {
			rmCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func rmCommandFunc(c *cli.Context, ki client.KeysAPI) {
	if len(c.Args()) == 0 {
		handleError(c, ExitBadArgs, errors.New("key required"))
	}
	key := c.Args()[0]
	recursive := c.Bool("recursive")
	dir := c.Bool("dir")
	prevValue := c.String("with-value")
	prevIndex := c.Int("with-index")

	ctx, cancel := contextWithTotalTimeout(c)
	resp, err := ki.Delete(ctx, key, &client.DeleteOptions{PrevIndex: uint64(prevIndex), PrevValue: prevValue, Dir: dir, Recursive: recursive})
	cancel()
	if err != nil {
		handleError(c, ExitServerError, err)
	}
	if !resp.Node.Dir || c.GlobalString("output") != "simple" {
		printResponseKey(resp, c.GlobalString("output"))
	}
}
func checkIntervals(reqs []*pb.RequestOp) (map[string]struct{}, adt.IntervalTree, error) {
	var dels adt.IntervalTree

	// collect deletes from this level; build first to check lower level overlapped puts
	for _, req := range reqs {
		tv, ok := req.Request.(*pb.RequestOp_RequestDeleteRange)
		if !ok {
			continue
		}
		dreq := tv.RequestDeleteRange
		if dreq == nil {
			continue
		}
		var iv adt.Interval
		if len(dreq.RangeEnd) != 0 {
			iv = adt.NewStringAffineInterval(string(dreq.Key), string(dreq.RangeEnd))
		} else {
			iv = adt.NewStringAffinePoint(string(dreq.Key))
		}
		dels.Insert(iv, struct{}{})
	}

	// collect children puts/deletes
	puts := make(map[string]struct{})
	for _, req := range reqs {
		tv, ok := req.Request.(*pb.RequestOp_RequestTxn)
		if !ok {
			continue
		}
		putsThen, delsThen, err := checkIntervals(tv.RequestTxn.Success)
		if err != nil {
			return nil, dels, err
		}
		putsElse, delsElse, err := checkIntervals(tv.RequestTxn.Failure)
		if err != nil {
			return nil, dels, err
		}
		for k := range putsThen {
			if _, ok := puts[k]; ok {
				return nil, dels, rpctypes.ErrGRPCDuplicateKey
			}
			if dels.Intersects(adt.NewStringAffinePoint(k)) {
				return nil, dels, rpctypes.ErrGRPCDuplicateKey
			}
			puts[k] = struct{}{}
		}
		for k := range putsElse {
			if _, ok := puts[k]; ok {
				// if key is from putsThen, overlap is OK since
				// either then/else are mutually exclusive
				if _, isSafe := putsThen[k]; !isSafe {
					return nil, dels, rpctypes.ErrGRPCDuplicateKey
				}
			}
			if dels.Intersects(adt.NewStringAffinePoint(k)) {
				return nil, dels, rpctypes.ErrGRPCDuplicateKey
			}
			puts[k] = struct{}{}
		}
		dels.Union(delsThen, adt.NewStringAffineInterval("\x00", ""))
		dels.Union(delsElse, adt.NewStringAffineInterval("\x00", ""))
	}

	// collect and check this level's puts
	for _, req := range reqs {
		tv, ok := req.Request.(*pb.RequestOp_RequestPut)
		if !ok || tv.RequestPut == nil {
			continue
		}
		k := string(tv.RequestPut.Key)
		if _, ok := puts[k]; ok {
			return nil, dels, rpctypes.ErrGRPCDuplicateKey
		}
		if dels.Intersects(adt.NewStringAffinePoint(k)) {
			return nil, dels, rpctypes.ErrGRPCDuplicateKey
		}
		puts[k] = struct{}{}
	}
	return puts, dels, nil
}
func ReportEventReceived(n int) {
	pendingEventsGauge.Sub(float64(n))
	totalEventsCounter.Add(float64(n))
}
func RegisterKVHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterKVHandlerClient(ctx, mux, etcdserverpb.NewKVClient(conn))
}
func RegisterWatchHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterWatchHandlerClient(ctx, mux, etcdserverpb.NewWatchClient(conn))
}
func RegisterLeaseHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterLeaseHandlerClient(ctx, mux, etcdserverpb.NewLeaseClient(conn))
}
func RegisterClusterHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterClusterHandlerClient(ctx, mux, etcdserverpb.NewClusterClient(conn))
}
func RegisterMaintenanceHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterMaintenanceHandlerClient(ctx, mux, etcdserverpb.NewMaintenanceClient(conn))
}
func RegisterAuthHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterAuthHandlerClient(ctx, mux, etcdserverpb.NewAuthClient(conn))
}
func startEtcd(cfg *embed.Config) (<-chan struct{}, <-chan error, error) {
	e, err := embed.StartEtcd(cfg)
	if err != nil {
		return nil, nil, err
	}
	osutil.RegisterInterruptHandler(e.Close)
	select {
	case <-e.Server.ReadyNotify(): // wait for e.Server to join the cluster
	case <-e.Server.StopNotify(): // publish aborted from 'ErrStopped'
	}
	return e.Server.StopNotify(), e.Err(), nil
}
func identifyDataDirOrDie(lg *zap.Logger, dir string) dirType {
	names, err := fileutil.ReadDir(dir)
	if err != nil {
		if os.IsNotExist(err) {
			return dirEmpty
		}
		if lg != nil {
			lg.Fatal("failed to list data directory", zap.String("dir", dir), zap.Error(err))
		} else {
			plog.Fatalf("error listing data dir: %s", dir)
		}
	}

	var m, p bool
	for _, name := range names {
		switch dirType(name) {
		case dirMember:
			m = true
		case dirProxy:
			p = true
		default:
			if lg != nil {
				lg.Warn(
					"found invalid file under data directory",
					zap.String("filename", name),
					zap.String("data-dir", dir),
				)
			} else {
				plog.Warningf("found invalid file/dir %s under data dir %s (Ignore this if you are upgrading etcd)", name, dir)
			}
		}
	}

	if m && p {
		if lg != nil {
			lg.Fatal("invalid datadir; both member and proxy directories exist")
		} else {
			plog.Fatal("invalid datadir. Both member and proxy directories exist.")
		}
	}
	if m {
		return dirMember
	}
	if p {
		return dirProxy
	}
	return dirEmpty
}
func openLast(lg *zap.Logger, dirpath string) (*fileutil.LockedFile, error) {
	names, err := readWALNames(lg, dirpath)
	if err != nil {
		return nil, err
	}
	last := filepath.Join(dirpath, names[len(names)-1])
	return fileutil.LockFile(last, os.O_RDWR, fileutil.PrivateFileMode)
}
func (l *leader) gotLeader() {
	l.mu.Lock()
	defer l.mu.Unlock()
	select {
	case <-l.leaderc:
		l.leaderc = make(chan struct{})
	default:
	}
}
func (l *leader) lostNotify() <-chan struct{} {
	l.mu.RLock()
	defer l.mu.RUnlock()
	return l.leaderc
}
func newGRPCProxyCommand() *cobra.Command {
	lpc := &cobra.Command{
		Use:   "grpc-proxy <subcommand>",
		Short: "grpc-proxy related command",
	}
	lpc.AddCommand(newGRPCProxyStartCommand())

	return lpc
}
func NewMemberCommand() *cobra.Command {
	mc := &cobra.Command{
		Use:   "member <subcommand>",
		Short: "Membership related commands",
	}

	mc.AddCommand(NewMemberAddCommand())
	mc.AddCommand(NewMemberRemoveCommand())
	mc.AddCommand(NewMemberUpdateCommand())
	mc.AddCommand(NewMemberListCommand())

	return mc
}
func NewMemberAddCommand() *cobra.Command {
	cc := &cobra.Command{
		Use:   "add <memberName> [options]",
		Short: "Adds a member into the cluster",

		Run: memberAddCommandFunc,
	}

	cc.Flags().StringVar(&memberPeerURLs, "peer-urls", "", "comma separated peer URLs for the new member.")

	return cc
}
func NewMemberRemoveCommand() *cobra.Command {
	cc := &cobra.Command{
		Use:   "remove <memberID>",
		Short: "Removes a member from the cluster",

		Run: memberRemoveCommandFunc,
	}

	return cc
}
func NewMemberUpdateCommand() *cobra.Command {
	cc := &cobra.Command{
		Use:   "update <memberID> [options]",
		Short: "Updates a member in the cluster",

		Run: memberUpdateCommandFunc,
	}

	cc.Flags().StringVar(&memberPeerURLs, "peer-urls", "", "comma separated peer URLs for the updated member.")

	return cc
}
func NewMemberListCommand() *cobra.Command {
	cc := &cobra.Command{
		Use:   "list",
		Short: "Lists all members in the cluster",
		Long: `When --write-out is set to simple, this command prints out comma-separated member lists for each endpoint.
The items in the lists are ID, Status, Name, Peer Addrs, Client Addrs.
`,

		Run: memberListCommandFunc,
	}

	return cc
}
func memberAddCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) < 1 {
		ExitWithError(ExitBadArgs, errors.New("member name not provided"))
	}
	if len(args) > 1 {
		ev := "too many arguments"
		for _, s := range args {
			if strings.HasPrefix(strings.ToLower(s), "http") {
				ev += fmt.Sprintf(`, did you mean --peer-urls=%s`, s)
			}
		}
		ExitWithError(ExitBadArgs, errors.New(ev))
	}
	newMemberName := args[0]

	if len(memberPeerURLs) == 0 {
		ExitWithError(ExitBadArgs, errors.New("member peer urls not provided"))
	}

	urls := strings.Split(memberPeerURLs, ",")
	ctx, cancel := commandCtx(cmd)
	cli := mustClientFromCmd(cmd)
	resp, err := cli.MemberAdd(ctx, urls)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	newID := resp.Member.ID

	display.MemberAdd(*resp)

	if _, ok := (display).(*simplePrinter); ok {
		ctx, cancel = commandCtx(cmd)
		listResp, err := cli.MemberList(ctx)
		// get latest member list; if there's failover new member might have outdated list
		for {
			if err != nil {
				ExitWithError(ExitError, err)
			}
			if listResp.Header.MemberId == resp.Header.MemberId {
				break
			}
			// quorum get to sync cluster list
			gresp, gerr := cli.Get(ctx, "_")
			if gerr != nil {
				ExitWithError(ExitError, err)
			}
			resp.Header.MemberId = gresp.Header.MemberId
			listResp, err = cli.MemberList(ctx)
		}
		cancel()

		conf := []string{}
		for _, memb := range listResp.Members {
			for _, u := range memb.PeerURLs {
				n := memb.Name
				if memb.ID == newID {
					n = newMemberName
				}
				conf = append(conf, fmt.Sprintf("%s=%s", n, u))
			}
		}

		fmt.Print("\n")
		fmt.Printf("ETCD_NAME=%q\n", newMemberName)
		fmt.Printf("ETCD_INITIAL_CLUSTER=%q\n", strings.Join(conf, ","))
		fmt.Printf("ETCD_INITIAL_ADVERTISE_PEER_URLS=%q\n", memberPeerURLs)
		fmt.Printf("ETCD_INITIAL_CLUSTER_STATE=\"existing\"\n")
	}
}
func memberRemoveCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("member ID is not provided"))
	}

	id, err := strconv.ParseUint(args[0], 16, 64)
	if err != nil {
		ExitWithError(ExitBadArgs, fmt.Errorf("bad member ID arg (%v), expecting ID in Hex", err))
	}

	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).MemberRemove(ctx, id)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	display.MemberRemove(id, *resp)
}
func memberUpdateCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("member ID is not provided"))
	}

	id, err := strconv.ParseUint(args[0], 16, 64)
	if err != nil {
		ExitWithError(ExitBadArgs, fmt.Errorf("bad member ID arg (%v), expecting ID in Hex", err))
	}

	if len(memberPeerURLs) == 0 {
		ExitWithError(ExitBadArgs, fmt.Errorf("member peer urls not provided"))
	}

	urls := strings.Split(memberPeerURLs, ",")

	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).MemberUpdate(ctx, id, urls)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.MemberUpdate(id, *resp)
}
func memberListCommandFunc(cmd *cobra.Command, args []string) {
	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).MemberList(ctx)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.MemberList(*resp)
}
func Open(lg *zap.Logger, dirpath string, snap walpb.Snapshot) (*WAL, error) {
	w, err := openAtIndex(lg, dirpath, snap, true)
	if err != nil {
		return nil, err
	}
	if w.dirFile, err = fileutil.OpenDir(w.dir); err != nil {
		return nil, err
	}
	return w, nil
}
func OpenForRead(lg *zap.Logger, dirpath string, snap walpb.Snapshot) (*WAL, error) {
	return openAtIndex(lg, dirpath, snap, false)
}
func Verify(lg *zap.Logger, walDir string, snap walpb.Snapshot) error {
	var metadata []byte
	var err error
	var match bool

	rec := &walpb.Record{}

	names, nameIndex, err := selectWALFiles(lg, walDir, snap)
	if err != nil {
		return err
	}

	// open wal files in read mode, so that there is no conflict
	// when the same WAL is opened elsewhere in write mode
	rs, _, closer, err := openWALFiles(lg, walDir, names, nameIndex, false)
	if err != nil {
		return err
	}

	// create a new decoder from the readers on the WAL files
	decoder := newDecoder(rs...)

	for err = decoder.decode(rec); err == nil; err = decoder.decode(rec) {
		switch rec.Type {
		case metadataType:
			if metadata != nil && !bytes.Equal(metadata, rec.Data) {
				return ErrMetadataConflict
			}
			metadata = rec.Data
		case crcType:
			crc := decoder.crc.Sum32()
			// Current crc of decoder must match the crc of the record.
			// We need not match 0 crc, since the decoder is a new one at this point.
			if crc != 0 && rec.Validate(crc) != nil {
				return ErrCRCMismatch
			}
			decoder.updateCRC(rec.Crc)
		case snapshotType:
			var loadedSnap walpb.Snapshot
			pbutil.MustUnmarshal(&loadedSnap, rec.Data)
			if loadedSnap.Index == snap.Index {
				if loadedSnap.Term != snap.Term {
					return ErrSnapshotMismatch
				}
				match = true
			}
		// We ignore all entry and state type records as these
		// are not necessary for validating the WAL contents
		case entryType:
		case stateType:
		default:
			return fmt.Errorf("unexpected block type %d", rec.Type)
		}
	}

	if closer != nil {
		closer()
	}

	// We do not have to read out all the WAL entries
	// as the decoder is opened in read mode.
	if err != io.EOF && err != io.ErrUnexpectedEOF {
		return err
	}

	if !match {
		return ErrSnapshotNotFound
	}

	return nil
}
func (w *WAL) Close() error {
	w.mu.Lock()
	defer w.mu.Unlock()

	if w.fp != nil {
		w.fp.Close()
		w.fp = nil
	}

	if w.tail() != nil {
		if err := w.sync(); err != nil {
			return err
		}
	}
	for _, l := range w.locks {
		if l == nil {
			continue
		}
		if err := l.Close(); err != nil {
			if w.lg != nil {
				w.lg.Warn("failed to close WAL", zap.Error(err))
			} else {
				plog.Errorf("failed to unlock during closing wal: %s", err)
			}
		}
	}

	return w.dirFile.Close()
}
func (w *watcher) notify(e *Event, originalPath bool, deleted bool) bool {
	// watcher is interested the path in three cases and under one condition
	// the condition is that the event happens after the watcher's sinceIndex

	// 1. the path at which the event happens is the path the watcher is watching at.
	// For example if the watcher is watching at "/foo" and the event happens at "/foo",
	// the watcher must be interested in that event.

	// 2. the watcher is a recursive watcher, it interests in the event happens after
	// its watching path. For example if watcher A watches at "/foo" and it is a recursive
	// one, it will interest in the event happens at "/foo/bar".

	// 3. when we delete a directory, we need to force notify all the watchers who watches
	// at the file we need to delete.
	// For example a watcher is watching at "/foo/bar". And we deletes "/foo". The watcher
	// should get notified even if "/foo" is not the path it is watching.
	if (w.recursive || originalPath || deleted) && e.Index() >= w.sinceIndex {
		// We cannot block here if the eventChan capacity is full, otherwise
		// etcd will hang. eventChan capacity is full when the rate of
		// notifications are higher than our send rate.
		// If this happens, we close the channel.
		select {
		case w.eventChan <- e:
		default:
			// We have missed a notification. Remove the watcher.
			// Removing the watcher also closes the eventChan.
			w.remove()
		}
		return true
	}
	return false
}
func (w *watcher) Remove() {
	w.hub.mutex.Lock()
	defer w.hub.mutex.Unlock()

	close(w.eventChan)
	if w.remove != nil {
		w.remove()
	}
}
func (s *v2v3Store) mkPathDepth(nodePath string, depth int) string {
	normalForm := path.Clean(path.Join("/", nodePath))
	n := strings.Count(normalForm, "/") + depth
	return fmt.Sprintf("%s/%03d/k/%s", s.pfx, n, normalForm)
}
func (s *v2v3Store) mkV2Node(kv *mvccpb.KeyValue) *v2store.NodeExtern {
	if kv == nil {
		return nil
	}
	n := &v2store.NodeExtern{
		Key:           s.mkNodePath(string(kv.Key)),
		Dir:           kv.Key[len(kv.Key)-1] == '/',
		CreatedIndex:  mkV2Rev(kv.CreateRevision),
		ModifiedIndex: mkV2Rev(kv.ModRevision),
	}
	if !n.Dir {
		v := string(kv.Value)
		n.Value = &v
	}
	return n
}
func prevKeyFromPuts(resp *clientv3.TxnResponse) *mvccpb.KeyValue {
	for _, r := range resp.Responses {
		pkv := r.GetResponsePut().PrevKv
		if pkv != nil && pkv.CreateRevision > 0 {
			return pkv
		}
	}
	return nil
}
func NewWeightedReport(r Report, precision string) Report {
	return &weightedReport{
		baseReport: r,
		report:     newReport(precision),
		results:    make(chan Result, 16),
	}
}
func NewURLsMapFromStringMap(m map[string]string, sep string) (URLsMap, error) {
	var err error
	um := URLsMap{}
	for k, v := range m {
		um[k], err = NewURLs(strings.Split(v, sep))
		if err != nil {
			return nil, err
		}
	}
	return um, nil
}
func (c URLsMap) String() string {
	var pairs []string
	for name, urls := range c {
		for _, url := range urls {
			pairs = append(pairs, fmt.Sprintf("%s=%s", name, url.String()))
		}
	}
	sort.Strings(pairs)
	return strings.Join(pairs, ",")
}
func (c URLsMap) URLs() []string {
	var urls []string
	for _, us := range c {
		for _, u := range us {
			urls = append(urls, u.String())
		}
	}
	sort.Strings(urls)
	return urls
}
func parse(s string) map[string][]string {
	m := make(map[string][]string)
	for s != "" {
		key := s
		if i := strings.IndexAny(key, ","); i >= 0 {
			key, s = key[:i], key[i+1:]
		} else {
			s = ""
		}
		if key == "" {
			continue
		}
		value := ""
		if i := strings.Index(key, "="); i >= 0 {
			key, value = key[:i], key[i+1:]
		}
		m[key] = append(m[key], value)
	}
	return m
}
func NewClientHandler(lg *zap.Logger, server etcdserver.ServerPeer, timeout time.Duration) http.Handler {
	mux := http.NewServeMux()
	etcdhttp.HandleBasic(mux, server)
	handleV2(lg, mux, server, timeout)
	return requestLogger(lg, mux)
}
func writeKeyEvent(w http.ResponseWriter, resp etcdserver.Response, noValueOnSuccess bool) error {
	ev := resp.Event
	if ev == nil {
		return errors.New("cannot write empty Event")
	}
	w.Header().Set("Content-Type", "application/json")
	w.Header().Set("X-Etcd-Index", fmt.Sprint(ev.EtcdIndex))
	w.Header().Set("X-Raft-Index", fmt.Sprint(resp.Index))
	w.Header().Set("X-Raft-Term", fmt.Sprint(resp.Term))

	if ev.IsCreated() {
		w.WriteHeader(http.StatusCreated)
	}

	ev = trimEventPrefix(ev, etcdserver.StoreKeysPrefix)
	if noValueOnSuccess &&
		(ev.Action == v2store.Set || ev.Action == v2store.CompareAndSwap ||
			ev.Action == v2store.Create || ev.Action == v2store.Update) {
		ev.Node = nil
		ev.PrevNode = nil
	}
	return json.NewEncoder(w).Encode(ev)
}
func writeKeyError(lg *zap.Logger, w http.ResponseWriter, err error) {
	if err == nil {
		return
	}
	switch e := err.(type) {
	case *v2error.Error:
		e.WriteTo(w)
	default:
		switch err {
		case etcdserver.ErrTimeoutDueToLeaderFail, etcdserver.ErrTimeoutDueToConnectionLost:
			if lg != nil {
				lg.Warn(
					"v2 response error",
					zap.String("internal-server-error", err.Error()),
				)
			} else {
				mlog.MergeError(err)
			}
		default:
			if lg != nil {
				lg.Warn(
					"unexpected v2 response error",
					zap.String("internal-server-error", err.Error()),
				)
			} else {
				mlog.MergeErrorf("got unexpected response error (%v)", err)
			}
		}
		ee := v2error.NewError(v2error.EcodeRaftInternal, err.Error(), 0)
		ee.WriteTo(w)
	}
}
func getUint64(form url.Values, key string) (i uint64, err error) {
	if vals, ok := form[key]; ok {
		i, err = strconv.ParseUint(vals[0], 10, 64)
	}
	return
}
func getBool(form url.Values, key string) (b bool, err error) {
	if vals, ok := form[key]; ok {
		b, err = strconv.ParseBool(vals[0])
	}
	return
}
func waitDeletes(ctx context.Context, client *v3.Client, pfx string, maxCreateRev int64) (*pb.ResponseHeader, error) {
	getOpts := append(v3.WithLastCreate(), v3.WithMaxCreateRev(maxCreateRev))
	for {
		resp, err := client.Get(ctx, pfx, getOpts...)
		if err != nil {
			return nil, err
		}
		if len(resp.Kvs) == 0 {
			return resp.Header, nil
		}
		lastKey := string(resp.Kvs[0].Key)
		if err = waitDelete(ctx, client, lastKey, resp.Header.Revision); err != nil {
			return nil, err
		}
	}
}
func AddOutputPaths(cfg zap.Config, outputPaths, errorOutputPaths []string) zap.Config {
	outputs := make(map[string]struct{})
	for _, v := range cfg.OutputPaths {
		outputs[v] = struct{}{}
	}
	for _, v := range outputPaths {
		outputs[v] = struct{}{}
	}
	outputSlice := make([]string, 0)
	if _, ok := outputs["/dev/null"]; ok {
		// "/dev/null" to discard all
		outputSlice = []string{"/dev/null"}
	} else {
		for k := range outputs {
			outputSlice = append(outputSlice, k)
		}
	}
	cfg.OutputPaths = outputSlice
	sort.Strings(cfg.OutputPaths)

	errOutputs := make(map[string]struct{})
	for _, v := range cfg.ErrorOutputPaths {
		errOutputs[v] = struct{}{}
	}
	for _, v := range errorOutputPaths {
		errOutputs[v] = struct{}{}
	}
	errOutputSlice := make([]string, 0)
	if _, ok := errOutputs["/dev/null"]; ok {
		// "/dev/null" to discard all
		errOutputSlice = []string{"/dev/null"}
	} else {
		for k := range errOutputs {
			errOutputSlice = append(errOutputSlice, k)
		}
	}
	cfg.ErrorOutputPaths = errOutputSlice
	sort.Strings(cfg.ErrorOutputPaths)

	return cfg
}
func NewConfig() *Config {
	lpurl, _ := url.Parse(DefaultListenPeerURLs)
	apurl, _ := url.Parse(DefaultInitialAdvertisePeerURLs)
	lcurl, _ := url.Parse(DefaultListenClientURLs)
	acurl, _ := url.Parse(DefaultAdvertiseClientURLs)
	cfg := &Config{
		MaxSnapFiles: DefaultMaxSnapshots,
		MaxWalFiles:  DefaultMaxWALs,

		Name: DefaultName,

		SnapshotCount:          etcdserver.DefaultSnapshotCount,
		SnapshotCatchUpEntries: etcdserver.DefaultSnapshotCatchUpEntries,

		MaxTxnOps:       DefaultMaxTxnOps,
		MaxRequestBytes: DefaultMaxRequestBytes,

		GRPCKeepAliveMinTime:  DefaultGRPCKeepAliveMinTime,
		GRPCKeepAliveInterval: DefaultGRPCKeepAliveInterval,
		GRPCKeepAliveTimeout:  DefaultGRPCKeepAliveTimeout,

		TickMs:                     100,
		ElectionMs:                 1000,
		InitialElectionTickAdvance: true,

		LPUrls: []url.URL{*lpurl},
		LCUrls: []url.URL{*lcurl},
		APUrls: []url.URL{*apurl},
		ACUrls: []url.URL{*acurl},

		ClusterState:        ClusterStateFlagNew,
		InitialClusterToken: "etcd-cluster",

		StrictReconfigCheck: DefaultStrictReconfigCheck,
		Metrics:             "basic",
		EnableV2:            DefaultEnableV2,

		CORS:          map[string]struct{}{"*": {}},
		HostWhitelist: map[string]struct{}{"*": {}},

		AuthToken:  "simple",
		BcryptCost: uint(bcrypt.DefaultCost),

		PreVote: false, // TODO: enable by default in v3.5

		loggerMu:            new(sync.RWMutex),
		logger:              nil,
		Logger:              "capnslog",
		DeprecatedLogOutput: []string{DefaultLogOutput},
		LogOutputs:          []string{DefaultLogOutput},
		Debug:               false,
		LogPkgLevels:        "",
	}
	cfg.InitialCluster = cfg.InitialClusterFromName(cfg.Name)
	return cfg
}
func (cfg *Config) PeerURLsMapAndToken(which string) (urlsmap types.URLsMap, token string, err error) {
	token = cfg.InitialClusterToken
	switch {
	case cfg.Durl != "":
		urlsmap = types.URLsMap{}
		// If using discovery, generate a temporary cluster based on
		// self's advertised peer URLs
		urlsmap[cfg.Name] = cfg.APUrls
		token = cfg.Durl

	case cfg.DNSCluster != "":
		clusterStrs, cerr := cfg.GetDNSClusterNames()
		lg := cfg.logger
		if cerr != nil {
			if lg != nil {
				lg.Warn("failed to resolve during SRV discovery", zap.Error(cerr))
			} else {
				plog.Errorf("couldn't resolve during SRV discovery (%v)", cerr)
			}
			return nil, "", cerr
		}
		for _, s := range clusterStrs {
			if lg != nil {
				lg.Info("got bootstrap from DNS for etcd-server", zap.String("node", s))
			} else {
				plog.Noticef("got bootstrap from DNS for etcd-server at %s", s)
			}
		}
		clusterStr := strings.Join(clusterStrs, ",")
		if strings.Contains(clusterStr, "https://") && cfg.PeerTLSInfo.TrustedCAFile == "" {
			cfg.PeerTLSInfo.ServerName = cfg.DNSCluster
		}
		urlsmap, err = types.NewURLsMap(clusterStr)
		// only etcd member must belong to the discovered cluster.
		// proxy does not need to belong to the discovered cluster.
		if which == "etcd" {
			if _, ok := urlsmap[cfg.Name]; !ok {
				return nil, "", fmt.Errorf("cannot find local etcd member %q in SRV records", cfg.Name)
			}
		}

	default:
		// We're statically configured, and cluster has appropriately been set.
		urlsmap, err = types.NewURLsMap(cfg.InitialCluster)
	}
	return urlsmap, token, err
}
func (cfg *Config) GetDNSClusterNames() ([]string, error) {
	var (
		clusterStrs       []string
		cerr              error
		serviceNameSuffix string
	)
	if cfg.DNSClusterServiceName != "" {
		serviceNameSuffix = "-" + cfg.DNSClusterServiceName
	}

	lg := cfg.GetLogger()

	// Use both etcd-server-ssl and etcd-server for discovery.
	// Combine the results if both are available.
	clusterStrs, cerr = srv.GetCluster("https", "etcd-server-ssl"+serviceNameSuffix, cfg.Name, cfg.DNSCluster, cfg.APUrls)
	if cerr != nil {
		clusterStrs = make([]string, 0)
	}
	if lg != nil {
		lg.Info(
			"get cluster for etcd-server-ssl SRV",
			zap.String("service-scheme", "https"),
			zap.String("service-name", "etcd-server-ssl"+serviceNameSuffix),
			zap.String("server-name", cfg.Name),
			zap.String("discovery-srv", cfg.DNSCluster),
			zap.Strings("advertise-peer-urls", cfg.getAPURLs()),
			zap.Strings("found-cluster", clusterStrs),
			zap.Error(cerr),
		)
	}

	defaultHTTPClusterStrs, httpCerr := srv.GetCluster("http", "etcd-server"+serviceNameSuffix, cfg.Name, cfg.DNSCluster, cfg.APUrls)
	if httpCerr != nil {
		clusterStrs = append(clusterStrs, defaultHTTPClusterStrs...)
	}
	if lg != nil {
		lg.Info(
			"get cluster for etcd-server SRV",
			zap.String("service-scheme", "http"),
			zap.String("service-name", "etcd-server"+serviceNameSuffix),
			zap.String("server-name", cfg.Name),
			zap.String("discovery-srv", cfg.DNSCluster),
			zap.Strings("advertise-peer-urls", cfg.getAPURLs()),
			zap.Strings("found-cluster", clusterStrs),
			zap.Error(httpCerr),
		)
	}

	return clusterStrs, cerr
}
func checkBindURLs(urls []url.URL) error {
	for _, url := range urls {
		if url.Scheme == "unix" || url.Scheme == "unixs" {
			continue
		}
		host, _, err := net.SplitHostPort(url.Host)
		if err != nil {
			return err
		}
		if host == "localhost" {
			// special case for local address
			// TODO: support /etc/hosts ?
			continue
		}
		if net.ParseIP(host) == nil {
			return fmt.Errorf("expected IP in URL for binding (%s)", url.String())
		}
	}
	return nil
}
func GetCluster(serviceScheme, service, name, dns string, apurls types.URLs) ([]string, error) {
	tempName := int(0)
	tcp2ap := make(map[string]url.URL)

	// First, resolve the apurls
	for _, url := range apurls {
		tcpAddr, err := resolveTCPAddr("tcp", url.Host)
		if err != nil {
			return nil, err
		}
		tcp2ap[tcpAddr.String()] = url
	}

	stringParts := []string{}
	updateNodeMap := func(service, scheme string) error {
		_, addrs, err := lookupSRV(service, "tcp", dns)
		if err != nil {
			return err
		}
		for _, srv := range addrs {
			port := fmt.Sprintf("%d", srv.Port)
			host := net.JoinHostPort(srv.Target, port)
			tcpAddr, terr := resolveTCPAddr("tcp", host)
			if terr != nil {
				err = terr
				continue
			}
			n := ""
			url, ok := tcp2ap[tcpAddr.String()]
			if ok {
				n = name
			}
			if n == "" {
				n = fmt.Sprintf("%d", tempName)
				tempName++
			}
			// SRV records have a trailing dot but URL shouldn't.
			shortHost := strings.TrimSuffix(srv.Target, ".")
			urlHost := net.JoinHostPort(shortHost, port)
			if ok && url.Scheme != scheme {
				err = fmt.Errorf("bootstrap at %s from DNS for %s has scheme mismatch with expected peer %s", scheme+"://"+urlHost, service, url.String())
			} else {
				stringParts = append(stringParts, fmt.Sprintf("%s=%s://%s", n, scheme, urlHost))
			}
		}
		if len(stringParts) == 0 {
			return err
		}
		return nil
	}

	err := updateNodeMap(service, serviceScheme)
	if err != nil {
		return nil, fmt.Errorf("error querying DNS SRV records for _%s %s", service, err)
	}
	return stringParts, nil
}
func GetClient(service, domain string, serviceName string) (*SRVClients, error) {
	var urls []*url.URL
	var srvs []*net.SRV

	updateURLs := func(service, scheme string) error {
		_, addrs, err := lookupSRV(service, "tcp", domain)
		if err != nil {
			return err
		}
		for _, srv := range addrs {
			urls = append(urls, &url.URL{
				Scheme: scheme,
				Host:   net.JoinHostPort(srv.Target, fmt.Sprintf("%d", srv.Port)),
			})
		}
		srvs = append(srvs, addrs...)
		return nil
	}

	errHTTPS := updateURLs(GetSRVService(service, serviceName, "https"), "https")
	errHTTP := updateURLs(GetSRVService(service, serviceName, "http"), "http")

	if errHTTPS != nil && errHTTP != nil {
		return nil, fmt.Errorf("dns lookup errors: %s and %s", errHTTPS, errHTTP)
	}

	endpoints := make([]string, len(urls))
	for i := range urls {
		endpoints[i] = urls[i].String()
	}
	return &SRVClients{Endpoints: endpoints, SRVs: srvs}, nil
}
func GetSRVService(service, serviceName string, scheme string) (SRVService string) {
	if scheme == "https" {
		service = fmt.Sprintf("%s-ssl", service)
	}

	if serviceName != "" {
		return fmt.Sprintf("%s-%s", service, serviceName)
	}
	return service
}
func ReadDir(d string, opts ...ReadDirOption) ([]string, error) {
	op := &ReadDirOp{}
	op.applyOpts(opts)

	dir, err := os.Open(d)
	if err != nil {
		return nil, err
	}
	defer dir.Close()

	names, err := dir.Readdirnames(-1)
	if err != nil {
		return nil, err
	}
	sort.Strings(names)

	if op.ext != "" {
		tss := make([]string, 0)
		for _, v := range names {
			if filepath.Ext(v) == op.ext {
				tss = append(tss, v)
			}
		}
		names = tss
	}
	return names, nil
}
func compact(c *v3.Client, rev int64) {
	fmt.Printf("Compacting with revision %d\n", rev)
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	_, err := c.Compact(ctx, rev, v3.WithCompactPhysical())
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	fmt.Printf("Compacted with revision %d\n", rev)
}
func defrag(c *v3.Client, ep string) {
	fmt.Printf("Defragmenting %q\n", ep)
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	_, err := c.Defragment(ctx, ep)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	fmt.Printf("Defragmented %q\n", ep)
}
func NewUserCommand() *cobra.Command {
	ac := &cobra.Command{
		Use:   "user <subcommand>",
		Short: "User related commands",
	}

	ac.AddCommand(newUserAddCommand())
	ac.AddCommand(newUserDeleteCommand())
	ac.AddCommand(newUserGetCommand())
	ac.AddCommand(newUserListCommand())
	ac.AddCommand(newUserChangePasswordCommand())
	ac.AddCommand(newUserGrantRoleCommand())
	ac.AddCommand(newUserRevokeRoleCommand())

	return ac
}
func userAddCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("user add command requires user name as its argument"))
	}

	var password string
	var user string

	if passwordFromFlag != "" {
		user = args[0]
		password = passwordFromFlag
	} else {
		splitted := strings.SplitN(args[0], ":", 2)
		if len(splitted) < 2 {
			user = args[0]
			if !passwordInteractive {
				fmt.Scanf("%s", &password)
			} else {
				password = readPasswordInteractive(args[0])
			}
		} else {
			user = splitted[0]
			password = splitted[1]
			if len(user) == 0 {
				ExitWithError(ExitBadArgs, fmt.Errorf("empty user name is not allowed"))
			}
		}
	}

	resp, err := mustClientFromCmd(cmd).Auth.UserAdd(context.TODO(), user, password)
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.UserAdd(user, *resp)
}
func userGetCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("user get command requires user name as its argument"))
	}

	name := args[0]
	client := mustClientFromCmd(cmd)
	resp, err := client.Auth.UserGet(context.TODO(), name)
	if err != nil {
		ExitWithError(ExitError, err)
	}

	if userShowDetail {
		fmt.Printf("User: %s\n", name)
		for _, role := range resp.Roles {
			fmt.Printf("\n")
			roleResp, err := client.Auth.RoleGet(context.TODO(), role)
			if err != nil {
				ExitWithError(ExitError, err)
			}
			display.RoleGet(role, *roleResp)
		}
	} else {
		display.UserGet(name, *resp)
	}
}
func userChangePasswordCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("user passwd command requires user name as its argument"))
	}

	var password string

	if !passwordInteractive {
		fmt.Scanf("%s", &password)
	} else {
		password = readPasswordInteractive(args[0])
	}

	resp, err := mustClientFromCmd(cmd).Auth.UserChangePassword(context.TODO(), args[0], password)
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.UserChangePassword(*resp)
}
func (eh *EventHistory) addEvent(e *Event) *Event {
	eh.rwl.Lock()
	defer eh.rwl.Unlock()

	eh.Queue.insert(e)

	eh.LastIndex = e.Index()

	eh.StartIndex = eh.Queue.Events[eh.Queue.Front].Index()

	return e
}
func (eh *EventHistory) scan(key string, recursive bool, index uint64) (*Event, *v2error.Error) {
	eh.rwl.RLock()
	defer eh.rwl.RUnlock()

	// index should be after the event history's StartIndex
	if index < eh.StartIndex {
		return nil,
			v2error.NewError(v2error.EcodeEventIndexCleared,
				fmt.Sprintf("the requested history has been cleared [%v/%v]",
					eh.StartIndex, index), 0)
	}

	// the index should come before the size of the queue minus the duplicate count
	if index > eh.LastIndex { // future index
		return nil, nil
	}

	offset := index - eh.StartIndex
	i := (eh.Queue.Front + int(offset)) % eh.Queue.Capacity

	for {
		e := eh.Queue.Events[i]

		if !e.Refresh {
			ok := e.Node.Key == key

			if recursive {
				// add tailing slash
				nkey := path.Clean(key)
				if nkey[len(nkey)-1] != '/' {
					nkey = nkey + "/"
				}

				ok = ok || strings.HasPrefix(e.Node.Key, nkey)
			}

			if (e.Action == Delete || e.Action == Expire) && e.PrevNode != nil && e.PrevNode.Dir {
				ok = ok || strings.HasPrefix(key, e.PrevNode.Key)
			}

			if ok {
				return e, nil
			}
		}

		i = (i + 1) % eh.Queue.Capacity

		if i == eh.Queue.Back {
			return nil, nil
		}
	}
}
func (eh *EventHistory) clone() *EventHistory {
	clonedQueue := eventQueue{
		Capacity: eh.Queue.Capacity,
		Events:   make([]*Event, eh.Queue.Capacity),
		Size:     eh.Queue.Size,
		Front:    eh.Queue.Front,
		Back:     eh.Queue.Back,
	}

	copy(clonedQueue.Events, eh.Queue.Events)
	return &EventHistory{
		StartIndex: eh.StartIndex,
		Queue:      clonedQueue,
		LastIndex:  eh.LastIndex,
	}

}
func openSnapshotBackend(cfg ServerConfig, ss *snap.Snapshotter, snapshot raftpb.Snapshot) (backend.Backend, error) {
	snapPath, err := ss.DBFilePath(snapshot.Metadata.Index)
	if err != nil {
		return nil, fmt.Errorf("failed to find database snapshot file (%v)", err)
	}
	if err := os.Rename(snapPath, cfg.backendPath()); err != nil {
		return nil, fmt.Errorf("failed to rename database snapshot file (%v)", err)
	}
	return openBackend(cfg), nil
}
func openBackend(cfg ServerConfig) backend.Backend {
	fn := cfg.backendPath()

	now, beOpened := time.Now(), make(chan backend.Backend)
	go func() {
		beOpened <- newBackend(cfg)
	}()

	select {
	case be := <-beOpened:
		if cfg.Logger != nil {
			cfg.Logger.Info("opened backend db", zap.String("path", fn), zap.Duration("took", time.Since(now)))
		}
		return be

	case <-time.After(10 * time.Second):
		if cfg.Logger != nil {
			cfg.Logger.Info(
				"db file is flocked by another process, or taking too long",
				zap.String("path", fn),
				zap.Duration("took", time.Since(now)),
			)
		} else {
			plog.Warningf("another etcd process is using %q and holds the file lock, or loading backend file is taking >10 seconds", fn)
			plog.Warningf("waiting for it to exit before starting...")
		}
	}

	return <-beOpened
}
func recoverSnapshotBackend(cfg ServerConfig, oldbe backend.Backend, snapshot raftpb.Snapshot) (backend.Backend, error) {
	var cIndex consistentIndex
	kv := mvcc.New(cfg.Logger, oldbe, &lease.FakeLessor{}, &cIndex)
	defer kv.Close()
	if snapshot.Metadata.Index <= kv.ConsistentIndex() {
		return oldbe, nil
	}
	oldbe.Close()
	return openSnapshotBackend(cfg, snap.New(cfg.Logger, cfg.SnapDir()), snapshot)
}
func NewUpdateCommand() cli.Command {
	return cli.Command{
		Name:      "update",
		Usage:     "update an existing key with a given value",
		ArgsUsage: "<key> <value>",
		Flags: []cli.Flag{
			cli.IntFlag{Name: "ttl", Value: 0, Usage: "key time-to-live in seconds"},
		},
		Action: func(c *cli.Context) error {
			updateCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func updateCommandFunc(c *cli.Context, ki client.KeysAPI) {
	if len(c.Args()) == 0 {
		handleError(c, ExitBadArgs, errors.New("key required"))
	}
	key := c.Args()[0]
	value, err := argOrStdin(c.Args(), os.Stdin, 1)
	if err != nil {
		handleError(c, ExitBadArgs, errors.New("value required"))
	}

	ttl := c.Int("ttl")

	ctx, cancel := contextWithTotalTimeout(c)
	resp, err := ki.Set(ctx, key, value, &client.SetOptions{TTL: time.Duration(ttl) * time.Second, PrevExist: client.PrevExist})
	cancel()
	if err != nil {
		handleError(c, ExitServerError, err)
	}

	printResponseKey(resp, c.GlobalString("output"))
}
func (q *statsQueue) frontAndBack() (*RequestStats, *RequestStats) {
	q.rwl.RLock()
	defer q.rwl.RUnlock()
	if q.size != 0 {
		return q.items[q.front], q.items[q.back]
	}
	return nil, nil
}
func (q *statsQueue) Insert(p *RequestStats) {
	q.rwl.Lock()
	defer q.rwl.Unlock()

	q.back = (q.back + 1) % queueCapacity

	if q.size == queueCapacity { //dequeue
		q.totalReqSize -= q.items[q.front].Size
		q.front = (q.back + 1) % queueCapacity
	} else {
		q.size++
	}

	q.items[q.back] = p
	q.totalReqSize += q.items[q.back].Size

}
func (q *statsQueue) Rate() (float64, float64) {
	front, back := q.frontAndBack()

	if front == nil || back == nil {
		return 0, 0
	}

	if time.Since(back.SendingTime) > time.Second {
		q.Clear()
		return 0, 0
	}

	sampleDuration := back.SendingTime.Sub(front.SendingTime)

	pr := float64(q.Len()) / float64(sampleDuration) * float64(time.Second)

	br := float64(q.ReqSize()) / float64(sampleDuration) * float64(time.Second)

	return pr, br
}
func (q *statsQueue) Clear() {
	q.rwl.Lock()
	defer q.rwl.Unlock()
	q.back = -1
	q.front = 0
	q.size = 0
	q.totalReqSize = 0
}
func UniqueStrings(slen uint, n int) (ss []string) {
	exist := make(map[string]struct{})
	ss = make([]string, 0, n)
	for len(ss) < n {
		s := randString(slen)
		if _, ok := exist[s]; !ok {
			ss = append(ss, s)
			exist[s] = struct{}{}
		}
	}
	return ss
}
func RandomStrings(slen uint, n int) (ss []string) {
	ss = make([]string, 0, n)
	for i := 0; i < n; i++ {
		ss = append(ss, randString(slen))
	}
	return ss
}
func IsKeyNotFound(err error) bool {
	if cErr, ok := err.(Error); ok {
		return cErr.Code == ErrorCodeKeyNotFound
	}
	return false
}
func IsRoleNotFound(err error) bool {
	if ae, ok := err.(authError); ok {
		return roleNotFoundRegExp.MatchString(ae.Message)
	}
	return false
}
func IsUserNotFound(err error) bool {
	if ae, ok := err.(authError); ok {
		return userNotFoundRegExp.MatchString(ae.Message)
	}
	return false
}
func JoinCluster(lg *zap.Logger, durl, dproxyurl string, id types.ID, config string) (string, error) {
	d, err := newDiscovery(lg, durl, dproxyurl, id)
	if err != nil {
		return "", err
	}
	return d.joinCluster(config)
}
func GetCluster(lg *zap.Logger, durl, dproxyurl string) (string, error) {
	d, err := newDiscovery(lg, durl, dproxyurl, 0)
	if err != nil {
		return "", err
	}
	return d.getCluster()
}
func newProxyFunc(lg *zap.Logger, proxy string) (func(*http.Request) (*url.URL, error), error) {
	if proxy == "" {
		return nil, nil
	}
	// Do a small amount of URL sanitization to help the user
	// Derived from net/http.ProxyFromEnvironment
	proxyURL, err := url.Parse(proxy)
	if err != nil || !strings.HasPrefix(proxyURL.Scheme, "http") {
		// proxy was bogus. Try prepending "http://" to it and
		// see if that parses correctly. If not, we ignore the
		// error and complain about the original one
		var err2 error
		proxyURL, err2 = url.Parse("http://" + proxy)
		if err2 == nil {
			err = nil
		}
	}
	if err != nil {
		return nil, fmt.Errorf("invalid proxy address %q: %v", proxy, err)
	}

	if lg != nil {
		lg.Info("running proxy with discovery", zap.String("proxy-url", proxyURL.String()))
	} else {
		plog.Infof("using proxy %q", proxyURL.String())
	}
	return http.ProxyURL(proxyURL), nil
}
func isSafeRetry(lg *zap.Logger, err error, callOpts *options) bool {
	if isContextError(err) {
		return false
	}
	switch callOpts.retryPolicy {
	case repeatable:
		return isSafeRetryImmutableRPC(err)
	case nonRepeatable:
		return isSafeRetryMutableRPC(err)
	default:
		lg.Warn("unrecognized retry policy", zap.String("retryPolicy", callOpts.retryPolicy.String()))
		return false
	}
}
func withRetryPolicy(rp retryPolicy) retryOption {
	return retryOption{applyFunc: func(o *options) {
		o.retryPolicy = rp
	}}
}
func withAuthRetry(retryAuth bool) retryOption {
	return retryOption{applyFunc: func(o *options) {
		o.retryAuth = retryAuth
	}}
}
func withMax(maxRetries uint) retryOption {
	return retryOption{applyFunc: func(o *options) {
		o.max = maxRetries
	}}
}
func withBackoff(bf backoffFunc) retryOption {
	return retryOption{applyFunc: func(o *options) {
		o.backoffFunc = bf
	}}
}
func (ss *ServerStats) RecvAppendReq(leader string, reqSize int) {
	ss.Lock()
	defer ss.Unlock()

	now := time.Now()

	ss.State = raft.StateFollower
	if leader != ss.LeaderInfo.Name {
		ss.LeaderInfo.Name = leader
		ss.LeaderInfo.StartTime = now
	}

	ss.recvRateQueue.Insert(
		&RequestStats{
			SendingTime: now,
			Size:        reqSize,
		},
	)
	ss.RecvAppendRequestCnt++
}
func (ss *ServerStats) SendAppendReq(reqSize int) {
	ss.Lock()
	defer ss.Unlock()

	ss.becomeLeader()

	ss.sendRateQueue.Insert(
		&RequestStats{
			SendingTime: time.Now(),
			Size:        reqSize,
		},
	)

	ss.SendAppendRequestCnt++
}
func (bb *bucketBuffer) merge(bbsrc *bucketBuffer) {
	for i := 0; i < bbsrc.used; i++ {
		bb.add(bbsrc.buf[i].key, bbsrc.buf[i].val)
	}
	if bb.used == bbsrc.used {
		return
	}
	if bytes.Compare(bb.buf[(bb.used-bbsrc.used)-1].key, bbsrc.buf[0].key) < 0 {
		return
	}

	sort.Stable(bb)

	// remove duplicates, using only newest update
	widx := 0
	for ridx := 1; ridx < bb.used; ridx++ {
		if !bytes.Equal(bb.buf[ridx].key, bb.buf[widx].key) {
			widx++
		}
		bb.buf[widx] = bb.buf[ridx]
	}
	bb.used = widx + 1
}
func deleteRevKey(kv v3.KV, key string, rev int64) (bool, error) {
	cmp := v3.Compare(v3.ModRevision(key), "=", rev)
	req := v3.OpDelete(key)
	txnresp, err := kv.Txn(context.TODO()).If(cmp).Then(req).Commit()
	if err != nil {
		return false, err
	} else if !txnresp.Succeeded {
		return false, nil
	}
	return true, nil
}
func isMemberBootstrapped(lg *zap.Logger, cl *membership.RaftCluster, member string, rt http.RoundTripper, timeout time.Duration) bool {
	rcl, err := getClusterFromRemotePeers(lg, getRemotePeerURLs(cl, member), timeout, false, rt)
	if err != nil {
		return false
	}
	id := cl.MemberByName(member).ID
	m := rcl.Member(id)
	if m == nil {
		return false
	}
	if len(m.ClientURLs) > 0 {
		return true
	}
	return false
}
func GetClusterFromRemotePeers(lg *zap.Logger, urls []string, rt http.RoundTripper) (*membership.RaftCluster, error) {
	return getClusterFromRemotePeers(lg, urls, 10*time.Second, true, rt)
}
func getClusterFromRemotePeers(lg *zap.Logger, urls []string, timeout time.Duration, logerr bool, rt http.RoundTripper) (*membership.RaftCluster, error) {
	cc := &http.Client{
		Transport: rt,
		Timeout:   timeout,
	}
	for _, u := range urls {
		addr := u + "/members"
		resp, err := cc.Get(addr)
		if err != nil {
			if logerr {
				if lg != nil {
					lg.Warn("failed to get cluster response", zap.String("address", addr), zap.Error(err))
				} else {
					plog.Warningf("could not get cluster response from %s: %v", u, err)
				}
			}
			continue
		}
		b, err := ioutil.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			if logerr {
				if lg != nil {
					lg.Warn("failed to read body of cluster response", zap.String("address", addr), zap.Error(err))
				} else {
					plog.Warningf("could not read the body of cluster response: %v", err)
				}
			}
			continue
		}
		var membs []*membership.Member
		if err = json.Unmarshal(b, &membs); err != nil {
			if logerr {
				if lg != nil {
					lg.Warn("failed to unmarshal cluster response", zap.String("address", addr), zap.Error(err))
				} else {
					plog.Warningf("could not unmarshal cluster response: %v", err)
				}
			}
			continue
		}
		id, err := types.IDFromString(resp.Header.Get("X-Etcd-Cluster-ID"))
		if err != nil {
			if logerr {
				if lg != nil {
					lg.Warn(
						"failed to parse cluster ID",
						zap.String("address", addr),
						zap.String("header", resp.Header.Get("X-Etcd-Cluster-ID")),
						zap.Error(err),
					)
				} else {
					plog.Warningf("could not parse the cluster ID from cluster res: %v", err)
				}
			}
			continue
		}

		// check the length of membership members
		// if the membership members are present then prepare and return raft cluster
		// if membership members are not present then the raft cluster formed will be
		// an invalid empty cluster hence return failed to get raft cluster member(s) from the given urls error
		if len(membs) > 0 {
			return membership.NewClusterFromMembers(lg, "", id, membs), nil
		}
		return nil, fmt.Errorf("failed to get raft cluster member(s) from the given URLs")
	}
	return nil, fmt.Errorf("could not retrieve cluster information from the given URLs")
}
func getRemotePeerURLs(cl *membership.RaftCluster, local string) []string {
	us := make([]string, 0)
	for _, m := range cl.Members() {
		if m.Name == local {
			continue
		}
		us = append(us, m.PeerURLs...)
	}
	sort.Strings(us)
	return us
}
func getVersions(lg *zap.Logger, cl *membership.RaftCluster, local types.ID, rt http.RoundTripper) map[string]*version.Versions {
	members := cl.Members()
	vers := make(map[string]*version.Versions)
	for _, m := range members {
		if m.ID == local {
			cv := "not_decided"
			if cl.Version() != nil {
				cv = cl.Version().String()
			}
			vers[m.ID.String()] = &version.Versions{Server: version.Version, Cluster: cv}
			continue
		}
		ver, err := getVersion(lg, m, rt)
		if err != nil {
			if lg != nil {
				lg.Warn("failed to get version", zap.String("remote-member-id", m.ID.String()), zap.Error(err))
			} else {
				plog.Warningf("cannot get the version of member %s (%v)", m.ID, err)
			}
			vers[m.ID.String()] = nil
		} else {
			vers[m.ID.String()] = ver
		}
	}
	return vers
}
func decideClusterVersion(lg *zap.Logger, vers map[string]*version.Versions) *semver.Version {
	var cv *semver.Version
	lv := semver.Must(semver.NewVersion(version.Version))

	for mid, ver := range vers {
		if ver == nil {
			return nil
		}
		v, err := semver.NewVersion(ver.Server)
		if err != nil {
			if lg != nil {
				lg.Warn(
					"failed to parse server version of remote member",
					zap.String("remote-member-id", mid),
					zap.String("remote-member-version", ver.Server),
					zap.Error(err),
				)
			} else {
				plog.Errorf("cannot understand the version of member %s (%v)", mid, err)
			}
			return nil
		}
		if lv.LessThan(*v) {
			if lg != nil {
				lg.Warn(
					"leader found higher-versioned member",
					zap.String("local-member-version", lv.String()),
					zap.String("remote-member-id", mid),
					zap.String("remote-member-version", ver.Server),
				)
			} else {
				plog.Warningf("the local etcd version %s is not up-to-date", lv.String())
				plog.Warningf("member %s has a higher version %s", mid, ver.Server)
			}
		}
		if cv == nil {
			cv = v
		} else if v.LessThan(*cv) {
			cv = v
		}
	}
	return cv
}
func getVersion(lg *zap.Logger, m *membership.Member, rt http.RoundTripper) (*version.Versions, error) {
	cc := &http.Client{
		Transport: rt,
	}
	var (
		err  error
		resp *http.Response
	)

	for _, u := range m.PeerURLs {
		addr := u + "/version"
		resp, err = cc.Get(addr)
		if err != nil {
			if lg != nil {
				lg.Warn(
					"failed to reach the peer URL",
					zap.String("address", addr),
					zap.String("remote-member-id", m.ID.String()),
					zap.Error(err),
				)
			} else {
				plog.Warningf("failed to reach the peerURL(%s) of member %s (%v)", u, m.ID, err)
			}
			continue
		}
		var b []byte
		b, err = ioutil.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			if lg != nil {
				lg.Warn(
					"failed to read body of response",
					zap.String("address", addr),
					zap.String("remote-member-id", m.ID.String()),
					zap.Error(err),
				)
			} else {
				plog.Warningf("failed to read out the response body from the peerURL(%s) of member %s (%v)", u, m.ID, err)
			}
			continue
		}
		var vers version.Versions
		if err = json.Unmarshal(b, &vers); err != nil {
			if lg != nil {
				lg.Warn(
					"failed to unmarshal response",
					zap.String("address", addr),
					zap.String("remote-member-id", m.ID.String()),
					zap.Error(err),
				)
			} else {
				plog.Warningf("failed to unmarshal the response body got from the peerURL(%s) of member %s (%v)", u, m.ID, err)
			}
			continue
		}
		return &vers, nil
	}
	return nil, err
}
func NewTimeoutDetector(maxDuration time.Duration) *TimeoutDetector {
	return &TimeoutDetector{
		maxDuration: maxDuration,
		records:     make(map[uint64]time.Time),
	}
}
func (td *TimeoutDetector) Reset() {
	td.mu.Lock()
	defer td.mu.Unlock()

	td.records = make(map[uint64]time.Time)
}
func (td *TimeoutDetector) Observe(which uint64) (bool, time.Duration) {
	td.mu.Lock()
	defer td.mu.Unlock()

	ok := true
	now := time.Now()
	exceed := time.Duration(0)

	if pt, found := td.records[which]; found {
		exceed = now.Sub(pt) - td.maxDuration
		if exceed > 0 {
			ok = false
		}
	}
	td.records[which] = now
	return ok, exceed
}
func NewPeerHandler(lg *zap.Logger, s etcdserver.ServerPeer) http.Handler {
	return newPeerHandler(lg, s.Cluster(), s.RaftHandler(), s.LeaseHandler())
}
func (ki *keyIndex) put(lg *zap.Logger, main int64, sub int64) {
	rev := revision{main: main, sub: sub}

	if !rev.GreaterThan(ki.modified) {
		if lg != nil {
			lg.Panic(
				"'put' with an unexpected smaller revision",
				zap.Int64("given-revision-main", rev.main),
				zap.Int64("given-revision-sub", rev.sub),
				zap.Int64("modified-revision-main", ki.modified.main),
				zap.Int64("modified-revision-sub", ki.modified.sub),
			)
		} else {
			plog.Panicf("store.keyindex: put with unexpected smaller revision [%v / %v]", rev, ki.modified)
		}
	}
	if len(ki.generations) == 0 {
		ki.generations = append(ki.generations, generation{})
	}
	g := &ki.generations[len(ki.generations)-1]
	if len(g.revs) == 0 { // create a new key
		keysGauge.Inc()
		g.created = rev
	}
	g.revs = append(g.revs, rev)
	g.ver++
	ki.modified = rev
}
func (ki *keyIndex) tombstone(lg *zap.Logger, main int64, sub int64) error {
	if ki.isEmpty() {
		if lg != nil {
			lg.Panic(
				"'tombstone' got an unexpected empty keyIndex",
				zap.String("key", string(ki.key)),
			)
		} else {
			plog.Panicf("store.keyindex: unexpected tombstone on empty keyIndex %s", string(ki.key))
		}
	}
	if ki.generations[len(ki.generations)-1].isEmpty() {
		return ErrRevisionNotFound
	}
	ki.put(lg, main, sub)
	ki.generations = append(ki.generations, generation{})
	keysGauge.Dec()
	return nil
}
func (ki *keyIndex) get(lg *zap.Logger, atRev int64) (modified, created revision, ver int64, err error) {
	if ki.isEmpty() {
		if lg != nil {
			lg.Panic(
				"'get' got an unexpected empty keyIndex",
				zap.String("key", string(ki.key)),
			)
		} else {
			plog.Panicf("store.keyindex: unexpected get on empty keyIndex %s", string(ki.key))
		}
	}
	g := ki.findGeneration(atRev)
	if g.isEmpty() {
		return revision{}, revision{}, 0, ErrRevisionNotFound
	}

	n := g.walk(func(rev revision) bool { return rev.main > atRev })
	if n != -1 {
		return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil
	}

	return revision{}, revision{}, 0, ErrRevisionNotFound
}
func (ki *keyIndex) since(lg *zap.Logger, rev int64) []revision {
	if ki.isEmpty() {
		if lg != nil {
			lg.Panic(
				"'since' got an unexpected empty keyIndex",
				zap.String("key", string(ki.key)),
			)
		} else {
			plog.Panicf("store.keyindex: unexpected get on empty keyIndex %s", string(ki.key))
		}
	}
	since := revision{rev, 0}
	var gi int
	// find the generations to start checking
	for gi = len(ki.generations) - 1; gi > 0; gi-- {
		g := ki.generations[gi]
		if g.isEmpty() {
			continue
		}
		if since.GreaterThan(g.created) {
			break
		}
	}

	var revs []revision
	var last int64
	for ; gi < len(ki.generations); gi++ {
		for _, r := range ki.generations[gi].revs {
			if since.GreaterThan(r) {
				continue
			}
			if r.main == last {
				// replace the revision with a new one that has higher sub value,
				// because the original one should not be seen by external
				revs[len(revs)-1] = r
				continue
			}
			revs = append(revs, r)
			last = r.main
		}
	}
	return revs
}
func (ki *keyIndex) keep(atRev int64, available map[revision]struct{}) {
	if ki.isEmpty() {
		return
	}

	genIdx, revIndex := ki.doCompact(atRev, available)
	g := &ki.generations[genIdx]
	if !g.isEmpty() {
		// remove any tombstone
		if revIndex == len(g.revs)-1 && genIdx != len(ki.generations)-1 {
			delete(available, g.revs[revIndex])
		}
	}
}
func (ki *keyIndex) findGeneration(rev int64) *generation {
	lastg := len(ki.generations) - 1
	cg := lastg

	for cg >= 0 {
		if len(ki.generations[cg].revs) == 0 {
			cg--
			continue
		}
		g := ki.generations[cg]
		if cg != lastg {
			if tomb := g.revs[len(g.revs)-1].main; tomb <= rev {
				return nil
			}
		}
		if g.revs[0].main <= rev {
			return &ki.generations[cg]
		}
		cg--
	}
	return nil
}
func (s *watchableStore) cancelWatcher(wa *watcher) {
	for {
		s.mu.Lock()
		if s.unsynced.delete(wa) {
			slowWatcherGauge.Dec()
			break
		} else if s.synced.delete(wa) {
			break
		} else if wa.compacted {
			break
		} else if wa.ch == nil {
			// already canceled (e.g., cancel/close race)
			break
		}

		if !wa.victim {
			panic("watcher not victim but not in watch groups")
		}

		var victimBatch watcherBatch
		for _, wb := range s.victims {
			if wb[wa] != nil {
				victimBatch = wb
				break
			}
		}
		if victimBatch != nil {
			slowWatcherGauge.Dec()
			delete(victimBatch, wa)
			break
		}

		// victim being processed so not accessible; retry
		s.mu.Unlock()
		time.Sleep(time.Millisecond)
	}

	watcherGauge.Dec()
	wa.ch = nil
	s.mu.Unlock()
}
func (s *watchableStore) syncWatchersLoop() {
	defer s.wg.Done()

	for {
		s.mu.RLock()
		st := time.Now()
		lastUnsyncedWatchers := s.unsynced.size()
		s.mu.RUnlock()

		unsyncedWatchers := 0
		if lastUnsyncedWatchers > 0 {
			unsyncedWatchers = s.syncWatchers()
		}
		syncDuration := time.Since(st)

		waitDuration := 100 * time.Millisecond
		// more work pending?
		if unsyncedWatchers != 0 && lastUnsyncedWatchers > unsyncedWatchers {
			// be fair to other store operations by yielding time taken
			waitDuration = syncDuration
		}

		select {
		case <-time.After(waitDuration):
		case <-s.stopc:
			return
		}
	}
}
func (s *watchableStore) syncVictimsLoop() {
	defer s.wg.Done()

	for {
		for s.moveVictims() != 0 {
			// try to update all victim watchers
		}
		s.mu.RLock()
		isEmpty := len(s.victims) == 0
		s.mu.RUnlock()

		var tickc <-chan time.Time
		if !isEmpty {
			tickc = time.After(10 * time.Millisecond)
		}

		select {
		case <-tickc:
		case <-s.victimc:
		case <-s.stopc:
			return
		}
	}
}
func (s *watchableStore) moveVictims() (moved int) {
	s.mu.Lock()
	victims := s.victims
	s.victims = nil
	s.mu.Unlock()

	var newVictim watcherBatch
	for _, wb := range victims {
		// try to send responses again
		for w, eb := range wb {
			// watcher has observed the store up to, but not including, w.minRev
			rev := w.minRev - 1
			if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: rev}) {
				pendingEventsGauge.Add(float64(len(eb.evs)))
			} else {
				if newVictim == nil {
					newVictim = make(watcherBatch)
				}
				newVictim[w] = eb
				continue
			}
			moved++
		}

		// assign completed victim watchers to unsync/sync
		s.mu.Lock()
		s.store.revMu.RLock()
		curRev := s.store.currentRev
		for w, eb := range wb {
			if newVictim != nil && newVictim[w] != nil {
				// couldn't send watch response; stays victim
				continue
			}
			w.victim = false
			if eb.moreRev != 0 {
				w.minRev = eb.moreRev
			}
			if w.minRev <= curRev {
				s.unsynced.add(w)
			} else {
				slowWatcherGauge.Dec()
				s.synced.add(w)
			}
		}
		s.store.revMu.RUnlock()
		s.mu.Unlock()
	}

	if len(newVictim) > 0 {
		s.mu.Lock()
		s.victims = append(s.victims, newVictim)
		s.mu.Unlock()
	}

	return moved
}
func kvsToEvents(lg *zap.Logger, wg *watcherGroup, revs, vals [][]byte) (evs []mvccpb.Event) {
	for i, v := range vals {
		var kv mvccpb.KeyValue
		if err := kv.Unmarshal(v); err != nil {
			if lg != nil {
				lg.Panic("failed to unmarshal mvccpb.KeyValue", zap.Error(err))
			} else {
				plog.Panicf("cannot unmarshal event: %v", err)
			}
		}

		if !wg.contains(string(kv.Key)) {
			continue
		}

		ty := mvccpb.PUT
		if isTombstone(revs[i]) {
			ty = mvccpb.DELETE
			// patch in mod revision so watchers won't skip
			kv.ModRevision = bytesToRev(revs[i]).main
		}
		evs = append(evs, mvccpb.Event{Kv: &kv, Type: ty})
	}
	return evs
}
func (s *watchableStore) notify(rev int64, evs []mvccpb.Event) {
	var victim watcherBatch
	for w, eb := range newWatcherBatch(&s.synced, evs) {
		if eb.revs != 1 {
			if s.store != nil && s.store.lg != nil {
				s.store.lg.Panic(
					"unexpected multiple revisions in watch notification",
					zap.Int("number-of-revisions", eb.revs),
				)
			} else {
				plog.Panicf("unexpected multiple revisions in notification")
			}
		}
		if w.send(WatchResponse{WatchID: w.id, Events: eb.evs, Revision: rev}) {
			pendingEventsGauge.Add(float64(len(eb.evs)))
		} else {
			// move slow watcher to victims
			w.minRev = rev + 1
			if victim == nil {
				victim = make(watcherBatch)
			}
			w.victim = true
			victim[w] = eb
			s.synced.delete(w)
			slowWatcherGauge.Inc()
		}
	}
	s.addVictim(victim)
}
func isOpFuncCalled(op string, opts []OpOption) bool {
	for _, opt := range opts {
		v := reflect.ValueOf(opt)
		if v.Kind() == reflect.Func {
			if opFunc := runtime.FuncForPC(v.Pointer()); opFunc != nil {
				if strings.Contains(opFunc.Name(), op) {
					return true
				}
			}
		}
	}
	return false
}
func (t *batchTx) UnsafePut(bucketName []byte, key []byte, value []byte) {
	t.unsafePut(bucketName, key, value, false)
}
func (t *batchTx) UnsafeSeqPut(bucketName []byte, key []byte, value []byte) {
	t.unsafePut(bucketName, key, value, true)
}
func (t *batchTx) UnsafeRange(bucketName, key, endKey []byte, limit int64) ([][]byte, [][]byte) {
	bucket := t.tx.Bucket(bucketName)
	if bucket == nil {
		if t.backend.lg != nil {
			t.backend.lg.Fatal(
				"failed to find a bucket",
				zap.String("bucket-name", string(bucketName)),
			)
		} else {
			plog.Fatalf("bucket %s does not exist", bucketName)
		}
	}
	return unsafeRange(bucket.Cursor(), key, endKey, limit)
}
func (t *batchTx) UnsafeDelete(bucketName []byte, key []byte) {
	bucket := t.tx.Bucket(bucketName)
	if bucket == nil {
		if t.backend.lg != nil {
			t.backend.lg.Fatal(
				"failed to find a bucket",
				zap.String("bucket-name", string(bucketName)),
			)
		} else {
			plog.Fatalf("bucket %s does not exist", bucketName)
		}
	}
	err := bucket.Delete(key)
	if err != nil {
		if t.backend.lg != nil {
			t.backend.lg.Fatal(
				"failed to delete a key",
				zap.String("bucket-name", string(bucketName)),
				zap.Error(err),
			)
		} else {
			plog.Fatalf("cannot delete key from bucket (%v)", err)
		}
	}
	t.pending++
}
func (t *batchTx) UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error {
	return unsafeForEach(t.tx, bucketName, visitor)
}
func (t *batchTx) Commit() {
	t.Lock()
	t.commit(false)
	t.Unlock()
}
func (t *batchTx) CommitAndStop() {
	t.Lock()
	t.commit(true)
	t.Unlock()
}
func (le *lessor) Renew(id LeaseID) (int64, error) {
	le.mu.RLock()
	if !le.isPrimary() {
		// forward renew request to primary instead of returning error.
		le.mu.RUnlock()
		return -1, ErrNotPrimary
	}

	demotec := le.demotec

	l := le.leaseMap[id]
	if l == nil {
		le.mu.RUnlock()
		return -1, ErrLeaseNotFound
	}
	// Clear remaining TTL when we renew if it is set
	clearRemainingTTL := le.cp != nil && l.remainingTTL > 0

	le.mu.RUnlock()
	if l.expired() {
		select {
		// A expired lease might be pending for revoking or going through
		// quorum to be revoked. To be accurate, renew request must wait for the
		// deletion to complete.
		case <-l.revokec:
			return -1, ErrLeaseNotFound
		// The expired lease might fail to be revoked if the primary changes.
		// The caller will retry on ErrNotPrimary.
		case <-demotec:
			return -1, ErrNotPrimary
		case <-le.stopC:
			return -1, ErrNotPrimary
		}
	}

	// Clear remaining TTL when we renew if it is set
	// By applying a RAFT entry only when the remainingTTL is already set, we limit the number
	// of RAFT entries written per lease to a max of 2 per checkpoint interval.
	if clearRemainingTTL {
		le.cp(context.Background(), &pb.LeaseCheckpointRequest{Checkpoints: []*pb.LeaseCheckpoint{{ID: int64(l.ID), Remaining_TTL: 0}}})
	}

	le.mu.Lock()
	l.refresh(0)
	item := &LeaseWithTime{id: l.ID, time: l.expiry.UnixNano()}
	heap.Push(&le.leaseHeap, item)
	le.mu.Unlock()

	leaseRenewed.Inc()
	return l.ttl, nil
}
func (le *lessor) Attach(id LeaseID, items []LeaseItem) error {
	le.mu.Lock()
	defer le.mu.Unlock()

	l := le.leaseMap[id]
	if l == nil {
		return ErrLeaseNotFound
	}

	l.mu.Lock()
	for _, it := range items {
		l.itemSet[it] = struct{}{}
		le.itemMap[it] = id
	}
	l.mu.Unlock()
	return nil
}
func (le *lessor) revokeExpiredLeases() {
	var ls []*Lease

	// rate limit
	revokeLimit := leaseRevokeRate / 2

	le.mu.RLock()
	if le.isPrimary() {
		ls = le.findExpiredLeases(revokeLimit)
	}
	le.mu.RUnlock()

	if len(ls) != 0 {
		select {
		case <-le.stopC:
			return
		case le.expiredC <- ls:
		default:
			// the receiver of expiredC is probably busy handling
			// other stuff
			// let's try this next time after 500ms
		}
	}
}
func (le *lessor) checkpointScheduledLeases() {
	var cps []*pb.LeaseCheckpoint

	// rate limit
	for i := 0; i < leaseCheckpointRate/2; i++ {
		le.mu.Lock()
		if le.isPrimary() {
			cps = le.findDueScheduledCheckpoints(maxLeaseCheckpointBatchSize)
		}
		le.mu.Unlock()

		if len(cps) != 0 {
			le.cp(context.Background(), &pb.LeaseCheckpointRequest{Checkpoints: cps})
		}
		if len(cps) < maxLeaseCheckpointBatchSize {
			return
		}
	}
}
func (le *lessor) expireExists() (l *Lease, ok bool, next bool) {
	if le.leaseHeap.Len() == 0 {
		return nil, false, false
	}

	item := le.leaseHeap[0]
	l = le.leaseMap[item.id]
	if l == nil {
		// lease has expired or been revoked
		// no need to revoke (nothing is expiry)
		heap.Pop(&le.leaseHeap) // O(log N)
		return nil, false, true
	}

	if time.Now().UnixNano() < item.time /* expiration time */ {
		// Candidate expirations are caught up, reinsert this item
		// and no need to revoke (nothing is expiry)
		return l, false, false
	}
	// if the lease is actually expired, add to the removal list. If it is not expired, we can ignore it because another entry will have been inserted into the heap

	heap.Pop(&le.leaseHeap) // O(log N)
	return l, true, false
}
func (le *lessor) findExpiredLeases(limit int) []*Lease {
	leases := make([]*Lease, 0, 16)

	for {
		l, ok, next := le.expireExists()
		if !ok && !next {
			break
		}
		if !ok {
			continue
		}
		if next {
			continue
		}

		if l.expired() {
			leases = append(leases, l)

			// reach expired limit
			if len(leases) == limit {
				break
			}
		}
	}

	return leases
}
func (l *Lease) refresh(extend time.Duration) {
	newExpiry := time.Now().Add(extend + time.Duration(l.RemainingTTL())*time.Second)
	l.expiryMu.Lock()
	defer l.expiryMu.Unlock()
	l.expiry = newExpiry
}
func (l *Lease) forever() {
	l.expiryMu.Lock()
	defer l.expiryMu.Unlock()
	l.expiry = forever
}
func (l *Lease) Keys() []string {
	l.mu.RLock()
	keys := make([]string, 0, len(l.itemSet))
	for k := range l.itemSet {
		keys = append(keys, k.Key)
	}
	l.mu.RUnlock()
	return keys
}
func (l *Lease) Remaining() time.Duration {
	l.expiryMu.RLock()
	defer l.expiryMu.RUnlock()
	if l.expiry.IsZero() {
		return time.Duration(math.MaxInt64)
	}
	return time.Until(l.expiry)
}
func NewCompactionCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "compaction [options] <revision>",
		Short: "Compacts the event history in etcd",
		Run:   compactionCommandFunc,
	}
	cmd.Flags().BoolVar(&compactPhysical, "physical", false, "'true' to wait for compaction to physically remove all old revisions")
	return cmd
}
func compactionCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("compaction command needs 1 argument"))
	}

	rev, err := strconv.ParseInt(args[0], 10, 64)
	if err != nil {
		ExitWithError(ExitError, err)
	}

	var opts []clientv3.CompactOption
	if compactPhysical {
		opts = append(opts, clientv3.WithCompactPhysical())
	}

	c := mustClientFromCmd(cmd)
	ctx, cancel := commandCtx(cmd)
	_, cerr := c.Compact(ctx, rev, opts...)
	cancel()
	if cerr != nil {
		ExitWithError(ExitError, cerr)
	}
	fmt.Println("compacted revision", rev)
}
func NewPutCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "put [options] <key> <value> (<value> can also be given from stdin)",
		Short: "Puts the given key into the store",
		Long: `
Puts the given key into the store.

When <value> begins with '-', <value> is interpreted as a flag.
Insert '--' for workaround:

$ put <key> -- <value>
$ put -- <key> <value>

If <value> isn't given as a command line argument and '--ignore-value' is not specified,
this command tries to read the value from standard input.

If <lease> isn't given as a command line argument and '--ignore-lease' is not specified,
this command tries to read the value from standard input.

For example,
$ cat file | put <key>
will store the content of the file to <key>.
`,
		Run: putCommandFunc,
	}
	cmd.Flags().StringVar(&leaseStr, "lease", "0", "lease ID (in hexadecimal) to attach to the key")
	cmd.Flags().BoolVar(&putPrevKV, "prev-kv", false, "return the previous key-value pair before modification")
	cmd.Flags().BoolVar(&putIgnoreVal, "ignore-value", false, "updates the key using its current value")
	cmd.Flags().BoolVar(&putIgnoreLease, "ignore-lease", false, "updates the key using its current lease")
	return cmd
}
func putCommandFunc(cmd *cobra.Command, args []string) {
	key, value, opts := getPutOp(args)

	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).Put(ctx, key, value, opts...)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	display.Put(*resp)
}
func NewHandler(t *http.Transport, urlsFunc GetProxyURLs, failureWait time.Duration, refreshInterval time.Duration) http.Handler {
	if t.TLSClientConfig != nil {
		// Enable http2, see Issue 5033.
		err := http2.ConfigureTransport(t)
		if err != nil {
			plog.Infof("Error enabling Transport HTTP/2 support: %v", err)
		}
	}

	p := &reverseProxy{
		director:  newDirector(urlsFunc, failureWait, refreshInterval),
		transport: t,
	}

	mux := http.NewServeMux()
	mux.Handle("/", p)
	mux.HandleFunc("/v2/config/local/proxy", p.configHandler)

	return mux
}
func NewReadonlyHandler(hdlr http.Handler) http.Handler {
	readonly := readonlyHandlerFunc(hdlr)
	return http.HandlerFunc(readonly)
}
func NewSetCommand() cli.Command {
	return cli.Command{
		Name:      "set",
		Usage:     "set the value of a key",
		ArgsUsage: "<key> <value>",
		Description: `Set sets the value of a key.

   When <value> begins with '-', <value> is interpreted as a flag.
   Insert '--' for workaround:

   $ set -- <key> <value>`,
		Flags: []cli.Flag{
			cli.IntFlag{Name: "ttl", Value: 0, Usage: "key time-to-live in seconds"},
			cli.StringFlag{Name: "swap-with-value", Value: "", Usage: "previous value"},
			cli.IntFlag{Name: "swap-with-index", Value: 0, Usage: "previous index"},
		},
		Action: func(c *cli.Context) error {
			setCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func setCommandFunc(c *cli.Context, ki client.KeysAPI) {
	if len(c.Args()) == 0 {
		handleError(c, ExitBadArgs, errors.New("key required"))
	}
	key := c.Args()[0]
	value, err := argOrStdin(c.Args(), os.Stdin, 1)
	if err != nil {
		handleError(c, ExitBadArgs, errors.New("value required"))
	}

	ttl := c.Int("ttl")
	prevValue := c.String("swap-with-value")
	prevIndex := c.Int("swap-with-index")

	ctx, cancel := contextWithTotalTimeout(c)
	resp, err := ki.Set(ctx, key, value, &client.SetOptions{TTL: time.Duration(ttl) * time.Second, PrevIndex: uint64(prevIndex), PrevValue: prevValue})
	cancel()
	if err != nil {
		handleError(c, ExitServerError, err)
	}

	printResponseKey(resp, c.GlobalString("output"))
}
func (rwm *RWMutex) waitOnLastRev(pfx string) (bool, error) {
	client := rwm.s.Client()
	// get key that's blocking myKey
	opts := append(v3.WithLastRev(), v3.WithMaxModRev(rwm.myKey.Revision()-1))
	lastKey, err := client.Get(rwm.ctx, pfx, opts...)
	if err != nil {
		return false, err
	}
	if len(lastKey.Kvs) == 0 {
		return true, nil
	}
	// wait for release on blocking key
	_, err = WaitEvents(
		client,
		string(lastKey.Kvs[0].Key),
		rwm.myKey.Revision(),
		[]mvccpb.Event_EventType{mvccpb.DELETE})
	return false, err
}
func GetDefaultInterfaces() (map[string]uint8, error) {
	return nil, fmt.Errorf("default host not supported on %s_%s", runtime.GOOS, runtime.GOARCH)
}
func NewSnapshotCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "snapshot <subcommand>",
		Short: "Manages etcd node snapshots",
	}
	cmd.AddCommand(NewSnapshotSaveCommand())
	cmd.AddCommand(NewSnapshotRestoreCommand())
	cmd.AddCommand(newSnapshotStatusCommand())
	return cmd
}
func NewMoveLeaderCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "move-leader <transferee-member-id>",
		Short: "Transfers leadership to another etcd cluster member.",
		Run:   transferLeadershipCommandFunc,
	}
	return cmd
}
func transferLeadershipCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 1 {
		ExitWithError(ExitBadArgs, fmt.Errorf("move-leader command needs 1 argument"))
	}
	target, err := strconv.ParseUint(args[0], 16, 64)
	if err != nil {
		ExitWithError(ExitBadArgs, err)
	}

	c := mustClientFromCmd(cmd)
	eps := c.Endpoints()
	c.Close()

	ctx, cancel := commandCtx(cmd)

	// find current leader
	var leaderCli *clientv3.Client
	var leaderID uint64
	for _, ep := range eps {
		cfg := clientConfigFromCmd(cmd)
		cfg.endpoints = []string{ep}
		cli := cfg.mustClient()
		resp, serr := cli.Status(ctx, ep)
		if serr != nil {
			ExitWithError(ExitError, serr)
		}

		if resp.Header.GetMemberId() == resp.Leader {
			leaderCli = cli
			leaderID = resp.Leader
			break
		}
		cli.Close()
	}
	if leaderCli == nil {
		ExitWithError(ExitBadArgs, fmt.Errorf("no leader endpoint given at %v", eps))
	}

	var resp *clientv3.MoveLeaderResponse
	resp, err = leaderCli.MoveLeader(ctx, target)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.MoveLeader(leaderID, target, *resp)
}
func OpenDir(path string) (*os.File, error) {
	fd, err := openDir(path)
	if err != nil {
		return nil, err
	}
	return os.NewFile(uintptr(fd), path), nil
}
func NewRemoveDirCommand() cli.Command {
	return cli.Command{
		Name:      "rmdir",
		Usage:     "removes the key if it is an empty directory or a key-value pair",
		ArgsUsage: "<key>",
		Action: func(c *cli.Context) error {
			rmdirCommandFunc(c, mustNewKeyAPI(c))
			return nil
		},
	}
}
func rmdirCommandFunc(c *cli.Context, ki client.KeysAPI) {
	if len(c.Args()) == 0 {
		handleError(c, ExitBadArgs, errors.New("key required"))
	}
	key := c.Args()[0]

	ctx, cancel := contextWithTotalTimeout(c)
	resp, err := ki.Delete(ctx, key, &client.DeleteOptions{Dir: true})
	cancel()
	if err != nil {
		handleError(c, ExitServerError, err)
	}

	if !resp.Node.Dir || c.GlobalString("output") != "simple" {
		printResponseKey(resp, c.GlobalString("output"))
	}
}
func NewDelCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "del [options] <key> [range_end]",
		Short: "Removes the specified key or range of keys [key, range_end)",
		Run:   delCommandFunc,
	}

	cmd.Flags().BoolVar(&delPrefix, "prefix", false, "delete keys with matching prefix")
	cmd.Flags().BoolVar(&delPrevKV, "prev-kv", false, "return deleted key-value pairs")
	cmd.Flags().BoolVar(&delFromKey, "from-key", false, "delete keys that are greater than or equal to the given key using byte compare")
	return cmd
}
func delCommandFunc(cmd *cobra.Command, args []string) {
	key, opts := getDelOp(args)
	ctx, cancel := commandCtx(cmd)
	resp, err := mustClientFromCmd(cmd).Delete(ctx, key, opts...)
	cancel()
	if err != nil {
		ExitWithError(ExitError, err)
	}
	display.Del(*resp)
}
func NewExpect(name string, arg ...string) (ep *ExpectProcess, err error) {
	// if env[] is nil, use current system env
	return NewExpectWithEnv(name, arg, nil)
}
func NewExpectWithEnv(name string, args []string, env []string) (ep *ExpectProcess, err error) {
	cmd := exec.Command(name, args...)
	cmd.Env = env
	ep = &ExpectProcess{
		cmd:        cmd,
		StopSignal: syscall.SIGKILL,
	}
	ep.cond = sync.NewCond(&ep.mu)
	ep.cmd.Stderr = ep.cmd.Stdout
	ep.cmd.Stdin = nil

	if ep.fpty, err = pty.Start(ep.cmd); err != nil {
		return nil, err
	}

	ep.wg.Add(1)
	go ep.read()
	return ep, nil
}
func (ep *ExpectProcess) ExpectFunc(f func(string) bool) (string, error) {
	ep.mu.Lock()
	for {
		for len(ep.lines) == 0 && ep.err == nil {
			ep.cond.Wait()
		}
		if len(ep.lines) == 0 {
			break
		}
		l := ep.lines[0]
		ep.lines = ep.lines[1:]
		if f(l) {
			ep.mu.Unlock()
			return l, nil
		}
	}
	ep.mu.Unlock()
	return "", ep.err
}
func (ep *ExpectProcess) Expect(s string) (string, error) {
	return ep.ExpectFunc(func(txt string) bool { return strings.Contains(txt, s) })
}
func (ep *ExpectProcess) LineCount() int {
	ep.mu.Lock()
	defer ep.mu.Unlock()
	return ep.count
}
func (ep *ExpectProcess) Signal(sig os.Signal) error {
	return ep.cmd.Process.Signal(sig)
}
func keyFunc(req *pb.RangeRequest) string {
	// TODO: use marshalTo to reduce allocation
	b, err := req.Marshal()
	if err != nil {
		panic(err)
	}
	return string(b)
}
func (c *cache) Add(req *pb.RangeRequest, resp *pb.RangeResponse) {
	key := keyFunc(req)

	c.mu.Lock()
	defer c.mu.Unlock()

	if req.Revision > c.compactedRev {
		c.lru.Add(key, resp)
	}
	// we do not need to invalidate a request with a revision specified.
	// so we do not need to add it into the reverse index.
	if req.Revision != 0 {
		return
	}

	var (
		iv  *adt.IntervalValue
		ivl adt.Interval
	)
	if len(req.RangeEnd) != 0 {
		ivl = adt.NewStringAffineInterval(string(req.Key), string(req.RangeEnd))
	} else {
		ivl = adt.NewStringAffinePoint(string(req.Key))
	}

	iv = c.cachedRanges.Find(ivl)

	if iv == nil {
		val := map[string]struct{}{key: {}}
		c.cachedRanges.Insert(ivl, val)
	} else {
		val := iv.Val.(map[string]struct{})
		val[key] = struct{}{}
		iv.Val = val
	}
}
func (c *cache) Get(req *pb.RangeRequest) (*pb.RangeResponse, error) {
	key := keyFunc(req)

	c.mu.Lock()
	defer c.mu.Unlock()

	if req.Revision > 0 && req.Revision < c.compactedRev {
		c.lru.Remove(key)
		return nil, ErrCompacted
	}

	if resp, ok := c.lru.Get(key); ok {
		return resp.(*pb.RangeResponse), nil
	}
	return nil, errors.New("not exist")
}
func (c *cache) Invalidate(key, endkey []byte) {
	c.mu.Lock()
	defer c.mu.Unlock()

	var (
		ivs []*adt.IntervalValue
		ivl adt.Interval
	)
	if len(endkey) == 0 {
		ivl = adt.NewStringAffinePoint(string(key))
	} else {
		ivl = adt.NewStringAffineInterval(string(key), string(endkey))
	}

	ivs = c.cachedRanges.Stab(ivl)
	for _, iv := range ivs {
		keys := iv.Val.(map[string]struct{})
		for key := range keys {
			c.lru.Remove(key)
		}
	}
	// delete after removing all keys since it is destructive to 'ivs'
	c.cachedRanges.Delete(ivl)
}
func (c *cache) Compact(revision int64) {
	c.mu.Lock()
	defer c.mu.Unlock()

	if revision > c.compactedRev {
		c.compactedRev = revision
	}
}
func NewUniqueURLsWithExceptions(s string, exceptions ...string) *UniqueURLs {
	us := &UniqueURLs{Values: make(map[string]struct{}), Allowed: make(map[string]struct{})}
	for _, v := range exceptions {
		us.Allowed[v] = struct{}{}
	}
	if s == "" {
		return us
	}
	if err := us.Set(s); err != nil {
		plog.Panicf("new UniqueURLs should never fail: %v", err)
	}
	return us
}
func UniqueURLsFromFlag(fs *flag.FlagSet, urlsFlagName string) []url.URL {
	return (*fs.Lookup(urlsFlagName).Value.(*UniqueURLs)).uss
}
func UniqueURLsMapFromFlag(fs *flag.FlagSet, urlsFlagName string) map[string]struct{} {
	return (*fs.Lookup(urlsFlagName).Value.(*UniqueURLs)).Values
}
func (b *Barrier) Hold() error {
	_, err := newKey(b.client, b.key, v3.NoLease)
	return err
}
func (b *Barrier) Release() error {
	_, err := b.client.Delete(b.ctx, b.key)
	return err
}
func (b *Barrier) Wait() error {
	resp, err := b.client.Get(b.ctx, b.key, v3.WithFirstKey()...)
	if err != nil {
		return err
	}
	if len(resp.Kvs) == 0 {
		// key already removed
		return nil
	}
	_, err = WaitEvents(
		b.client,
		b.key,
		resp.Header.Revision,
		[]mvccpb.Event_EventType{mvccpb.PUT, mvccpb.DELETE})
	return err
}
func NewLockRacerCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "lock-racer [name of lock (defaults to 'racers')]",
		Short: "Performs lock race operation",
		Run:   runRacerFunc,
	}
	cmd.Flags().IntVar(&totalClientConnections, "total-client-connections", 10, "total number of client connections")
	return cmd
}
func (m *Member) ElectionTimeout() time.Duration {
	return time.Duration(m.Etcd.ElectionTimeoutMs) * time.Millisecond
}
func (m *Member) DialEtcdGRPCServer(opts ...grpc.DialOption) (*grpc.ClientConn, error) {
	dialOpts := []grpc.DialOption{
		grpc.WithTimeout(5 * time.Second),
		grpc.WithBlock(),
	}

	secure := false
	for _, cu := range m.Etcd.AdvertiseClientURLs {
		u, err := url.Parse(cu)
		if err != nil {
			return nil, err
		}
		if u.Scheme == "https" { // TODO: handle unix
			secure = true
		}
	}

	if secure {
		// assume save TLS assets are already stord on disk
		tlsInfo := transport.TLSInfo{
			CertFile:      m.ClientCertPath,
			KeyFile:       m.ClientKeyPath,
			TrustedCAFile: m.ClientTrustedCAPath,

			// TODO: remove this with generated certs
			// only need it for auto TLS
			InsecureSkipVerify: true,
		}
		tlsConfig, err := tlsInfo.ClientConfig()
		if err != nil {
			return nil, err
		}
		creds := credentials.NewTLS(tlsConfig)
		dialOpts = append(dialOpts, grpc.WithTransportCredentials(creds))
	} else {
		dialOpts = append(dialOpts, grpc.WithInsecure())
	}
	dialOpts = append(dialOpts, opts...)
	return grpc.Dial(m.EtcdClientEndpoint, dialOpts...)
}
func (m *Member) CreateEtcdClientConfig(opts ...grpc.DialOption) (cfg *clientv3.Config, err error) {
	secure := false
	for _, cu := range m.Etcd.AdvertiseClientURLs {
		var u *url.URL
		u, err = url.Parse(cu)
		if err != nil {
			return nil, err
		}
		if u.Scheme == "https" { // TODO: handle unix
			secure = true
		}
	}

	cfg = &clientv3.Config{
		Endpoints:   []string{m.EtcdClientEndpoint},
		DialTimeout: 10 * time.Second,
		DialOptions: opts,
	}
	if secure {
		// assume save TLS assets are already stord on disk
		tlsInfo := transport.TLSInfo{
			CertFile:      m.ClientCertPath,
			KeyFile:       m.ClientKeyPath,
			TrustedCAFile: m.ClientTrustedCAPath,

			// TODO: remove this with generated certs
			// only need it for auto TLS
			InsecureSkipVerify: true,
		}
		var tlsConfig *tls.Config
		tlsConfig, err = tlsInfo.ClientConfig()
		if err != nil {
			return nil, err
		}
		cfg.TLS = tlsConfig
	}
	return cfg, err
}
func (m *Member) CreateEtcdClient(opts ...grpc.DialOption) (*clientv3.Client, error) {
	cfg, err := m.CreateEtcdClientConfig(opts...)
	if err != nil {
		return nil, err
	}
	return clientv3.New(*cfg)
}
func (m *Member) CheckCompact(rev int64) error {
	cli, err := m.CreateEtcdClient()
	if err != nil {
		return fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	defer cli.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	wch := cli.Watch(ctx, "\x00", clientv3.WithFromKey(), clientv3.WithRev(rev-1))
	wr, ok := <-wch
	cancel()

	if !ok {
		return fmt.Errorf("watch channel terminated (endpoint %q)", m.EtcdClientEndpoint)
	}
	if wr.CompactRevision != rev {
		return fmt.Errorf("got compact revision %v, wanted %v (endpoint %q)", wr.CompactRevision, rev, m.EtcdClientEndpoint)
	}

	return nil
}
func (m *Member) Defrag() error {
	cli, err := m.CreateEtcdClient()
	if err != nil {
		return fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	defer cli.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
	_, err = cli.Defragment(ctx, m.EtcdClientEndpoint)
	cancel()
	return err
}
func (m *Member) RevHash() (int64, int64, error) {
	conn, err := m.DialEtcdGRPCServer()
	if err != nil {
		return 0, 0, err
	}
	defer conn.Close()

	mt := pb.NewMaintenanceClient(conn)
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	resp, err := mt.Hash(ctx, &pb.HashRequest{}, grpc.FailFast(false))
	cancel()

	if err != nil {
		return 0, 0, err
	}

	return resp.Header.Revision, int64(resp.Hash), nil
}
func (m *Member) Rev(ctx context.Context) (int64, error) {
	cli, err := m.CreateEtcdClient()
	if err != nil {
		return 0, fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	defer cli.Close()

	resp, err := cli.Status(ctx, m.EtcdClientEndpoint)
	if err != nil {
		return 0, err
	}
	return resp.Header.Revision, nil
}
func (m *Member) Compact(rev int64, timeout time.Duration) error {
	cli, err := m.CreateEtcdClient()
	if err != nil {
		return fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	defer cli.Close()

	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	_, err = cli.Compact(ctx, rev, clientv3.WithCompactPhysical())
	cancel()
	return err
}
func (m *Member) IsLeader() (bool, error) {
	cli, err := m.CreateEtcdClient()
	if err != nil {
		return false, fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	defer cli.Close()

	resp, err := cli.Status(context.Background(), m.EtcdClientEndpoint)
	if err != nil {
		return false, err
	}
	return resp.Header.MemberId == resp.Leader, nil
}
func (m *Member) WriteHealthKey() error {
	cli, err := m.CreateEtcdClient()
	if err != nil {
		return fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	defer cli.Close()

	// give enough time-out in case expensive requests (range/delete) are pending
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	_, err = cli.Put(ctx, "health", "good")
	cancel()
	if err != nil {
		return fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}
	return nil
}
func (m *Member) SaveSnapshot(lg *zap.Logger) (err error) {
	// remove existing snapshot first
	if err = os.RemoveAll(m.SnapshotPath); err != nil {
		return err
	}

	var ccfg *clientv3.Config
	ccfg, err = m.CreateEtcdClientConfig()
	if err != nil {
		return fmt.Errorf("%v (%q)", err, m.EtcdClientEndpoint)
	}

	lg.Info(
		"snapshot save START",
		zap.String("member-name", m.Etcd.Name),
		zap.Strings("member-client-urls", m.Etcd.AdvertiseClientURLs),
		zap.String("snapshot-path", m.SnapshotPath),
	)
	now := time.Now()
	mgr := snapshot.NewV3(lg)
	if err = mgr.Save(context.Background(), *ccfg, m.SnapshotPath); err != nil {
		return err
	}
	took := time.Since(now)

	var fi os.FileInfo
	fi, err = os.Stat(m.SnapshotPath)
	if err != nil {
		return err
	}
	var st snapshot.Status
	st, err = mgr.Status(m.SnapshotPath)
	if err != nil {
		return err
	}
	m.SnapshotInfo = &SnapshotInfo{
		MemberName:        m.Etcd.Name,
		MemberClientURLs:  m.Etcd.AdvertiseClientURLs,
		SnapshotPath:      m.SnapshotPath,
		SnapshotFileSize:  humanize.Bytes(uint64(fi.Size())),
		SnapshotTotalSize: humanize.Bytes(uint64(st.TotalSize)),
		SnapshotTotalKey:  int64(st.TotalKey),
		SnapshotHash:      int64(st.Hash),
		SnapshotRevision:  st.Revision,
		Took:              fmt.Sprintf("%v", took),
	}
	lg.Info(
		"snapshot save END",
		zap.String("member-name", m.SnapshotInfo.MemberName),
		zap.Strings("member-client-urls", m.SnapshotInfo.MemberClientURLs),
		zap.String("snapshot-path", m.SnapshotPath),
		zap.String("snapshot-file-size", m.SnapshotInfo.SnapshotFileSize),
		zap.String("snapshot-total-size", m.SnapshotInfo.SnapshotTotalSize),
		zap.Int64("snapshot-total-key", m.SnapshotInfo.SnapshotTotalKey),
		zap.Int64("snapshot-hash", m.SnapshotInfo.SnapshotHash),
		zap.Int64("snapshot-revision", m.SnapshotInfo.SnapshotRevision),
		zap.String("took", m.SnapshotInfo.Took),
	)
	return nil
}
func (m *Member) RestoreSnapshot(lg *zap.Logger) (err error) {
	if err = os.RemoveAll(m.EtcdOnSnapshotRestore.DataDir); err != nil {
		return err
	}
	if err = os.RemoveAll(m.EtcdOnSnapshotRestore.WALDir); err != nil {
		return err
	}

	lg.Info(
		"snapshot restore START",
		zap.String("member-name", m.Etcd.Name),
		zap.Strings("member-client-urls", m.Etcd.AdvertiseClientURLs),
		zap.String("snapshot-path", m.SnapshotPath),
	)
	now := time.Now()
	mgr := snapshot.NewV3(lg)
	err = mgr.Restore(snapshot.RestoreConfig{
		SnapshotPath:        m.SnapshotInfo.SnapshotPath,
		Name:                m.EtcdOnSnapshotRestore.Name,
		OutputDataDir:       m.EtcdOnSnapshotRestore.DataDir,
		OutputWALDir:        m.EtcdOnSnapshotRestore.WALDir,
		PeerURLs:            m.EtcdOnSnapshotRestore.AdvertisePeerURLs,
		InitialCluster:      m.EtcdOnSnapshotRestore.InitialCluster,
		InitialClusterToken: m.EtcdOnSnapshotRestore.InitialClusterToken,
		SkipHashCheck:       false,
		// TODO: set SkipHashCheck it true, to recover from existing db file
	})
	took := time.Since(now)
	lg.Info(
		"snapshot restore END",
		zap.String("member-name", m.SnapshotInfo.MemberName),
		zap.Strings("member-client-urls", m.SnapshotInfo.MemberClientURLs),
		zap.String("snapshot-path", m.SnapshotPath),
		zap.String("snapshot-file-size", m.SnapshotInfo.SnapshotFileSize),
		zap.String("snapshot-total-size", m.SnapshotInfo.SnapshotTotalSize),
		zap.Int64("snapshot-total-key", m.SnapshotInfo.SnapshotTotalKey),
		zap.Int64("snapshot-hash", m.SnapshotInfo.SnapshotHash),
		zap.Int64("snapshot-revision", m.SnapshotInfo.SnapshotRevision),
		zap.String("took", took.String()),
		zap.Error(err),
	)
	return err
}
func NewWatcher(w clientv3.Watcher, prefix string) clientv3.Watcher {
	return &watcherPrefix{Watcher: w, pfx: prefix, stopc: make(chan struct{})}
}
func NewRawNode(config *Config, peers []Peer) (*RawNode, error) {
	if config.ID == 0 {
		panic("config.ID must not be zero")
	}
	r := newRaft(config)
	rn := &RawNode{
		raft: r,
	}
	lastIndex, err := config.Storage.LastIndex()
	if err != nil {
		panic(err) // TODO(bdarnell)
	}
	// If the log is empty, this is a new RawNode (like StartNode); otherwise it's
	// restoring an existing RawNode (like RestartNode).
	// TODO(bdarnell): rethink RawNode initialization and whether the application needs
	// to be able to tell us when it expects the RawNode to exist.
	if lastIndex == 0 {
		r.becomeFollower(1, None)
		ents := make([]pb.Entry, len(peers))
		for i, peer := range peers {
			cc := pb.ConfChange{Type: pb.ConfChangeAddNode, NodeID: peer.ID, Context: peer.Context}
			data, err := cc.Marshal()
			if err != nil {
				panic("unexpected marshal error")
			}

			ents[i] = pb.Entry{Type: pb.EntryConfChange, Term: 1, Index: uint64(i + 1), Data: data}
		}
		r.raftLog.append(ents...)
		r.raftLog.committed = uint64(len(ents))
		for _, peer := range peers {
			r.addNode(peer.ID)
		}
	}

	// Set the initial hard and soft states after performing all initialization.
	rn.prevSoftSt = r.softState()
	if lastIndex == 0 {
		rn.prevHardSt = emptyState
	} else {
		rn.prevHardSt = r.hardState()
	}

	return rn, nil
}
func (rn *RawNode) Campaign() error {
	return rn.raft.Step(pb.Message{
		Type: pb.MsgHup,
	})
}
func (rn *RawNode) Propose(data []byte) error {
	return rn.raft.Step(pb.Message{
		Type: pb.MsgProp,
		From: rn.raft.id,
		Entries: []pb.Entry{
			{Data: data},
		}})
}
func (rn *RawNode) ProposeConfChange(cc pb.ConfChange) error {
	data, err := cc.Marshal()
	if err != nil {
		return err
	}
	return rn.raft.Step(pb.Message{
		Type: pb.MsgProp,
		Entries: []pb.Entry{
			{Type: pb.EntryConfChange, Data: data},
		},
	})
}
func (rn *RawNode) ApplyConfChange(cc pb.ConfChange) *pb.ConfState {
	if cc.NodeID == None {
		return &pb.ConfState{Nodes: rn.raft.nodes(), Learners: rn.raft.learnerNodes()}
	}
	switch cc.Type {
	case pb.ConfChangeAddNode:
		rn.raft.addNode(cc.NodeID)
	case pb.ConfChangeAddLearnerNode:
		rn.raft.addLearner(cc.NodeID)
	case pb.ConfChangeRemoveNode:
		rn.raft.removeNode(cc.NodeID)
	case pb.ConfChangeUpdateNode:
	default:
		panic("unexpected conf type")
	}
	return &pb.ConfState{Nodes: rn.raft.nodes(), Learners: rn.raft.learnerNodes()}
}
func (rn *RawNode) Step(m pb.Message) error {
	// ignore unexpected local messages receiving over network
	if IsLocalMsg(m.Type) {
		return ErrStepLocalMsg
	}
	if pr := rn.raft.getProgress(m.From); pr != nil || !IsResponseMsg(m.Type) {
		return rn.raft.Step(m)
	}
	return ErrStepPeerNotFound
}
func (rn *RawNode) Ready() Ready {
	rd := rn.newReady()
	rn.raft.msgs = nil
	rn.raft.reduceUncommittedSize(rd.CommittedEntries)
	return rd
}
func (rn *RawNode) WithProgress(visitor func(id uint64, typ ProgressType, pr Progress)) {
	for id, pr := range rn.raft.prs {
		pr := *pr
		pr.ins = nil
		visitor(id, ProgressTypePeer, pr)
	}
	for id, pr := range rn.raft.learnerPrs {
		pr := *pr
		pr.ins = nil
		visitor(id, ProgressTypeLearner, pr)
	}
}
func (rn *RawNode) ReportUnreachable(id uint64) {
	_ = rn.raft.Step(pb.Message{Type: pb.MsgUnreachable, From: id})
}
func (rn *RawNode) ReportSnapshot(id uint64, status SnapshotStatus) {
	rej := status == SnapshotFailure

	_ = rn.raft.Step(pb.Message{Type: pb.MsgSnapStatus, From: id, Reject: rej})
}
func (rn *RawNode) TransferLeader(transferee uint64) {
	_ = rn.raft.Step(pb.Message{Type: pb.MsgTransferLeader, From: transferee})
}
func (rn *RawNode) ReadIndex(rctx []byte) {
	_ = rn.raft.Step(pb.Message{Type: pb.MsgReadIndex, Entries: []pb.Entry{{Data: rctx}}})
}
func printcURL(req *http.Request) error {
	if !cURLDebug {
		return nil
	}
	var (
		command string
		b       []byte
		err     error
	)

	if req.URL != nil {
		command = fmt.Sprintf("curl -X %s %s", req.Method, req.URL.String())
	}

	if req.Body != nil {
		b, err = ioutil.ReadAll(req.Body)
		if err != nil {
			return err
		}
		command += fmt.Sprintf(" -d %q", string(b))
	}

	fmt.Fprintf(os.Stderr, "cURL Command: %s\n", command)

	// reset body
	body := bytes.NewBuffer(b)
	req.Body = ioutil.NopCloser(body)

	return nil
}
func StartNode(c *Config, peers []Peer) Node {
	r := newRaft(c)
	// become the follower at term 1 and apply initial configuration
	// entries of term 1
	r.becomeFollower(1, None)
	for _, peer := range peers {
		cc := pb.ConfChange{Type: pb.ConfChangeAddNode, NodeID: peer.ID, Context: peer.Context}
		d, err := cc.Marshal()
		if err != nil {
			panic("unexpected marshal error")
		}
		e := pb.Entry{Type: pb.EntryConfChange, Term: 1, Index: r.raftLog.lastIndex() + 1, Data: d}
		r.raftLog.append(e)
	}
	// Mark these initial entries as committed.
	// TODO(bdarnell): These entries are still unstable; do we need to preserve
	// the invariant that committed < unstable?
	r.raftLog.committed = r.raftLog.lastIndex()
	// Now apply them, mainly so that the application can call Campaign
	// immediately after StartNode in tests. Note that these nodes will
	// be added to raft twice: here and when the application's Ready
	// loop calls ApplyConfChange. The calls to addNode must come after
	// all calls to raftLog.append so progress.next is set after these
	// bootstrapping entries (it is an error if we try to append these
	// entries since they have already been committed).
	// We do not set raftLog.applied so the application will be able
	// to observe all conf changes via Ready.CommittedEntries.
	for _, peer := range peers {
		r.addNode(peer.ID)
	}

	n := newNode()
	n.logger = c.Logger
	go n.run(r)
	return &n
}
func RestartNode(c *Config) Node {
	r := newRaft(c)

	n := newNode()
	n.logger = c.Logger
	go n.run(r)
	return &n
}
func (n *node) Tick() {
	select {
	case n.tickc <- struct{}{}:
	case <-n.done:
	default:
		n.logger.Warningf("A tick missed to fire. Node blocks too long!")
	}
}
func MustSync(st, prevst pb.HardState, entsnum int) bool {
	// Persistent state on all servers:
	// (Updated on stable storage before responding to RPCs)
	// currentTerm
	// votedFor
	// log entries[]
	return entsnum != 0 || st.Vote != prevst.Vote || st.Term != prevst.Term
}
func NewGRPC17Health(
	eps []string,
	timeout time.Duration,
	dialFunc DialFunc,
) *GRPC17Health {
	notifyCh := make(chan []grpc.Address)
	addrs := eps2addrs(eps)
	hb := &GRPC17Health{
		addrs:              addrs,
		eps:                eps,
		notifyCh:           notifyCh,
		readyc:             make(chan struct{}),
		healthCheck:        func(ep string) (bool, error) { return grpcHealthCheck(ep, dialFunc) },
		unhealthyHostPorts: make(map[string]time.Time),
		upc:                make(chan struct{}),
		stopc:              make(chan struct{}),
		downc:              make(chan struct{}),
		donec:              make(chan struct{}),
		updateAddrsC:       make(chan NotifyMsg),
		hostPort2ep:        getHostPort2ep(eps),
	}
	if timeout < minHealthRetryDuration {
		timeout = minHealthRetryDuration
	}
	hb.healthCheckTimeout = timeout

	close(hb.downc)
	go hb.updateNotifyLoop()
	hb.wg.Add(1)
	go func() {
		defer hb.wg.Done()
		hb.updateUnhealthy()
	}()
	return hb
}
func (b *GRPC17Health) NeedUpdate() bool {
	// updating notifyCh can trigger new connections,
	// need update addrs if all connections are down
	// or addrs does not include pinAddr.
	b.mu.RLock()
	update := !hasAddr(b.addrs, b.pinAddr)
	b.mu.RUnlock()
	return update
}
func dflSignal(sig syscall.Signal) {
	// clearing out the sigact sets the signal to SIG_DFL
	var sigactBuf [32]uint64
	ptr := unsafe.Pointer(&sigactBuf)
	syscall.Syscall6(uintptr(syscall.SYS_RT_SIGACTION), uintptr(sig), uintptr(ptr), 0, 8, 0, 0)
}
func New(namespaces ...string) Store {
	s := newStore(namespaces...)
	s.clock = clockwork.NewRealClock()
	return s
}
func (s *store) Index() uint64 {
	s.worldLock.RLock()
	defer s.worldLock.RUnlock()
	return s.CurrentIndex
}
func (s *store) Get(nodePath string, recursive, sorted bool) (*Event, error) {
	var err *v2error.Error

	s.worldLock.RLock()
	defer s.worldLock.RUnlock()

	defer func() {
		if err == nil {
			s.Stats.Inc(GetSuccess)
			if recursive {
				reportReadSuccess(GetRecursive)
			} else {
				reportReadSuccess(Get)
			}
			return
		}

		s.Stats.Inc(GetFail)
		if recursive {
			reportReadFailure(GetRecursive)
		} else {
			reportReadFailure(Get)
		}
	}()

	n, err := s.internalGet(nodePath)
	if err != nil {
		return nil, err
	}

	e := newEvent(Get, nodePath, n.ModifiedIndex, n.CreatedIndex)
	e.EtcdIndex = s.CurrentIndex
	e.Node.loadInternalNode(n, recursive, sorted, s.clock)

	return e, nil
}
func (s *store) Create(nodePath string, dir bool, value string, unique bool, expireOpts TTLOptionSet) (*Event, error) {
	var err *v2error.Error

	s.worldLock.Lock()
	defer s.worldLock.Unlock()

	defer func() {
		if err == nil {
			s.Stats.Inc(CreateSuccess)
			reportWriteSuccess(Create)
			return
		}

		s.Stats.Inc(CreateFail)
		reportWriteFailure(Create)
	}()

	e, err := s.internalCreate(nodePath, dir, value, unique, false, expireOpts.ExpireTime, Create)
	if err != nil {
		return nil, err
	}

	e.EtcdIndex = s.CurrentIndex
	s.WatcherHub.notify(e)

	return e, nil
}
func (s *store) Set(nodePath string, dir bool, value string, expireOpts TTLOptionSet) (*Event, error) {
	var err *v2error.Error

	s.worldLock.Lock()
	defer s.worldLock.Unlock()

	defer func() {
		if err == nil {
			s.Stats.Inc(SetSuccess)
			reportWriteSuccess(Set)
			return
		}

		s.Stats.Inc(SetFail)
		reportWriteFailure(Set)
	}()

	// Get prevNode value
	n, getErr := s.internalGet(nodePath)
	if getErr != nil && getErr.ErrorCode != v2error.EcodeKeyNotFound {
		err = getErr
		return nil, err
	}

	if expireOpts.Refresh {
		if getErr != nil {
			err = getErr
			return nil, err
		}
		value = n.Value
	}

	// Set new value
	e, err := s.internalCreate(nodePath, dir, value, false, true, expireOpts.ExpireTime, Set)
	if err != nil {
		return nil, err
	}
	e.EtcdIndex = s.CurrentIndex

	// Put prevNode into event
	if getErr == nil {
		prev := newEvent(Get, nodePath, n.ModifiedIndex, n.CreatedIndex)
		prev.Node.loadInternalNode(n, false, false, s.clock)
		e.PrevNode = prev.Node
	}

	if !expireOpts.Refresh {
		s.WatcherHub.notify(e)
	} else {
		e.SetRefresh()
		s.WatcherHub.add(e)
	}

	return e, nil
}
func getCompareFailCause(n *node, which int, prevValue string, prevIndex uint64) string {
	switch which {
	case CompareIndexNotMatch:
		return fmt.Sprintf("[%v != %v]", prevIndex, n.ModifiedIndex)
	case CompareValueNotMatch:
		return fmt.Sprintf("[%v != %v]", prevValue, n.Value)
	default:
		return fmt.Sprintf("[%v != %v] [%v != %v]", prevValue, n.Value, prevIndex, n.ModifiedIndex)
	}
}
func (s *store) Delete(nodePath string, dir, recursive bool) (*Event, error) {
	var err *v2error.Error

	s.worldLock.Lock()
	defer s.worldLock.Unlock()

	defer func() {
		if err == nil {
			s.Stats.Inc(DeleteSuccess)
			reportWriteSuccess(Delete)
			return
		}

		s.Stats.Inc(DeleteFail)
		reportWriteFailure(Delete)
	}()

	nodePath = path.Clean(path.Join("/", nodePath))
	// we do not allow the user to change "/"
	if s.readonlySet.Contains(nodePath) {
		return nil, v2error.NewError(v2error.EcodeRootROnly, "/", s.CurrentIndex)
	}

	// recursive implies dir
	if recursive {
		dir = true
	}

	n, err := s.internalGet(nodePath)
	if err != nil { // if the node does not exist, return error
		return nil, err
	}

	nextIndex := s.CurrentIndex + 1
	e := newEvent(Delete, nodePath, nextIndex, n.CreatedIndex)
	e.EtcdIndex = nextIndex
	e.PrevNode = n.Repr(false, false, s.clock)
	eNode := e.Node

	if n.IsDir() {
		eNode.Dir = true
	}

	callback := func(path string) { // notify function
		// notify the watchers with deleted set true
		s.WatcherHub.notifyWatchers(e, path, true)
	}

	err = n.Remove(dir, recursive, callback)
	if err != nil {
		return nil, err
	}

	// update etcd index
	s.CurrentIndex++

	s.WatcherHub.notify(e)

	return e, nil
}
func (s *store) walk(nodePath string, walkFunc func(prev *node, component string) (*node, *v2error.Error)) (*node, *v2error.Error) {
	components := strings.Split(nodePath, "/")

	curr := s.Root
	var err *v2error.Error

	for i := 1; i < len(components); i++ {
		if len(components[i]) == 0 { // ignore empty string
			return curr, nil
		}

		curr, err = walkFunc(curr, components[i])
		if err != nil {
			return nil, err
		}
	}

	return curr, nil
}
func (s *store) internalGet(nodePath string) (*node, *v2error.Error) {
	nodePath = path.Clean(path.Join("/", nodePath))

	walkFunc := func(parent *node, name string) (*node, *v2error.Error) {

		if !parent.IsDir() {
			err := v2error.NewError(v2error.EcodeNotDir, parent.Path, s.CurrentIndex)
			return nil, err
		}

		child, ok := parent.Children[name]
		if ok {
			return child, nil
		}

		return nil, v2error.NewError(v2error.EcodeKeyNotFound, path.Join(parent.Path, name), s.CurrentIndex)
	}

	f, err := s.walk(nodePath, walkFunc)

	if err != nil {
		return nil, err
	}
	return f, nil
}
func (s *store) DeleteExpiredKeys(cutoff time.Time) {
	s.worldLock.Lock()
	defer s.worldLock.Unlock()

	for {
		node := s.ttlKeyHeap.top()
		if node == nil || node.ExpireTime.After(cutoff) {
			break
		}

		s.CurrentIndex++
		e := newEvent(Expire, node.Path, s.CurrentIndex, node.CreatedIndex)
		e.EtcdIndex = s.CurrentIndex
		e.PrevNode = node.Repr(false, false, s.clock)
		if node.IsDir() {
			e.Node.Dir = true
		}

		callback := func(path string) { // notify function
			// notify the watchers with deleted set true
			s.WatcherHub.notifyWatchers(e, path, true)
		}

		s.ttlKeyHeap.pop()
		node.Remove(true, true, callback)

		reportExpiredKey()
		s.Stats.Inc(ExpireCount)

		s.WatcherHub.notify(e)
	}

}
func (s *store) checkDir(parent *node, dirName string) (*node, *v2error.Error) {
	node, ok := parent.Children[dirName]

	if ok {
		if node.IsDir() {
			return node, nil
		}

		return nil, v2error.NewError(v2error.EcodeNotDir, node.Path, s.CurrentIndex)
	}

	n := newDir(s, path.Join(parent.Path, dirName), s.CurrentIndex+1, parent, Permanent)

	parent.Children[dirName] = n

	return n, nil
}
func (s *store) Save() ([]byte, error) {
	b, err := json.Marshal(s.Clone())
	if err != nil {
		return nil, err
	}

	return b, nil
}
func (s *store) Recovery(state []byte) error {
	s.worldLock.Lock()
	defer s.worldLock.Unlock()
	err := json.Unmarshal(state, s)

	if err != nil {
		return err
	}

	s.ttlKeyHeap = newTtlKeyHeap()

	s.Root.recoverAndclean()
	return nil
}
func (g *Generator) Next() uint64 {
	suffix := atomic.AddUint64(&g.suffix, 1)
	id := g.prefix | lowbit(suffix, suffixLen)
	return id
}
func NewMakeMirrorCommand() *cobra.Command {
	c := &cobra.Command{
		Use:   "make-mirror [options] <destination>",
		Short: "Makes a mirror at the destination etcd cluster",
		Run:   makeMirrorCommandFunc,
	}

	c.Flags().StringVar(&mmprefix, "prefix", "", "Key-value prefix to mirror")
	c.Flags().StringVar(&mmdestprefix, "dest-prefix", "", "destination prefix to mirror a prefix to a different prefix in the destination cluster")
	c.Flags().BoolVar(&mmnodestprefix, "no-dest-prefix", false, "mirror key-values to the root of the destination cluster")
	c.Flags().StringVar(&mmcert, "dest-cert", "", "Identify secure client using this TLS certificate file for the destination cluster")
	c.Flags().StringVar(&mmkey, "dest-key", "", "Identify secure client using this TLS key file")
	c.Flags().StringVar(&mmcacert, "dest-cacert", "", "Verify certificates of TLS enabled secure servers using this CA bundle")
	// TODO: secure by default when etcd enables secure gRPC by default.
	c.Flags().BoolVar(&mminsecureTr, "dest-insecure-transport", true, "Disable transport security for client connections")

	return c
}
func NewZapCoreLoggerBuilder(lg *zap.Logger, cr zapcore.Core, syncer zapcore.WriteSyncer) func(*Config) error {
	return func(cfg *Config) error {
		cfg.loggerMu.Lock()
		defer cfg.loggerMu.Unlock()
		cfg.logger = lg
		cfg.loggerConfig = nil
		cfg.loggerCore = cr
		cfg.loggerWriteSyncer = syncer

		grpcLogOnce.Do(func() {
			grpclog.SetLoggerV2(logutil.NewGRPCLoggerV2FromZapCore(cr, syncer))
		})
		return nil
	}
}
func NewSyncer(c *clientv3.Client, prefix string, rev int64) Syncer {
	return &syncer{c: c, prefix: prefix, rev: rev}
}
func DropPort(port int) error {
	cmdStr := fmt.Sprintf("sudo iptables -A OUTPUT -p tcp --destination-port %d -j DROP", port)
	if _, err := exec.Command("/bin/sh", "-c", cmdStr).Output(); err != nil {
		return err
	}
	cmdStr = fmt.Sprintf("sudo iptables -A INPUT -p tcp --destination-port %d -j DROP", port)
	_, err := exec.Command("/bin/sh", "-c", cmdStr).Output()
	return err
}
func SetLatency(ms, rv int) error {
	ifces, err := GetDefaultInterfaces()
	if err != nil {
		return err
	}

	if rv > ms {
		rv = 1
	}
	for ifce := range ifces {
		cmdStr := fmt.Sprintf("sudo tc qdisc add dev %s root netem delay %dms %dms distribution normal", ifce, ms, rv)
		_, err = exec.Command("/bin/sh", "-c", cmdStr).Output()
		if err != nil {
			// the rule has already been added. Overwrite it.
			cmdStr = fmt.Sprintf("sudo tc qdisc change dev %s root netem delay %dms %dms distribution normal", ifce, ms, rv)
			_, err = exec.Command("/bin/sh", "-c", cmdStr).Output()
			if err != nil {
				return err
			}
		}
	}
	return nil
}
func RemoveLatency() error {
	ifces, err := GetDefaultInterfaces()
	if err != nil {
		return err
	}
	for ifce := range ifces {
		_, err = exec.Command("/bin/sh", "-c", fmt.Sprintf("sudo tc qdisc del dev %s root netem", ifce)).Output()
		if err != nil {
			return err
		}
	}
	return nil
}
func NewTxnCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "txn [options]",
		Short: "Txn processes all the requests in one transaction",
		Run:   txnCommandFunc,
	}
	cmd.Flags().BoolVarP(&txnInteractive, "interactive", "i", false, "Input transaction in interactive mode")
	return cmd
}
func txnCommandFunc(cmd *cobra.Command, args []string) {
	if len(args) != 0 {
		ExitWithError(ExitBadArgs, fmt.Errorf("txn command does not accept argument"))
	}

	reader := bufio.NewReader(os.Stdin)

	txn := mustClientFromCmd(cmd).Txn(context.Background())
	promptInteractive("compares:")
	txn.If(readCompares(reader)...)
	promptInteractive("success requests (get, put, del):")
	txn.Then(readOps(reader)...)
	promptInteractive("failure requests (get, put, del):")
	txn.Else(readOps(reader)...)

	resp, err := txn.Commit()
	if err != nil {
		ExitWithError(ExitError, err)
	}

	display.Txn(*resp)
}
func New(
	lg *zap.Logger,
	mode string,
	retention time.Duration,
	rg RevGetter,
	c Compactable,
) (Compactor, error) {
	switch mode {
	case ModePeriodic:
		return newPeriodic(lg, clockwork.NewRealClock(), retention, rg, c), nil
	case ModeRevision:
		return newRevision(lg, clockwork.NewRealClock(), int64(retention), rg, c), nil
	default:
		return nil, fmt.Errorf("unsupported compaction mode %s", mode)
	}
}
func printResponseKey(resp *client.Response, format string) {
	// Format the result.
	switch format {
	case "simple":
		if resp.Action != "delete" {
			fmt.Println(resp.Node.Value)
		} else {
			fmt.Println("PrevNode.Value:", resp.PrevNode.Value)
		}
	case "extended":
		// Extended prints in a rfc2822 style format
		fmt.Println("Key:", resp.Node.Key)
		fmt.Println("Created-Index:", resp.Node.CreatedIndex)
		fmt.Println("Modified-Index:", resp.Node.ModifiedIndex)

		if resp.PrevNode != nil {
			fmt.Println("PrevNode.Value:", resp.PrevNode.Value)
		}

		fmt.Println("TTL:", resp.Node.TTL)
		fmt.Println("Index:", resp.Index)
		if resp.Action != "delete" {
			fmt.Println("")
			fmt.Println(resp.Node.Value)
		}
	case "json":
		b, err := json.Marshal(resp)
		if err != nil {
			panic(err)
		}
		fmt.Println(string(b))
	default:
		fmt.Fprintln(os.Stderr, "Unsupported output format:", format)
	}
}
func RegisterLockHandler(ctx context.Context, mux *runtime.ServeMux, conn *grpc.ClientConn) error {
	return RegisterLockHandlerClient(ctx, mux, v3lockpb.NewLockClient(conn))
}
func grpcHandlerFunc(grpcServer *grpc.Server, otherHandler http.Handler) http.Handler {
	if otherHandler == nil {
		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
			grpcServer.ServeHTTP(w, r)
		})
	}
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if r.ProtoMajor == 2 && strings.Contains(r.Header.Get("Content-Type"), "application/grpc") {
			grpcServer.ServeHTTP(w, r)
		} else {
			otherHandler.ServeHTTP(w, r)
		}
	})
}
func addCORSHeader(w http.ResponseWriter, origin string) {
	w.Header().Add("Access-Control-Allow-Methods", "POST, GET, OPTIONS, PUT, DELETE")
	w.Header().Add("Access-Control-Allow-Origin", origin)
	w.Header().Add("Access-Control-Allow-Headers", "accept, content-type, authorization")
}
func (txn *txnLeasing) fallback(ops []v3.Op) (fbOps []v3.Op) {
	for _, op := range ops {
		if op.IsGet() {
			continue
		}
		lkey, lend := txn.lkv.pfx+string(op.KeyBytes()), ""
		if len(op.RangeBytes()) > 0 {
			lend = txn.lkv.pfx + string(op.RangeBytes())
		}
		fbOps = append(fbOps, v3.OpGet(lkey, v3.WithRange(lend)))
	}
	return fbOps
}
func IDFromString(s string) (ID, error) {
	i, err := strconv.ParseUint(s, 16, 64)
	return ID(i), err
}
func (guard *directoryLockGuard) release() error {
	var err error
	if !guard.readOnly {
		// It's important that we remove the pid file first.
		err = os.Remove(guard.path)
	}

	if closeErr := guard.f.Close(); err == nil {
		err = closeErr
	}
	guard.path = ""
	guard.f = nil

	return err
}
func (v *ValueStruct) EncodedSize() uint16 {
	sz := len(v.Value) + 2 // meta, usermeta.
	if v.ExpiresAt == 0 {
		return uint16(sz + 1)
	}

	enc := sizeVarint(v.ExpiresAt)
	return uint16(sz + enc)
}
func (v *ValueStruct) Decode(b []byte) {
	v.Meta = b[0]
	v.UserMeta = b[1]
	var sz int
	v.ExpiresAt, sz = binary.Uvarint(b[2:])
	v.Value = b[2+sz:]
}
func NewMergeIterator(iters []Iterator, reversed bool) *MergeIterator {
	m := &MergeIterator{all: iters, reversed: reversed}
	m.h = make(elemHeap, 0, len(iters))
	m.initHeap()
	return m
}
func (s *MergeIterator) initHeap() {
	s.h = s.h[:0]
	for idx, itr := range s.all {
		if !itr.Valid() {
			continue
		}
		e := &elem{itr: itr, nice: idx, reversed: s.reversed}
		s.h = append(s.h, e)
	}
	heap.Init(&s.h)
	for len(s.h) > 0 {
		it := s.h[0].itr
		if it == nil || !it.Valid() {
			heap.Pop(&s.h)
			continue
		}
		s.storeKey(s.h[0].itr)
		break
	}
}
func (s *MergeIterator) Valid() bool {
	if s == nil {
		return false
	}
	if len(s.h) == 0 {
		return false
	}
	return s.h[0].itr.Valid()
}
func (s *MergeIterator) Key() []byte {
	if len(s.h) == 0 {
		return nil
	}
	return s.h[0].itr.Key()
}
func (s *MergeIterator) Value() ValueStruct {
	if len(s.h) == 0 {
		return ValueStruct{}
	}
	return s.h[0].itr.Value()
}
func (s *MergeIterator) Next() {
	if len(s.h) == 0 {
		return
	}

	smallest := s.h[0].itr
	smallest.Next()

	for len(s.h) > 0 {
		smallest = s.h[0].itr
		if !smallest.Valid() {
			heap.Pop(&s.h)
			continue
		}

		heap.Fix(&s.h, 0)
		smallest = s.h[0].itr
		if smallest.Valid() {
			if !bytes.Equal(smallest.Key(), s.curKey) {
				break
			}
			smallest.Next()
		}
	}
	if !smallest.Valid() {
		return
	}
	s.storeKey(smallest)
}
func (s *MergeIterator) Seek(key []byte) {
	for _, itr := range s.all {
		itr.Seek(key)
	}
	s.initHeap()
}
func (s *MergeIterator) Close() error {
	for _, itr := range s.all {
		if err := itr.Close(); err != nil {
			return errors.Wrap(err, "MergeIterator")
		}
	}
	return nil
}
func (p valuePointer) Encode(b []byte) []byte {
	binary.BigEndian.PutUint32(b[:4], p.Fid)
	binary.BigEndian.PutUint32(b[4:8], p.Len)
	binary.BigEndian.PutUint32(b[8:12], p.Offset)
	return b[:vptrSize]
}
func (h *header) Decode(buf []byte) {
	h.klen = binary.BigEndian.Uint32(buf[0:4])
	h.vlen = binary.BigEndian.Uint32(buf[4:8])
	h.expiresAt = binary.BigEndian.Uint64(buf[8:16])
	h.meta = buf[16]
	h.userMeta = buf[17]
}
func encodeEntry(e *Entry, buf *bytes.Buffer) (int, error) {
	h := header{
		klen:      uint32(len(e.Key)),
		vlen:      uint32(len(e.Value)),
		expiresAt: e.ExpiresAt,
		meta:      e.meta,
		userMeta:  e.UserMeta,
	}

	var headerEnc [headerBufSize]byte
	h.Encode(headerEnc[:])

	hash := crc32.New(y.CastagnoliCrcTable)

	buf.Write(headerEnc[:])
	hash.Write(headerEnc[:])

	buf.Write(e.Key)
	hash.Write(e.Key)

	buf.Write(e.Value)
	hash.Write(e.Value)

	var crcBuf [crc32.Size]byte
	binary.BigEndian.PutUint32(crcBuf[:], hash.Sum32())
	buf.Write(crcBuf[:])

	return len(headerEnc) + len(e.Key) + len(e.Value) + len(crcBuf), nil
}
func (wb *WriteBatch) SetEntry(e *Entry) error {
	wb.Lock()
	defer wb.Unlock()

	if err := wb.txn.SetEntry(e); err != ErrTxnTooBig {
		return err
	}
	// Txn has reached it's zenith. Commit now.
	if cerr := wb.commit(); cerr != nil {
		return cerr
	}
	// This time the error must not be ErrTxnTooBig, otherwise, we make the
	// error permanent.
	if err := wb.txn.SetEntry(e); err != nil {
		wb.err = err
		return err
	}
	return nil
}
func (wb *WriteBatch) Set(k, v []byte, meta byte) error {
	e := &Entry{Key: k, Value: v, UserMeta: meta}
	return wb.SetEntry(e)
}
func (wb *WriteBatch) SetWithTTL(key, val []byte, dur time.Duration) error {
	expire := time.Now().Add(dur).Unix()
	e := &Entry{Key: key, Value: val, ExpiresAt: uint64(expire)}
	return wb.SetEntry(e)
}
func (wb *WriteBatch) Delete(k []byte) error {
	wb.Lock()
	defer wb.Unlock()

	if err := wb.txn.Delete(k); err != ErrTxnTooBig {
		return err
	}
	if err := wb.commit(); err != nil {
		return err
	}
	if err := wb.txn.Delete(k); err != nil {
		wb.err = err
		return err
	}
	return nil
}
func (wb *WriteBatch) commit() error {
	if wb.err != nil {
		return wb.err
	}
	// Get a new txn before we commit this one. So, the new txn doesn't need
	// to wait for this one to commit.
	wb.wg.Add(1)
	wb.txn.CommitWith(wb.callback)
	wb.txn = wb.db.newTransaction(true, true)
	// See comment about readTs in NewWriteBatch.
	wb.txn.readTs = wb.db.orc.readMark.DoneUntil()
	return wb.err
}
func (wb *WriteBatch) Flush() error {
	wb.Lock()
	_ = wb.commit()
	wb.txn.Discard()
	wb.Unlock()

	wb.wg.Wait()
	// Safe to access error without any synchronization here.
	return wb.err
}
func (wb *WriteBatch) Error() error {
	wb.Lock()
	defer wb.Unlock()
	return wb.err
}
func (db *DB) getMemTables() ([]*skl.Skiplist, func()) {
	db.RLock()
	defer db.RUnlock()

	tables := make([]*skl.Skiplist, len(db.imm)+1)

	// Get mutable memtable.
	tables[0] = db.mt
	tables[0].IncrRef()

	// Get immutable memtables.
	last := len(db.imm) - 1
	for i := range db.imm {
		tables[i+1] = db.imm[last-i]
		tables[i+1].IncrRef()
	}
	return tables, func() {
		for _, tbl := range tables {
			tbl.DecrRef()
		}
	}
}
func (db *DB) writeRequests(reqs []*request) error {
	if len(reqs) == 0 {
		return nil
	}

	done := func(err error) {
		for _, r := range reqs {
			r.Err = err
			r.Wg.Done()
		}
	}
	db.elog.Printf("writeRequests called. Writing to value log")

	err := db.vlog.write(reqs)
	if err != nil {
		done(err)
		return err
	}

	db.elog.Printf("Writing to memtable")
	var count int
	for _, b := range reqs {
		if len(b.Entries) == 0 {
			continue
		}
		count += len(b.Entries)
		var i uint64
		for err = db.ensureRoomForWrite(); err == errNoRoom; err = db.ensureRoomForWrite() {
			i++
			if i%100 == 0 {
				db.elog.Printf("Making room for writes")
			}
			// We need to poll a bit because both hasRoomForWrite and the flusher need access to s.imm.
			// When flushChan is full and you are blocked there, and the flusher is trying to update s.imm,
			// you will get a deadlock.
			time.Sleep(10 * time.Millisecond)
		}
		if err != nil {
			done(err)
			return errors.Wrap(err, "writeRequests")
		}
		if err := db.writeToLSM(b); err != nil {
			done(err)
			return errors.Wrap(err, "writeRequests")
		}
		db.updateHead(b.Ptrs)
	}
	done(nil)
	db.elog.Printf("%d entries written", count)
	return nil
}
func (db *DB) ensureRoomForWrite() error {
	var err error
	db.Lock()
	defer db.Unlock()
	if db.mt.MemSize() < db.opt.MaxTableSize {
		return nil
	}

	y.AssertTrue(db.mt != nil) // A nil mt indicates that DB is being closed.
	select {
	case db.flushChan <- flushTask{mt: db.mt, vptr: db.vhead}:
		db.elog.Printf("Flushing value log to disk if async mode.")
		// Ensure value log is synced to disk so this memtable's contents wouldn't be lost.
		err = db.vlog.sync(db.vhead.Fid)
		if err != nil {
			return err
		}

		db.elog.Printf("Flushing memtable, mt.size=%d size of flushChan: %d\n",
			db.mt.MemSize(), len(db.flushChan))
		// We manage to push this task. Let's modify imm.
		db.imm = append(db.imm, db.mt)
		db.mt = skl.NewSkiplist(arenaSize(db.opt))
		// New memtable is empty. We certainly have room.
		return nil
	default:
		// We need to do this to unlock and allow the flusher to modify imm.
		return errNoRoom
	}
}
func writeLevel0Table(ft flushTask, f io.Writer) error {
	iter := ft.mt.NewIterator()
	defer iter.Close()
	b := table.NewTableBuilder()
	defer b.Close()
	for iter.SeekToFirst(); iter.Valid(); iter.Next() {
		if len(ft.dropPrefix) > 0 && bytes.HasPrefix(iter.Key(), ft.dropPrefix) {
			continue
		}
		if err := b.Add(iter.Key(), iter.Value()); err != nil {
			return err
		}
	}
	_, err := f.Write(b.Finish())
	return err
}
func (db *DB) handleFlushTask(ft flushTask) error {
	if !ft.mt.Empty() {
		// Store badger head even if vptr is zero, need it for readTs
		db.opt.Debugf("Storing value log head: %+v\n", ft.vptr)
		db.elog.Printf("Storing offset: %+v\n", ft.vptr)
		offset := make([]byte, vptrSize)
		ft.vptr.Encode(offset)

		// Pick the max commit ts, so in case of crash, our read ts would be higher than all the
		// commits.
		headTs := y.KeyWithTs(head, db.orc.nextTs())
		ft.mt.Put(headTs, y.ValueStruct{Value: offset})

		// Also store lfDiscardStats before flushing memtables
		discardStatsKey := y.KeyWithTs(lfDiscardStatsKey, 1)
		ft.mt.Put(discardStatsKey, y.ValueStruct{Value: db.vlog.encodedDiscardStats()})
	}

	fileID := db.lc.reserveFileID()
	fd, err := y.CreateSyncedFile(table.NewFilename(fileID, db.opt.Dir), true)
	if err != nil {
		return y.Wrap(err)
	}

	// Don't block just to sync the directory entry.
	dirSyncCh := make(chan error)
	go func() { dirSyncCh <- syncDir(db.opt.Dir) }()

	err = writeLevel0Table(ft, fd)
	dirSyncErr := <-dirSyncCh

	if err != nil {
		db.elog.Errorf("ERROR while writing to level 0: %v", err)
		return err
	}
	if dirSyncErr != nil {
		// Do dir sync as best effort. No need to return due to an error there.
		db.elog.Errorf("ERROR while syncing level directory: %v", dirSyncErr)
	}

	tbl, err := table.OpenTable(fd, db.opt.TableLoadingMode, nil)
	if err != nil {
		db.elog.Printf("ERROR while opening table: %v", err)
		return err
	}
	// We own a ref on tbl.
	err = db.lc.addLevel0Table(tbl) // This will incrRef (if we don't error, sure)
	tbl.DecrRef()                   // Releases our ref.
	return err
}
func (db *DB) flushMemtable(lc *y.Closer) error {
	defer lc.Done()

	for ft := range db.flushChan {
		if ft.mt == nil {
			// We close db.flushChan now, instead of sending a nil ft.mt.
			continue
		}
		for {
			err := db.handleFlushTask(ft)
			if err == nil {
				// Update s.imm. Need a lock.
				db.Lock()
				// This is a single-threaded operation. ft.mt corresponds to the head of
				// db.imm list. Once we flush it, we advance db.imm. The next ft.mt
				// which would arrive here would match db.imm[0], because we acquire a
				// lock over DB when pushing to flushChan.
				// TODO: This logic is dirty AF. Any change and this could easily break.
				y.AssertTrue(ft.mt == db.imm[0])
				db.imm = db.imm[1:]
				ft.mt.DecrRef() // Return memory.
				db.Unlock()

				break
			}
			// Encountered error. Retry indefinitely.
			db.opt.Errorf("Failure while flushing memtable to disk: %v. Retrying...\n", err)
			time.Sleep(time.Second)
		}
	}
	return nil
}
func (db *DB) calculateSize() {
	newInt := func(val int64) *expvar.Int {
		v := new(expvar.Int)
		v.Add(val)
		return v
	}

	totalSize := func(dir string) (int64, int64) {
		var lsmSize, vlogSize int64
		err := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {
			if err != nil {
				return err
			}
			ext := filepath.Ext(path)
			if ext == ".sst" {
				lsmSize += info.Size()
			} else if ext == ".vlog" {
				vlogSize += info.Size()
			}
			return nil
		})
		if err != nil {
			db.elog.Printf("Got error while calculating total size of directory: %s", dir)
		}
		return lsmSize, vlogSize
	}

	lsmSize, vlogSize := totalSize(db.opt.Dir)
	y.LSMSize.Set(db.opt.Dir, newInt(lsmSize))
	// If valueDir is different from dir, we'd have to do another walk.
	if db.opt.ValueDir != db.opt.Dir {
		_, vlogSize = totalSize(db.opt.ValueDir)
	}
	y.VlogSize.Set(db.opt.Dir, newInt(vlogSize))
}
func (db *DB) Size() (lsm, vlog int64) {
	if y.LSMSize.Get(db.opt.Dir) == nil {
		lsm, vlog = 0, 0
		return
	}
	lsm = y.LSMSize.Get(db.opt.Dir).(*expvar.Int).Value()
	vlog = y.VlogSize.Get(db.opt.Dir).(*expvar.Int).Value()
	return
}
func (seq *Sequence) Next() (uint64, error) {
	seq.Lock()
	defer seq.Unlock()
	if seq.next >= seq.leased {
		if err := seq.updateLease(); err != nil {
			return 0, err
		}
	}
	val := seq.next
	seq.next++
	return val, nil
}
func (seq *Sequence) Release() error {
	seq.Lock()
	defer seq.Unlock()
	err := seq.db.Update(func(txn *Txn) error {
		var buf [8]byte
		binary.BigEndian.PutUint64(buf[:], seq.next)
		return txn.Set(seq.key, buf[:])
	})
	if err != nil {
		return err
	}
	seq.leased = seq.next
	return nil
}
func (db *DB) KeySplits(prefix []byte) []string {
	var splits []string
	for _, ti := range db.Tables() {
		// We don't use ti.Left, because that has a tendency to store !badger
		// keys.
		if bytes.HasPrefix(ti.Right, prefix) {
			splits = append(splits, string(ti.Right))
		}
	}
	sort.Strings(splits)
	return splits
}
func (db *DB) Flatten(workers int) error {
	db.stopCompactions()
	defer db.startCompactions()

	compactAway := func(cp compactionPriority) error {
		db.opt.Infof("Attempting to compact with %+v\n", cp)
		errCh := make(chan error, 1)
		for i := 0; i < workers; i++ {
			go func() {
				errCh <- db.lc.doCompact(cp)
			}()
		}
		var success int
		var rerr error
		for i := 0; i < workers; i++ {
			err := <-errCh
			if err != nil {
				rerr = err
				db.opt.Warningf("While running doCompact with %+v. Error: %v\n", cp, err)
			} else {
				success++
			}
		}
		if success == 0 {
			return rerr
		}
		// We could do at least one successful compaction. So, we'll consider this a success.
		db.opt.Infof("%d compactor(s) succeeded. One or more tables from level %d compacted.\n",
			success, cp.level)
		return nil
	}

	hbytes := func(sz int64) string {
		return humanize.Bytes(uint64(sz))
	}

	for {
		db.opt.Infof("\n")
		var levels []int
		for i, l := range db.lc.levels {
			sz := l.getTotalSize()
			db.opt.Infof("Level: %d. %8s Size. %8s Max.\n",
				i, hbytes(l.getTotalSize()), hbytes(l.maxTotalSize))
			if sz > 0 {
				levels = append(levels, i)
			}
		}
		if len(levels) <= 1 {
			prios := db.lc.pickCompactLevels()
			if len(prios) == 0 || prios[0].score <= 1.0 {
				db.opt.Infof("All tables consolidated into one level. Flattening done.\n")
				return nil
			}
			if err := compactAway(prios[0]); err != nil {
				return err
			}
			continue
		}
		// Create an artificial compaction priority, to ensure that we compact the level.
		cp := compactionPriority{level: levels[0], score: 1.71}
		if err := compactAway(cp); err != nil {
			return err
		}
	}
}
func Mmap(fd *os.File, writable bool, size int64) ([]byte, error) {
	mtype := unix.PROT_READ
	if writable {
		mtype |= unix.PROT_WRITE
	}
	return unix.Mmap(int(fd.Fd()), 0, int(size), mtype, unix.MAP_SHARED)
}
func Madvise(b []byte, readahead bool) error {
	flags := unix.MADV_NORMAL
	if !readahead {
		flags = unix.MADV_RANDOM
	}
	return madvise(b, flags)
}
func (o *oracle) setDiscardTs(ts uint64) {
	o.Lock()
	defer o.Unlock()
	o.discardTs = ts
}
func (o *oracle) hasConflict(txn *Txn) bool {
	if len(txn.reads) == 0 {
		return false
	}
	for _, ro := range txn.reads {
		// A commit at the read timestamp is expected.
		// But, any commit after the read timestamp should cause a conflict.
		if ts, has := o.commits[ro]; has && ts > txn.readTs {
			return true
		}
	}
	return false
}
func (txn *Txn) Set(key, val []byte) error {
	e := &Entry{
		Key:   key,
		Value: val,
	}
	return txn.SetEntry(e)
}
func (txn *Txn) SetWithMeta(key, val []byte, meta byte) error {
	e := &Entry{Key: key, Value: val, UserMeta: meta}
	return txn.SetEntry(e)
}
func (txn *Txn) Delete(key []byte) error {
	e := &Entry{
		Key:  key,
		meta: bitDelete,
	}
	return txn.modify(e)
}
func (txn *Txn) Get(key []byte) (item *Item, rerr error) {
	if len(key) == 0 {
		return nil, ErrEmptyKey
	} else if txn.discarded {
		return nil, ErrDiscardedTxn
	}

	item = new(Item)
	if txn.update {
		if e, has := txn.pendingWrites[string(key)]; has && bytes.Equal(key, e.Key) {
			if isDeletedOrExpired(e.meta, e.ExpiresAt) {
				return nil, ErrKeyNotFound
			}
			// Fulfill from cache.
			item.meta = e.meta
			item.val = e.Value
			item.userMeta = e.UserMeta
			item.key = key
			item.status = prefetched
			item.version = txn.readTs
			item.expiresAt = e.ExpiresAt
			// We probably don't need to set db on item here.
			return item, nil
		}
		// Only track reads if this is update txn. No need to track read if txn serviced it
		// internally.
		txn.addReadKey(key)
	}

	seek := y.KeyWithTs(key, txn.readTs)
	vs, err := txn.db.get(seek)
	if err != nil {
		return nil, errors.Wrapf(err, "DB::Get key: %q", key)
	}
	if vs.Value == nil && vs.Meta == 0 {
		return nil, ErrKeyNotFound
	}
	if isDeletedOrExpired(vs.Meta, vs.ExpiresAt) {
		return nil, ErrKeyNotFound
	}

	item.key = key
	item.version = vs.Version
	item.meta = vs.Meta
	item.userMeta = vs.UserMeta
	item.db = txn.db
	item.vptr = vs.Value // TODO: Do we need to copy this over?
	item.txn = txn
	item.expiresAt = vs.ExpiresAt
	return item, nil
}
func (txn *Txn) CommitWith(cb func(error)) {
	txn.commitPrecheck() // Precheck before discarding txn.
	defer txn.Discard()

	if cb == nil {
		panic("Nil callback provided to CommitWith")
	}

	if len(txn.writes) == 0 {
		// Do not run these callbacks from here, because the CommitWith and the
		// callback might be acquiring the same locks. Instead run the callback
		// from another goroutine.
		go runTxnCallback(&txnCb{user: cb, err: nil})
		return
	}

	commitCb, err := txn.commitAndSend()
	if err != nil {
		go runTxnCallback(&txnCb{user: cb, err: err})
		return
	}

	go runTxnCallback(&txnCb{user: cb, commit: commitCb})
}
func (db *DB) View(fn func(txn *Txn) error) error {
	var txn *Txn
	if db.opt.managedTxns {
		txn = db.NewTransactionAt(math.MaxUint64, false)
	} else {
		txn = db.NewTransaction(false)
	}
	defer txn.Discard()

	return fn(txn)
}
func (db *DB) Update(fn func(txn *Txn) error) error {
	if db.opt.managedTxns {
		panic("Update can only be used with managedDB=false.")
	}
	txn := db.NewTransaction(true)
	defer txn.Discard()

	if err := fn(txn); err != nil {
		return err
	}

	return txn.Commit()
}
func (itr *blockIterator) Seek(key []byte, whence int) {
	itr.err = nil

	switch whence {
	case origin:
		itr.Reset()
	case current:
	}

	var done bool
	for itr.Init(); itr.Valid(); itr.Next() {
		k := itr.Key()
		if y.CompareKeys(k, key) >= 0 {
			// We are done as k is >= key.
			done = true
			break
		}
	}
	if !done {
		itr.err = io.EOF
	}
}
func (itr *blockIterator) SeekToLast() {
	itr.err = nil
	for itr.Init(); itr.Valid(); itr.Next() {
	}
	itr.Prev()
}
func (itr *blockIterator) parseKV(h header) {
	if cap(itr.key) < int(h.plen+h.klen) {
		sz := int(h.plen) + int(h.klen) // Convert to int before adding to avoid uint16 overflow.
		itr.key = make([]byte, 2*sz)
	}
	itr.key = itr.key[:h.plen+h.klen]
	copy(itr.key, itr.baseKey[:h.plen])
	copy(itr.key[h.plen:], itr.data[itr.pos:itr.pos+uint32(h.klen)])
	itr.pos += uint32(h.klen)

	if itr.pos+uint32(h.vlen) > uint32(len(itr.data)) {
		itr.err = errors.Errorf("Value exceeded size of block: %d %d %d %d %v",
			itr.pos, h.klen, h.vlen, len(itr.data), h)
		return
	}
	itr.val = y.SafeCopy(itr.val, itr.data[itr.pos:itr.pos+uint32(h.vlen)])
	itr.pos += uint32(h.vlen)
}
func (t *Table) NewIterator(reversed bool) *Iterator {
	t.IncrRef() // Important.
	ti := &Iterator{t: t, reversed: reversed}
	ti.next()
	return ti
}
func (itr *Iterator) seekFrom(key []byte, whence int) {
	itr.err = nil
	switch whence {
	case origin:
		itr.reset()
	case current:
	}

	idx := sort.Search(len(itr.t.blockIndex), func(idx int) bool {
		ko := itr.t.blockIndex[idx]
		return y.CompareKeys(ko.key, key) > 0
	})
	if idx == 0 {
		// The smallest key in our table is already strictly > key. We can return that.
		// This is like a SeekToFirst.
		itr.seekHelper(0, key)
		return
	}

	// block[idx].smallest is > key.
	// Since idx>0, we know block[idx-1].smallest is <= key.
	// There are two cases.
	// 1) Everything in block[idx-1] is strictly < key. In this case, we should go to the first
	//    element of block[idx].
	// 2) Some element in block[idx-1] is >= key. We should go to that element.
	itr.seekHelper(idx-1, key)
	if itr.err == io.EOF {
		// Case 1. Need to visit block[idx].
		if idx == len(itr.t.blockIndex) {
			// If idx == len(itr.t.blockIndex), then input key is greater than ANY element of table.
			// There's nothing we can do. Valid() should return false as we seek to end of table.
			return
		}
		// Since block[idx].smallest is > key. This is essentially a block[idx].SeekToFirst.
		itr.seekHelper(idx, key)
	}
	// Case 2: No need to do anything. We already did the seek in block[idx-1].
}
func (itr *Iterator) seekForPrev(key []byte) {
	// TODO: Optimize this. We shouldn't have to take a Prev step.
	itr.seekFrom(key, origin)
	if !bytes.Equal(itr.Key(), key) {
		itr.prev()
	}
}
func (itr *Iterator) Value() (ret y.ValueStruct) {
	ret.Decode(itr.bi.Value())
	return
}
func (itr *Iterator) Seek(key []byte) {
	if !itr.reversed {
		itr.seek(key)
	} else {
		itr.seekForPrev(key)
	}
}
func NewConcatIterator(tbls []*Table, reversed bool) *ConcatIterator {
	iters := make([]*Iterator, len(tbls))
	for i := 0; i < len(tbls); i++ {
		iters[i] = tbls[i].NewIterator(reversed)
	}
	return &ConcatIterator{
		reversed: reversed,
		iters:    iters,
		tables:   tbls,
		idx:      -1, // Not really necessary because s.it.Valid()=false, but good to have.
	}
}
func (s *ConcatIterator) Valid() bool {
	return s.cur != nil && s.cur.Valid()
}
func (s *ConcatIterator) Next() {
	s.cur.Next()
	if s.cur.Valid() {
		// Nothing to do. Just stay with the current table.
		return
	}
	for { // In case there are empty tables.
		if !s.reversed {
			s.setIdx(s.idx + 1)
		} else {
			s.setIdx(s.idx - 1)
		}
		if s.cur == nil {
			// End of list. Valid will become false.
			return
		}
		s.cur.Rewind()
		if s.cur.Valid() {
			break
		}
	}
}
func (s *ConcatIterator) Close() error {
	for _, it := range s.iters {
		if err := it.Close(); err != nil {
			return errors.Wrap(err, "ConcatIterator")
		}
	}
	return nil
}
func OpenExistingFile(filename string, flags uint32) (*os.File, error) {
	openFlags := os.O_RDWR
	if flags&ReadOnly != 0 {
		openFlags = os.O_RDONLY
	}

	if flags&Sync != 0 {
		openFlags |= datasyncFileFlag
	}
	return os.OpenFile(filename, openFlags, 0)
}
func Copy(a []byte) []byte {
	b := make([]byte, len(a))
	copy(b, a)
	return b
}
func KeyWithTs(key []byte, ts uint64) []byte {
	out := make([]byte, len(key)+8)
	copy(out, key)
	binary.BigEndian.PutUint64(out[len(key):], math.MaxUint64-ts)
	return out
}
func ParseTs(key []byte) uint64 {
	if len(key) <= 8 {
		return 0
	}
	return math.MaxUint64 - binary.BigEndian.Uint64(key[len(key)-8:])
}
func ParseKey(key []byte) []byte {
	if key == nil {
		return nil
	}

	AssertTrue(len(key) > 8)
	return key[:len(key)-8]
}
func SameKey(src, dst []byte) bool {
	if len(src) != len(dst) {
		return false
	}
	return bytes.Equal(ParseKey(src), ParseKey(dst))
}
func FixedDuration(d time.Duration) string {
	str := fmt.Sprintf("%02ds", int(d.Seconds())%60)
	if d >= time.Minute {
		str = fmt.Sprintf("%02dm", int(d.Minutes())%60) + str
	}
	if d >= time.Hour {
		str = fmt.Sprintf("%02dh", int(d.Hours())) + str
	}
	return str
}
func NewCloser(initial int) *Closer {
	ret := &Closer{closed: make(chan struct{})}
	ret.waiting.Add(initial)
	return ret
}
func NewThrottle(max int) *Throttle {
	return &Throttle{
		ch:    make(chan struct{}, max),
		errCh: make(chan error, max),
	}
}
func (t *Throttle) Do() error {
	for {
		select {
		case t.ch <- struct{}{}:
			t.wg.Add(1)
			return nil
		case err := <-t.errCh:
			if err != nil {
				return err
			}
		}
	}
}
func (t *Throttle) Done(err error) {
	if err != nil {
		t.errCh <- err
	}
	select {
	case <-t.ch:
	default:
		panic("Throttle Do Done mismatch")
	}
	t.wg.Done()
}
func (t *Throttle) Finish() error {
	t.wg.Wait()
	close(t.ch)
	close(t.errCh)
	for err := range t.errCh {
		if err != nil {
			return err
		}
	}
	return nil
}
func (db *DB) SetDiscardTs(ts uint64) {
	if !db.opt.managedTxns {
		panic("Cannot use SetDiscardTs with managedDB=false.")
	}
	db.orc.setDiscardTs(ts)
}
func (lf *logFile) openReadOnly() error {
	var err error
	lf.fd, err = os.OpenFile(lf.path, os.O_RDONLY, 0666)
	if err != nil {
		return errors.Wrapf(err, "Unable to open %q as RDONLY.", lf.path)
	}

	fi, err := lf.fd.Stat()
	if err != nil {
		return errors.Wrapf(err, "Unable to check stat for %q", lf.path)
	}
	y.AssertTrue(fi.Size() <= math.MaxUint32)
	lf.size = uint32(fi.Size())

	if err = lf.mmap(fi.Size()); err != nil {
		_ = lf.fd.Close()
		return y.Wrapf(err, "Unable to map file")
	}

	return nil
}
func (vlog *valueLog) iterate(lf *logFile, offset uint32, fn logEntry) (uint32, error) {
	fi, err := lf.fd.Stat()
	if err != nil {
		return 0, err
	}
	if int64(offset) == fi.Size() {
		// We're at the end of the file already. No need to do anything.
		return offset, nil
	}
	if vlog.opt.ReadOnly {
		// We're not at the end of the file. We'd need to replay the entries, or
		// possibly truncate the file.
		return 0, ErrReplayNeeded
	}

	// We're not at the end of the file. Let's Seek to the offset and start reading.
	if _, err := lf.fd.Seek(int64(offset), io.SeekStart); err != nil {
		return 0, errFile(err, lf.path, "Unable to seek")
	}

	reader := bufio.NewReader(lf.fd)
	read := &safeRead{
		k:            make([]byte, 10),
		v:            make([]byte, 10),
		recordOffset: offset,
	}

	var lastCommit uint64
	var validEndOffset uint32
	for {
		e, err := read.Entry(reader)
		if err == io.EOF {
			break
		} else if err == io.ErrUnexpectedEOF || err == errTruncate {
			break
		} else if err != nil {
			return 0, err
		} else if e == nil {
			continue
		}

		var vp valuePointer
		vp.Len = uint32(headerBufSize + len(e.Key) + len(e.Value) + crc32.Size)
		read.recordOffset += vp.Len

		vp.Offset = e.offset
		vp.Fid = lf.fid

		if e.meta&bitTxn > 0 {
			txnTs := y.ParseTs(e.Key)
			if lastCommit == 0 {
				lastCommit = txnTs
			}
			if lastCommit != txnTs {
				break
			}

		} else if e.meta&bitFinTxn > 0 {
			txnTs, err := strconv.ParseUint(string(e.Value), 10, 64)
			if err != nil || lastCommit != txnTs {
				break
			}
			// Got the end of txn. Now we can store them.
			lastCommit = 0
			validEndOffset = read.recordOffset

		} else {
			if lastCommit != 0 {
				// This is most likely an entry which was moved as part of GC.
				// We shouldn't get this entry in the middle of a transaction.
				break
			}
			validEndOffset = read.recordOffset
		}

		if err := fn(*e, vp); err != nil {
			if err == errStop {
				break
			}
			return 0, errFile(err, lf.path, "Iteration function")
		}
	}
	return validEndOffset, nil
}
func (vlog *valueLog) sortedFids() []uint32 {
	toBeDeleted := make(map[uint32]struct{})
	for _, fid := range vlog.filesToBeDeleted {
		toBeDeleted[fid] = struct{}{}
	}
	ret := make([]uint32, 0, len(vlog.filesMap))
	for fid := range vlog.filesMap {
		if _, ok := toBeDeleted[fid]; !ok {
			ret = append(ret, fid)
		}
	}
	sort.Slice(ret, func(i, j int) bool {
		return ret[i] < ret[j]
	})
	return ret
}
func (vlog *valueLog) write(reqs []*request) error {
	vlog.filesLock.RLock()
	maxFid := atomic.LoadUint32(&vlog.maxFid)
	curlf := vlog.filesMap[maxFid]
	vlog.filesLock.RUnlock()

	var buf bytes.Buffer
	toDisk := func() error {
		if buf.Len() == 0 {
			return nil
		}
		vlog.elog.Printf("Flushing %d blocks of total size: %d", len(reqs), buf.Len())
		n, err := curlf.fd.Write(buf.Bytes())
		if err != nil {
			return errors.Wrapf(err, "Unable to write to value log file: %q", curlf.path)
		}
		buf.Reset()
		y.NumWrites.Add(1)
		y.NumBytesWritten.Add(int64(n))
		vlog.elog.Printf("Done")
		atomic.AddUint32(&vlog.writableLogOffset, uint32(n))

		if vlog.woffset() > uint32(vlog.opt.ValueLogFileSize) ||
			vlog.numEntriesWritten > vlog.opt.ValueLogMaxEntries {
			var err error
			if err = curlf.doneWriting(vlog.woffset()); err != nil {
				return err
			}

			newid := atomic.AddUint32(&vlog.maxFid, 1)
			y.AssertTruef(newid > 0, "newid has overflown uint32: %v", newid)
			newlf, err := vlog.createVlogFile(newid)
			if err != nil {
				return err
			}
			curlf = newlf
		}
		return nil
	}

	for i := range reqs {
		b := reqs[i]
		b.Ptrs = b.Ptrs[:0]
		for j := range b.Entries {
			e := b.Entries[j]
			var p valuePointer

			p.Fid = curlf.fid
			// Use the offset including buffer length so far.
			p.Offset = vlog.woffset() + uint32(buf.Len())
			plen, err := encodeEntry(e, &buf) // Now encode the entry into buffer.
			if err != nil {
				return err
			}
			p.Len = uint32(plen)
			b.Ptrs = append(b.Ptrs, p)
		}
		vlog.numEntriesWritten += uint32(len(b.Entries))
		// We write to disk here so that all entries that are part of the same transaction are
		// written to the same vlog file.
		writeNow :=
			vlog.woffset()+uint32(buf.Len()) > uint32(vlog.opt.ValueLogFileSize) ||
				vlog.numEntriesWritten > uint32(vlog.opt.ValueLogMaxEntries)
		if writeNow {
			if err := toDisk(); err != nil {
				return err
			}
		}
	}
	return toDisk()
}
func (vlog *valueLog) populateDiscardStats() error {
	discardStatsKey := y.KeyWithTs(lfDiscardStatsKey, math.MaxUint64)
	vs, err := vlog.db.get(discardStatsKey)
	if err != nil {
		return err
	}

	// check if value is Empty
	if vs.Value == nil || len(vs.Value) == 0 {
		vlog.lfDiscardStats = &lfDiscardStats{m: make(map[uint32]int64)}
		return nil
	}

	var statsMap map[uint32]int64
	if err := json.Unmarshal(vs.Value, &statsMap); err != nil {
		return err
	}
	vlog.opt.Debugf("Value Log Discard stats: %v", statsMap)
	vlog.lfDiscardStats = &lfDiscardStats{m: statsMap}
	return nil
}
func (db *DB) Backup(w io.Writer, since uint64) (uint64, error) {
	stream := db.NewStream()
	stream.LogPrefix = "DB.Backup"
	return stream.Backup(w, since)
}
func (st *Stream) ToList(key []byte, itr *Iterator) (*pb.KVList, error) {
	list := &pb.KVList{}
	for ; itr.Valid(); itr.Next() {
		item := itr.Item()
		if item.IsDeletedOrExpired() {
			break
		}
		if !bytes.Equal(key, item.Key()) {
			// Break out on the first encounter with another key.
			break
		}

		valCopy, err := item.ValueCopy(nil)
		if err != nil {
			return nil, err
		}
		kv := &pb.KV{
			Key:       item.KeyCopy(nil),
			Value:     valCopy,
			UserMeta:  []byte{item.UserMeta()},
			Version:   item.Version(),
			ExpiresAt: item.ExpiresAt(),
		}
		list.Kv = append(list.Kv, kv)
		if st.db.opt.NumVersionsToKeep == 1 {
			break
		}

		if item.DiscardEarlierVersions() {
			break
		}
	}
	return list, nil
}
func (st *Stream) produceRanges(ctx context.Context) {
	splits := st.db.KeySplits(st.Prefix)
	start := y.SafeCopy(nil, st.Prefix)
	for _, key := range splits {
		st.rangeCh <- keyRange{left: start, right: y.SafeCopy(nil, []byte(key))}
		start = y.SafeCopy(nil, []byte(key))
	}
	// Edge case: prefix is empty and no splits exist. In that case, we should have at least one
	// keyRange output.
	st.rangeCh <- keyRange{left: start}
	close(st.rangeCh)
}
func (st *Stream) produceKVs(ctx context.Context) error {
	var size int
	var txn *Txn
	if st.readTs > 0 {
		txn = st.db.NewTransactionAt(st.readTs, false)
	} else {
		txn = st.db.NewTransaction(false)
	}
	defer txn.Discard()

	iterate := func(kr keyRange) error {
		iterOpts := DefaultIteratorOptions
		iterOpts.AllVersions = true
		iterOpts.Prefix = st.Prefix
		iterOpts.PrefetchValues = false
		itr := txn.NewIterator(iterOpts)
		defer itr.Close()

		outList := new(pb.KVList)
		var prevKey []byte
		for itr.Seek(kr.left); itr.Valid(); {
			// it.Valid would only return true for keys with the provided Prefix in iterOpts.
			item := itr.Item()
			if bytes.Equal(item.Key(), prevKey) {
				itr.Next()
				continue
			}
			prevKey = append(prevKey[:0], item.Key()...)

			// Check if we reached the end of the key range.
			if len(kr.right) > 0 && bytes.Compare(item.Key(), kr.right) >= 0 {
				break
			}
			// Check if we should pick this key.
			if st.ChooseKey != nil && !st.ChooseKey(item) {
				continue
			}

			// Now convert to key value.
			list, err := st.KeyToList(item.KeyCopy(nil), itr)
			if err != nil {
				return err
			}
			if list == nil || len(list.Kv) == 0 {
				continue
			}
			outList.Kv = append(outList.Kv, list.Kv...)
			size += list.Size()
			if size >= pageSize {
				st.kvChan <- outList
				outList = new(pb.KVList)
				size = 0
			}
		}
		if len(outList.Kv) > 0 {
			st.kvChan <- outList
		}
		return nil
	}

	for {
		select {
		case kr, ok := <-st.rangeCh:
			if !ok {
				// Done with the keys.
				return nil
			}
			if err := iterate(kr); err != nil {
				return err
			}
		case <-ctx.Done():
			return ctx.Err()
		}
	}
}
func (st *Stream) Orchestrate(ctx context.Context) error {
	st.rangeCh = make(chan keyRange, 3) // Contains keys for posting lists.

	// kvChan should only have a small capacity to ensure that we don't buffer up too much data if
	// sending is slow. Page size is set to 4MB, which is used to lazily cap the size of each
	// KVList. To get around 64MB buffer, we can set the channel size to 16.
	st.kvChan = make(chan *pb.KVList, 16)

	if st.KeyToList == nil {
		st.KeyToList = st.ToList
	}

	// Picks up ranges from Badger, and sends them to rangeCh.
	go st.produceRanges(ctx)

	errCh := make(chan error, 1) // Stores error by consumeKeys.
	var wg sync.WaitGroup
	for i := 0; i < st.NumGo; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			// Picks up ranges from rangeCh, generates KV lists, and sends them to kvChan.
			if err := st.produceKVs(ctx); err != nil {
				select {
				case errCh <- err:
				default:
				}
			}
		}()
	}

	// Pick up key-values from kvChan and send to stream.
	kvErr := make(chan error, 1)
	go func() {
		// Picks up KV lists from kvChan, and sends them to Output.
		kvErr <- st.streamKVs(ctx)
	}()
	wg.Wait()        // Wait for produceKVs to be over.
	close(st.kvChan) // Now we can close kvChan.

	select {
	case err := <-errCh: // Check error from produceKVs.
		return err
	default:
	}

	// Wait for key streaming to be over.
	err := <-kvErr
	return err
}
func (db *DB) NewStream() *Stream {
	if db.opt.managedTxns {
		panic("This API can not be called in managed mode.")
	}
	return db.newStream()
}
func (db *DB) NewStreamAt(readTs uint64) *Stream {
	if !db.opt.managedTxns {
		panic("This API can only be called in managed mode.")
	}
	stream := db.newStream()
	stream.readTs = readTs
	return stream
}
func (t *Table) DecrRef() error {
	newRef := atomic.AddInt32(&t.ref, -1)
	if newRef == 0 {
		// We can safely delete this file, because for all the current files, we always have
		// at least one reference pointing to them.

		// It's necessary to delete windows files
		if t.loadingMode == options.MemoryMap {
			y.Munmap(t.mmap)
		}
		if err := t.fd.Truncate(0); err != nil {
			// This is very important to let the FS know that the file is deleted.
			return err
		}
		filename := t.fd.Name()
		if err := t.fd.Close(); err != nil {
			return err
		}
		if err := os.Remove(filename); err != nil {
			return err
		}
	}
	return nil
}
func ParseFileID(name string) (uint64, bool) {
	name = path.Base(name)
	if !strings.HasSuffix(name, fileSuffix) {
		return 0, false
	}
	//	suffix := name[len(fileSuffix):]
	name = strings.TrimSuffix(name, fileSuffix)
	id, err := strconv.Atoi(name)
	if err != nil {
		return 0, false
	}
	y.AssertTrue(id >= 0)
	return uint64(id), true
}
func (db *DB) PrintHistogram(keyPrefix []byte) {
	if db == nil {
		fmt.Println("\nCannot build histogram: DB is nil.")
		return
	}
	histogram := db.buildHistogram(keyPrefix)
	fmt.Printf("Histogram of key sizes (in bytes)\n")
	histogram.keySizeHistogram.printHistogram()
	fmt.Printf("Histogram of value sizes (in bytes)\n")
	histogram.valueSizeHistogram.printHistogram()
}
func newSizeHistogram() *sizeHistogram {
	// TODO(ibrahim): find appropriate bin size.
	keyBins := createHistogramBins(1, 16)
	valueBins := createHistogramBins(1, 30)
	return &sizeHistogram{
		keySizeHistogram: histogramData{
			bins:        keyBins,
			countPerBin: make([]int64, len(keyBins)+1),
			max:         math.MinInt64,
			min:         math.MaxInt64,
			sum:         0,
		},
		valueSizeHistogram: histogramData{
			bins:        valueBins,
			countPerBin: make([]int64, len(valueBins)+1),
			max:         math.MinInt64,
			min:         math.MaxInt64,
			sum:         0,
		},
	}
}
func (db *DB) buildHistogram(keyPrefix []byte) *sizeHistogram {
	txn := db.NewTransaction(false)
	defer txn.Discard()

	itr := txn.NewIterator(DefaultIteratorOptions)
	defer itr.Close()

	badgerHistogram := newSizeHistogram()

	// Collect key and value sizes.
	for itr.Seek(keyPrefix); itr.ValidForPrefix(keyPrefix); itr.Next() {
		item := itr.Item()
		badgerHistogram.keySizeHistogram.Update(item.KeySize())
		badgerHistogram.valueSizeHistogram.Update(item.ValueSize())
	}
	return badgerHistogram
}
func (histogram histogramData) printHistogram() {
	fmt.Printf("Total count: %d\n", histogram.totalCount)
	fmt.Printf("Min value: %d\n", histogram.min)
	fmt.Printf("Max value: %d\n", histogram.max)
	fmt.Printf("Mean: %.2f\n", float64(histogram.sum)/float64(histogram.totalCount))
	fmt.Printf("%24s %9s\n", "Range", "Count")

	numBins := len(histogram.bins)
	for index, count := range histogram.countPerBin {
		if count == 0 {
			continue
		}

		// The last bin represents the bin that contains the range from
		// the last bin up to infinity so it's processed differently than the
		// other bins.
		if index == len(histogram.countPerBin)-1 {
			lowerBound := int(histogram.bins[numBins-1])
			fmt.Printf("[%10d, %10s) %9d\n", lowerBound, "infinity", count)
			continue
		}

		upperBound := int(histogram.bins[index])
		lowerBound := 0
		if index > 0 {
			lowerBound = int(histogram.bins[index-1])
		}

		fmt.Printf("[%10d, %10d) %9d\n", lowerBound, upperBound, count)
	}
	fmt.Println()
}
func (w *WaterMark) Init(closer *Closer) {
	w.markCh = make(chan mark, 100)
	w.elog = trace.NewEventLog("Watermark", w.Name)
	go w.process(closer)
}
func (w *WaterMark) Begin(index uint64) {
	atomic.StoreUint64(&w.lastIndex, index)
	w.markCh <- mark{index: index, done: false}
}
func (w *WaterMark) BeginMany(indices []uint64) {
	atomic.StoreUint64(&w.lastIndex, indices[len(indices)-1])
	w.markCh <- mark{index: 0, indices: indices, done: false}
}
func (w *WaterMark) Done(index uint64) {
	w.markCh <- mark{index: index, done: true}
}
func (w *WaterMark) DoneMany(indices []uint64) {
	w.markCh <- mark{index: 0, indices: indices, done: true}
}
func (w *WaterMark) SetDoneUntil(val uint64) {
	atomic.StoreUint64(&w.doneUntil, val)
}
func (w *WaterMark) WaitForMark(ctx context.Context, index uint64) error {
	if w.DoneUntil() >= index {
		return nil
	}
	waitCh := make(chan struct{})
	w.markCh <- mark{index: index, waiter: waitCh}

	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-waitCh:
		return nil
	}
}
func (h header) Encode(b []byte) {
	binary.BigEndian.PutUint16(b[0:2], h.plen)
	binary.BigEndian.PutUint16(b[2:4], h.klen)
	binary.BigEndian.PutUint16(b[4:6], h.vlen)
	binary.BigEndian.PutUint32(b[6:10], h.prev)
}
func (h *header) Decode(buf []byte) int {
	h.plen = binary.BigEndian.Uint16(buf[0:2])
	h.klen = binary.BigEndian.Uint16(buf[2:4])
	h.vlen = binary.BigEndian.Uint16(buf[4:6])
	h.prev = binary.BigEndian.Uint32(buf[6:10])
	return h.Size()
}
func NewTableBuilder() *Builder {
	return &Builder{
		keyBuf:     newBuffer(1 << 20),
		buf:        newBuffer(1 << 20),
		prevOffset: math.MaxUint32, // Used for the first element!
	}
}
func (b Builder) keyDiff(newKey []byte) []byte {
	var i int
	for i = 0; i < len(newKey) && i < len(b.baseKey); i++ {
		if newKey[i] != b.baseKey[i] {
			break
		}
	}
	return newKey[i:]
}
func (b *Builder) Add(key []byte, value y.ValueStruct) error {
	if b.counter >= restartInterval {
		b.finishBlock()
		// Start a new block. Initialize the block.
		b.restarts = append(b.restarts, uint32(b.buf.Len()))
		b.counter = 0
		b.baseKey = []byte{}
		b.baseOffset = uint32(b.buf.Len())
		b.prevOffset = math.MaxUint32 // First key-value pair of block has header.prev=MaxInt.
	}
	b.addHelper(key, value)
	return nil // Currently, there is no meaningful error.
}
func (b *Builder) blockIndex() []byte {
	// Store the end offset, so we know the length of the final block.
	b.restarts = append(b.restarts, uint32(b.buf.Len()))

	// Add 4 because we want to write out number of restarts at the end.
	sz := 4*len(b.restarts) + 4
	out := make([]byte, sz)
	buf := out
	for _, r := range b.restarts {
		binary.BigEndian.PutUint32(buf[:4], r)
		buf = buf[4:]
	}
	binary.BigEndian.PutUint32(buf[:4], uint32(len(b.restarts)))
	return out
}
func (b *Builder) Finish() []byte {
	bf := bbloom.New(float64(b.keyCount), 0.01)
	var klen [2]byte
	key := make([]byte, 1024)
	for {
		if _, err := b.keyBuf.Read(klen[:]); err == io.EOF {
			break
		} else if err != nil {
			y.Check(err)
		}
		kl := int(binary.BigEndian.Uint16(klen[:]))
		if cap(key) < kl {
			key = make([]byte, 2*int(kl)) // 2 * uint16 will overflow
		}
		key = key[:kl]
		y.Check2(b.keyBuf.Read(key))
		bf.Add(key)
	}

	b.finishBlock() // This will never start a new block.
	index := b.blockIndex()
	b.buf.Write(index)

	// Write bloom filter.
	bdata := bf.JSONMarshal()
	n, err := b.buf.Write(bdata)
	y.Check(err)
	var buf [4]byte
	binary.BigEndian.PutUint32(buf[:], uint32(n))
	b.buf.Write(buf[:])

	return b.buf.Bytes()
}
func (opt *Options) Errorf(format string, v ...interface{}) {
	if opt.Logger == nil {
		return
	}
	opt.Logger.Errorf(format, v...)
}
func (opt *Options) Infof(format string, v ...interface{}) {
	if opt.Logger == nil {
		return
	}
	opt.Logger.Infof(format, v...)
}
func (s *Skiplist) DecrRef() {
	newRef := atomic.AddInt32(&s.ref, -1)
	if newRef > 0 {
		return
	}

	s.arena.reset()
	// Indicate we are closed. Good for testing.  Also, lets GC reclaim memory. Race condition
	// here would suggest we are accessing skiplist when we are supposed to have no reference!
	s.arena = nil
}
func NewSkiplist(arenaSize int64) *Skiplist {
	arena := newArena(arenaSize)
	head := newNode(arena, nil, y.ValueStruct{}, maxHeight)
	return &Skiplist{
		height: 1,
		head:   head,
		arena:  arena,
		ref:    1,
	}
}
func (s *Skiplist) Put(key []byte, v y.ValueStruct) {
	// Since we allow overwrite, we may not need to create a new node. We might not even need to
	// increase the height. Let's defer these actions.

	listHeight := s.getHeight()
	var prev [maxHeight + 1]*node
	var next [maxHeight + 1]*node
	prev[listHeight] = s.head
	next[listHeight] = nil
	for i := int(listHeight) - 1; i >= 0; i-- {
		// Use higher level to speed up for current level.
		prev[i], next[i] = s.findSpliceForLevel(key, prev[i+1], i)
		if prev[i] == next[i] {
			prev[i].setValue(s.arena, v)
			return
		}
	}

	// We do need to create a new node.
	height := randomHeight()
	x := newNode(s.arena, key, v, height)

	// Try to increase s.height via CAS.
	listHeight = s.getHeight()
	for height > int(listHeight) {
		if atomic.CompareAndSwapInt32(&s.height, listHeight, int32(height)) {
			// Successfully increased skiplist.height.
			break
		}
		listHeight = s.getHeight()
	}

	// We always insert from the base level and up. After you add a node in base level, we cannot
	// create a node in the level above because it would have discovered the node in the base level.
	for i := 0; i < height; i++ {
		for {
			if prev[i] == nil {
				y.AssertTrue(i > 1) // This cannot happen in base level.
				// We haven't computed prev, next for this level because height exceeds old listHeight.
				// For these levels, we expect the lists to be sparse, so we can just search from head.
				prev[i], next[i] = s.findSpliceForLevel(key, s.head, i)
				// Someone adds the exact same key before we are able to do so. This can only happen on
				// the base level. But we know we are not on the base level.
				y.AssertTrue(prev[i] != next[i])
			}
			nextOffset := s.arena.getNodeOffset(next[i])
			x.tower[i] = nextOffset
			if prev[i].casNextOffset(i, nextOffset, s.arena.getNodeOffset(x)) {
				// Managed to insert x between prev[i] and next[i]. Go to the next level.
				break
			}
			// CAS failed. We need to recompute prev and next.
			// It is unlikely to be helpful to try to use a different level as we redo the search,
			// because it is unlikely that lots of nodes are inserted between prev[i] and next[i].
			prev[i], next[i] = s.findSpliceForLevel(key, prev[i], i)
			if prev[i] == next[i] {
				y.AssertTruef(i == 0, "Equality can happen only on base level: %d", i)
				prev[i].setValue(s.arena, v)
				return
			}
		}
	}
}
func (s *Skiplist) Get(key []byte) y.ValueStruct {
	n, _ := s.findNear(key, false, true) // findGreaterOrEqual.
	if n == nil {
		return y.ValueStruct{}
	}

	nextKey := s.arena.getKey(n.keyOffset, n.keySize)
	if !y.SameKey(key, nextKey) {
		return y.ValueStruct{}
	}

	valOffset, valSize := n.getValueOffset()
	vs := s.arena.getVal(valOffset, valSize)
	vs.Version = y.ParseTs(nextKey)
	return vs
}
func (s *Iterator) Key() []byte {
	return s.list.arena.getKey(s.n.keyOffset, s.n.keySize)
}
func (s *Iterator) Value() y.ValueStruct {
	valOffset, valSize := s.n.getValueOffset()
	return s.list.arena.getVal(valOffset, valSize)
}
func (s *Iterator) Next() {
	y.AssertTrue(s.Valid())
	s.n = s.list.getNext(s.n, 0)
}
func (s *Iterator) Prev() {
	y.AssertTrue(s.Valid())
	s.n, _ = s.list.findNear(s.Key(), true, false) // find <. No equality allowed.
}
func (s *Iterator) Seek(target []byte) {
	s.n, _ = s.list.findNear(target, false, true) // find >=.
}
func (s *Iterator) SeekForPrev(target []byte) {
	s.n, _ = s.list.findNear(target, true, true) // find <=.
}
func (s *Skiplist) NewUniIterator(reversed bool) *UniIterator {
	return &UniIterator{
		iter:     s.NewIterator(),
		reversed: reversed,
	}
}
func (s *UniIterator) Next() {
	if !s.reversed {
		s.iter.Next()
	} else {
		s.iter.Prev()
	}
}
func (s *UniIterator) Seek(key []byte) {
	if !s.reversed {
		s.iter.Seek(key)
	} else {
		s.iter.SeekForPrev(key)
	}
}
func (m *Manifest) asChanges() []*pb.ManifestChange {
	changes := make([]*pb.ManifestChange, 0, len(m.Tables))
	for id, tm := range m.Tables {
		changes = append(changes, newCreateChange(id, int(tm.Level), tm.Checksum))
	}
	return changes
}
func (mf *manifestFile) rewrite() error {
	// In Windows the files should be closed before doing a Rename.
	if err := mf.fp.Close(); err != nil {
		return err
	}
	fp, netCreations, err := helpRewrite(mf.directory, &mf.manifest)
	if err != nil {
		return err
	}
	mf.fp = fp
	mf.manifest.Creations = netCreations
	mf.manifest.Deletions = 0

	return nil
}
func (s *levelHandler) validate() error {
	if s.level == 0 {
		return nil
	}

	s.RLock()
	defer s.RUnlock()
	numTables := len(s.tables)
	for j := 1; j < numTables; j++ {
		if j >= len(s.tables) {
			return errors.Errorf("Level %d, j=%d numTables=%d", s.level, j, numTables)
		}

		if y.CompareKeys(s.tables[j-1].Biggest(), s.tables[j].Smallest()) >= 0 {
			return errors.Errorf(
				"Inter: Biggest(j-1) \n%s\n vs Smallest(j): \n%s\n: level=%d j=%d numTables=%d",
				hex.Dump(s.tables[j-1].Biggest()), hex.Dump(s.tables[j].Smallest()),
				s.level, j, numTables)
		}

		if y.CompareKeys(s.tables[j].Smallest(), s.tables[j].Biggest()) > 0 {
			return errors.Errorf(
				"Intra: %q vs %q: level=%d j=%d numTables=%d",
				s.tables[j].Smallest(), s.tables[j].Biggest(), s.level, j, numTables)
		}
	}
	return nil
}
func acquireDirectoryLock(dirPath string, pidFileName string, readOnly bool) (*directoryLockGuard, error) {
	if readOnly {
		return nil, ErrWindowsNotSupported
	}

	// Convert to absolute path so that Release still works even if we do an unbalanced
	// chdir in the meantime.
	absLockFilePath, err := filepath.Abs(filepath.Join(dirPath, pidFileName))
	if err != nil {
		return nil, errors.Wrap(err, "Cannot get absolute path for pid lock file")
	}

	// This call creates a file handler in memory that only one process can use at a time. When
	// that process ends, the file is deleted by the system.
	// FILE_ATTRIBUTE_TEMPORARY is used to tell Windows to try to create the handle in memory.
	// FILE_FLAG_DELETE_ON_CLOSE is not specified in syscall_windows.go but tells Windows to delete
	// the file when all processes holding the handler are closed.
	// XXX: this works but it's a bit klunky. i'd prefer to use LockFileEx but it needs unsafe pkg.
	h, err := syscall.CreateFile(
		syscall.StringToUTF16Ptr(absLockFilePath), 0, 0, nil,
		syscall.OPEN_ALWAYS,
		uint32(FILE_ATTRIBUTE_TEMPORARY|FILE_FLAG_DELETE_ON_CLOSE),
		0)
	if err != nil {
		return nil, errors.Wrapf(err,
			"Cannot create lock file %q.  Another process is using this Badger database",
			absLockFilePath)
	}

	return &directoryLockGuard{h: h, path: absLockFilePath}, nil
}
func (g *directoryLockGuard) release() error {
	g.path = ""
	return syscall.CloseHandle(g.h)
}
func AssertTruef(b bool, format string, args ...interface{}) {
	if !b {
		log.Fatalf("%+v", errors.Errorf(format, args...))
	}
}
func Wrapf(err error, format string, args ...interface{}) error {
	if !debugMode {
		if err == nil {
			return nil
		}
		return fmt.Errorf(format+" error: %+v", append(args, err)...)
	}
	return errors.Wrapf(err, format, args...)
}
func (s *levelHandler) initTables(tables []*table.Table) {
	s.Lock()
	defer s.Unlock()

	s.tables = tables
	s.totalSize = 0
	for _, t := range tables {
		s.totalSize += t.Size()
	}

	if s.level == 0 {
		// Key range will overlap. Just sort by fileID in ascending order
		// because newer tables are at the end of level 0.
		sort.Slice(s.tables, func(i, j int) bool {
			return s.tables[i].ID() < s.tables[j].ID()
		})
	} else {
		// Sort tables by keys.
		sort.Slice(s.tables, func(i, j int) bool {
			return y.CompareKeys(s.tables[i].Smallest(), s.tables[j].Smallest()) < 0
		})
	}
}
func (s *levelHandler) deleteTables(toDel []*table.Table) error {
	s.Lock() // s.Unlock() below

	toDelMap := make(map[uint64]struct{})
	for _, t := range toDel {
		toDelMap[t.ID()] = struct{}{}
	}

	// Make a copy as iterators might be keeping a slice of tables.
	var newTables []*table.Table
	for _, t := range s.tables {
		_, found := toDelMap[t.ID()]
		if !found {
			newTables = append(newTables, t)
			continue
		}
		s.totalSize -= t.Size()
	}
	s.tables = newTables

	s.Unlock() // Unlock s _before_ we DecrRef our tables, which can be slow.

	return decrRefs(toDel)
}
func (s *levelHandler) tryAddLevel0Table(t *table.Table) bool {
	y.AssertTrue(s.level == 0)
	// Need lock as we may be deleting the first table during a level 0 compaction.
	s.Lock()
	defer s.Unlock()
	if len(s.tables) >= s.db.opt.NumLevelZeroTablesStall {
		return false
	}

	s.tables = append(s.tables, t)
	t.IncrRef()
	s.totalSize += t.Size()

	return true
}
func (s *levelHandler) getTableForKey(key []byte) ([]*table.Table, func() error) {
	s.RLock()
	defer s.RUnlock()

	if s.level == 0 {
		// For level 0, we need to check every table. Remember to make a copy as s.tables may change
		// once we exit this function, and we don't want to lock s.tables while seeking in tables.
		// CAUTION: Reverse the tables.
		out := make([]*table.Table, 0, len(s.tables))
		for i := len(s.tables) - 1; i >= 0; i-- {
			out = append(out, s.tables[i])
			s.tables[i].IncrRef()
		}
		return out, func() error {
			for _, t := range out {
				if err := t.DecrRef(); err != nil {
					return err
				}
			}
			return nil
		}
	}
	// For level >= 1, we can do a binary search as key range does not overlap.
	idx := sort.Search(len(s.tables), func(i int) bool {
		return y.CompareKeys(s.tables[i].Biggest(), key) >= 0
	})
	if idx >= len(s.tables) {
		// Given key is strictly > than every element we have.
		return nil, func() error { return nil }
	}
	tbl := s.tables[idx]
	tbl.IncrRef()
	return []*table.Table{tbl}, tbl.DecrRef
}
func (s *levelHandler) get(key []byte) (y.ValueStruct, error) {
	tables, decr := s.getTableForKey(key)
	keyNoTs := y.ParseKey(key)

	var maxVs y.ValueStruct
	for _, th := range tables {
		if th.DoesNotHave(keyNoTs) {
			y.NumLSMBloomHits.Add(s.strLevel, 1)
			continue
		}

		it := th.NewIterator(false)
		defer it.Close()

		y.NumLSMGets.Add(s.strLevel, 1)
		it.Seek(key)
		if !it.Valid() {
			continue
		}
		if y.SameKey(key, it.Key()) {
			if version := y.ParseTs(it.Key()); maxVs.Version < version {
				maxVs = it.Value()
				maxVs.Version = version
			}
		}
	}
	return maxVs, decr()
}
func (s *levelHandler) overlappingTables(_ levelHandlerRLocked, kr keyRange) (int, int) {
	if len(kr.left) == 0 || len(kr.right) == 0 {
		return 0, 0
	}
	left := sort.Search(len(s.tables), func(i int) bool {
		return y.CompareKeys(kr.left, s.tables[i].Biggest()) <= 0
	})
	right := sort.Search(len(s.tables), func(i int) bool {
		return y.CompareKeys(kr.right, s.tables[i].Smallest()) < 0
	})
	return left, right
}
func (item *Item) String() string {
	return fmt.Sprintf("key=%q, version=%d, meta=%x", item.Key(), item.Version(), item.meta)
}
func (item *Item) KeyCopy(dst []byte) []byte {
	return y.SafeCopy(dst, item.key)
}
func (item *Item) ValueSize() int64 {
	if !item.hasValue() {
		return 0
	}
	if (item.meta & bitValuePointer) == 0 {
		return int64(len(item.vptr))
	}
	var vp valuePointer
	vp.Decode(item.vptr)

	klen := int64(len(item.key) + 8) // 8 bytes for timestamp.
	return int64(vp.Len) - klen - headerBufSize - crc32.Size
}
func (txn *Txn) NewKeyIterator(key []byte, opt IteratorOptions) *Iterator {
	if len(opt.Prefix) > 0 {
		panic("opt.Prefix should be nil for NewKeyIterator.")
	}
	opt.Prefix = key // This key must be without the timestamp.
	opt.prefixIsKey = true
	return txn.NewIterator(opt)
}
func (it *Iterator) Valid() bool {
	if it.item == nil {
		return false
	}
	return bytes.HasPrefix(it.item.key, it.opt.Prefix)
}
func (it *Iterator) ValidForPrefix(prefix []byte) bool {
	return it.Valid() && bytes.HasPrefix(it.item.key, prefix)
}
func (it *Iterator) Close() {
	if it.closed {
		return
	}
	it.closed = true

	it.iitr.Close()
	// It is important to wait for the fill goroutines to finish. Otherwise, we might leave zombie
	// goroutines behind, which are waiting to acquire file read locks after DB has been closed.
	waitFor := func(l list) {
		item := l.pop()
		for item != nil {
			item.wg.Wait()
			item = l.pop()
		}
	}
	waitFor(it.waste)
	waitFor(it.data)

	// TODO: We could handle this error.
	_ = it.txn.db.vlog.decrIteratorCount()
	atomic.AddInt32(&it.txn.numIterators, -1)
}
func (it *Iterator) parseItem() bool {
	mi := it.iitr
	key := mi.Key()

	setItem := func(item *Item) {
		if it.item == nil {
			it.item = item
		} else {
			it.data.push(item)
		}
	}

	// Skip badger keys.
	if !it.opt.internalAccess && bytes.HasPrefix(key, badgerPrefix) {
		mi.Next()
		return false
	}

	// Skip any versions which are beyond the readTs.
	version := y.ParseTs(key)
	if version > it.readTs {
		mi.Next()
		return false
	}

	if it.opt.AllVersions {
		// Return deleted or expired values also, otherwise user can't figure out
		// whether the key was deleted.
		item := it.newItem()
		it.fill(item)
		setItem(item)
		mi.Next()
		return true
	}

	// If iterating in forward direction, then just checking the last key against current key would
	// be sufficient.
	if !it.opt.Reverse {
		if y.SameKey(it.lastKey, key) {
			mi.Next()
			return false
		}
		// Only track in forward direction.
		// We should update lastKey as soon as we find a different key in our snapshot.
		// Consider keys: a 5, b 7 (del), b 5. When iterating, lastKey = a.
		// Then we see b 7, which is deleted. If we don't store lastKey = b, we'll then return b 5,
		// which is wrong. Therefore, update lastKey here.
		it.lastKey = y.SafeCopy(it.lastKey, mi.Key())
	}

FILL:
	// If deleted, advance and return.
	vs := mi.Value()
	if isDeletedOrExpired(vs.Meta, vs.ExpiresAt) {
		mi.Next()
		return false
	}

	item := it.newItem()
	it.fill(item)
	// fill item based on current cursor position. All Next calls have returned, so reaching here
	// means no Next was called.

	mi.Next()                           // Advance but no fill item yet.
	if !it.opt.Reverse || !mi.Valid() { // Forward direction, or invalid.
		setItem(item)
		return true
	}

	// Reverse direction.
	nextTs := y.ParseTs(mi.Key())
	mik := y.ParseKey(mi.Key())
	if nextTs <= it.readTs && bytes.Equal(mik, item.key) {
		// This is a valid potential candidate.
		goto FILL
	}
	// Ignore the next candidate. Return the current one.
	setItem(item)
	return true
}
func (it *Iterator) Seek(key []byte) {
	for i := it.data.pop(); i != nil; i = it.data.pop() {
		i.wg.Wait()
		it.waste.push(i)
	}

	it.lastKey = it.lastKey[:0]
	if len(key) == 0 {
		key = it.opt.Prefix
	}
	if len(key) == 0 {
		it.iitr.Rewind()
		it.prefetch()
		return
	}

	if !it.opt.Reverse {
		key = y.KeyWithTs(key, it.txn.readTs)
	} else {
		key = y.KeyWithTs(key, 0)
	}
	it.iitr.Seek(key)
	it.prefetch()
}
func (db *DB) GetMergeOperator(key []byte,
	f MergeFunc, dur time.Duration) *MergeOperator {
	op := &MergeOperator{
		f:      f,
		db:     db,
		key:    key,
		closer: y.NewCloser(1),
	}

	go op.runCompactions(dur)
	return op
}
func (op *MergeOperator) Get() ([]byte, error) {
	op.RLock()
	defer op.RUnlock()
	var existing []byte
	err := op.db.View(func(txn *Txn) (err error) {
		existing, err = op.iterateAndMerge(txn)
		return err
	})
	if err == errNoMerge {
		return existing, nil
	}
	return existing, err
}
func (cs *compactStatus) compareAndAdd(_ thisAndNextLevelRLocked, cd compactDef) bool {
	cs.Lock()
	defer cs.Unlock()

	level := cd.thisLevel.level

	y.AssertTruef(level < len(cs.levels)-1, "Got level %d. Max levels: %d", level, len(cs.levels))
	thisLevel := cs.levels[level]
	nextLevel := cs.levels[level+1]

	if thisLevel.overlapsWith(cd.thisRange) {
		return false
	}
	if nextLevel.overlapsWith(cd.nextRange) {
		return false
	}
	// Check whether this level really needs compaction or not. Otherwise, we'll end up
	// running parallel compactions for the same level.
	// Update: We should not be checking size here. Compaction priority already did the size checks.
	// Here we should just be executing the wish of others.

	thisLevel.ranges = append(thisLevel.ranges, cd.thisRange)
	nextLevel.ranges = append(nextLevel.ranges, cd.nextRange)
	thisLevel.delSize += cd.thisSize
	return true
}
func newArena(n int64) *Arena {
	// Don't store data at position 0 in order to reserve offset=0 as a kind
	// of nil pointer.
	out := &Arena{
		n:   1,
		buf: make([]byte, n),
	}
	return out
}
func (s *Arena) putNode(height int) uint32 {
	// Compute the amount of the tower that will never be used, since the height
	// is less than maxHeight.
	unusedSize := (maxHeight - height) * offsetSize

	// Pad the allocation with enough bytes to ensure pointer alignment.
	l := uint32(MaxNodeSize - unusedSize + nodeAlign)
	n := atomic.AddUint32(&s.n, l)
	y.AssertTruef(int(n) <= len(s.buf),
		"Arena too small, toWrite:%d newTotal:%d limit:%d",
		l, n, len(s.buf))

	// Return the aligned offset.
	m := (n - l + uint32(nodeAlign)) & ^uint32(nodeAlign)
	return m
}
func (s *Arena) getNode(offset uint32) *node {
	if offset == 0 {
		return nil
	}

	return (*node)(unsafe.Pointer(&s.buf[offset]))
}
func (s *Arena) getKey(offset uint32, size uint16) []byte {
	return s.buf[offset : offset+uint32(size)]
}
func (s *Arena) getVal(offset uint32, size uint16) (ret y.ValueStruct) {
	ret.Decode(s.buf[offset : offset+uint32(size)])
	return
}
func (s *Arena) getNodeOffset(nd *node) uint32 {
	if nd == nil {
		return 0
	}

	return uint32(uintptr(unsafe.Pointer(nd)) - uintptr(unsafe.Pointer(&s.buf[0])))
}
func init() {
	NumReads = expvar.NewInt("badger_disk_reads_total")
	NumWrites = expvar.NewInt("badger_disk_writes_total")
	NumBytesRead = expvar.NewInt("badger_read_bytes")
	NumBytesWritten = expvar.NewInt("badger_written_bytes")
	NumLSMGets = expvar.NewMap("badger_lsm_level_gets_total")
	NumLSMBloomHits = expvar.NewMap("badger_lsm_bloom_hits_total")
	NumGets = expvar.NewInt("badger_gets_total")
	NumPuts = expvar.NewInt("badger_puts_total")
	NumBlockedPuts = expvar.NewInt("badger_blocked_puts_total")
	NumMemtableGets = expvar.NewInt("badger_memtable_gets_total")
	LSMSize = expvar.NewMap("badger_lsm_size_bytes")
	VlogSize = expvar.NewMap("badger_vlog_size_bytes")
	PendingWrites = expvar.NewMap("badger_pending_writes_total")
}
func revertToManifest(kv *DB, mf *Manifest, idMap map[uint64]struct{}) error {
	// 1. Check all files in manifest exist.
	for id := range mf.Tables {
		if _, ok := idMap[id]; !ok {
			return fmt.Errorf("file does not exist for table %d", id)
		}
	}

	// 2. Delete files that shouldn't exist.
	for id := range idMap {
		if _, ok := mf.Tables[id]; !ok {
			kv.elog.Printf("Table file %d not referenced in MANIFEST\n", id)
			filename := table.NewFilename(id, kv.opt.Dir)
			if err := os.Remove(filename); err != nil {
				return y.Wrapf(err, "While removing table %d", id)
			}
		}
	}

	return nil
}
func (s *levelsController) dropTree() (int, error) {
	// First pick all tables, so we can create a manifest changelog.
	var all []*table.Table
	for _, l := range s.levels {
		l.RLock()
		all = append(all, l.tables...)
		l.RUnlock()
	}
	if len(all) == 0 {
		return 0, nil
	}

	// Generate the manifest changes.
	changes := []*pb.ManifestChange{}
	for _, table := range all {
		changes = append(changes, newDeleteChange(table.ID()))
	}
	changeSet := pb.ManifestChangeSet{Changes: changes}
	if err := s.kv.manifest.addChanges(changeSet.Changes); err != nil {
		return 0, err
	}

	// Now that manifest has been successfully written, we can delete the tables.
	for _, l := range s.levels {
		l.Lock()
		l.totalSize = 0
		l.tables = l.tables[:0]
		l.Unlock()
	}
	for _, table := range all {
		if err := table.DecrRef(); err != nil {
			return 0, err
		}
	}
	return len(all), nil
}
func (s *levelsController) dropPrefix(prefix []byte) error {
	opt := s.kv.opt
	for _, l := range s.levels {
		l.RLock()
		if l.level == 0 {
			size := len(l.tables)
			l.RUnlock()

			if size > 0 {
				cp := compactionPriority{
					level: 0,
					score: 1.74,
					// A unique number greater than 1.0 does two things. Helps identify this
					// function in logs, and forces a compaction.
					dropPrefix: prefix,
				}
				if err := s.doCompact(cp); err != nil {
					opt.Warningf("While compacting level 0: %v", err)
					return nil
				}
			}
			continue
		}

		var tables []*table.Table
		for _, table := range l.tables {
			var absent bool
			switch {
			case bytes.HasPrefix(table.Smallest(), prefix):
			case bytes.HasPrefix(table.Biggest(), prefix):
			case bytes.Compare(prefix, table.Smallest()) > 0 &&
				bytes.Compare(prefix, table.Biggest()) < 0:
			default:
				absent = true
			}
			if !absent {
				tables = append(tables, table)
			}
		}
		l.RUnlock()
		if len(tables) == 0 {
			continue
		}

		cd := compactDef{
			elog:       trace.New(fmt.Sprintf("Badger.L%d", l.level), "Compact"),
			thisLevel:  l,
			nextLevel:  l,
			top:        []*table.Table{},
			bot:        tables,
			dropPrefix: prefix,
		}
		if err := s.runCompactDef(l.level, cd); err != nil {
			opt.Warningf("While running compact def: %+v. Error: %v", cd, err)
			return err
		}
	}
	return nil
}
func (s *levelsController) isLevel0Compactable() bool {
	return s.levels[0].numTables() >= s.kv.opt.NumLevelZeroTables
}
func (s *levelsController) doCompact(p compactionPriority) error {
	l := p.level
	y.AssertTrue(l+1 < s.kv.opt.MaxLevels) // Sanity check.

	cd := compactDef{
		elog:       trace.New(fmt.Sprintf("Badger.L%d", l), "Compact"),
		thisLevel:  s.levels[l],
		nextLevel:  s.levels[l+1],
		dropPrefix: p.dropPrefix,
	}
	cd.elog.SetMaxEvents(100)
	defer cd.elog.Finish()

	s.kv.opt.Infof("Got compaction priority: %+v", p)

	// While picking tables to be compacted, both levels' tables are expected to
	// remain unchanged.
	if l == 0 {
		if !s.fillTablesL0(&cd) {
			return errFillTables
		}

	} else {
		if !s.fillTables(&cd) {
			return errFillTables
		}
	}
	defer s.cstatus.delete(cd) // Remove the ranges from compaction status.

	s.kv.opt.Infof("Running for level: %d\n", cd.thisLevel.level)
	s.cstatus.toLog(cd.elog)
	if err := s.runCompactDef(l, cd); err != nil {
		// This compaction couldn't be done successfully.
		s.kv.opt.Warningf("LOG Compact FAILED with error: %+v: %+v", err, cd)
		return err
	}

	s.cstatus.toLog(cd.elog)
	s.kv.opt.Infof("Compaction for level: %d DONE", cd.thisLevel.level)
	return nil
}
func (s *levelsController) get(key []byte, maxVs *y.ValueStruct) (y.ValueStruct, error) {
	// It's important that we iterate the levels from 0 on upward.  The reason is, if we iterated
	// in opposite order, or in parallel (naively calling all the h.RLock() in some order) we could
	// read level L's tables post-compaction and level L+1's tables pre-compaction.  (If we do
	// parallelize this, we will need to call the h.RLock() function by increasing order of level
	// number.)
	version := y.ParseTs(key)
	for _, h := range s.levels {
		vs, err := h.get(key) // Calls h.RLock() and h.RUnlock().
		if err != nil {
			return y.ValueStruct{}, errors.Wrapf(err, "get key: %q", key)
		}
		if vs.Value == nil && vs.Meta == 0 {
			continue
		}
		if maxVs == nil || vs.Version == version {
			return vs, nil
		}
		if maxVs.Version < vs.Version {
			*maxVs = vs
		}
	}
	if maxVs != nil {
		return *maxVs, nil
	}
	return y.ValueStruct{}, nil
}
func seekTotal(txn *badger.Txn) ([]account, error) {
	expected := uint64(numAccounts) * uint64(initialBal)
	var accounts []account

	var total uint64
	for i := 0; i < numAccounts; i++ {
		item, err := txn.Get(key(i))
		if err != nil {
			log.Printf("Error for account: %d. err=%v. key=%q\n", i, err, key(i))
			return accounts, err
		}
		val, err := item.ValueCopy(nil)
		if err != nil {
			return accounts, err
		}
		acc := account{
			Id:  i,
			Bal: toUint64(val),
		}
		accounts = append(accounts, acc)
		total += acc.Bal
	}
	if total != expected {
		log.Printf("Balance did NOT match up. Expected: %d. Received: %d",
			expected, total)
		atomic.AddInt32(&stopAll, 1)
		return accounts, errFailure
	}
	return accounts, nil
}
func findFirstInvalidTxn(db *badger.DB, lowTs, highTs uint64) uint64 {
	checkAt := func(ts uint64) error {
		txn := db.NewTransactionAt(ts, false)
		_, err := seekTotal(txn)
		txn.Discard()
		return err
	}

	if highTs-lowTs < 1 {
		log.Printf("Checking at lowTs: %d\n", lowTs)
		err := checkAt(lowTs)
		if err == errFailure {
			fmt.Printf("Violation at ts: %d\n", lowTs)
			return lowTs
		} else if err != nil {
			log.Printf("Error at lowTs: %d. Err=%v\n", lowTs, err)
			return 0
		}
		fmt.Printf("No violation found at ts: %d\n", lowTs)
		return 0
	}

	midTs := (lowTs + highTs) / 2
	log.Println()
	log.Printf("Checking. low=%d. high=%d. mid=%d\n", lowTs, highTs, midTs)
	err := checkAt(midTs)
	if err == badger.ErrKeyNotFound || err == nil {
		// If no failure, move to higher ts.
		return findFirstInvalidTxn(db, midTs+1, highTs)
	}
	// Found an error.
	return findFirstInvalidTxn(db, lowTs, midTs)
}
func (m *InmemSnapshotStore) Create(version SnapshotVersion, index, term uint64,
	configuration Configuration, configurationIndex uint64, trans Transport) (SnapshotSink, error) {
	// We only support version 1 snapshots at this time.
	if version != 1 {
		return nil, fmt.Errorf("unsupported snapshot version %d", version)
	}

	name := snapshotName(term, index)

	m.Lock()
	defer m.Unlock()

	sink := &InmemSnapshotSink{
		meta: SnapshotMeta{
			Version:            version,
			ID:                 name,
			Index:              index,
			Term:               term,
			Peers:              encodePeers(configuration, trans),
			Configuration:      configuration,
			ConfigurationIndex: configurationIndex,
		},
		contents: &bytes.Buffer{},
	}
	m.hasSnapshot = true
	m.latest = sink

	return sink, nil
}
func (m *InmemSnapshotStore) List() ([]*SnapshotMeta, error) {
	m.RLock()
	defer m.RUnlock()

	if !m.hasSnapshot {
		return []*SnapshotMeta{}, nil
	}
	return []*SnapshotMeta{&m.latest.meta}, nil
}
func (m *InmemSnapshotStore) Open(id string) (*SnapshotMeta, io.ReadCloser, error) {
	m.RLock()
	defer m.RUnlock()

	if m.latest.meta.ID != id {
		return nil, nil, fmt.Errorf("[ERR] snapshot: failed to open snapshot id: %s", id)
	}

	return &m.latest.meta, ioutil.NopCloser(m.latest.contents), nil
}
func (s *InmemSnapshotSink) Write(p []byte) (n int, err error) {
	written, err := io.Copy(s.contents, bytes.NewReader(p))
	s.meta.Size += written
	return int(written), err
}
func NewFileSnapshotStoreWithLogger(base string, retain int, logger *log.Logger) (*FileSnapshotStore, error) {
	if retain < 1 {
		return nil, fmt.Errorf("must retain at least one snapshot")
	}
	if logger == nil {
		logger = log.New(os.Stderr, "", log.LstdFlags)
	}

	// Ensure our path exists
	path := filepath.Join(base, snapPath)
	if err := os.MkdirAll(path, 0755); err != nil && !os.IsExist(err) {
		return nil, fmt.Errorf("snapshot path not accessible: %v", err)
	}

	// Setup the store
	store := &FileSnapshotStore{
		path:   path,
		retain: retain,
		logger: logger,
	}

	// Do a permissions test
	if err := store.testPermissions(); err != nil {
		return nil, fmt.Errorf("permissions test failed: %v", err)
	}
	return store, nil
}
func NewFileSnapshotStore(base string, retain int, logOutput io.Writer) (*FileSnapshotStore, error) {
	if logOutput == nil {
		logOutput = os.Stderr
	}
	return NewFileSnapshotStoreWithLogger(base, retain, log.New(logOutput, "", log.LstdFlags))
}
func snapshotName(term, index uint64) string {
	now := time.Now()
	msec := now.UnixNano() / int64(time.Millisecond)
	return fmt.Sprintf("%d-%d-%d", term, index, msec)
}
func (f *FileSnapshotStore) Create(version SnapshotVersion, index, term uint64,
	configuration Configuration, configurationIndex uint64, trans Transport) (SnapshotSink, error) {
	// We only support version 1 snapshots at this time.
	if version != 1 {
		return nil, fmt.Errorf("unsupported snapshot version %d", version)
	}

	// Create a new path
	name := snapshotName(term, index)
	path := filepath.Join(f.path, name+tmpSuffix)
	f.logger.Printf("[INFO] snapshot: Creating new snapshot at %s", path)

	// Make the directory
	if err := os.MkdirAll(path, 0755); err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to make snapshot directory: %v", err)
		return nil, err
	}

	// Create the sink
	sink := &FileSnapshotSink{
		store:     f,
		logger:    f.logger,
		dir:       path,
		parentDir: f.path,
		meta: fileSnapshotMeta{
			SnapshotMeta: SnapshotMeta{
				Version:            version,
				ID:                 name,
				Index:              index,
				Term:               term,
				Peers:              encodePeers(configuration, trans),
				Configuration:      configuration,
				ConfigurationIndex: configurationIndex,
			},
			CRC: nil,
		},
	}

	// Write out the meta data
	if err := sink.writeMeta(); err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to write metadata: %v", err)
		return nil, err
	}

	// Open the state file
	statePath := filepath.Join(path, stateFilePath)
	fh, err := os.Create(statePath)
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to create state file: %v", err)
		return nil, err
	}
	sink.stateFile = fh

	// Create a CRC64 hash
	sink.stateHash = crc64.New(crc64.MakeTable(crc64.ECMA))

	// Wrap both the hash and file in a MultiWriter with buffering
	multi := io.MultiWriter(sink.stateFile, sink.stateHash)
	sink.buffered = bufio.NewWriter(multi)

	// Done
	return sink, nil
}
func (f *FileSnapshotStore) List() ([]*SnapshotMeta, error) {
	// Get the eligible snapshots
	snapshots, err := f.getSnapshots()
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to get snapshots: %v", err)
		return nil, err
	}

	var snapMeta []*SnapshotMeta
	for _, meta := range snapshots {
		snapMeta = append(snapMeta, &meta.SnapshotMeta)
		if len(snapMeta) == f.retain {
			break
		}
	}
	return snapMeta, nil
}
func (f *FileSnapshotStore) getSnapshots() ([]*fileSnapshotMeta, error) {
	// Get the eligible snapshots
	snapshots, err := ioutil.ReadDir(f.path)
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to scan snapshot dir: %v", err)
		return nil, err
	}

	// Populate the metadata
	var snapMeta []*fileSnapshotMeta
	for _, snap := range snapshots {
		// Ignore any files
		if !snap.IsDir() {
			continue
		}

		// Ignore any temporary snapshots
		dirName := snap.Name()
		if strings.HasSuffix(dirName, tmpSuffix) {
			f.logger.Printf("[WARN] snapshot: Found temporary snapshot: %v", dirName)
			continue
		}

		// Try to read the meta data
		meta, err := f.readMeta(dirName)
		if err != nil {
			f.logger.Printf("[WARN] snapshot: Failed to read metadata for %v: %v", dirName, err)
			continue
		}

		// Make sure we can understand this version.
		if meta.Version < SnapshotVersionMin || meta.Version > SnapshotVersionMax {
			f.logger.Printf("[WARN] snapshot: Snapshot version for %v not supported: %d", dirName, meta.Version)
			continue
		}

		// Append, but only return up to the retain count
		snapMeta = append(snapMeta, meta)
	}

	// Sort the snapshot, reverse so we get new -> old
	sort.Sort(sort.Reverse(snapMetaSlice(snapMeta)))

	return snapMeta, nil
}
func (f *FileSnapshotStore) readMeta(name string) (*fileSnapshotMeta, error) {
	// Open the meta file
	metaPath := filepath.Join(f.path, name, metaFilePath)
	fh, err := os.Open(metaPath)
	if err != nil {
		return nil, err
	}
	defer fh.Close()

	// Buffer the file IO
	buffered := bufio.NewReader(fh)

	// Read in the JSON
	meta := &fileSnapshotMeta{}
	dec := json.NewDecoder(buffered)
	if err := dec.Decode(meta); err != nil {
		return nil, err
	}
	return meta, nil
}
func (f *FileSnapshotStore) Open(id string) (*SnapshotMeta, io.ReadCloser, error) {
	// Get the metadata
	meta, err := f.readMeta(id)
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to get meta data to open snapshot: %v", err)
		return nil, nil, err
	}

	// Open the state file
	statePath := filepath.Join(f.path, id, stateFilePath)
	fh, err := os.Open(statePath)
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to open state file: %v", err)
		return nil, nil, err
	}

	// Create a CRC64 hash
	stateHash := crc64.New(crc64.MakeTable(crc64.ECMA))

	// Compute the hash
	_, err = io.Copy(stateHash, fh)
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to read state file: %v", err)
		fh.Close()
		return nil, nil, err
	}

	// Verify the hash
	computed := stateHash.Sum(nil)
	if bytes.Compare(meta.CRC, computed) != 0 {
		f.logger.Printf("[ERR] snapshot: CRC checksum failed (stored: %v computed: %v)",
			meta.CRC, computed)
		fh.Close()
		return nil, nil, fmt.Errorf("CRC mismatch")
	}

	// Seek to the start
	if _, err := fh.Seek(0, 0); err != nil {
		f.logger.Printf("[ERR] snapshot: State file seek failed: %v", err)
		fh.Close()
		return nil, nil, err
	}

	// Return a buffered file
	buffered := &bufferedFile{
		bh: bufio.NewReader(fh),
		fh: fh,
	}

	return &meta.SnapshotMeta, buffered, nil
}
func (f *FileSnapshotStore) ReapSnapshots() error {
	snapshots, err := f.getSnapshots()
	if err != nil {
		f.logger.Printf("[ERR] snapshot: Failed to get snapshots: %v", err)
		return err
	}

	for i := f.retain; i < len(snapshots); i++ {
		path := filepath.Join(f.path, snapshots[i].ID)
		f.logger.Printf("[INFO] snapshot: reaping snapshot %v", path)
		if err := os.RemoveAll(path); err != nil {
			f.logger.Printf("[ERR] snapshot: Failed to reap snapshot %v: %v", path, err)
			return err
		}
	}
	return nil
}
func (s *FileSnapshotSink) Write(b []byte) (int, error) {
	return s.buffered.Write(b)
}
func (s *FileSnapshotSink) Close() error {
	// Make sure close is idempotent
	if s.closed {
		return nil
	}
	s.closed = true

	// Close the open handles
	if err := s.finalize(); err != nil {
		s.logger.Printf("[ERR] snapshot: Failed to finalize snapshot: %v", err)
		if delErr := os.RemoveAll(s.dir); delErr != nil {
			s.logger.Printf("[ERR] snapshot: Failed to delete temporary snapshot directory at path %v: %v", s.dir, delErr)
			return delErr
		}
		return err
	}

	// Write out the meta data
	if err := s.writeMeta(); err != nil {
		s.logger.Printf("[ERR] snapshot: Failed to write metadata: %v", err)
		return err
	}

	// Move the directory into place
	newPath := strings.TrimSuffix(s.dir, tmpSuffix)
	if err := os.Rename(s.dir, newPath); err != nil {
		s.logger.Printf("[ERR] snapshot: Failed to move snapshot into place: %v", err)
		return err
	}

	if runtime.GOOS != "windows" { //skipping fsync for directory entry edits on Windows, only needed for *nix style file systems
		parentFH, err := os.Open(s.parentDir)
		defer parentFH.Close()
		if err != nil {
			s.logger.Printf("[ERR] snapshot: Failed to open snapshot parent directory %v, error: %v", s.parentDir, err)
			return err
		}

		if err = parentFH.Sync(); err != nil {
			s.logger.Printf("[ERR] snapshot: Failed syncing parent directory %v, error: %v", s.parentDir, err)
			return err
		}
	}

	// Reap any old snapshots
	if err := s.store.ReapSnapshots(); err != nil {
		return err
	}

	return nil
}
func (s *FileSnapshotSink) Cancel() error {
	// Make sure close is idempotent
	if s.closed {
		return nil
	}
	s.closed = true

	// Close the open handles
	if err := s.finalize(); err != nil {
		s.logger.Printf("[ERR] snapshot: Failed to finalize snapshot: %v", err)
		return err
	}

	// Attempt to remove all artifacts
	return os.RemoveAll(s.dir)
}
func (s *FileSnapshotSink) finalize() error {
	// Flush any remaining data
	if err := s.buffered.Flush(); err != nil {
		return err
	}

	// Sync to force fsync to disk
	if err := s.stateFile.Sync(); err != nil {
		return err
	}

	// Get the file size
	stat, statErr := s.stateFile.Stat()

	// Close the file
	if err := s.stateFile.Close(); err != nil {
		return err
	}

	// Set the file size, check after we close
	if statErr != nil {
		return statErr
	}
	s.meta.Size = stat.Size()

	// Set the CRC
	s.meta.CRC = s.stateHash.Sum(nil)
	return nil
}
func (s *FileSnapshotSink) writeMeta() error {
	// Open the meta file
	metaPath := filepath.Join(s.dir, metaFilePath)
	fh, err := os.Create(metaPath)
	if err != nil {
		return err
	}
	defer fh.Close()

	// Buffer the file IO
	buffered := bufio.NewWriter(fh)

	// Write out as JSON
	enc := json.NewEncoder(buffered)
	if err := enc.Encode(&s.meta); err != nil {
		return err
	}

	if err = buffered.Flush(); err != nil {
		return err
	}

	if err = fh.Sync(); err != nil {
		return err
	}

	return nil
}
func NewNetworkTransportWithConfig(
	config *NetworkTransportConfig,
) *NetworkTransport {
	if config.Logger == nil {
		config.Logger = log.New(os.Stderr, "", log.LstdFlags)
	}
	trans := &NetworkTransport{
		connPool:              make(map[ServerAddress][]*netConn),
		consumeCh:             make(chan RPC),
		logger:                config.Logger,
		maxPool:               config.MaxPool,
		shutdownCh:            make(chan struct{}),
		stream:                config.Stream,
		timeout:               config.Timeout,
		TimeoutScale:          DefaultTimeoutScale,
		serverAddressProvider: config.ServerAddressProvider,
	}

	// Create the connection context and then start our listener.
	trans.setupStreamContext()
	go trans.listen()

	return trans
}
func (n *NetworkTransport) setupStreamContext() {
	ctx, cancel := context.WithCancel(context.Background())
	n.streamCtx = ctx
	n.streamCancel = cancel
}
func (n *NetworkTransport) getStreamContext() context.Context {
	n.streamCtxLock.RLock()
	defer n.streamCtxLock.RUnlock()
	return n.streamCtx
}
func (n *NetworkTransport) SetHeartbeatHandler(cb func(rpc RPC)) {
	n.heartbeatFnLock.Lock()
	defer n.heartbeatFnLock.Unlock()
	n.heartbeatFn = cb
}
func (n *NetworkTransport) CloseStreams() {
	n.connPoolLock.Lock()
	defer n.connPoolLock.Unlock()

	// Close all the connections in the connection pool and then remove their
	// entry.
	for k, e := range n.connPool {
		for _, conn := range e {
			conn.Release()
		}

		delete(n.connPool, k)
	}

	// Cancel the existing connections and create a new context. Both these
	// operations must always be done with the lock held otherwise we can create
	// connection handlers that are holding a context that will never be
	// cancelable.
	n.streamCtxLock.Lock()
	n.streamCancel()
	n.setupStreamContext()
	n.streamCtxLock.Unlock()
}
func (n *NetworkTransport) Close() error {
	n.shutdownLock.Lock()
	defer n.shutdownLock.Unlock()

	if !n.shutdown {
		close(n.shutdownCh)
		n.stream.Close()
		n.shutdown = true
	}
	return nil
}
func (n *NetworkTransport) getPooledConn(target ServerAddress) *netConn {
	n.connPoolLock.Lock()
	defer n.connPoolLock.Unlock()

	conns, ok := n.connPool[target]
	if !ok || len(conns) == 0 {
		return nil
	}

	var conn *netConn
	num := len(conns)
	conn, conns[num-1] = conns[num-1], nil
	n.connPool[target] = conns[:num-1]
	return conn
}
func (n *NetworkTransport) getConnFromAddressProvider(id ServerID, target ServerAddress) (*netConn, error) {
	address := n.getProviderAddressOrFallback(id, target)
	return n.getConn(address)
}
func (n *NetworkTransport) getConn(target ServerAddress) (*netConn, error) {
	// Check for a pooled conn
	if conn := n.getPooledConn(target); conn != nil {
		return conn, nil
	}

	// Dial a new connection
	conn, err := n.stream.Dial(target, n.timeout)
	if err != nil {
		return nil, err
	}

	// Wrap the conn
	netConn := &netConn{
		target: target,
		conn:   conn,
		r:      bufio.NewReader(conn),
		w:      bufio.NewWriter(conn),
	}

	// Setup encoder/decoders
	netConn.dec = codec.NewDecoder(netConn.r, &codec.MsgpackHandle{})
	netConn.enc = codec.NewEncoder(netConn.w, &codec.MsgpackHandle{})

	// Done
	return netConn, nil
}
func (n *NetworkTransport) returnConn(conn *netConn) {
	n.connPoolLock.Lock()
	defer n.connPoolLock.Unlock()

	key := conn.target
	conns, _ := n.connPool[key]

	if !n.IsShutdown() && len(conns) < n.maxPool {
		n.connPool[key] = append(conns, conn)
	} else {
		conn.Release()
	}
}
func (n *NetworkTransport) listen() {
	const baseDelay = 5 * time.Millisecond
	const maxDelay = 1 * time.Second

	var loopDelay time.Duration
	for {
		// Accept incoming connections
		conn, err := n.stream.Accept()
		if err != nil {
			if loopDelay == 0 {
				loopDelay = baseDelay
			} else {
				loopDelay *= 2
			}

			if loopDelay > maxDelay {
				loopDelay = maxDelay
			}

			if !n.IsShutdown() {
				n.logger.Printf("[ERR] raft-net: Failed to accept connection: %v", err)
			}

			select {
			case <-n.shutdownCh:
				return
			case <-time.After(loopDelay):
				continue
			}
		}
		// No error, reset loop delay
		loopDelay = 0

		n.logger.Printf("[DEBUG] raft-net: %v accepted connection from: %v", n.LocalAddr(), conn.RemoteAddr())

		// Handle the connection in dedicated routine
		go n.handleConn(n.getStreamContext(), conn)
	}
}
func (n *NetworkTransport) handleConn(connCtx context.Context, conn net.Conn) {
	defer conn.Close()
	r := bufio.NewReader(conn)
	w := bufio.NewWriter(conn)
	dec := codec.NewDecoder(r, &codec.MsgpackHandle{})
	enc := codec.NewEncoder(w, &codec.MsgpackHandle{})

	for {
		select {
		case <-connCtx.Done():
			n.logger.Println("[DEBUG] raft-net: stream layer is closed")
			return
		default:
		}

		if err := n.handleCommand(r, dec, enc); err != nil {
			if err != io.EOF {
				n.logger.Printf("[ERR] raft-net: Failed to decode incoming command: %v", err)
			}
			return
		}
		if err := w.Flush(); err != nil {
			n.logger.Printf("[ERR] raft-net: Failed to flush response: %v", err)
			return
		}
	}
}
func (n *NetworkTransport) handleCommand(r *bufio.Reader, dec *codec.Decoder, enc *codec.Encoder) error {
	// Get the rpc type
	rpcType, err := r.ReadByte()
	if err != nil {
		return err
	}

	// Create the RPC object
	respCh := make(chan RPCResponse, 1)
	rpc := RPC{
		RespChan: respCh,
	}

	// Decode the command
	isHeartbeat := false
	switch rpcType {
	case rpcAppendEntries:
		var req AppendEntriesRequest
		if err := dec.Decode(&req); err != nil {
			return err
		}
		rpc.Command = &req

		// Check if this is a heartbeat
		if req.Term != 0 && req.Leader != nil &&
			req.PrevLogEntry == 0 && req.PrevLogTerm == 0 &&
			len(req.Entries) == 0 && req.LeaderCommitIndex == 0 {
			isHeartbeat = true
		}

	case rpcRequestVote:
		var req RequestVoteRequest
		if err := dec.Decode(&req); err != nil {
			return err
		}
		rpc.Command = &req

	case rpcInstallSnapshot:
		var req InstallSnapshotRequest
		if err := dec.Decode(&req); err != nil {
			return err
		}
		rpc.Command = &req
		rpc.Reader = io.LimitReader(r, req.Size)

	default:
		return fmt.Errorf("unknown rpc type %d", rpcType)
	}

	// Check for heartbeat fast-path
	if isHeartbeat {
		n.heartbeatFnLock.Lock()
		fn := n.heartbeatFn
		n.heartbeatFnLock.Unlock()
		if fn != nil {
			fn(rpc)
			goto RESP
		}
	}

	// Dispatch the RPC
	select {
	case n.consumeCh <- rpc:
	case <-n.shutdownCh:
		return ErrTransportShutdown
	}

	// Wait for response
RESP:
	select {
	case resp := <-respCh:
		// Send the error first
		respErr := ""
		if resp.Error != nil {
			respErr = resp.Error.Error()
		}
		if err := enc.Encode(respErr); err != nil {
			return err
		}

		// Send the response
		if err := enc.Encode(resp.Response); err != nil {
			return err
		}
	case <-n.shutdownCh:
		return ErrTransportShutdown
	}
	return nil
}
func decodeResponse(conn *netConn, resp interface{}) (bool, error) {
	// Decode the error if any
	var rpcError string
	if err := conn.dec.Decode(&rpcError); err != nil {
		conn.Release()
		return false, err
	}

	// Decode the response
	if err := conn.dec.Decode(resp); err != nil {
		conn.Release()
		return false, err
	}

	// Format an error if any
	if rpcError != "" {
		return true, fmt.Errorf(rpcError)
	}
	return true, nil
}
func sendRPC(conn *netConn, rpcType uint8, args interface{}) error {
	// Write the request type
	if err := conn.w.WriteByte(rpcType); err != nil {
		conn.Release()
		return err
	}

	// Send the request
	if err := conn.enc.Encode(args); err != nil {
		conn.Release()
		return err
	}

	// Flush
	if err := conn.w.Flush(); err != nil {
		conn.Release()
		return err
	}
	return nil
}
func newNetPipeline(trans *NetworkTransport, conn *netConn) *netPipeline {
	n := &netPipeline{
		conn:         conn,
		trans:        trans,
		doneCh:       make(chan AppendFuture, rpcMaxPipeline),
		inprogressCh: make(chan *appendFuture, rpcMaxPipeline),
		shutdownCh:   make(chan struct{}),
	}
	go n.decodeResponses()
	return n
}
func (n *netPipeline) decodeResponses() {
	timeout := n.trans.timeout
	for {
		select {
		case future := <-n.inprogressCh:
			if timeout > 0 {
				n.conn.conn.SetReadDeadline(time.Now().Add(timeout))
			}

			_, err := decodeResponse(n.conn, future.resp)
			future.respond(err)
			select {
			case n.doneCh <- future:
			case <-n.shutdownCh:
				return
			}
		case <-n.shutdownCh:
			return
		}
	}
}
func (n *netPipeline) AppendEntries(args *AppendEntriesRequest, resp *AppendEntriesResponse) (AppendFuture, error) {
	// Create a new future
	future := &appendFuture{
		start: time.Now(),
		args:  args,
		resp:  resp,
	}
	future.init()

	// Add a send timeout
	if timeout := n.trans.timeout; timeout > 0 {
		n.conn.conn.SetWriteDeadline(time.Now().Add(timeout))
	}

	// Send the RPC
	if err := sendRPC(n.conn, rpcAppendEntries, future.args); err != nil {
		return nil, err
	}

	// Hand-off for decoding, this can also cause back-pressure
	// to prevent too many inflight requests
	select {
	case n.inprogressCh <- future:
		return future, nil
	case <-n.shutdownCh:
		return nil, ErrPipelineShutdown
	}
}
func (n *netPipeline) Close() error {
	n.shutdownLock.Lock()
	defer n.shutdownLock.Unlock()
	if n.shutdown {
		return nil
	}

	// Release the connection
	n.conn.Release()

	n.shutdown = true
	close(n.shutdownCh)
	return nil
}
func NewObserver(channel chan Observation, blocking bool, filter FilterFn) *Observer {
	return &Observer{
		channel:  channel,
		blocking: blocking,
		filter:   filter,
		id:       atomic.AddUint64(&nextObserverID, 1),
	}
}
func (r *Raft) RegisterObserver(or *Observer) {
	r.observersLock.Lock()
	defer r.observersLock.Unlock()
	r.observers[or.id] = or
}
func (r *Raft) DeregisterObserver(or *Observer) {
	r.observersLock.Lock()
	defer r.observersLock.Unlock()
	delete(r.observers, or.id)
}
func (r *Raft) observe(o interface{}) {
	// In general observers should not block. But in any case this isn't
	// disastrous as we only hold a read lock, which merely prevents
	// registration / deregistration of observers.
	r.observersLock.RLock()
	defer r.observersLock.RUnlock()
	for _, or := range r.observers {
		// It's wasteful to do this in the loop, but for the common case
		// where there are no observers we won't create any objects.
		ob := Observation{Raft: r, Data: o}
		if or.filter != nil && !or.filter(&ob) {
			continue
		}
		if or.channel == nil {
			continue
		}
		if or.blocking {
			or.channel <- ob
			atomic.AddUint64(&or.numObserved, 1)
		} else {
			select {
			case or.channel <- ob:
				atomic.AddUint64(&or.numObserved, 1)
			default:
				atomic.AddUint64(&or.numDropped, 1)
			}
		}
	}
}
func NewInmemStore() *InmemStore {
	i := &InmemStore{
		logs:  make(map[uint64]*Log),
		kv:    make(map[string][]byte),
		kvInt: make(map[string]uint64),
	}
	return i
}
func (i *InmemStore) FirstIndex() (uint64, error) {
	i.l.RLock()
	defer i.l.RUnlock()
	return i.lowIndex, nil
}
func (i *InmemStore) LastIndex() (uint64, error) {
	i.l.RLock()
	defer i.l.RUnlock()
	return i.highIndex, nil
}
func (i *InmemStore) GetLog(index uint64, log *Log) error {
	i.l.RLock()
	defer i.l.RUnlock()
	l, ok := i.logs[index]
	if !ok {
		return ErrLogNotFound
	}
	*log = *l
	return nil
}
func (i *InmemStore) StoreLog(log *Log) error {
	return i.StoreLogs([]*Log{log})
}
func (i *InmemStore) StoreLogs(logs []*Log) error {
	i.l.Lock()
	defer i.l.Unlock()
	for _, l := range logs {
		i.logs[l.Index] = l
		if i.lowIndex == 0 {
			i.lowIndex = l.Index
		}
		if l.Index > i.highIndex {
			i.highIndex = l.Index
		}
	}
	return nil
}
func (i *InmemStore) DeleteRange(min, max uint64) error {
	i.l.Lock()
	defer i.l.Unlock()
	for j := min; j <= max; j++ {
		delete(i.logs, j)
	}
	if min <= i.lowIndex {
		i.lowIndex = max + 1
	}
	if max >= i.highIndex {
		i.highIndex = min - 1
	}
	if i.lowIndex > i.highIndex {
		i.lowIndex = 0
		i.highIndex = 0
	}
	return nil
}
func (i *InmemStore) Set(key []byte, val []byte) error {
	i.l.Lock()
	defer i.l.Unlock()
	i.kv[string(key)] = val
	return nil
}
func (i *InmemStore) Get(key []byte) ([]byte, error) {
	i.l.RLock()
	defer i.l.RUnlock()
	val := i.kv[string(key)]
	if val == nil {
		return nil, errors.New("not found")
	}
	return val, nil
}
func (i *InmemStore) SetUint64(key []byte, val uint64) error {
	i.l.Lock()
	defer i.l.Unlock()
	i.kvInt[string(key)] = val
	return nil
}
func (i *InmemStore) GetUint64(key []byte) (uint64, error) {
	i.l.RLock()
	defer i.l.RUnlock()
	return i.kvInt[string(key)], nil
}
func NewLogCache(capacity int, store LogStore) (*LogCache, error) {
	if capacity <= 0 {
		return nil, fmt.Errorf("capacity must be positive")
	}
	c := &LogCache{
		store: store,
		cache: make([]*Log, capacity),
	}
	return c, nil
}
func (i *InmemTransport) Connect(peer ServerAddress, t Transport) {
	trans := t.(*InmemTransport)
	i.Lock()
	defer i.Unlock()
	i.peers[peer] = trans
}
func (i *InmemTransport) Disconnect(peer ServerAddress) {
	i.Lock()
	defer i.Unlock()
	delete(i.peers, peer)

	// Disconnect any pipelines
	n := len(i.pipelines)
	for idx := 0; idx < n; idx++ {
		if i.pipelines[idx].peerAddr == peer {
			i.pipelines[idx].Close()
			i.pipelines[idx], i.pipelines[n-1] = i.pipelines[n-1], nil
			idx--
			n--
		}
	}
	i.pipelines = i.pipelines[:n]
}
func (i *InmemTransport) DisconnectAll() {
	i.Lock()
	defer i.Unlock()
	i.peers = make(map[ServerAddress]*InmemTransport)

	// Handle pipelines
	for _, pipeline := range i.pipelines {
		pipeline.Close()
	}
	i.pipelines = nil
}
func (r *RPC) Respond(resp interface{}, err error) {
	r.RespChan <- RPCResponse{resp, err}
}
func (u *userSnapshotFuture) Open() (*SnapshotMeta, io.ReadCloser, error) {
	if u.opener == nil {
		return nil, nil, fmt.Errorf("no snapshot available")
	} else {
		// Invalidate the opener so it can't get called multiple times,
		// which isn't generally safe.
		defer func() {
			u.opener = nil
		}()
		return u.opener()
	}
}
func (v *verifyFuture) vote(leader bool) {
	v.voteLock.Lock()
	defer v.voteLock.Unlock()

	// Guard against having notified already
	if v.notifyCh == nil {
		return
	}

	if leader {
		v.votes++
		if v.votes >= v.quorumSize {
			v.notifyCh <- v
			v.notifyCh = nil
		}
	} else {
		v.notifyCh <- v
		v.notifyCh = nil
	}
}
func (s *followerReplication) notifyAll(leader bool) {
	// Clear the waiting notifies minimizing lock time
	s.notifyLock.Lock()
	n := s.notify
	s.notify = make(map[*verifyFuture]struct{})
	s.notifyLock.Unlock()

	// Submit our votes
	for v, _ := range n {
		v.vote(leader)
	}
}
func (s *followerReplication) cleanNotify(v *verifyFuture) {
	s.notifyLock.Lock()
	delete(s.notify, v)
	s.notifyLock.Unlock()
}
func (s *followerReplication) LastContact() time.Time {
	s.lastContactLock.RLock()
	last := s.lastContact
	s.lastContactLock.RUnlock()
	return last
}
func (s *followerReplication) setLastContact() {
	s.lastContactLock.Lock()
	s.lastContact = time.Now()
	s.lastContactLock.Unlock()
}
func (r *Raft) replicate(s *followerReplication) {
	// Start an async heartbeating routing
	stopHeartbeat := make(chan struct{})
	defer close(stopHeartbeat)
	r.goFunc(func() { r.heartbeat(s, stopHeartbeat) })

RPC:
	shouldStop := false
	for !shouldStop {
		select {
		case maxIndex := <-s.stopCh:
			// Make a best effort to replicate up to this index
			if maxIndex > 0 {
				r.replicateTo(s, maxIndex)
			}
			return
		case <-s.triggerCh:
			lastLogIdx, _ := r.getLastLog()
			shouldStop = r.replicateTo(s, lastLogIdx)
		// This is _not_ our heartbeat mechanism but is to ensure
		// followers quickly learn the leader's commit index when
		// raft commits stop flowing naturally. The actual heartbeats
		// can't do this to keep them unblocked by disk IO on the
		// follower. See https://github.com/hashicorp/raft/issues/282.
		case <-randomTimeout(r.conf.CommitTimeout):
			lastLogIdx, _ := r.getLastLog()
			shouldStop = r.replicateTo(s, lastLogIdx)
		}

		// If things looks healthy, switch to pipeline mode
		if !shouldStop && s.allowPipeline {
			goto PIPELINE
		}
	}
	return

PIPELINE:
	// Disable until re-enabled
	s.allowPipeline = false

	// Replicates using a pipeline for high performance. This method
	// is not able to gracefully recover from errors, and so we fall back
	// to standard mode on failure.
	if err := r.pipelineReplicate(s); err != nil {
		if err != ErrPipelineReplicationNotSupported {
			r.logger.Error(fmt.Sprintf("Failed to start pipeline replication to %s: %s", s.peer, err))
		}
	}
	goto RPC
}
func (r *Raft) pipelineReplicate(s *followerReplication) error {
	// Create a new pipeline
	pipeline, err := r.trans.AppendEntriesPipeline(s.peer.ID, s.peer.Address)
	if err != nil {
		return err
	}
	defer pipeline.Close()

	// Log start and stop of pipeline
	r.logger.Info(fmt.Sprintf("pipelining replication to peer %v", s.peer))
	defer r.logger.Info(fmt.Sprintf("aborting pipeline replication to peer %v", s.peer))

	// Create a shutdown and finish channel
	stopCh := make(chan struct{})
	finishCh := make(chan struct{})

	// Start a dedicated decoder
	r.goFunc(func() { r.pipelineDecode(s, pipeline, stopCh, finishCh) })

	// Start pipeline sends at the last good nextIndex
	nextIndex := s.nextIndex

	shouldStop := false
SEND:
	for !shouldStop {
		select {
		case <-finishCh:
			break SEND
		case maxIndex := <-s.stopCh:
			// Make a best effort to replicate up to this index
			if maxIndex > 0 {
				r.pipelineSend(s, pipeline, &nextIndex, maxIndex)
			}
			break SEND
		case <-s.triggerCh:
			lastLogIdx, _ := r.getLastLog()
			shouldStop = r.pipelineSend(s, pipeline, &nextIndex, lastLogIdx)
		case <-randomTimeout(r.conf.CommitTimeout):
			lastLogIdx, _ := r.getLastLog()
			shouldStop = r.pipelineSend(s, pipeline, &nextIndex, lastLogIdx)
		}
	}

	// Stop our decoder, and wait for it to finish
	close(stopCh)
	select {
	case <-finishCh:
	case <-r.shutdownCh:
	}
	return nil
}
func (r *Raft) pipelineSend(s *followerReplication, p AppendPipeline, nextIdx *uint64, lastIndex uint64) (shouldStop bool) {
	// Create a new append request
	req := new(AppendEntriesRequest)
	if err := r.setupAppendEntries(s, req, *nextIdx, lastIndex); err != nil {
		return true
	}

	// Pipeline the append entries
	if _, err := p.AppendEntries(req, new(AppendEntriesResponse)); err != nil {
		r.logger.Error(fmt.Sprintf("Failed to pipeline AppendEntries to %v: %v", s.peer, err))
		return true
	}

	// Increase the next send log to avoid re-sending old logs
	if n := len(req.Entries); n > 0 {
		last := req.Entries[n-1]
		*nextIdx = last.Index + 1
	}
	return false
}
func (r *Raft) pipelineDecode(s *followerReplication, p AppendPipeline, stopCh, finishCh chan struct{}) {
	defer close(finishCh)
	respCh := p.Consumer()
	for {
		select {
		case ready := <-respCh:
			req, resp := ready.Request(), ready.Response()
			appendStats(string(s.peer.ID), ready.Start(), float32(len(req.Entries)))

			// Check for a newer term, stop running
			if resp.Term > req.Term {
				r.handleStaleTerm(s)
				return
			}

			// Update the last contact
			s.setLastContact()

			// Abort pipeline if not successful
			if !resp.Success {
				return
			}

			// Update our replication state
			updateLastAppended(s, req)
		case <-stopCh:
			return
		}
	}
}
func (r *Raft) setupAppendEntries(s *followerReplication, req *AppendEntriesRequest, nextIndex, lastIndex uint64) error {
	req.RPCHeader = r.getRPCHeader()
	req.Term = s.currentTerm
	req.Leader = r.trans.EncodePeer(r.localID, r.localAddr)
	req.LeaderCommitIndex = r.getCommitIndex()
	if err := r.setPreviousLog(req, nextIndex); err != nil {
		return err
	}
	if err := r.setNewLogs(req, nextIndex, lastIndex); err != nil {
		return err
	}
	return nil
}
func (r *Raft) setPreviousLog(req *AppendEntriesRequest, nextIndex uint64) error {
	// Guard for the first index, since there is no 0 log entry
	// Guard against the previous index being a snapshot as well
	lastSnapIdx, lastSnapTerm := r.getLastSnapshot()
	if nextIndex == 1 {
		req.PrevLogEntry = 0
		req.PrevLogTerm = 0

	} else if (nextIndex - 1) == lastSnapIdx {
		req.PrevLogEntry = lastSnapIdx
		req.PrevLogTerm = lastSnapTerm

	} else {
		var l Log
		if err := r.logs.GetLog(nextIndex-1, &l); err != nil {
			r.logger.Error(fmt.Sprintf("Failed to get log at index %d: %v", nextIndex-1, err))
			return err
		}

		// Set the previous index and term (0 if nextIndex is 1)
		req.PrevLogEntry = l.Index
		req.PrevLogTerm = l.Term
	}
	return nil
}
func (r *Raft) setNewLogs(req *AppendEntriesRequest, nextIndex, lastIndex uint64) error {
	// Append up to MaxAppendEntries or up to the lastIndex
	req.Entries = make([]*Log, 0, r.conf.MaxAppendEntries)
	maxIndex := min(nextIndex+uint64(r.conf.MaxAppendEntries)-1, lastIndex)
	for i := nextIndex; i <= maxIndex; i++ {
		oldLog := new(Log)
		if err := r.logs.GetLog(i, oldLog); err != nil {
			r.logger.Error(fmt.Sprintf("Failed to get log at index %d: %v", i, err))
			return err
		}
		req.Entries = append(req.Entries, oldLog)
	}
	return nil
}
func appendStats(peer string, start time.Time, logs float32) {
	metrics.MeasureSince([]string{"raft", "replication", "appendEntries", "rpc", peer}, start)
	metrics.IncrCounter([]string{"raft", "replication", "appendEntries", "logs", peer}, logs)
}
func (r *Raft) handleStaleTerm(s *followerReplication) {
	r.logger.Error(fmt.Sprintf("peer %v has newer term, stopping replication", s.peer))
	s.notifyAll(false) // No longer leader
	asyncNotifyCh(s.stepDown)
}
func (t *transport) AppendEntries(id raft.ServerID, target raft.ServerAddress, args *raft.AppendEntriesRequest, resp *raft.AppendEntriesResponse) error {
	ae := appendEntries{
		source:      t.node,
		target:      target,
		firstIndex:  firstIndex(args),
		lastIndex:   lastIndex(args),
		commitIndex: args.LeaderCommitIndex,
	}
	if len(t.ae) < cap(t.ae) {
		t.ae = append(t.ae, ae)
	}
	return t.sendRPC(string(target), args, resp)
}
func (t *transport) RequestVote(id raft.ServerID, target raft.ServerAddress, args *raft.RequestVoteRequest, resp *raft.RequestVoteResponse) error {
	return t.sendRPC(string(target), args, resp)
}
func (t *transport) InstallSnapshot(id raft.ServerID, target raft.ServerAddress, args *raft.InstallSnapshotRequest, resp *raft.InstallSnapshotResponse, data io.Reader) error {
	t.log.Printf("INSTALL SNAPSHOT *************************************")
	return errors.New("huh")
}
func (t *transport) EncodePeer(id raft.ServerID, p raft.ServerAddress) []byte {
	return []byte(p)
}
func (t *transport) DecodePeer(p []byte) raft.ServerAddress {
	return raft.ServerAddress(p)
}
func (p *pipeline) AppendEntries(args *raft.AppendEntriesRequest, resp *raft.AppendEntriesResponse) (raft.AppendFuture, error) {
	e := &appendEntry{
		req:      args,
		res:      resp,
		start:    time.Now(),
		ready:    make(chan error),
		consumer: p.consumer,
	}
	p.work <- e
	return e, nil
}
func ReadPeersJSON(path string) (Configuration, error) {
	// Read in the file.
	buf, err := ioutil.ReadFile(path)
	if err != nil {
		return Configuration{}, err
	}

	// Parse it as JSON.
	var peers []string
	dec := json.NewDecoder(bytes.NewReader(buf))
	if err := dec.Decode(&peers); err != nil {
		return Configuration{}, err
	}

	// Map it into the new-style configuration structure. We can only specify
	// voter roles here, and the ID has to be the same as the address.
	var configuration Configuration
	for _, peer := range peers {
		server := Server{
			Suffrage: Voter,
			ID:       ServerID(peer),
			Address:  ServerAddress(peer),
		}
		configuration.Servers = append(configuration.Servers, server)
	}

	// We should only ingest valid configurations.
	if err := checkConfiguration(configuration); err != nil {
		return Configuration{}, err
	}
	return configuration, nil
}
func ReadConfigJSON(path string) (Configuration, error) {
	// Read in the file.
	buf, err := ioutil.ReadFile(path)
	if err != nil {
		return Configuration{}, err
	}

	// Parse it as JSON.
	var peers []configEntry
	dec := json.NewDecoder(bytes.NewReader(buf))
	if err := dec.Decode(&peers); err != nil {
		return Configuration{}, err
	}

	// Map it into the new-style configuration structure.
	var configuration Configuration
	for _, peer := range peers {
		suffrage := Voter
		if peer.NonVoter {
			suffrage = Nonvoter
		}
		server := Server{
			Suffrage: suffrage,
			ID:       peer.ID,
			Address:  peer.Address,
		}
		configuration.Servers = append(configuration.Servers, server)
	}

	// We should only ingest valid configurations.
	if err := checkConfiguration(configuration); err != nil {
		return Configuration{}, err
	}
	return configuration, nil
}
func NewTCPTransport(
	bindAddr string,
	advertise net.Addr,
	maxPool int,
	timeout time.Duration,
	logOutput io.Writer,
) (*NetworkTransport, error) {
	return newTCPTransport(bindAddr, advertise, func(stream StreamLayer) *NetworkTransport {
		return NewNetworkTransport(stream, maxPool, timeout, logOutput)
	})
}
func NewTCPTransportWithLogger(
	bindAddr string,
	advertise net.Addr,
	maxPool int,
	timeout time.Duration,
	logger *log.Logger,
) (*NetworkTransport, error) {
	return newTCPTransport(bindAddr, advertise, func(stream StreamLayer) *NetworkTransport {
		return NewNetworkTransportWithLogger(stream, maxPool, timeout, logger)
	})
}
func NewTCPTransportWithConfig(
	bindAddr string,
	advertise net.Addr,
	config *NetworkTransportConfig,
) (*NetworkTransport, error) {
	return newTCPTransport(bindAddr, advertise, func(stream StreamLayer) *NetworkTransport {
		config.Stream = stream
		return NewNetworkTransportWithConfig(config)
	})
}
func (t *TCPStreamLayer) Dial(address ServerAddress, timeout time.Duration) (net.Conn, error) {
	return net.DialTimeout("tcp", string(address), timeout)
}
func (t *TCPStreamLayer) Accept() (c net.Conn, err error) {
	return t.listener.Accept()
}
func (t *TCPStreamLayer) Addr() net.Addr {
	// Use an advertise addr if provided
	if t.advertise != nil {
		return t.advertise
	}
	return t.listener.Addr()
}
func (r *Raft) restoreSnapshot() error {
	snapshots, err := r.snapshots.List()
	if err != nil {
		r.logger.Error(fmt.Sprintf("Failed to list snapshots: %v", err))
		return err
	}

	// Try to load in order of newest to oldest
	for _, snapshot := range snapshots {
		_, source, err := r.snapshots.Open(snapshot.ID)
		if err != nil {
			r.logger.Error(fmt.Sprintf("Failed to open snapshot %v: %v", snapshot.ID, err))
			continue
		}
		defer source.Close()

		if err := r.fsm.Restore(source); err != nil {
			r.logger.Error(fmt.Sprintf("Failed to restore snapshot %v: %v", snapshot.ID, err))
			continue
		}

		// Log success
		r.logger.Info(fmt.Sprintf("Restored from snapshot %v", snapshot.ID))

		// Update the lastApplied so we don't replay old logs
		r.setLastApplied(snapshot.Index)

		// Update the last stable snapshot info
		r.setLastSnapshot(snapshot.Index, snapshot.Term)

		// Update the configuration
		if snapshot.Version > 0 {
			r.configurations.committed = snapshot.Configuration
			r.configurations.committedIndex = snapshot.ConfigurationIndex
			r.configurations.latest = snapshot.Configuration
			r.configurations.latestIndex = snapshot.ConfigurationIndex
		} else {
			configuration := decodePeers(snapshot.Peers, r.trans)
			r.configurations.committed = configuration
			r.configurations.committedIndex = snapshot.Index
			r.configurations.latest = configuration
			r.configurations.latestIndex = snapshot.Index
		}

		// Success!
		return nil
	}

	// If we had snapshots and failed to load them, its an error
	if len(snapshots) > 0 {
		return fmt.Errorf("failed to load any existing snapshots")
	}
	return nil
}
func (r *Raft) BootstrapCluster(configuration Configuration) Future {
	bootstrapReq := &bootstrapFuture{}
	bootstrapReq.init()
	bootstrapReq.configuration = configuration
	select {
	case <-r.shutdownCh:
		return errorFuture{ErrRaftShutdown}
	case r.bootstrapCh <- bootstrapReq:
		return bootstrapReq
	}
}
func (r *Raft) Leader() ServerAddress {
	r.leaderLock.RLock()
	leader := r.leader
	r.leaderLock.RUnlock()
	return leader
}
func (r *Raft) Apply(cmd []byte, timeout time.Duration) ApplyFuture {
	metrics.IncrCounter([]string{"raft", "apply"}, 1)
	var timer <-chan time.Time
	if timeout > 0 {
		timer = time.After(timeout)
	}

	// Create a log future, no index or term yet
	logFuture := &logFuture{
		log: Log{
			Type: LogCommand,
			Data: cmd,
		},
	}
	logFuture.init()

	select {
	case <-timer:
		return errorFuture{ErrEnqueueTimeout}
	case <-r.shutdownCh:
		return errorFuture{ErrRaftShutdown}
	case r.applyCh <- logFuture:
		return logFuture
	}
}
func (r *Raft) Barrier(timeout time.Duration) Future {
	metrics.IncrCounter([]string{"raft", "barrier"}, 1)
	var timer <-chan time.Time
	if timeout > 0 {
		timer = time.After(timeout)
	}

	// Create a log future, no index or term yet
	logFuture := &logFuture{
		log: Log{
			Type: LogBarrier,
		},
	}
	logFuture.init()

	select {
	case <-timer:
		return errorFuture{ErrEnqueueTimeout}
	case <-r.shutdownCh:
		return errorFuture{ErrRaftShutdown}
	case r.applyCh <- logFuture:
		return logFuture
	}
}
func (r *Raft) VerifyLeader() Future {
	metrics.IncrCounter([]string{"raft", "verify_leader"}, 1)
	verifyFuture := &verifyFuture{}
	verifyFuture.init()
	select {
	case <-r.shutdownCh:
		return errorFuture{ErrRaftShutdown}
	case r.verifyCh <- verifyFuture:
		return verifyFuture
	}
}
func (r *Raft) AddVoter(id ServerID, address ServerAddress, prevIndex uint64, timeout time.Duration) IndexFuture {
	if r.protocolVersion < 2 {
		return errorFuture{ErrUnsupportedProtocol}
	}

	return r.requestConfigChange(configurationChangeRequest{
		command:       AddStaging,
		serverID:      id,
		serverAddress: address,
		prevIndex:     prevIndex,
	}, timeout)
}
func (r *Raft) RemoveServer(id ServerID, prevIndex uint64, timeout time.Duration) IndexFuture {
	if r.protocolVersion < 2 {
		return errorFuture{ErrUnsupportedProtocol}
	}

	return r.requestConfigChange(configurationChangeRequest{
		command:   RemoveServer,
		serverID:  id,
		prevIndex: prevIndex,
	}, timeout)
}
func (r *Raft) Shutdown() Future {
	r.shutdownLock.Lock()
	defer r.shutdownLock.Unlock()

	if !r.shutdown {
		close(r.shutdownCh)
		r.shutdown = true
		r.setState(Shutdown)
		return &shutdownFuture{r}
	}

	// avoid closing transport twice
	return &shutdownFuture{nil}
}
func (r *Raft) Snapshot() SnapshotFuture {
	future := &userSnapshotFuture{}
	future.init()
	select {
	case r.userSnapshotCh <- future:
		return future
	case <-r.shutdownCh:
		future.respond(ErrRaftShutdown)
		return future
	}
}
func (r *Raft) Restore(meta *SnapshotMeta, reader io.Reader, timeout time.Duration) error {
	metrics.IncrCounter([]string{"raft", "restore"}, 1)
	var timer <-chan time.Time
	if timeout > 0 {
		timer = time.After(timeout)
	}

	// Perform the restore.
	restore := &userRestoreFuture{
		meta:   meta,
		reader: reader,
	}
	restore.init()
	select {
	case <-timer:
		return ErrEnqueueTimeout
	case <-r.shutdownCh:
		return ErrRaftShutdown
	case r.userRestoreCh <- restore:
		// If the restore is ingested then wait for it to complete.
		if err := restore.Error(); err != nil {
			return err
		}
	}

	// Apply a no-op log entry. Waiting for this allows us to wait until the
	// followers have gotten the restore and replicated at least this new
	// entry, which shows that we've also faulted and installed the
	// snapshot with the contents of the restore.
	noop := &logFuture{
		log: Log{
			Type: LogNoop,
		},
	}
	noop.init()
	select {
	case <-timer:
		return ErrEnqueueTimeout
	case <-r.shutdownCh:
		return ErrRaftShutdown
	case r.applyCh <- noop:
		return noop.Error()
	}
}
func (r *Raft) String() string {
	return fmt.Sprintf("Node at %s [%v]", r.localAddr, r.getState())
}
func (r *Raft) LastContact() time.Time {
	r.lastContactLock.RLock()
	last := r.lastContact
	r.lastContactLock.RUnlock()
	return last
}
func (a *LoggerAdapter) Logf(s string, v ...interface{}) {
	a.log.Printf(s, v...)
}
func containsNode(nodes []*raftNode, n *raftNode) bool {
	for _, rn := range nodes {
		if rn == n {
			return true
		}
	}
	return false
}
func (c *cluster) LeaderPlus(n int) []*raftNode {
	r := make([]*raftNode, 0, n+1)
	ldr := c.Leader(time.Second)
	if ldr != nil {
		r = append(r, ldr)
	}
	if len(r) >= n {
		return r
	}
	for _, node := range c.nodes {
		if !containsNode(r, node) {
			r = append(r, node)
			if len(r) >= n {
				return r
			}
		}
	}
	return r
}
func (c *cluster) WaitTilUptoDate(t *testing.T, maxWait time.Duration) {
	idx := c.lastApplySuccess.Index()
	start := time.Now()
	for true {
		allAtIdx := true
		for i := 0; i < len(c.nodes); i++ {
			nodeAppliedIdx := c.nodes[i].raft.AppliedIndex()
			if nodeAppliedIdx < idx {
				allAtIdx = false
				break
			} else if nodeAppliedIdx > idx {
				allAtIdx = false
				idx = nodeAppliedIdx
				break
			}
		}
		if allAtIdx {
			t.Logf("All nodes have appliedIndex=%d", idx)
			return
		}
		if time.Now().Sub(start) > maxWait {
			t.Fatalf("Gave up waiting for all nodes to reach raft Index %d, [currently at %v]", idx, c.appliedIndexes())
		}
		time.Sleep(time.Millisecond * 10)
	}
}
func assertLogEntryEqual(t *testing.T, node string, exp *raft.Log, act *raft.Log) bool {
	res := true
	if exp.Term != act.Term {
		t.Errorf("Log Entry at Index %d for node %v has mismatched terms %d/%d", exp.Index, node, exp.Term, act.Term)
		res = false
	}
	if exp.Index != act.Index {
		t.Errorf("Node %v, Log Entry should be Index %d,but is %d", node, exp.Index, act.Index)
		res = false
	}
	if exp.Type != act.Type {
		t.Errorf("Node %v, Log Entry at Index %d should have type %v but is %v", node, exp.Index, exp.Type, act.Type)
		res = false
	}
	if !bytes.Equal(exp.Data, act.Data) {
		t.Errorf("Node %v, Log Entry at Index %d should have data %v, but has %v", node, exp.Index, exp.Data, act.Data)
		res = false
	}
	return res
}
func (r *Raft) runFSM() {
	var lastIndex, lastTerm uint64

	commit := func(req *commitTuple) {
		// Apply the log if a command
		var resp interface{}
		if req.log.Type == LogCommand {
			start := time.Now()
			resp = r.fsm.Apply(req.log)
			metrics.MeasureSince([]string{"raft", "fsm", "apply"}, start)
		}

		// Update the indexes
		lastIndex = req.log.Index
		lastTerm = req.log.Term

		// Invoke the future if given
		if req.future != nil {
			req.future.response = resp
			req.future.respond(nil)
		}
	}

	restore := func(req *restoreFuture) {
		// Open the snapshot
		meta, source, err := r.snapshots.Open(req.ID)
		if err != nil {
			req.respond(fmt.Errorf("failed to open snapshot %v: %v", req.ID, err))
			return
		}

		// Attempt to restore
		start := time.Now()
		if err := r.fsm.Restore(source); err != nil {
			req.respond(fmt.Errorf("failed to restore snapshot %v: %v", req.ID, err))
			source.Close()
			return
		}
		source.Close()
		metrics.MeasureSince([]string{"raft", "fsm", "restore"}, start)

		// Update the last index and term
		lastIndex = meta.Index
		lastTerm = meta.Term
		req.respond(nil)
	}

	snapshot := func(req *reqSnapshotFuture) {
		// Is there something to snapshot?
		if lastIndex == 0 {
			req.respond(ErrNothingNewToSnapshot)
			return
		}

		// Start a snapshot
		start := time.Now()
		snap, err := r.fsm.Snapshot()
		metrics.MeasureSince([]string{"raft", "fsm", "snapshot"}, start)

		// Respond to the request
		req.index = lastIndex
		req.term = lastTerm
		req.snapshot = snap
		req.respond(err)
	}

	for {
		select {
		case ptr := <-r.fsmMutateCh:
			switch req := ptr.(type) {
			case *commitTuple:
				commit(req)

			case *restoreFuture:
				restore(req)

			default:
				panic(fmt.Errorf("bad type passed to fsmMutateCh: %#v", ptr))
			}

		case req := <-r.fsmSnapshotCh:
			snapshot(req)

		case <-r.shutdownCh:
			return
		}
	}
}
func (c *Configuration) Clone() (copy Configuration) {
	copy.Servers = append(copy.Servers, c.Servers...)
	return
}
func (c *configurations) Clone() (copy configurations) {
	copy.committed = c.committed.Clone()
	copy.committedIndex = c.committedIndex
	copy.latest = c.latest.Clone()
	copy.latestIndex = c.latestIndex
	return
}
func hasVote(configuration Configuration, id ServerID) bool {
	for _, server := range configuration.Servers {
		if server.ID == id {
			return server.Suffrage == Voter
		}
	}
	return false
}
func checkConfiguration(configuration Configuration) error {
	idSet := make(map[ServerID]bool)
	addressSet := make(map[ServerAddress]bool)
	var voters int
	for _, server := range configuration.Servers {
		if server.ID == "" {
			return fmt.Errorf("Empty ID in configuration: %v", configuration)
		}
		if server.Address == "" {
			return fmt.Errorf("Empty address in configuration: %v", server)
		}
		if idSet[server.ID] {
			return fmt.Errorf("Found duplicate ID in configuration: %v", server.ID)
		}
		idSet[server.ID] = true
		if addressSet[server.Address] {
			return fmt.Errorf("Found duplicate address in configuration: %v", server.Address)
		}
		addressSet[server.Address] = true
		if server.Suffrage == Voter {
			voters++
		}
	}
	if voters == 0 {
		return fmt.Errorf("Need at least one voter in configuration: %v", configuration)
	}
	return nil
}
func nextConfiguration(current Configuration, currentIndex uint64, change configurationChangeRequest) (Configuration, error) {
	if change.prevIndex > 0 && change.prevIndex != currentIndex {
		return Configuration{}, fmt.Errorf("Configuration changed since %v (latest is %v)", change.prevIndex, currentIndex)
	}

	configuration := current.Clone()
	switch change.command {
	case AddStaging:
		// TODO: barf on new address?
		newServer := Server{
			// TODO: This should add the server as Staging, to be automatically
			// promoted to Voter later. However, the promotion to Voter is not yet
			// implemented, and doing so is not trivial with the way the leader loop
			// coordinates with the replication goroutines today. So, for now, the
			// server will have a vote right away, and the Promote case below is
			// unused.
			Suffrage: Voter,
			ID:       change.serverID,
			Address:  change.serverAddress,
		}
		found := false
		for i, server := range configuration.Servers {
			if server.ID == change.serverID {
				if server.Suffrage == Voter {
					configuration.Servers[i].Address = change.serverAddress
				} else {
					configuration.Servers[i] = newServer
				}
				found = true
				break
			}
		}
		if !found {
			configuration.Servers = append(configuration.Servers, newServer)
		}
	case AddNonvoter:
		newServer := Server{
			Suffrage: Nonvoter,
			ID:       change.serverID,
			Address:  change.serverAddress,
		}
		found := false
		for i, server := range configuration.Servers {
			if server.ID == change.serverID {
				if server.Suffrage != Nonvoter {
					configuration.Servers[i].Address = change.serverAddress
				} else {
					configuration.Servers[i] = newServer
				}
				found = true
				break
			}
		}
		if !found {
			configuration.Servers = append(configuration.Servers, newServer)
		}
	case DemoteVoter:
		for i, server := range configuration.Servers {
			if server.ID == change.serverID {
				configuration.Servers[i].Suffrage = Nonvoter
				break
			}
		}
	case RemoveServer:
		for i, server := range configuration.Servers {
			if server.ID == change.serverID {
				configuration.Servers = append(configuration.Servers[:i], configuration.Servers[i+1:]...)
				break
			}
		}
	case Promote:
		for i, server := range configuration.Servers {
			if server.ID == change.serverID && server.Suffrage == Staging {
				configuration.Servers[i].Suffrage = Voter
				break
			}
		}
	}

	// Make sure we didn't do something bad like remove the last voter
	if err := checkConfiguration(configuration); err != nil {
		return Configuration{}, err
	}

	return configuration, nil
}
func encodePeers(configuration Configuration, trans Transport) []byte {
	// Gather up all the voters, other suffrage types are not supported by
	// this data format.
	var encPeers [][]byte
	for _, server := range configuration.Servers {
		if server.Suffrage == Voter {
			encPeers = append(encPeers, trans.EncodePeer(server.ID, server.Address))
		}
	}

	// Encode the entire array.
	buf, err := encodeMsgPack(encPeers)
	if err != nil {
		panic(fmt.Errorf("failed to encode peers: %v", err))
	}

	return buf.Bytes()
}
func decodePeers(buf []byte, trans Transport) Configuration {
	// Decode the buffer first.
	var encPeers [][]byte
	if err := decodeMsgPack(buf, &encPeers); err != nil {
		panic(fmt.Errorf("failed to decode peers: %v", err))
	}

	// Deserialize each peer.
	var servers []Server
	for _, enc := range encPeers {
		p := trans.DecodePeer(enc)
		servers = append(servers, Server{
			Suffrage: Voter,
			ID:       ServerID(p),
			Address:  ServerAddress(p),
		})
	}

	return Configuration{
		Servers: servers,
	}
}
func encodeConfiguration(configuration Configuration) []byte {
	buf, err := encodeMsgPack(configuration)
	if err != nil {
		panic(fmt.Errorf("failed to encode configuration: %v", err))
	}
	return buf.Bytes()
}
func decodeConfiguration(buf []byte) Configuration {
	var configuration Configuration
	if err := decodeMsgPack(buf, &configuration); err != nil {
		panic(fmt.Errorf("failed to decode configuration: %v", err))
	}
	return configuration
}
func (r *raftState) goFunc(f func()) {
	r.routinesGroup.Add(1)
	go func() {
		defer r.routinesGroup.Done()
		f()
	}()
}
func (r *raftState) getLastIndex() uint64 {
	r.lastLock.Lock()
	defer r.lastLock.Unlock()
	return max(r.lastLogIndex, r.lastSnapshotIndex)
}
func (r *raftState) getLastEntry() (uint64, uint64) {
	r.lastLock.Lock()
	defer r.lastLock.Unlock()
	if r.lastLogIndex >= r.lastSnapshotIndex {
		return r.lastLogIndex, r.lastLogTerm
	}
	return r.lastSnapshotIndex, r.lastSnapshotTerm
}
func (r *Raft) checkRPCHeader(rpc RPC) error {
	// Get the header off the RPC message.
	wh, ok := rpc.Command.(WithRPCHeader)
	if !ok {
		return fmt.Errorf("RPC does not have a header")
	}
	header := wh.GetRPCHeader()

	// First check is to just make sure the code can understand the
	// protocol at all.
	if header.ProtocolVersion < ProtocolVersionMin ||
		header.ProtocolVersion > ProtocolVersionMax {
		return ErrUnsupportedProtocol
	}

	// Second check is whether we should support this message, given the
	// current protocol we are configured to run. This will drop support
	// for protocol version 0 starting at protocol version 2, which is
	// currently what we want, and in general support one version back. We
	// may need to revisit this policy depending on how future protocol
	// changes evolve.
	if header.ProtocolVersion < r.conf.ProtocolVersion-1 {
		return ErrUnsupportedProtocol
	}

	return nil
}
func (r *Raft) setLeader(leader ServerAddress) {
	r.leaderLock.Lock()
	oldLeader := r.leader
	r.leader = leader
	r.leaderLock.Unlock()
	if oldLeader != leader {
		r.observe(LeaderObservation{leader: leader})
	}
}
func (r *Raft) requestConfigChange(req configurationChangeRequest, timeout time.Duration) IndexFuture {
	var timer <-chan time.Time
	if timeout > 0 {
		timer = time.After(timeout)
	}
	future := &configurationChangeFuture{
		req: req,
	}
	future.init()
	select {
	case <-timer:
		return errorFuture{ErrEnqueueTimeout}
	case r.configurationChangeCh <- future:
		return future
	case <-r.shutdownCh:
		return errorFuture{ErrRaftShutdown}
	}
}
func (r *Raft) run() {
	for {
		// Check if we are doing a shutdown
		select {
		case <-r.shutdownCh:
			// Clear the leader to prevent forwarding
			r.setLeader("")
			return
		default:
		}

		// Enter into a sub-FSM
		switch r.getState() {
		case Follower:
			r.runFollower()
		case Candidate:
			r.runCandidate()
		case Leader:
			r.runLeader()
		}
	}
}
func (r *Raft) runFollower() {
	didWarn := false
	r.logger.Info(fmt.Sprintf("%v entering Follower state (Leader: %q)", r, r.Leader()))
	metrics.IncrCounter([]string{"raft", "state", "follower"}, 1)
	heartbeatTimer := randomTimeout(r.conf.HeartbeatTimeout)
	for {
		select {
		case rpc := <-r.rpcCh:
			r.processRPC(rpc)

		case c := <-r.configurationChangeCh:
			// Reject any operations since we are not the leader
			c.respond(ErrNotLeader)

		case a := <-r.applyCh:
			// Reject any operations since we are not the leader
			a.respond(ErrNotLeader)

		case v := <-r.verifyCh:
			// Reject any operations since we are not the leader
			v.respond(ErrNotLeader)

		case r := <-r.userRestoreCh:
			// Reject any restores since we are not the leader
			r.respond(ErrNotLeader)

		case c := <-r.configurationsCh:
			c.configurations = r.configurations.Clone()
			c.respond(nil)

		case b := <-r.bootstrapCh:
			b.respond(r.liveBootstrap(b.configuration))

		case <-heartbeatTimer:
			// Restart the heartbeat timer
			heartbeatTimer = randomTimeout(r.conf.HeartbeatTimeout)

			// Check if we have had a successful contact
			lastContact := r.LastContact()
			if time.Now().Sub(lastContact) < r.conf.HeartbeatTimeout {
				continue
			}

			// Heartbeat failed! Transition to the candidate state
			lastLeader := r.Leader()
			r.setLeader("")

			if r.configurations.latestIndex == 0 {
				if !didWarn {
					r.logger.Warn("no known peers, aborting election")
					didWarn = true
				}
			} else if r.configurations.latestIndex == r.configurations.committedIndex &&
				!hasVote(r.configurations.latest, r.localID) {
				if !didWarn {
					r.logger.Warn("not part of stable configuration, aborting election")
					didWarn = true
				}
			} else {
				r.logger.Warn(fmt.Sprintf("Heartbeat timeout from %q reached, starting election", lastLeader))
				metrics.IncrCounter([]string{"raft", "transition", "heartbeat_timeout"}, 1)
				r.setState(Candidate)
				return
			}

		case <-r.shutdownCh:
			return
		}
	}
}
func (r *Raft) liveBootstrap(configuration Configuration) error {
	// Use the pre-init API to make the static updates.
	err := BootstrapCluster(&r.conf, r.logs, r.stable, r.snapshots,
		r.trans, configuration)
	if err != nil {
		return err
	}

	// Make the configuration live.
	var entry Log
	if err := r.logs.GetLog(1, &entry); err != nil {
		panic(err)
	}
	r.setCurrentTerm(1)
	r.setLastLog(entry.Index, entry.Term)
	r.processConfigurationLogEntry(&entry)
	return nil
}
func (r *Raft) runCandidate() {
	r.logger.Info(fmt.Sprintf("%v entering Candidate state in term %v", r, r.getCurrentTerm()+1))
	metrics.IncrCounter([]string{"raft", "state", "candidate"}, 1)

	// Start vote for us, and set a timeout
	voteCh := r.electSelf()
	electionTimer := randomTimeout(r.conf.ElectionTimeout)

	// Tally the votes, need a simple majority
	grantedVotes := 0
	votesNeeded := r.quorumSize()
	r.logger.Debug(fmt.Sprintf("Votes needed: %d", votesNeeded))

	for r.getState() == Candidate {
		select {
		case rpc := <-r.rpcCh:
			r.processRPC(rpc)

		case vote := <-voteCh:
			// Check if the term is greater than ours, bail
			if vote.Term > r.getCurrentTerm() {
				r.logger.Debug("Newer term discovered, fallback to follower")
				r.setState(Follower)
				r.setCurrentTerm(vote.Term)
				return
			}

			// Check if the vote is granted
			if vote.Granted {
				grantedVotes++
				r.logger.Debug(fmt.Sprintf("Vote granted from %s in term %v. Tally: %d",
					vote.voterID, vote.Term, grantedVotes))
			}

			// Check if we've become the leader
			if grantedVotes >= votesNeeded {
				r.logger.Info(fmt.Sprintf("Election won. Tally: %d", grantedVotes))
				r.setState(Leader)
				r.setLeader(r.localAddr)
				return
			}

		case c := <-r.configurationChangeCh:
			// Reject any operations since we are not the leader
			c.respond(ErrNotLeader)

		case a := <-r.applyCh:
			// Reject any operations since we are not the leader
			a.respond(ErrNotLeader)

		case v := <-r.verifyCh:
			// Reject any operations since we are not the leader
			v.respond(ErrNotLeader)

		case r := <-r.userRestoreCh:
			// Reject any restores since we are not the leader
			r.respond(ErrNotLeader)

		case c := <-r.configurationsCh:
			c.configurations = r.configurations.Clone()
			c.respond(nil)

		case b := <-r.bootstrapCh:
			b.respond(ErrCantBootstrap)

		case <-electionTimer:
			// Election failed! Restart the election. We simply return,
			// which will kick us back into runCandidate
			r.logger.Warn("Election timeout reached, restarting election")
			return

		case <-r.shutdownCh:
			return
		}
	}
}
func (r *Raft) runLeader() {
	r.logger.Info(fmt.Sprintf("%v entering Leader state", r))
	metrics.IncrCounter([]string{"raft", "state", "leader"}, 1)

	// Notify that we are the leader
	asyncNotifyBool(r.leaderCh, true)

	// Push to the notify channel if given
	if notify := r.conf.NotifyCh; notify != nil {
		select {
		case notify <- true:
		case <-r.shutdownCh:
		}
	}

	// Setup leader state
	r.leaderState.commitCh = make(chan struct{}, 1)
	r.leaderState.commitment = newCommitment(r.leaderState.commitCh,
		r.configurations.latest,
		r.getLastIndex()+1 /* first index that may be committed in this term */)
	r.leaderState.inflight = list.New()
	r.leaderState.replState = make(map[ServerID]*followerReplication)
	r.leaderState.notify = make(map[*verifyFuture]struct{})
	r.leaderState.stepDown = make(chan struct{}, 1)

	// Cleanup state on step down
	defer func() {
		// Since we were the leader previously, we update our
		// last contact time when we step down, so that we are not
		// reporting a last contact time from before we were the
		// leader. Otherwise, to a client it would seem our data
		// is extremely stale.
		r.setLastContact()

		// Stop replication
		for _, p := range r.leaderState.replState {
			close(p.stopCh)
		}

		// Respond to all inflight operations
		for e := r.leaderState.inflight.Front(); e != nil; e = e.Next() {
			e.Value.(*logFuture).respond(ErrLeadershipLost)
		}

		// Respond to any pending verify requests
		for future := range r.leaderState.notify {
			future.respond(ErrLeadershipLost)
		}

		// Clear all the state
		r.leaderState.commitCh = nil
		r.leaderState.commitment = nil
		r.leaderState.inflight = nil
		r.leaderState.replState = nil
		r.leaderState.notify = nil
		r.leaderState.stepDown = nil

		// If we are stepping down for some reason, no known leader.
		// We may have stepped down due to an RPC call, which would
		// provide the leader, so we cannot always blank this out.
		r.leaderLock.Lock()
		if r.leader == r.localAddr {
			r.leader = ""
		}
		r.leaderLock.Unlock()

		// Notify that we are not the leader
		asyncNotifyBool(r.leaderCh, false)

		// Push to the notify channel if given
		if notify := r.conf.NotifyCh; notify != nil {
			select {
			case notify <- false:
			case <-r.shutdownCh:
				// On shutdown, make a best effort but do not block
				select {
				case notify <- false:
				default:
				}
			}
		}
	}()

	// Start a replication routine for each peer
	r.startStopReplication()

	// Dispatch a no-op log entry first. This gets this leader up to the latest
	// possible commit index, even in the absence of client commands. This used
	// to append a configuration entry instead of a noop. However, that permits
	// an unbounded number of uncommitted configurations in the log. We now
	// maintain that there exists at most one uncommitted configuration entry in
	// any log, so we have to do proper no-ops here.
	noop := &logFuture{
		log: Log{
			Type: LogNoop,
		},
	}
	r.dispatchLogs([]*logFuture{noop})

	// Sit in the leader loop until we step down
	r.leaderLoop()
}
func (r *Raft) startStopReplication() {
	inConfig := make(map[ServerID]bool, len(r.configurations.latest.Servers))
	lastIdx := r.getLastIndex()

	// Start replication goroutines that need starting
	for _, server := range r.configurations.latest.Servers {
		if server.ID == r.localID {
			continue
		}
		inConfig[server.ID] = true
		if _, ok := r.leaderState.replState[server.ID]; !ok {
			r.logger.Info(fmt.Sprintf("Added peer %v, starting replication", server.ID))
			s := &followerReplication{
				peer:        server,
				commitment:  r.leaderState.commitment,
				stopCh:      make(chan uint64, 1),
				triggerCh:   make(chan struct{}, 1),
				currentTerm: r.getCurrentTerm(),
				nextIndex:   lastIdx + 1,
				lastContact: time.Now(),
				notify:      make(map[*verifyFuture]struct{}),
				notifyCh:    make(chan struct{}, 1),
				stepDown:    r.leaderState.stepDown,
			}
			r.leaderState.replState[server.ID] = s
			r.goFunc(func() { r.replicate(s) })
			asyncNotifyCh(s.triggerCh)
		}
	}

	// Stop replication goroutines that need stopping
	for serverID, repl := range r.leaderState.replState {
		if inConfig[serverID] {
			continue
		}
		// Replicate up to lastIdx and stop
		r.logger.Info(fmt.Sprintf("Removed peer %v, stopping replication after %v", serverID, lastIdx))
		repl.stopCh <- lastIdx
		close(repl.stopCh)
		delete(r.leaderState.replState, serverID)
	}
}
func (r *Raft) configurationChangeChIfStable() chan *configurationChangeFuture {
	// Have to wait until:
	// 1. The latest configuration is committed, and
	// 2. This leader has committed some entry (the noop) in this term
	//    https://groups.google.com/forum/#!msg/raft-dev/t4xj6dJTP6E/d2D9LrWRza8J
	if r.configurations.latestIndex == r.configurations.committedIndex &&
		r.getCommitIndex() >= r.leaderState.commitment.startIndex {
		return r.configurationChangeCh
	}
	return nil
}
func (r *Raft) verifyLeader(v *verifyFuture) {
	// Current leader always votes for self
	v.votes = 1

	// Set the quorum size, hot-path for single node
	v.quorumSize = r.quorumSize()
	if v.quorumSize == 1 {
		v.respond(nil)
		return
	}

	// Track this request
	v.notifyCh = r.verifyCh
	r.leaderState.notify[v] = struct{}{}

	// Trigger immediate heartbeats
	for _, repl := range r.leaderState.replState {
		repl.notifyLock.Lock()
		repl.notify[v] = struct{}{}
		repl.notifyLock.Unlock()
		asyncNotifyCh(repl.notifyCh)
	}
}
func (r *Raft) checkLeaderLease() time.Duration {
	// Track contacted nodes, we can always contact ourself
	contacted := 1

	// Check each follower
	var maxDiff time.Duration
	now := time.Now()
	for peer, f := range r.leaderState.replState {
		diff := now.Sub(f.LastContact())
		if diff <= r.conf.LeaderLeaseTimeout {
			contacted++
			if diff > maxDiff {
				maxDiff = diff
			}
		} else {
			// Log at least once at high value, then debug. Otherwise it gets very verbose.
			if diff <= 3*r.conf.LeaderLeaseTimeout {
				r.logger.Warn(fmt.Sprintf("Failed to contact %v in %v", peer, diff))
			} else {
				r.logger.Debug(fmt.Sprintf("Failed to contact %v in %v", peer, diff))
			}
		}
		metrics.AddSample([]string{"raft", "leader", "lastContact"}, float32(diff/time.Millisecond))
	}

	// Verify we can contact a quorum
	quorum := r.quorumSize()
	if contacted < quorum {
		r.logger.Warn("Failed to contact quorum of nodes, stepping down")
		r.setState(Follower)
		metrics.IncrCounter([]string{"raft", "transition", "leader_lease_timeout"}, 1)
	}
	return maxDiff
}
func (r *Raft) restoreUserSnapshot(meta *SnapshotMeta, reader io.Reader) error {
	defer metrics.MeasureSince([]string{"raft", "restoreUserSnapshot"}, time.Now())

	// Sanity check the version.
	version := meta.Version
	if version < SnapshotVersionMin || version > SnapshotVersionMax {
		return fmt.Errorf("unsupported snapshot version %d", version)
	}

	// We don't support snapshots while there's a config change
	// outstanding since the snapshot doesn't have a means to
	// represent this state.
	committedIndex := r.configurations.committedIndex
	latestIndex := r.configurations.latestIndex
	if committedIndex != latestIndex {
		return fmt.Errorf("cannot restore snapshot now, wait until the configuration entry at %v has been applied (have applied %v)",
			latestIndex, committedIndex)
	}

	// Cancel any inflight requests.
	for {
		e := r.leaderState.inflight.Front()
		if e == nil {
			break
		}
		e.Value.(*logFuture).respond(ErrAbortedByRestore)
		r.leaderState.inflight.Remove(e)
	}

	// We will overwrite the snapshot metadata with the current term,
	// an index that's greater than the current index, or the last
	// index in the snapshot. It's important that we leave a hole in
	// the index so we know there's nothing in the Raft log there and
	// replication will fault and send the snapshot.
	term := r.getCurrentTerm()
	lastIndex := r.getLastIndex()
	if meta.Index > lastIndex {
		lastIndex = meta.Index
	}
	lastIndex++

	// Dump the snapshot. Note that we use the latest configuration,
	// not the one that came with the snapshot.
	sink, err := r.snapshots.Create(version, lastIndex, term,
		r.configurations.latest, r.configurations.latestIndex, r.trans)
	if err != nil {
		return fmt.Errorf("failed to create snapshot: %v", err)
	}
	n, err := io.Copy(sink, reader)
	if err != nil {
		sink.Cancel()
		return fmt.Errorf("failed to write snapshot: %v", err)
	}
	if n != meta.Size {
		sink.Cancel()
		return fmt.Errorf("failed to write snapshot, size didn't match (%d != %d)", n, meta.Size)
	}
	if err := sink.Close(); err != nil {
		return fmt.Errorf("failed to close snapshot: %v", err)
	}
	r.logger.Info(fmt.Sprintf("Copied %d bytes to local snapshot", n))

	// Restore the snapshot into the FSM. If this fails we are in a
	// bad state so we panic to take ourselves out.
	fsm := &restoreFuture{ID: sink.ID()}
	fsm.init()
	select {
	case r.fsmMutateCh <- fsm:
	case <-r.shutdownCh:
		return ErrRaftShutdown
	}
	if err := fsm.Error(); err != nil {
		panic(fmt.Errorf("failed to restore snapshot: %v", err))
	}

	// We set the last log so it looks like we've stored the empty
	// index we burned. The last applied is set because we made the
	// FSM take the snapshot state, and we store the last snapshot
	// in the stable store since we created a snapshot as part of
	// this process.
	r.setLastLog(lastIndex, term)
	r.setLastApplied(lastIndex)
	r.setLastSnapshot(lastIndex, term)

	r.logger.Info(fmt.Sprintf("Restored user snapshot (index %d)", lastIndex))
	return nil
}
func (r *Raft) appendConfigurationEntry(future *configurationChangeFuture) {
	configuration, err := nextConfiguration(r.configurations.latest, r.configurations.latestIndex, future.req)
	if err != nil {
		future.respond(err)
		return
	}

	r.logger.Info(fmt.Sprintf("Updating configuration with %s (%v, %v) to %+v",
		future.req.command, future.req.serverID, future.req.serverAddress, configuration.Servers))

	// In pre-ID compatibility mode we translate all configuration changes
	// in to an old remove peer message, which can handle all supported
	// cases for peer changes in the pre-ID world (adding and removing
	// voters). Both add peer and remove peer log entries are handled
	// similarly on old Raft servers, but remove peer does extra checks to
	// see if a leader needs to step down. Since they both assert the full
	// configuration, then we can safely call remove peer for everything.
	if r.protocolVersion < 2 {
		future.log = Log{
			Type: LogRemovePeerDeprecated,
			Data: encodePeers(configuration, r.trans),
		}
	} else {
		future.log = Log{
			Type: LogConfiguration,
			Data: encodeConfiguration(configuration),
		}
	}

	r.dispatchLogs([]*logFuture{&future.logFuture})
	index := future.Index()
	r.configurations.latest = configuration
	r.configurations.latestIndex = index
	r.leaderState.commitment.setConfiguration(configuration)
	r.startStopReplication()
}
func (r *Raft) dispatchLogs(applyLogs []*logFuture) {
	now := time.Now()
	defer metrics.MeasureSince([]string{"raft", "leader", "dispatchLog"}, now)

	term := r.getCurrentTerm()
	lastIndex := r.getLastIndex()

	n := len(applyLogs)
	logs := make([]*Log, n)
	metrics.SetGauge([]string{"raft", "leader", "dispatchNumLogs"}, float32(n))

	for idx, applyLog := range applyLogs {
		applyLog.dispatch = now
		lastIndex++
		applyLog.log.Index = lastIndex
		applyLog.log.Term = term
		logs[idx] = &applyLog.log
		r.leaderState.inflight.PushBack(applyLog)
	}

	// Write the log entry locally
	if err := r.logs.StoreLogs(logs); err != nil {
		r.logger.Error(fmt.Sprintf("Failed to commit logs: %v", err))
		for _, applyLog := range applyLogs {
			applyLog.respond(err)
		}
		r.setState(Follower)
		return
	}
	r.leaderState.commitment.match(r.localID, lastIndex)

	// Update the last log since it's on disk now
	r.setLastLog(lastIndex, term)

	// Notify the replicators of the new log
	for _, f := range r.leaderState.replState {
		asyncNotifyCh(f.triggerCh)
	}
}
func (r *Raft) processLogs(index uint64, future *logFuture) {
	// Reject logs we've applied already
	lastApplied := r.getLastApplied()
	if index <= lastApplied {
		r.logger.Warn(fmt.Sprintf("Skipping application of old log: %d", index))
		return
	}

	// Apply all the preceding logs
	for idx := r.getLastApplied() + 1; idx <= index; idx++ {
		// Get the log, either from the future or from our log store
		if future != nil && future.log.Index == idx {
			r.processLog(&future.log, future)
		} else {
			l := new(Log)
			if err := r.logs.GetLog(idx, l); err != nil {
				r.logger.Error(fmt.Sprintf("Failed to get log at %d: %v", idx, err))
				panic(err)
			}
			r.processLog(l, nil)
		}

		// Update the lastApplied index and term
		r.setLastApplied(idx)
	}
}
func (r *Raft) processLog(l *Log, future *logFuture) {
	switch l.Type {
	case LogBarrier:
		// Barrier is handled by the FSM
		fallthrough

	case LogCommand:
		// Forward to the fsm handler
		select {
		case r.fsmMutateCh <- &commitTuple{l, future}:
		case <-r.shutdownCh:
			if future != nil {
				future.respond(ErrRaftShutdown)
			}
		}

		// Return so that the future is only responded to
		// by the FSM handler when the application is done
		return

	case LogConfiguration:
	case LogAddPeerDeprecated:
	case LogRemovePeerDeprecated:
	case LogNoop:
		// Ignore the no-op

	default:
		panic(fmt.Errorf("unrecognized log type: %#v", l))
	}

	// Invoke the future if given
	if future != nil {
		future.respond(nil)
	}
}
func (r *Raft) processRPC(rpc RPC) {
	if err := r.checkRPCHeader(rpc); err != nil {
		rpc.Respond(nil, err)
		return
	}

	switch cmd := rpc.Command.(type) {
	case *AppendEntriesRequest:
		r.appendEntries(rpc, cmd)
	case *RequestVoteRequest:
		r.requestVote(rpc, cmd)
	case *InstallSnapshotRequest:
		r.installSnapshot(rpc, cmd)
	default:
		r.logger.Error(fmt.Sprintf("Got unexpected command: %#v", rpc.Command))
		rpc.Respond(nil, fmt.Errorf("unexpected command"))
	}
}
func (r *Raft) processHeartbeat(rpc RPC) {
	defer metrics.MeasureSince([]string{"raft", "rpc", "processHeartbeat"}, time.Now())

	// Check if we are shutdown, just ignore the RPC
	select {
	case <-r.shutdownCh:
		return
	default:
	}

	// Ensure we are only handling a heartbeat
	switch cmd := rpc.Command.(type) {
	case *AppendEntriesRequest:
		r.appendEntries(rpc, cmd)
	default:
		r.logger.Error(fmt.Sprintf("Expected heartbeat, got command: %#v", rpc.Command))
		rpc.Respond(nil, fmt.Errorf("unexpected command"))
	}
}
func (r *Raft) setLastContact() {
	r.lastContactLock.Lock()
	r.lastContact = time.Now()
	r.lastContactLock.Unlock()
}
func (r *Raft) persistVote(term uint64, candidate []byte) error {
	if err := r.stable.SetUint64(keyLastVoteTerm, term); err != nil {
		return err
	}
	if err := r.stable.Set(keyLastVoteCand, candidate); err != nil {
		return err
	}
	return nil
}
func (r *Raft) setCurrentTerm(t uint64) {
	// Persist to disk first
	if err := r.stable.SetUint64(keyCurrentTerm, t); err != nil {
		panic(fmt.Errorf("failed to save current term: %v", err))
	}
	r.raftState.setCurrentTerm(t)
}
func (r *Raft) setState(state RaftState) {
	r.setLeader("")
	oldState := r.raftState.getState()
	r.raftState.setState(state)
	if oldState != state {
		r.observe(state)
	}
}
func (c *commitment) getCommitIndex() uint64 {
	c.Lock()
	defer c.Unlock()
	return c.commitIndex
}
func (c *commitment) recalculate() {
	if len(c.matchIndexes) == 0 {
		return
	}

	matched := make([]uint64, 0, len(c.matchIndexes))
	for _, idx := range c.matchIndexes {
		matched = append(matched, idx)
	}
	sort.Sort(uint64Slice(matched))
	quorumMatchIndex := matched[(len(matched)-1)/2]

	if quorumMatchIndex > c.commitIndex && quorumMatchIndex >= c.startIndex {
		c.commitIndex = quorumMatchIndex
		asyncNotifyCh(c.commitCh)
	}
}
func randomTimeout(minVal time.Duration) <-chan time.Time {
	if minVal == 0 {
		return nil
	}
	extra := (time.Duration(rand.Int63()) % minVal)
	return time.After(minVal + extra)
}
func generateUUID() string {
	buf := make([]byte, 16)
	if _, err := crand.Read(buf); err != nil {
		panic(fmt.Errorf("failed to read random bytes: %v", err))
	}

	return fmt.Sprintf("%08x-%04x-%04x-%04x-%12x",
		buf[0:4],
		buf[4:6],
		buf[6:8],
		buf[8:10],
		buf[10:16])
}
func decodeMsgPack(buf []byte, out interface{}) error {
	r := bytes.NewBuffer(buf)
	hd := codec.MsgpackHandle{}
	dec := codec.NewDecoder(r, &hd)
	return dec.Decode(out)
}
func encodeMsgPack(in interface{}) (*bytes.Buffer, error) {
	buf := bytes.NewBuffer(nil)
	hd := codec.MsgpackHandle{}
	enc := codec.NewEncoder(buf, &hd)
	err := enc.Encode(in)
	return buf, err
}
func backoff(base time.Duration, round, limit uint64) time.Duration {
	power := min(round, limit)
	for power > 2 {
		base *= 2
		power--
	}
	return base
}
func newApplySource(seed string) *applySource {
	h := fnv.New32()
	h.Write([]byte(seed))
	s := &applySource{seed: int64(h.Sum32())}
	s.reset()
	return s
}
func (a *applySource) reset() {
	a.rnd = rand.New(rand.NewSource(a.seed))
}
func DefaultConfig() *Config {
	return &Config{
		ProtocolVersion:    ProtocolVersionMax,
		HeartbeatTimeout:   1000 * time.Millisecond,
		ElectionTimeout:    1000 * time.Millisecond,
		CommitTimeout:      50 * time.Millisecond,
		MaxAppendEntries:   64,
		ShutdownOnRemove:   true,
		TrailingLogs:       10240,
		SnapshotInterval:   120 * time.Second,
		SnapshotThreshold:  8192,
		LeaderLeaseTimeout: 500 * time.Millisecond,
		LogLevel:           "DEBUG",
	}
}
func ValidateConfig(config *Config) error {
	// We don't actually support running as 0 in the library any more, but
	// we do understand it.
	protocolMin := ProtocolVersionMin
	if protocolMin == 0 {
		protocolMin = 1
	}
	if config.ProtocolVersion < protocolMin ||
		config.ProtocolVersion > ProtocolVersionMax {
		return fmt.Errorf("Protocol version %d must be >= %d and <= %d",
			config.ProtocolVersion, protocolMin, ProtocolVersionMax)
	}
	if len(config.LocalID) == 0 {
		return fmt.Errorf("LocalID cannot be empty")
	}
	if config.HeartbeatTimeout < 5*time.Millisecond {
		return fmt.Errorf("Heartbeat timeout is too low")
	}
	if config.ElectionTimeout < 5*time.Millisecond {
		return fmt.Errorf("Election timeout is too low")
	}
	if config.CommitTimeout < time.Millisecond {
		return fmt.Errorf("Commit timeout is too low")
	}
	if config.MaxAppendEntries <= 0 {
		return fmt.Errorf("MaxAppendEntries must be positive")
	}
	if config.MaxAppendEntries > 1024 {
		return fmt.Errorf("MaxAppendEntries is too large")
	}
	if config.SnapshotInterval < 5*time.Millisecond {
		return fmt.Errorf("Snapshot interval is too low")
	}
	if config.LeaderLeaseTimeout < 5*time.Millisecond {
		return fmt.Errorf("Leader lease timeout is too low")
	}
	if config.LeaderLeaseTimeout > config.HeartbeatTimeout {
		return fmt.Errorf("Leader lease timeout cannot be larger than heartbeat timeout")
	}
	if config.ElectionTimeout < config.HeartbeatTimeout {
		return fmt.Errorf("Election timeout must be equal or greater than Heartbeat Timeout")
	}
	return nil
}
func (r *Raft) runSnapshots() {
	for {
		select {
		case <-randomTimeout(r.conf.SnapshotInterval):
			// Check if we should snapshot
			if !r.shouldSnapshot() {
				continue
			}

			// Trigger a snapshot
			if _, err := r.takeSnapshot(); err != nil {
				r.logger.Error(fmt.Sprintf("Failed to take snapshot: %v", err))
			}

		case future := <-r.userSnapshotCh:
			// User-triggered, run immediately
			id, err := r.takeSnapshot()
			if err != nil {
				r.logger.Error(fmt.Sprintf("Failed to take snapshot: %v", err))
			} else {
				future.opener = func() (*SnapshotMeta, io.ReadCloser, error) {
					return r.snapshots.Open(id)
				}
			}
			future.respond(err)

		case <-r.shutdownCh:
			return
		}
	}
}
func (r *Raft) shouldSnapshot() bool {
	// Check the last snapshot index
	lastSnap, _ := r.getLastSnapshot()

	// Check the last log index
	lastIdx, err := r.logs.LastIndex()
	if err != nil {
		r.logger.Error(fmt.Sprintf("Failed to get last log index: %v", err))
		return false
	}

	// Compare the delta to the threshold
	delta := lastIdx - lastSnap
	return delta >= r.conf.SnapshotThreshold
}
func (r *Raft) takeSnapshot() (string, error) {
	defer metrics.MeasureSince([]string{"raft", "snapshot", "takeSnapshot"}, time.Now())

	// Create a request for the FSM to perform a snapshot.
	snapReq := &reqSnapshotFuture{}
	snapReq.init()

	// Wait for dispatch or shutdown.
	select {
	case r.fsmSnapshotCh <- snapReq:
	case <-r.shutdownCh:
		return "", ErrRaftShutdown
	}

	// Wait until we get a response
	if err := snapReq.Error(); err != nil {
		if err != ErrNothingNewToSnapshot {
			err = fmt.Errorf("failed to start snapshot: %v", err)
		}
		return "", err
	}
	defer snapReq.snapshot.Release()

	// Make a request for the configurations and extract the committed info.
	// We have to use the future here to safely get this information since
	// it is owned by the main thread.
	configReq := &configurationsFuture{}
	configReq.init()
	select {
	case r.configurationsCh <- configReq:
	case <-r.shutdownCh:
		return "", ErrRaftShutdown
	}
	if err := configReq.Error(); err != nil {
		return "", err
	}
	committed := configReq.configurations.committed
	committedIndex := configReq.configurations.committedIndex

	// We don't support snapshots while there's a config change outstanding
	// since the snapshot doesn't have a means to represent this state. This
	// is a little weird because we need the FSM to apply an index that's
	// past the configuration change, even though the FSM itself doesn't see
	// the configuration changes. It should be ok in practice with normal
	// application traffic flowing through the FSM. If there's none of that
	// then it's not crucial that we snapshot, since there's not much going
	// on Raft-wise.
	if snapReq.index < committedIndex {
		return "", fmt.Errorf("cannot take snapshot now, wait until the configuration entry at %v has been applied (have applied %v)",
			committedIndex, snapReq.index)
	}

	// Create a new snapshot.
	r.logger.Info(fmt.Sprintf("Starting snapshot up to %d", snapReq.index))
	start := time.Now()
	version := getSnapshotVersion(r.protocolVersion)
	sink, err := r.snapshots.Create(version, snapReq.index, snapReq.term, committed, committedIndex, r.trans)
	if err != nil {
		return "", fmt.Errorf("failed to create snapshot: %v", err)
	}
	metrics.MeasureSince([]string{"raft", "snapshot", "create"}, start)

	// Try to persist the snapshot.
	start = time.Now()
	if err := snapReq.snapshot.Persist(sink); err != nil {
		sink.Cancel()
		return "", fmt.Errorf("failed to persist snapshot: %v", err)
	}
	metrics.MeasureSince([]string{"raft", "snapshot", "persist"}, start)

	// Close and check for error.
	if err := sink.Close(); err != nil {
		return "", fmt.Errorf("failed to close snapshot: %v", err)
	}

	// Update the last stable snapshot info.
	r.setLastSnapshot(snapReq.index, snapReq.term)

	// Compact the logs.
	if err := r.compactLogs(snapReq.index); err != nil {
		return "", err
	}

	r.logger.Info(fmt.Sprintf("Snapshot to %d complete", snapReq.index))
	return sink.ID(), nil
}
func (r *Raft) compactLogs(snapIdx uint64) error {
	defer metrics.MeasureSince([]string{"raft", "compactLogs"}, time.Now())
	// Determine log ranges to compact
	minLog, err := r.logs.FirstIndex()
	if err != nil {
		return fmt.Errorf("failed to get first log index: %v", err)
	}

	// Check if we have enough logs to truncate
	lastLogIdx, _ := r.getLastLog()
	if lastLogIdx <= r.conf.TrailingLogs {
		return nil
	}

	// Truncate up to the end of the snapshot, or `TrailingLogs`
	// back from the head, which ever is further back. This ensures
	// at least `TrailingLogs` entries, but does not allow logs
	// after the snapshot to be removed.
	maxLog := min(snapIdx, lastLogIdx-r.conf.TrailingLogs)

	// Log this
	r.logger.Info(fmt.Sprintf("Compacting logs from %d to %d", minLog, maxLog))

	// Compact the logs
	if err := r.logs.DeleteRange(minLog, maxLog); err != nil {
		return fmt.Errorf("log compaction failed: %v", err)
	}
	return nil
}
func WebpackCheck(r *Runner) error {
	fmt.Println("~~~ Checking webpack.config.js ~~~")

	if !r.App.WithWebpack {
		return nil
	}

	box := webpack.Templates

	f, err := box.FindString("webpack.config.js.tmpl")
	if err != nil {
		return err
	}

	tmpl, err := template.New("webpack").Parse(f)
	if err != nil {
		return err
	}

	bb := &bytes.Buffer{}
	err = tmpl.Execute(bb, map[string]interface{}{
		"opts": &webpack.Options{
			App: r.App,
		},
	})
	if err != nil {
		return err
	}

	b, err := ioutil.ReadFile("webpack.config.js")
	if err != nil {
		return err
	}

	if string(b) == bb.String() {
		return nil
	}

	if !ask("Your webpack.config.js file is different from the latest Buffalo template.\nWould you like to replace yours with the latest template?") {
		fmt.Println("\tSkipping webpack.config.js")
		return nil
	}

	wf, err := os.Create("webpack.config.js")
	if err != nil {
		return err
	}
	_, err = wf.Write(bb.Bytes())
	if err != nil {
		return err
	}
	return wf.Close()
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, err
	}

	if opts.Provider == "none" {
		return g, nil
	}

	box := packr.New("buffalo:genny:vcs", "../vcs/templates")
	s, err := box.FindString("ignore.tmpl")
	if err != nil {
		return g, err
	}

	p := opts.Provider
	n := fmt.Sprintf(".%signore", p)
	g.File(genny.NewFileS(n, s))
	g.Command(exec.Command(p, "init"))

	args := []string{"add", "."}
	if p == "bzr" {
		// Ensure Bazaar is as quiet as Git
		args = append(args, "-q")
	}
	g.Command(exec.Command(p, args...))
	g.Command(exec.Command(p, "commit", "-q", "-m", "Initial Commit"))
	return g, nil
}
func UnixSocket(addr string) (*Listener, error) {
	listener, err := net.Listen("unix", addr)
	if err != nil {
		return nil, err
	}
	return &Listener{
		Server:   &http.Server{},
		Listener: listener,
	}, nil
}
func (e ErrorHandlers) Get(status int) ErrorHandler {
	if eh, ok := e[status]; ok {
		return eh
	}
	if eh, ok := e[0]; ok {
		return eh
	}
	return defaultErrorHandler
}
func (a *App) PanicHandler(next Handler) Handler {
	return func(c Context) error {
		defer func() { //catch or finally
			r := recover()
			var err error
			if r != nil { //catch
				switch t := r.(type) {
				case error:
					err = t
				case string:
					err = errors.New(t)
				default:
					err = errors.New(fmt.Sprint(t))
				}
				err = err
				events.EmitError(events.ErrPanic, err,
					map[string]interface{}{
						"context": c,
						"app":     a,
					},
				)
				eh := a.ErrorHandlers.Get(500)
				eh(500, err, c)
			}
		}()
		return next(c)
	}
}
func (s templateRenderer) partialFeeder(name string) (string, error) {
	ct := strings.ToLower(s.contentType)

	d, f := filepath.Split(name)
	name = filepath.Join(d, "_"+f)
	name = fixExtension(name, ct)

	return s.TemplatesBox.FindString(name)
}
func New(opts Options) *Engine {
	if opts.Helpers == nil {
		opts.Helpers = map[string]interface{}{}
	}

	if opts.TemplateEngines == nil {
		opts.TemplateEngines = map[string]TemplateEngine{}
	}
	if _, ok := opts.TemplateEngines["html"]; !ok {
		opts.TemplateEngines["html"] = plush.BuffaloRenderer
	}
	if _, ok := opts.TemplateEngines["text"]; !ok {
		opts.TemplateEngines["text"] = plush.BuffaloRenderer
	}
	if _, ok := opts.TemplateEngines["txt"]; !ok {
		opts.TemplateEngines["txt"] = plush.BuffaloRenderer
	}
	if _, ok := opts.TemplateEngines["js"]; !ok {
		opts.TemplateEngines["js"] = plush.BuffaloRenderer
	}
	if _, ok := opts.TemplateEngines["md"]; !ok {
		opts.TemplateEngines["md"] = MDTemplateEngine
	}
	if _, ok := opts.TemplateEngines["tmpl"]; !ok {
		opts.TemplateEngines["tmpl"] = GoTemplateEngine
	}

	if opts.DefaultContentType == "" {
		opts.DefaultContentType = "text/html; charset=utf-8"
	}

	e := &Engine{
		Options: opts,
	}
	return e
}
func (m *Message) WriteTo(w io.Writer) (int64, error) {
	mw := &messageWriter{w: w}
	mw.writeMessage(m)
	return mw.n, mw.err
}
func (sm SMTPSender) Send(message Message) error {
	gm := gomail.NewMessage()

	gm.SetHeader("From", message.From)
	gm.SetHeader("To", message.To...)
	gm.SetHeader("Subject", message.Subject)
	gm.SetHeader("Cc", message.CC...)
	gm.SetHeader("Bcc", message.Bcc...)

	sm.addBodies(message, gm)
	sm.addAttachments(message, gm)

	for field, value := range message.Headers {
		gm.SetHeader(field, value)
	}

	err := sm.Dialer.DialAndSend(gm)

	if err != nil {
		return err
	}

	return nil
}
func NewSMTPSender(host string, port string, user string, password string) (SMTPSender, error) {
	iport, err := strconv.Atoi(port)

	if err != nil {
		return SMTPSender{}, errors.New("invalid port for the SMTP mail")
	}

	dialer := &gomail.Dialer{
		Host: host,
		Port: iport,
	}

	if user != "" {
		dialer.Username = user
		dialer.Password = password
	}

	return SMTPSender{
		Dialer: dialer,
	}, nil
}
func (d *DefaultContext) Param(key string) string {
	return d.Params().Get(key)
}
func (d *DefaultContext) Set(key string, value interface{}) {
	d.moot.Lock()
	d.data[key] = value
	d.moot.Unlock()
}
func (d *DefaultContext) Value(key interface{}) interface{} {
	if k, ok := key.(string); ok {
		d.moot.RLock()
		defer d.moot.RUnlock()
		if v, ok := d.data[k]; ok {
			return v
		}
	}
	return d.Context.Value(key)
}
func (d *DefaultContext) Redirect(status int, url string, args ...interface{}) error {
	d.Flash().persist(d.Session())

	if strings.HasSuffix(url, "Path()") {
		if len(args) > 1 {
			return fmt.Errorf("you must pass only a map[string]interface{} to a route path: %T", args)
		}
		var m map[string]interface{}
		if len(args) == 1 {
			rv := reflect.Indirect(reflect.ValueOf(args[0]))
			if !rv.Type().ConvertibleTo(mapType) {
				return fmt.Errorf("you must pass only a map[string]interface{} to a route path: %T", args)
			}
			m = rv.Convert(mapType).Interface().(map[string]interface{})
		}
		h, ok := d.Value(strings.TrimSuffix(url, "()")).(RouteHelperFunc)
		if !ok {
			return fmt.Errorf("could not find a route helper named %s", url)
		}
		url, err := h(m)
		if err != nil {
			return err
		}
		http.Redirect(d.Response(), d.Request(), string(url), status)
		return nil
	}

	if len(args) > 0 {
		url = fmt.Sprintf(url, args...)
	}
	http.Redirect(d.Response(), d.Request(), url, status)
	return nil
}
func (d *DefaultContext) File(name string) (binding.File, error) {
	req := d.Request()
	if err := req.ParseMultipartForm(5 * 1024 * 1024); err != nil {
		return binding.File{}, err
	}
	f, h, err := req.FormFile(name)
	bf := binding.File{
		File:       f,
		FileHeader: h,
	}
	if err != nil {
		return bf, err
	}
	return bf, nil
}
func (d *DefaultContext) MarshalJSON() ([]byte, error) {
	m := map[string]interface{}{}
	data := d.Data()
	for k, v := range data {
		// don't try and marshal ourself
		if _, ok := v.(*DefaultContext); ok {
			continue
		}
		if _, err := json.Marshal(v); err == nil {
			// it can be marshaled, so add it:
			m[k] = v
		}
	}
	return json.Marshal(m)
}
func New(opts *Options) (*genny.Group, error) {
	if err := opts.Validate(); err != nil {
		return nil, err
	}

	gg, err := core.New(opts.Options)
	if err != nil {
		return gg, err
	}

	g := genny.New()
	data := map[string]interface{}{
		"opts": opts,
	}

	helpers := template.FuncMap{}

	t := gogen.TemplateTransformer(data, helpers)
	g.Transformer(t)
	g.Box(packr.New("buffalo:genny:newapp:api", "../api/templates"))

	gg.Add(g)

	return gg, nil
}
func New(opts Options) *App {
	LoadPlugins()
	envy.Load()
	opts = optionsWithDefaults(opts)

	a := &App{
		Options: opts,
		ErrorHandlers: ErrorHandlers{
			404: defaultErrorHandler,
			500: defaultErrorHandler,
		},
		router:   mux.NewRouter(),
		moot:     &sync.RWMutex{},
		routes:   RouteList{},
		children: []*App{},
	}

	dem := a.defaultErrorMiddleware
	a.Middleware = newMiddlewareStack(dem)

	notFoundHandler := func(errorf string, code int) http.HandlerFunc {
		return func(res http.ResponseWriter, req *http.Request) {
			c := a.newContext(RouteInfo{}, res, req)
			err := fmt.Errorf(errorf, req.Method, req.URL.Path)
			a.ErrorHandlers.Get(code)(code, err, c)
		}
	}

	a.router.NotFoundHandler = notFoundHandler("path not found: %s %s", 404)
	a.router.MethodNotAllowedHandler = notFoundHandler("method not found: %s %s", 405)

	if a.MethodOverride == nil {
		a.MethodOverride = MethodOverride
	}
	a.Use(a.PanicHandler)
	a.Use(RequestLogger)
	a.Use(sessionSaver)

	return a
}
func DeprecrationsCheck(r *Runner) error {
	fmt.Println("~~~ Checking for deprecations ~~~")
	b, err := ioutil.ReadFile("main.go")
	if err != nil {
		return err
	}
	if bytes.Contains(b, []byte("app.Start")) {
		r.Warnings = append(r.Warnings, "app.Start has been removed in v0.11.0. Use app.Serve Instead. [main.go]")
	}

	return filepath.Walk(filepath.Join(r.App.Root, "actions"), func(path string, info os.FileInfo, _ error) error {
		if info.IsDir() {
			return nil
		}

		if filepath.Ext(path) != ".go" {
			return nil
		}

		b, err := ioutil.ReadFile(path)
		if err != nil {
			return err
		}
		if bytes.Contains(b, []byte("Websocket()")) {
			r.Warnings = append(r.Warnings, fmt.Sprintf("buffalo.Context#Websocket has been deprecated in v0.11.0, and removed in v0.12.0. Use github.com/gorilla/websocket directly. [%s]", path))
		}
		if bytes.Contains(b, []byte("meta.Name")) {
			r.Warnings = append(r.Warnings, fmt.Sprintf("meta.Name has been deprecated in v0.11.0, and removed in v0.12.0. Use github.com/markbates/inflect.Name directly. [%s]", path))
		}
		if bytes.Contains(b, []byte("generators.Find(")) {
			r.Warnings = append(r.Warnings, fmt.Sprintf("generators.Find(string) has been deprecated in v0.11.0, and removed in v0.12.0. Use generators.FindByBox() instead. [%s]", path))
		}
		// i18n middleware changes in v0.11.1
		if bytes.Contains(b, []byte("T.CookieName")) {
			b = bytes.Replace(b, []byte("T.CookieName"), []byte("T.LanguageExtractorOptions[\"CookieName\"]"), -1)
		}
		if bytes.Contains(b, []byte("T.SessionName")) {
			b = bytes.Replace(b, []byte("T.SessionName"), []byte("T.LanguageExtractorOptions[\"SessionName\"]"), -1)
		}
		if bytes.Contains(b, []byte("T.LanguageFinder=")) || bytes.Contains(b, []byte("T.LanguageFinder ")) {
			r.Warnings = append(r.Warnings, fmt.Sprintf("i18n.Translator#LanguageFinder has been deprecated in v0.11.1, and has been removed in v0.12.0. Use i18n.Translator#LanguageExtractors instead. [%s]", path))
		}
		ioutil.WriteFile(path, b, 0664)

		return nil
	})
}
func (v BaseResource) List(c Context) error {
	return c.Error(404, errors.New("resource not implemented"))
}
func (s funcRenderer) Render(w io.Writer, data Data) error {
	return s.renderFunc(w, data)
}
func (s *Session) Save() error {
	return s.Session.Save(s.req, s.res)
}
func (s *Session) GetOnce(name interface{}) interface{} {
	if x, ok := s.Session.Values[name]; ok {
		s.Delete(name)
		return x
	}
	return nil
}
func (s *Session) Set(name, value interface{}) {
	s.Session.Values[name] = value
}
func (s *Session) Clear() {
	for k := range s.Session.Values {
		s.Delete(k)
	}
}
func (a *App) getSession(r *http.Request, w http.ResponseWriter) *Session {
	if a.root != nil {
		return a.root.getSession(r, w)
	}
	session, _ := a.SessionStore.Get(r, a.SessionName)
	return &Session{
		Session: session,
		req:     r,
		res:     w,
	}
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, err
	}

	data := map[string]interface{}{
		"opts": opts,
	}
	t := gogen.TemplateTransformer(data, template.FuncMap{})
	g.Transformer(t)

	g.RunFn(func(r *genny.Runner) error {
		return genFile(r, opts)
	})
	return g, nil
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, err
	}

	g.RunFn(func(r *genny.Runner) error {
		if _, err := r.LookPath("npm"); err != nil {
			return errors.New("could not find npm executable")
		}
		return nil
	})

	g.Box(Templates)

	data := map[string]interface{}{
		"opts": opts,
	}
	t := gogen.TemplateTransformer(data, gogen.TemplateHelpers)
	g.Transformer(t)
	g.Transformer(genny.Dot())

	g.RunFn(func(r *genny.Runner) error {
		return installPkgs(r, opts)
	})

	return g, nil
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()
	if err := opts.Validate(); err != nil {
		return g, err
	}
	g.Box(packr.New("buffalo:genny:refresh", "../refresh/templates"))

	ctx := plush.NewContext()
	ctx.Set("app", opts.App)
	g.Transformer(plushgen.Transformer(ctx))
	g.Transformer(genny.Dot())
	return g, nil
}
func NewMessage(settings ...MessageSetting) *Message {
	m := &Message{
		header:   make(header),
		charset:  "UTF-8",
		encoding: QuotedPrintable,
	}

	m.applySettings(settings)

	if m.encoding == Base64 {
		m.hEncoder = bEncoding
	} else {
		m.hEncoder = qEncoding
	}

	return m
}
func (m *Message) Reset() {
	for k := range m.header {
		delete(m.header, k)
	}
	m.parts = nil
	m.attachments = nil
	m.embedded = nil
}
func (m *Message) SetHeader(field string, value ...string) {
	m.encodeHeader(value)
	m.header[field] = value
}
func (m *Message) SetHeaders(h map[string][]string) {
	for k, v := range h {
		m.SetHeader(k, v...)
	}
}
func (m *Message) SetAddressHeader(field, address, name string) {
	m.header[field] = []string{m.FormatAddress(address, name)}
}
func (m *Message) FormatAddress(address, name string) string {
	if name == "" {
		return address
	}

	enc := m.encodeString(name)
	if enc == name {
		m.buf.WriteByte('"')
		for i := 0; i < len(name); i++ {
			b := name[i]
			if b == '\\' || b == '"' {
				m.buf.WriteByte('\\')
			}
			m.buf.WriteByte(b)
		}
		m.buf.WriteByte('"')
	} else if hasSpecials(name) {
		m.buf.WriteString(bEncoding.Encode(m.charset, name))
	} else {
		m.buf.WriteString(enc)
	}
	m.buf.WriteString(" <")
	m.buf.WriteString(address)
	m.buf.WriteByte('>')

	addr := m.buf.String()
	m.buf.Reset()
	return addr
}
func (m *Message) SetDateHeader(field string, date time.Time) {
	m.header[field] = []string{m.FormatDate(date)}
}
func (m *Message) FormatDate(date time.Time) string {
	return date.Format(time.RFC1123Z)
}
func (m *Message) SetBody(contentType, body string, settings ...PartSetting) {
	m.SetBodyWriter(contentType, newCopier(body), settings...)
}
func SetPartEncoding(e Encoding) PartSetting {
	return PartSetting(func(p *part) {
		p.encoding = e
	})
}
func SetHeader(h map[string][]string) FileSetting {
	return func(f *file) {
		for k, v := range h {
			f.Header[k] = v
		}
	}
}
func SetCopyFunc(f func(io.Writer) error) FileSetting {
	return func(fi *file) {
		fi.CopyFunc = f
	}
}
func (m *Message) AttachReader(name string, r io.Reader, settings ...FileSetting) {
	m.attachments = m.appendFile(m.attachments, fileFromReader(name, r), settings)
}
func (m *Message) Attach(filename string, settings ...FileSetting) {
	m.attachments = m.appendFile(m.attachments, fileFromFilename(filename), settings)
}
func (m *Message) EmbedReader(name string, r io.Reader, settings ...FileSetting) {
	m.embedded = m.appendFile(m.embedded, fileFromReader(name, r), settings)
}
func (m *Message) Embed(filename string, settings ...FileSetting) {
	m.embedded = m.appendFile(m.embedded, fileFromFilename(filename), settings)
}
func ValidateTemplates(walk packd.Walker, tvs []TemplateValidator) genny.RunFn {
	if len(tvs) == 0 {
		return func(r *genny.Runner) error {
			return nil
		}
	}
	return func(r *genny.Runner) error {
		var errs []string
		err := packd.SkipWalker(walk, packd.CommonSkipPrefixes, func(path string, file packd.File) error {
			info, err := file.FileInfo()
			if err != nil {
				return err
			}
			if info.IsDir() {
				return nil
			}

			f := genny.NewFile(path, file)
			for _, tv := range tvs {
				err := safe.Run(func() {
					if err := tv(f); err != nil {
						errs = append(errs, fmt.Sprintf("template error in file %s: %s", path, err.Error()))
					}
				})
				if err != nil {
					return err
				}
			}

			return nil
		})
		if err != nil {
			return err
		}
		if len(errs) == 0 {
			return nil
		}
		return errors.New(strings.Join(errs, "\n"))
	}
}
func PlushValidator(f genny.File) error {
	if !genny.HasExt(f, ".html", ".md", ".plush") {
		return nil
	}
	_, err := plush.Parse(f.String())
	return err
}
func (opts *Options) Validate() error {
	if opts.App.IsZero() {
		opts.App = meta.New(".")
	}

	if len(opts.Name.String()) == 0 {
		return errors.New("you must supply a name for your mailer")
	}
	return nil
}
func LoadPlugins() error {
	var err error
	oncer.Do("events.LoadPlugins", func() {
		// don't send plugins events during testing
		if envy.Get("GO_ENV", "development") == "test" {
			return
		}
		plugs, err := plugins.Available()
		if err != nil {
			err = err
			return
		}
		for _, cmds := range plugs {
			for _, c := range cmds {
				if c.BuffaloCommand != "events" {
					continue
				}
				err := func(c plugins.Command) error {
					return safe.RunE(func() error {
						n := fmt.Sprintf("[PLUGIN] %s %s", c.Binary, c.Name)
						fn := func(e events.Event) {
							b, err := json.Marshal(e)
							if err != nil {
								fmt.Println("error trying to marshal event", e, err)
								return
							}
							cmd := exec.Command(c.Binary, c.UseCommand, string(b))
							cmd.Stderr = os.Stderr
							cmd.Stdout = os.Stdout
							cmd.Stdin = os.Stdin
							if err := cmd.Run(); err != nil {
								fmt.Println("error trying to send event", strings.Join(cmd.Args, " "), err)
							}
						}
						_, err := events.NamedListen(n, events.Filter(c.ListenFor, fn))
						if err != nil {
							return err
						}
						return nil
					})
				}(c)
				if err != nil {
					err = err
					return
				}
			}

		}
	})
	return err
}
func (w *Response) WriteHeader(i int) {
	w.Status = i
	w.ResponseWriter.WriteHeader(i)
}
func (w *Response) Write(b []byte) (int, error) {
	w.Size = binary.Size(b)
	return w.ResponseWriter.Write(b)
}
func (w *Response) Flush() {
	if f, ok := w.ResponseWriter.(http.Flusher); ok {
		f.Flush()
	}
}
func (w *Response) CloseNotify() <-chan bool {
	if cn, ok := w.ResponseWriter.(closeNotifier); ok {
		return cn.CloseNotify()
	}
	return nil
}
func Run() error {
	fmt.Printf("! This updater will attempt to update your application to Buffalo version: %s\n", runtime.Version)
	if !ask("Do you wish to continue?") {
		fmt.Println("~~~ cancelling update ~~~")
		return nil
	}

	r := &Runner{
		App:      meta.New("."),
		Warnings: []string{},
	}

	defer func() {
		if len(r.Warnings) == 0 {
			return
		}

		fmt.Println("\n\n----------------------------")
		fmt.Printf("!!! (%d) Warnings Were Found !!!\n\n", len(r.Warnings))
		for _, w := range r.Warnings {
			fmt.Printf("[WARNING]: %s\n", w)
		}
	}()

	for _, c := range checks {
		if err := c(r); err != nil {
			return err
		}
	}
	return nil
}
func onlyRelevantFiles(p string, fi os.FileInfo, err error, fn func(p string) error) error {
	if err != nil {
		return err
	}

	if fi.IsDir() {
		base := filepath.Base(p)
		if strings.HasPrefix(base, "_") {
			return filepath.SkipDir
		}
		for _, n := range []string{"vendor", "node_modules", ".git"} {
			if base == n {
				return filepath.SkipDir
			}
		}
		return nil
	}

	ext := filepath.Ext(p)
	if ext != ".go" {
		return nil
	}

	return fn(p)
}
func GoTemplateEngine(input string, data map[string]interface{}, helpers map[string]interface{}) (string, error) {
	// since go templates don't have the concept of an optional map argument like Plush does
	// add this "null" map so it can be used in templates like this:
	// {{ partial "flash.html" .nilOpts }}
	data["nilOpts"] = map[string]interface{}{}

	t := template.New(input)
	if helpers != nil {
		t = t.Funcs(helpers)
	}

	t, err := t.Parse(input)
	if err != nil {
		return "", err
	}

	bb := &bytes.Buffer{}
	err = t.Execute(bb, data)
	return bb.String(), err
}
func (a *App) GET(p string, h Handler) *RouteInfo {
	return a.addRoute("GET", p, h)
}
func (a *App) Redirect(status int, from, to string) *RouteInfo {
	return a.GET(from, func(c Context) error {
		return c.Redirect(status, to)
	})
}
func (a *App) ANY(p string, h Handler) {
	a.GET(p, h)
	a.POST(p, h)
	a.PUT(p, h)
	a.PATCH(p, h)
	a.HEAD(p, h)
	a.OPTIONS(p, h)
	a.DELETE(p, h)
}
func (a *App) buildRouteName(p string) string {
	if p == "/" || p == "" {
		return "root"
	}

	resultParts := []string{}
	parts := strings.Split(p, "/")

	for index, part := range parts {

		if strings.Contains(part, "{") || part == "" {
			continue
		}

		shouldSingularize := (len(parts) > index+1) && strings.Contains(parts[index+1], "{")
		if shouldSingularize {
			part = flect.Singularize(part)
		}

		if parts[index] == "new" || parts[index] == "edit" {
			resultParts = append([]string{part}, resultParts...)
			continue
		}

		if index > 0 && strings.Contains(parts[index-1], "}") {
			resultParts = append(resultParts, part)
			continue
		}

		resultParts = append(resultParts, part)
	}

	if len(resultParts) == 0 {
		return "unnamed"
	}

	underscore := strings.TrimSpace(strings.Join(resultParts, "_"))
	return name.VarCase(underscore)
}
func New(opts *Options) (*genny.Group, error) {
	gg := &genny.Group{}

	if err := opts.Validate(); err != nil {
		return gg, err
	}

	if !opts.SkipInit {
		g, err := initGenerator(opts)
		if err != nil {
			return gg, err
		}
		gg.Add(g)
	}

	g := genny.New()
	h := template.FuncMap{}
	data := map[string]interface{}{
		"opts": opts,
	}
	t := gogen.TemplateTransformer(data, h)
	g.Transformer(t)

	fn := opts.Name.File().String()
	g.File(genny.NewFileS("mailers/"+fn+".go.tmpl", mailerTmpl))
	g.File(genny.NewFileS("templates/mail/"+fn+".html.tmpl", mailTmpl))
	gg.Add(g)

	return gg, nil
}
func NewDialer(host string, port int, username, password string) *Dialer {
	return &Dialer{
		Host:         host,
		Port:         port,
		Username:     username,
		Password:     password,
		SSL:          port == 465,
		Timeout:      10 * time.Second,
		RetryFailure: true,
	}
}
func (d *Dialer) Dial() (SendCloser, error) {
	conn, err := NetDialTimeout("tcp", addr(d.Host, d.Port), d.Timeout)
	if err != nil {
		return nil, err
	}

	if d.SSL {
		conn = tlsClient(conn, d.tlsConfig())
	}

	c, err := smtpNewClient(conn, d.Host)
	if err != nil {
		return nil, err
	}

	if d.Timeout > 0 {
		conn.SetDeadline(time.Now().Add(d.Timeout))
	}

	if d.LocalName != "" {
		if err := c.Hello(d.LocalName); err != nil {
			return nil, err
		}
	}

	if !d.SSL && d.StartTLSPolicy != NoStartTLS {
		ok, _ := c.Extension("STARTTLS")
		if !ok && d.StartTLSPolicy == MandatoryStartTLS {
			err := StartTLSUnsupportedError{
				Policy: d.StartTLSPolicy}
			return nil, err
		}

		if ok {
			if err := c.StartTLS(d.tlsConfig()); err != nil {
				c.Close()
				return nil, err
			}
		}
	}

	if d.Auth == nil && d.Username != "" {
		if ok, auths := c.Extension("AUTH"); ok {
			if strings.Contains(auths, "CRAM-MD5") {
				d.Auth = smtp.CRAMMD5Auth(d.Username, d.Password)
			} else if strings.Contains(auths, "LOGIN") &&
				!strings.Contains(auths, "PLAIN") {
				d.Auth = &loginAuth{
					username: d.Username,
					password: d.Password,
					host:     d.Host,
				}
			} else {
				d.Auth = smtp.PlainAuth("", d.Username, d.Password, d.Host)
			}
		}
	}

	if d.Auth != nil {
		if err = c.Auth(d.Auth); err != nil {
			c.Close()
			return nil, err
		}
	}

	return &smtpSender{c, conn, d}, nil
}
func (d *Dialer) DialAndSend(m ...*Message) error {
	s, err := d.Dial()
	if err != nil {
		return err
	}
	defer s.Close()

	return Send(s, m...)
}
func (f Flash) Set(key string, values []string) {
	f.data[key] = values
}
func (f Flash) Add(key, value string) {
	if len(f.data[key]) == 0 {
		f.data[key] = []string{value}
		return
	}

	f.data[key] = append(f.data[key], value)
}
func (f Flash) persist(session *Session) {
	b, _ := json.Marshal(f.data)
	session.Set(flashKey, b)
	session.Save()
}
func newFlash(session *Session) *Flash {
	result := &Flash{
		data: map[string][]string{},
	}

	if session.Session != nil {
		if f := session.Get(flashKey); f != nil {
			json.Unmarshal(f.([]byte), &result.data)
		}
	}
	return result
}
func (c *Cookies) Get(name string) (string, error) {
	ck, err := c.req.Cookie(name)
	if err != nil {
		return "", err
	}

	return ck.Value, nil
}
func (c *Cookies) Set(name, value string, maxAge time.Duration) {
	ck := http.Cookie{
		Name:   name,
		Value:  value,
		MaxAge: int(maxAge.Seconds()),
	}

	http.SetCookie(c.res, &ck)
}
func (c *Cookies) SetWithExpirationTime(name, value string, expires time.Time) {
	ck := http.Cookie{
		Name:    name,
		Value:   value,
		Expires: expires,
	}

	http.SetCookie(c.res, &ck)
}
func (c *Cookies) Delete(name string) {
	ck := http.Cookie{
		Name:  name,
		Value: "v",
		// Setting a time in the distant past, like the unix epoch, removes the cookie,
		// since it has long expired.
		Expires: time.Unix(0, 0),
	}

	http.SetCookie(c.res, &ck)
}
func NewMessage() Message {
	return Message{
		Context: context.Background(),
		Headers: map[string]string{},
		Data:    render.Data{},
		moot:    &sync.RWMutex{},
	}
}
func NewFromData(data render.Data) Message {
	d := render.Data{}
	for k, v := range data {
		d[k] = v
	}
	m := NewMessage()
	m.Data = d
	return m
}
func New(c buffalo.Context) Message {
	m := NewFromData(c.Data())
	m.Context = c
	return m
}
func (es *EventSource) CloseNotify() <-chan bool {
	if cn, ok := es.w.(closeNotifier); ok {
		return cn.CloseNotify()
	}
	return nil
}
func NewEventSource(w http.ResponseWriter) (*EventSource, error) {
	es := &EventSource{w: w}
	var ok bool
	es.fl, ok = w.(http.Flusher)
	if !ok {
		return es, errors.New("streaming is not supported")
	}

	es.w.Header().Set("Content-Type", "text/event-stream")
	es.w.Header().Set("Cache-Control", "no-cache")
	es.w.Header().Set("Connection", "keep-alive")
	es.w.Header().Set("Access-Control-Allow-Origin", "*")
	return es, nil
}
func NewSimpleWithContext(ctx context.Context) *Simple {
	ctx, cancel := context.WithCancel(ctx)

	l := logrus.New()
	l.Level = logrus.InfoLevel
	l.Formatter = &logrus.TextFormatter{}

	return &Simple{
		Logger:   l,
		ctx:      ctx,
		cancel:   cancel,
		handlers: map[string]Handler{},
		moot:     &sync.Mutex{},
	}
}
func (w *Simple) Register(name string, h Handler) error {
	w.moot.Lock()
	defer w.moot.Unlock()
	if _, ok := w.handlers[name]; ok {
		return fmt.Errorf("handler already mapped for name %s", name)
	}
	w.handlers[name] = h
	return nil
}
func (w *Simple) Start(ctx context.Context) error {
	w.Logger.Info("Starting Simple Background Worker")
	w.ctx, w.cancel = context.WithCancel(ctx)
	return nil
}
func (w Simple) Stop() error {
	w.Logger.Info("Stopping Simple Background Worker")
	w.cancel()
	return nil
}
func (w Simple) Perform(job Job) error {
	w.Logger.Debugf("Performing job %s", job)
	if job.Handler == "" {
		err := fmt.Errorf("no handler name given for %s", job)
		w.Logger.Error(err)
		return err
	}
	w.moot.Lock()
	defer w.moot.Unlock()
	if h, ok := w.handlers[job.Handler]; ok {
		go func() {
			err := safe.RunE(func() error {
				return h(job.Args)
			})

			if err != nil {
				w.Logger.Error(err)
			}
			w.Logger.Debugf("Completed job %s", job)
		}()
		return nil
	}
	err := fmt.Errorf("no handler mapped for name %s", job.Handler)
	w.Logger.Error(err)
	return err
}
func (w Simple) PerformAt(job Job, t time.Time) error {
	return w.PerformIn(job, time.Until(t))
}
func (w Simple) PerformIn(job Job, d time.Duration) error {
	go func() {
		select {
		case <-time.After(d):
			w.Perform(job)
		case <-w.ctx.Done():
			w.cancel()
		}
	}()
	return nil
}
func (ri RouteInfo) String() string {
	b, _ := json.MarshalIndent(ri, "", "  ")
	return string(b)
}
func (ri *RouteInfo) Alias(aliases ...string) *RouteInfo {
	ri.Aliases = append(ri.Aliases, aliases...)
	for _, a := range aliases {
		ri.App.router.Handle(a, ri).Methods(ri.Method)
	}
	return ri
}
func (ri *RouteInfo) Name(name string) *RouteInfo {
	routeIndex := -1
	for index, route := range ri.App.Routes() {
		if route.Path == ri.Path && route.Method == ri.Method {
			routeIndex = index
			break
		}
	}

	name = flect.Camelize(name)

	if !strings.HasSuffix(name, "Path") {
		name = name + "Path"
	}

	ri.PathName = name
	if routeIndex != -1 {
		ri.App.Routes()[routeIndex] = reflect.ValueOf(ri).Interface().(*RouteInfo)
	}

	return ri
}
func (ri *RouteInfo) BuildPathHelper() RouteHelperFunc {
	cRoute := ri
	return func(opts map[string]interface{}) (template.HTML, error) {
		pairs := []string{}
		for k, v := range opts {
			pairs = append(pairs, k)
			pairs = append(pairs, fmt.Sprintf("%v", v))
		}

		url, err := cRoute.MuxRoute.URL(pairs...)
		if err != nil {
			return "", errors.Wrapf(err, "missing parameters for %v", cRoute.Path)
		}

		result := url.Path
		result = addExtraParamsTo(result, opts)

		return template.HTML(result), nil
	}
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, err
	}

	g.Transformer(genny.Replace("-no-pop", ""))
	g.Transformer(genny.Dot())

	box := packr.New("buffalo:genny:ci", "../ci/templates")

	var fname string
	switch opts.Provider {
	case "travis", "travis-ci":
		fname = "-dot-travis.yml.tmpl"
	case "gitlab", "gitlab-ci":
		if opts.App.WithPop {
			fname = "-dot-gitlab-ci.yml.tmpl"
		} else {
			fname = "-dot-gitlab-ci-no-pop.yml.tmpl"
		}
	default:
		return g, fmt.Errorf("could not find a template for %s", opts.Provider)
	}

	f, err := box.FindString(fname)
	if err != nil {
		return g, err
	}

	g.File(genny.NewFileS(fname, f))

	data := map[string]interface{}{
		"opts": opts,
	}

	if opts.DBType == "postgres" {
		data["testDbUrl"] = "postgres://postgres:postgres@postgres:5432/" + opts.App.Name.File().String() + "_test?sslmode=disable"
	} else if opts.DBType == "mysql" {
		data["testDbUrl"] = "mysql://root:root@(mysql:3306)/" + opts.App.Name.File().String() + "_test?parseTime=true&multiStatements=true&readTimeout=1s"
	} else {
		data["testDbUrl"] = ""
	}

	helpers := template.FuncMap{}

	t := gogen.TemplateTransformer(data, helpers)
	g.Transformer(t)

	return g, nil
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, err
	}

	g.RunFn(construct(opts))
	return g, nil
}
func RegisterCustomDecoder(fn CustomTypeDecoder, types []interface{}, fields []interface{}) {
	rawFunc := (func([]string) (interface{}, error))(fn)
	decoder.RegisterCustomType(rawFunc, types, fields)
}
func (ms *MiddlewareStack) Replace(mw1 MiddlewareFunc, mw2 MiddlewareFunc) {
	m1k := funcKey(mw1)
	stack := []MiddlewareFunc{}
	for _, mw := range ms.stack {
		if funcKey(mw) == m1k {
			stack = append(stack, mw2)
		} else {
			stack = append(stack, mw)
		}
	}
	ms.stack = stack
}
func (a *App) Routes() RouteList {
	if a.root != nil {
		return a.root.routes
	}
	return a.routes
}
func WrapBuffaloHandler(h Handler) http.Handler {
	a := New(Options{})
	// it doesn't matter what we actually map it
	// GET, POST, etc... we just need the underlying
	// RouteInfo, which implements http.Handler
	ri := a.GET("/", h)
	return ri
}
func PackageJSONCheck(r *Runner) error {
	fmt.Println("~~~ Checking package.json ~~~")

	if !r.App.WithWebpack {
		return nil
	}

	box := webpack.Templates

	f, err := box.FindString("package.json.tmpl")
	if err != nil {
		return err
	}

	tmpl, err := template.New("package.json").Parse(f)
	if err != nil {
		return err
	}

	bb := &bytes.Buffer{}
	err = tmpl.Execute(bb, map[string]interface{}{
		"opts": &webpack.Options{
			App: r.App,
		},
	})
	if err != nil {
		return err
	}

	b, err := ioutil.ReadFile("package.json")
	if err != nil {
		return err
	}

	if string(b) == bb.String() {
		return nil
	}

	if !ask("Your package.json file is different from the latest Buffalo template.\nWould you like to REPLACE yours with the latest template?") {
		fmt.Println("\tskipping package.json")
		return nil
	}

	pf, err := os.Create("package.json")
	if err != nil {
		return err
	}
	_, err = pf.Write(bb.Bytes())
	if err != nil {
		return err
	}
	err = pf.Close()
	if err != nil {
		return err
	}

	os.RemoveAll(filepath.Join(r.App.Root, "node_modules"))
	var cmd *exec.Cmd
	if r.App.WithYarn {
		cmd = exec.Command("yarnpkg", "install")
	} else {
		cmd = exec.Command("npm", "install")
	}

	cmd.Stdin = os.Stdin
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	return cmd.Run()
}
func (c ImportConverter) match(importpath string) (string, bool) {
	for key, value := range c.Data {
		if !strings.HasPrefix(importpath, key) {
			continue
		}

		result := strings.Replace(importpath, key, value, 1)
		return result, true
	}

	return importpath, false
}
func Send(s Sender, msg ...*Message) error {
	for i, m := range msg {
		if err := send(s, m); err != nil {
			return &SendError{Cause: err, Index: uint(i)}
		}
	}

	return nil
}
func (opts Options) Last(n name.Ident) bool {
	return opts.Parts[len(opts.Parts)-1].String() == n.String()
}
func (a *App) Stop(err error) error {
	a.cancel()
	if err != nil && errors.Cause(err) != context.Canceled {
		a.Logger.Error(err)
		return err
	}
	return nil
}
func DepEnsure(r *Runner) error {
	if r.App.WithPop {
		upkg = append(upkg, "github.com/gobuffalo/fizz", "github.com/gobuffalo/pop")
	}
	if !r.App.WithDep {
		fmt.Println("~~~ Running go get ~~~")
		return modGetUpdate(r)
	}

	fmt.Println("~~~ Running dep ensure ~~~")
	return runDepEnsure(r)
}
func (b BuildInfo) String() string {
	return fmt.Sprintf("%s (%s)", b.Version, b.Time)
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, err
	}

	if !opts.SkipTemplates {
		core := packr.New("github.com/gobuffalo/buffalo/genny/resource/templates/core", "../resource/templates/core")

		if err := g.Box(core); err != nil {
			return g, err
		}
	}

	var abox packd.Box
	if opts.SkipModel {
		abox = packr.New("github.com/gobuffalo/buffalo/genny/resource/templates/standard", "../resource/templates/standard")
	} else {
		abox = packr.New("github.com/gobuffalo/buffalo/genny/resource/templates/use_model", "../resource/templates/use_model")
	}

	if err := g.Box(abox); err != nil {
		return g, err
	}

	pres := presenter{
		App:   opts.App,
		Name:  name.New(opts.Name),
		Model: name.New(opts.Model),
		Attrs: opts.Attrs,
	}
	x := pres.Name.Resource().File().String()
	folder := pres.Name.Folder().Pluralize().String()
	g.Transformer(genny.Replace("resource-name", x))
	g.Transformer(genny.Replace("resource-use_model", x))
	g.Transformer(genny.Replace("folder-name", folder))

	data := map[string]interface{}{
		"opts":    pres,
		"actions": actions(opts),
		"folder":  folder,
	}
	helpers := template.FuncMap{
		"camelize": func(s string) string {
			return flect.Camelize(s)
		},
	}
	g.Transformer(gogen.TemplateTransformer(data, helpers))

	g.RunFn(installPop(opts))

	g.RunFn(addResource(pres))
	return g, nil
}
func (m *Message) AddBody(r render.Renderer, data render.Data) error {
	buf := bytes.NewBuffer([]byte{})
	err := r.Render(buf, m.merge(data))

	if err != nil {
		return err
	}

	m.Bodies = append(m.Bodies, Body{
		Content:     buf.String(),
		ContentType: r.ContentType(),
	})

	return nil
}
func (m *Message) AddBodies(data render.Data, renderers ...render.Renderer) error {
	for _, r := range renderers {
		err := m.AddBody(r, data)
		if err != nil {
			return err
		}
	}

	return nil
}
func (m *Message) AddAttachment(name, contentType string, r io.Reader) error {
	m.Attachments = append(m.Attachments, Attachment{
		Name:        name,
		ContentType: contentType,
		Reader:      r,
		Embedded:    false,
	})

	return nil
}
func (m *Message) AddEmbedded(name string, r io.Reader) error {
	m.Attachments = append(m.Attachments, Attachment{
		Name:     name,
		Reader:   r,
		Embedded: true,
	})

	return nil
}
func (m *Message) SetHeader(field, value string) {
	m.Headers[field] = value
}
func New(opts *Options) (*genny.Group, error) {
	if err := opts.Validate(); err != nil {
		return nil, err
	}

	gg, err := core.New(opts.Options)
	if err != nil {
		return gg, err
	}

	g := genny.New()
	g.Transformer(genny.Dot())
	data := map[string]interface{}{
		"opts": opts,
	}

	helpers := template.FuncMap{}

	t := gogen.TemplateTransformer(data, helpers)
	g.Transformer(t)
	g.Box(packr.New("buffalo:genny:newapp:web", "../web/templates"))

	gg.Add(g)

	if opts.Webpack != nil {
		// add the webpack generator
		g, err = webpack.New(opts.Webpack)
		if err != nil {
			return gg, err
		}
		gg.Add(g)
	}

	if opts.Standard != nil {
		// add the standard generator
		g, err = standard.New(opts.Standard)
		if err != nil {
			return gg, err
		}
		gg.Add(g)
	}

	return gg, nil
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()
	g.Box(packr.New("buffalo:genny:assets:standard", "../standard/templates"))

	data := map[string]interface{}{}
	h := template.FuncMap{}
	t := gogen.TemplateTransformer(data, h)
	g.Transformer(t)

	g.RunFn(func(r *genny.Runner) error {
		f, err := r.FindFile("templates/application.html")
		if err != nil {
			return err
		}

		s := strings.Replace(f.String(), "</title>", "</title>\n"+bs4, 1)
		return r.File(genny.NewFileS(f.Name(), s))
	})

	return g, nil
}
func New(opts *Options) (*genny.Generator, error) {
	g := genny.New()

	if err := opts.Validate(); err != nil {
		return g, errors.WithStack(err)
	}

	g.RunFn(appDetails(opts))

	cBox := packr.Folder(filepath.Join(opts.App.Root, "config"))
	g.RunFn(configs(opts, cBox))

	aBox := packr.Folder(opts.App.Root)
	g.RunFn(pkgChecks(opts, aBox))

	return g, nil
}
func Cleanup(opts *Options) genny.RunFn {
	return func(r *genny.Runner) error {
		defer os.RemoveAll(filepath.Join(opts.Root, "a"))
		if err := jam.Clean(); err != nil {
			return err
		}

		var err error
		opts.rollback.Range(func(k, v interface{}) bool {
			f := genny.NewFileS(k.(string), v.(string))
			r.Logger.Debugf("Rollback: %s", f.Name())
			if err = r.File(f); err != nil {
				return false
			}
			r.Disk.Remove(f.Name())
			return true
		})
		if err != nil {
			return err
		}
		for _, f := range r.Disk.Files() {
			if err := r.Disk.Delete(f.Name()); err != nil {
				return err
			}
		}
		if envy.Mods() {
			if err := r.Exec(exec.Command(genny.GoBin(), "mod", "tidy")); err != nil {
				return err
			}
		}
		return nil
	}
}
func MDTemplateEngine(input string, data map[string]interface{}, helpers map[string]interface{}) (string, error) {
	if ct, ok := data["contentType"].(string); ok && ct == "text/plain" {
		return plush.BuffaloRenderer(input, data, helpers)
	}
	source := github_flavored_markdown.Markdown([]byte(input))
	source = []byte(html.UnescapeString(string(source)))
	return plush.BuffaloRenderer(string(source), data, helpers)
}
func Update(fg FileGetter, kc corev1.ConfigMapInterface, name, namespace string, updates []ConfigMapUpdate, logger *logrus.Entry) error {
	cm, getErr := kc.Get(name, metav1.GetOptions{})
	isNotFound := errors.IsNotFound(getErr)
	if getErr != nil && !isNotFound {
		return fmt.Errorf("failed to fetch current state of configmap: %v", getErr)
	}

	if cm == nil || isNotFound {
		cm = &coreapi.ConfigMap{
			ObjectMeta: metav1.ObjectMeta{
				Name:      name,
				Namespace: namespace,
			},
		}
	}
	if cm.Data == nil {
		cm.Data = map[string]string{}
	}
	if cm.BinaryData == nil {
		cm.BinaryData = map[string][]byte{}
	}

	for _, upd := range updates {
		if upd.Filename == "" {
			logger.WithField("key", upd.Key).Debug("Deleting key.")
			delete(cm.Data, upd.Key)
			delete(cm.BinaryData, upd.Key)
			continue
		}

		content, err := fg.GetFile(upd.Filename)
		if err != nil {
			return fmt.Errorf("get file err: %v", err)
		}
		logger.WithFields(logrus.Fields{"key": upd.Key, "filename": upd.Filename}).Debug("Populating key.")
		value := content
		if upd.GZIP {
			buff := bytes.NewBuffer([]byte{})
			// TODO: this error is wildly unlikely for anything that
			// would actually fit in a configmap, we could just as well return
			// the error instead of falling back to the raw content
			z := gzip.NewWriter(buff)
			if _, err := z.Write(content); err != nil {
				logger.WithError(err).Error("failed to gzip content, falling back to raw")
			} else {
				if err := z.Close(); err != nil {
					logger.WithError(err).Error("failed to flush gzipped content (!?), falling back to raw")
				} else {
					value = buff.Bytes()
				}
			}
		}
		if utf8.ValidString(string(value)) {
			delete(cm.BinaryData, upd.Key)
			cm.Data[upd.Key] = string(value)
		} else {
			delete(cm.Data, upd.Key)
			cm.BinaryData[upd.Key] = value
		}
	}

	var updateErr error
	var verb string
	if getErr != nil && isNotFound {
		verb = "create"
		_, updateErr = kc.Create(cm)
	} else {
		verb = "update"
		_, updateErr = kc.Update(cm)
	}
	if updateErr != nil {
		return fmt.Errorf("%s config map err: %v", verb, updateErr)
	}
	return nil
}
func FilterChanges(cfg plugins.ConfigUpdater, changes []github.PullRequestChange, log *logrus.Entry) map[ConfigMapID][]ConfigMapUpdate {
	toUpdate := map[ConfigMapID][]ConfigMapUpdate{}
	for _, change := range changes {
		var cm plugins.ConfigMapSpec
		found := false

		for key, configMap := range cfg.Maps {
			var matchErr error
			found, matchErr = zglob.Match(key, change.Filename)
			if matchErr != nil {
				// Should not happen, log matchErr and continue
				log.WithError(matchErr).Info("key matching error")
				continue
			}

			if found {
				cm = configMap
				break
			}
		}

		if !found {
			continue // This file does not define a configmap
		}

		// Yes, update the configmap with the contents of this file
		for _, ns := range append(cm.Namespaces) {
			id := ConfigMapID{Name: cm.Name, Namespace: ns}
			key := cm.Key
			if key == "" {
				key = path.Base(change.Filename)
				// if the key changed, we need to remove the old key
				if change.Status == github.PullRequestFileRenamed {
					oldKey := path.Base(change.PreviousFilename)
					// not setting the filename field will cause the key to be
					// deleted
					toUpdate[id] = append(toUpdate[id], ConfigMapUpdate{Key: oldKey})
				}
			}
			if change.Status == github.PullRequestFileRemoved {
				toUpdate[id] = append(toUpdate[id], ConfigMapUpdate{Key: key})
			} else {
				gzip := cfg.GZIP
				if cm.GZIP != nil {
					gzip = *cm.GZIP
				}
				toUpdate[id] = append(toUpdate[id], ConfigMapUpdate{Key: key, Filename: change.Filename, GZIP: gzip})
			}
		}
	}
	return toUpdate
}
func getLabelsFromREMatches(matches [][]string) (labels []string) {
	for _, match := range matches {
		for _, label := range strings.Split(match[0], " ")[1:] {
			label = strings.ToLower(match[1] + "/" + strings.TrimSpace(label))
			labels = append(labels, label)
		}
	}
	return
}
func getLabelsFromGenericMatches(matches [][]string, additionalLabels []string) []string {
	if len(additionalLabels) == 0 {
		return nil
	}
	var labels []string
	for _, match := range matches {
		parts := strings.Split(match[0], " ")
		if ((parts[0] != "/label") && (parts[0] != "/remove-label")) || len(parts) != 2 {
			continue
		}
		for _, l := range additionalLabels {
			if l == parts[1] {
				labels = append(labels, parts[1])
			}
		}
	}
	return labels
}
func (ca *Agent) Start(prowConfig, jobConfig string) error {
	c, err := Load(prowConfig, jobConfig)
	if err != nil {
		return err
	}
	ca.Set(c)
	go func() {
		var lastModTime time.Time
		// Rarely, if two changes happen in the same second, mtime will
		// be the same for the second change, and an mtime-based check would
		// fail. Reload periodically just in case.
		skips := 0
		for range time.Tick(1 * time.Second) {
			if skips < 600 {
				// Check if the file changed to see if it needs to be re-read.
				// os.Stat follows symbolic links, which is how ConfigMaps work.
				prowStat, err := os.Stat(prowConfig)
				if err != nil {
					logrus.WithField("prowConfig", prowConfig).WithError(err).Error("Error loading prow config.")
					continue
				}

				recentModTime := prowStat.ModTime()

				// TODO(krzyzacy): allow empty jobConfig till fully migrate config to subdirs
				if jobConfig != "" {
					jobConfigStat, err := os.Stat(jobConfig)
					if err != nil {
						logrus.WithField("jobConfig", jobConfig).WithError(err).Error("Error loading job configs.")
						continue
					}

					if jobConfigStat.ModTime().After(recentModTime) {
						recentModTime = jobConfigStat.ModTime()
					}
				}

				if !recentModTime.After(lastModTime) {
					skips++
					continue // file hasn't been modified
				}
				lastModTime = recentModTime
			}
			if c, err := Load(prowConfig, jobConfig); err != nil {
				logrus.WithField("prowConfig", prowConfig).
					WithField("jobConfig", jobConfig).
					WithError(err).Error("Error loading config.")
			} else {
				skips = 0
				ca.Set(c)
			}
		}
	}()
	return nil
}
func (ca *Agent) Subscribe(subscription DeltaChan) {
	ca.mut.Lock()
	defer ca.mut.Unlock()
	ca.subscriptions = append(ca.subscriptions, subscription)
}
func (ca *Agent) Config() *Config {
	ca.mut.RLock()
	defer ca.mut.RUnlock()
	return ca.c
}
func (ca *Agent) Set(c *Config) {
	ca.mut.Lock()
	defer ca.mut.Unlock()
	var oldConfig Config
	if ca.c != nil {
		oldConfig = *ca.c
	}
	delta := Delta{oldConfig, *c}
	ca.c = c
	for _, subscription := range ca.subscriptions {
		go func(sub DeltaChan) { // wait a minute to send each event
			end := time.NewTimer(time.Minute)
			select {
			case sub <- delta:
			case <-end.C:
			}
			if !end.Stop() { // prevent new events
				<-end.C // drain the pending event
			}
		}(subscription)
	}
}
func (f *FakeClient) IsMember(org, user string) (bool, error) {
	for _, m := range f.OrgMembers[org] {
		if m == user {
			return true, nil
		}
	}
	return false, nil
}
func (f *FakeClient) ListIssueComments(owner, repo string, number int) ([]github.IssueComment, error) {
	return append([]github.IssueComment{}, f.IssueComments[number]...), nil
}
func (f *FakeClient) ListPullRequestComments(owner, repo string, number int) ([]github.ReviewComment, error) {
	return append([]github.ReviewComment{}, f.PullRequestComments[number]...), nil
}
func (f *FakeClient) ListReviews(owner, repo string, number int) ([]github.Review, error) {
	return append([]github.Review{}, f.Reviews[number]...), nil
}
func (f *FakeClient) ListIssueEvents(owner, repo string, number int) ([]github.ListedIssueEvent, error) {
	return append([]github.ListedIssueEvent{}, f.IssueEvents[number]...), nil
}
func (f *FakeClient) CreateComment(owner, repo string, number int, comment string) error {
	f.IssueCommentsAdded = append(f.IssueCommentsAdded, fmt.Sprintf("%s/%s#%d:%s", owner, repo, number, comment))
	f.IssueComments[number] = append(f.IssueComments[number], github.IssueComment{
		ID:   f.IssueCommentID,
		Body: comment,
		User: github.User{Login: botName},
	})
	f.IssueCommentID++
	return nil
}
func (f *FakeClient) CreateReview(org, repo string, number int, r github.DraftReview) error {
	f.Reviews[number] = append(f.Reviews[number], github.Review{
		ID:   f.ReviewID,
		User: github.User{Login: botName},
		Body: r.Body,
	})
	f.ReviewID++
	return nil
}
func (f *FakeClient) CreateCommentReaction(org, repo string, ID int, reaction string) error {
	f.CommentReactionsAdded = append(f.CommentReactionsAdded, fmt.Sprintf("%s/%s#%d:%s", org, repo, ID, reaction))
	return nil
}
func (f *FakeClient) CreateIssueReaction(org, repo string, ID int, reaction string) error {
	f.IssueReactionsAdded = append(f.IssueReactionsAdded, fmt.Sprintf("%s/%s#%d:%s", org, repo, ID, reaction))
	return nil
}
func (f *FakeClient) DeleteComment(owner, repo string, ID int) error {
	f.IssueCommentsDeleted = append(f.IssueCommentsDeleted, fmt.Sprintf("%s/%s#%d", owner, repo, ID))
	for num, ics := range f.IssueComments {
		for i, ic := range ics {
			if ic.ID == ID {
				f.IssueComments[num] = append(ics[:i], ics[i+1:]...)
				return nil
			}
		}
	}
	return fmt.Errorf("could not find issue comment %d", ID)
}
func (f *FakeClient) DeleteStaleComments(org, repo string, number int, comments []github.IssueComment, isStale func(github.IssueComment) bool) error {
	if comments == nil {
		comments, _ = f.ListIssueComments(org, repo, number)
	}
	for _, comment := range comments {
		if isStale(comment) {
			if err := f.DeleteComment(org, repo, comment.ID); err != nil {
				return fmt.Errorf("failed to delete stale comment with ID '%d'", comment.ID)
			}
		}
	}
	return nil
}
func (f *FakeClient) GetPullRequest(owner, repo string, number int) (*github.PullRequest, error) {
	val, exists := f.PullRequests[number]
	if !exists {
		return nil, fmt.Errorf("Pull request number %d does not exit", number)
	}
	return val, nil
}
func (f *FakeClient) GetPullRequestChanges(org, repo string, number int) ([]github.PullRequestChange, error) {
	return f.PullRequestChanges[number], nil
}
func (f *FakeClient) GetRef(owner, repo, ref string) (string, error) {
	return TestRef, nil
}
func (f *FakeClient) DeleteRef(owner, repo, ref string) error {
	f.RefsDeleted = append(f.RefsDeleted, struct{ Org, Repo, Ref string }{Org: owner, Repo: repo, Ref: ref})
	return nil
}
func (f *FakeClient) GetSingleCommit(org, repo, SHA string) (github.SingleCommit, error) {
	return f.Commits[SHA], nil
}
func (f *FakeClient) CreateStatus(owner, repo, SHA string, s github.Status) error {
	if f.CreatedStatuses == nil {
		f.CreatedStatuses = make(map[string][]github.Status)
	}
	statuses := f.CreatedStatuses[SHA]
	var updated bool
	for i := range statuses {
		if statuses[i].Context == s.Context {
			statuses[i] = s
			updated = true
		}
	}
	if !updated {
		statuses = append(statuses, s)
	}
	f.CreatedStatuses[SHA] = statuses
	return nil
}
func (f *FakeClient) ListStatuses(org, repo, ref string) ([]github.Status, error) {
	return f.CreatedStatuses[ref], nil
}
func (f *FakeClient) GetCombinedStatus(owner, repo, ref string) (*github.CombinedStatus, error) {
	return f.CombinedStatuses[ref], nil
}
func (f *FakeClient) GetRepoLabels(owner, repo string) ([]github.Label, error) {
	la := []github.Label{}
	for _, l := range f.RepoLabelsExisting {
		la = append(la, github.Label{Name: l})
	}
	return la, nil
}
func (f *FakeClient) GetIssueLabels(owner, repo string, number int) ([]github.Label, error) {
	re := regexp.MustCompile(fmt.Sprintf(`^%s/%s#%d:(.*)$`, owner, repo, number))
	la := []github.Label{}
	allLabels := sets.NewString(f.IssueLabelsExisting...)
	allLabels.Insert(f.IssueLabelsAdded...)
	allLabels.Delete(f.IssueLabelsRemoved...)
	for _, l := range allLabels.List() {
		groups := re.FindStringSubmatch(l)
		if groups != nil {
			la = append(la, github.Label{Name: groups[1]})
		}
	}
	return la, nil
}
func (f *FakeClient) AddLabel(owner, repo string, number int, label string) error {
	labelString := fmt.Sprintf("%s/%s#%d:%s", owner, repo, number, label)
	if sets.NewString(f.IssueLabelsAdded...).Has(labelString) {
		return fmt.Errorf("cannot add %v to %s/%s/#%d", label, owner, repo, number)
	}
	if f.RepoLabelsExisting == nil {
		f.IssueLabelsAdded = append(f.IssueLabelsAdded, labelString)
		return nil
	}
	for _, l := range f.RepoLabelsExisting {
		if label == l {
			f.IssueLabelsAdded = append(f.IssueLabelsAdded, labelString)
			return nil
		}
	}
	return fmt.Errorf("cannot add %v to %s/%s/#%d", label, owner, repo, number)
}
func (f *FakeClient) RemoveLabel(owner, repo string, number int, label string) error {
	labelString := fmt.Sprintf("%s/%s#%d:%s", owner, repo, number, label)
	if !sets.NewString(f.IssueLabelsRemoved...).Has(labelString) {
		f.IssueLabelsRemoved = append(f.IssueLabelsRemoved, labelString)
		return nil
	}
	return fmt.Errorf("cannot remove %v from %s/%s/#%d", label, owner, repo, number)
}
func (f *FakeClient) FindIssues(query, sort string, asc bool) ([]github.Issue, error) {
	return f.Issues, nil
}
func (f *FakeClient) AssignIssue(owner, repo string, number int, assignees []string) error {
	var m github.MissingUsers
	for _, a := range assignees {
		if a == "not-in-the-org" {
			m.Users = append(m.Users, a)
			continue
		}
		f.AssigneesAdded = append(f.AssigneesAdded, fmt.Sprintf("%s/%s#%d:%s", owner, repo, number, a))
	}
	if m.Users == nil {
		return nil
	}
	return m
}
func (f *FakeClient) GetFile(org, repo, file, commit string) ([]byte, error) {
	contents, ok := f.RemoteFiles[file]
	if !ok {
		return nil, fmt.Errorf("could not find file %s", file)
	}
	if commit == "" {
		if master, ok := contents["master"]; ok {
			return []byte(master), nil
		}

		return nil, fmt.Errorf("could not find file %s in master", file)
	}

	if content, ok := contents[commit]; ok {
		return []byte(content), nil
	}

	return nil, fmt.Errorf("could not find file %s with ref %s", file, commit)
}
func (f *FakeClient) ListTeams(org string) ([]github.Team, error) {
	return []github.Team{
		{
			ID:   0,
			Name: "Admins",
		},
		{
			ID:   42,
			Name: "Leads",
		},
	}, nil
}
func (f *FakeClient) ListTeamMembers(teamID int, role string) ([]github.TeamMember, error) {
	if role != github.RoleAll {
		return nil, fmt.Errorf("unsupported role %v (only all supported)", role)
	}
	teams := map[int][]github.TeamMember{
		0:  {{Login: "default-sig-lead"}},
		42: {{Login: "sig-lead"}},
	}
	members, ok := teams[teamID]
	if !ok {
		return []github.TeamMember{}, nil
	}
	return members, nil
}
func (f *FakeClient) IsCollaborator(org, repo, login string) (bool, error) {
	normed := github.NormLogin(login)
	for _, collab := range f.Collaborators {
		if github.NormLogin(collab) == normed {
			return true, nil
		}
	}
	return false, nil
}
func (f *FakeClient) ListCollaborators(org, repo string) ([]github.User, error) {
	result := make([]github.User, 0, len(f.Collaborators))
	for _, login := range f.Collaborators {
		result = append(result, github.User{Login: login})
	}
	return result, nil
}
func (f *FakeClient) ClearMilestone(org, repo string, issueNum int) error {
	f.Milestone = 0
	return nil
}
func (f *FakeClient) SetMilestone(org, repo string, issueNum, milestoneNum int) error {
	if milestoneNum < 0 {
		return fmt.Errorf("Milestone Numbers Cannot Be Negative")
	}
	f.Milestone = milestoneNum
	return nil
}
func (f *FakeClient) ListMilestones(org, repo string) ([]github.Milestone, error) {
	milestones := []github.Milestone{}
	for k, v := range f.MilestoneMap {
		milestones = append(milestones, github.Milestone{Title: k, Number: v})
	}
	return milestones, nil
}
func (f *FakeClient) ListPRCommits(org, repo string, prNumber int) ([]github.RepositoryCommit, error) {
	k := fmt.Sprintf("%s/%s#%d", org, repo, prNumber)
	return f.CommitMap[k], nil
}
func (f *FakeClient) GetRepoProjects(owner, repo string) ([]github.Project, error) {
	return f.RepoProjects[fmt.Sprintf("%s/%s", owner, repo)], nil
}
func (f *FakeClient) GetOrgProjects(org string) ([]github.Project, error) {
	return f.RepoProjects[fmt.Sprintf("%s/*", org)], nil
}
func (f *FakeClient) GetProjectColumns(projectID int) ([]github.ProjectColumn, error) {
	// Get project name
	for _, projects := range f.RepoProjects {
		for _, project := range projects {
			if projectID == project.ID {
				return f.ProjectColumnsMap[project.Name], nil
			}
		}
	}
	return nil, fmt.Errorf("Cannot find project ID")
}
func (f *FakeClient) CreateProjectCard(columnID int, projectCard github.ProjectCard) (*github.ProjectCard, error) {
	if f.ColumnCardsMap == nil {
		f.ColumnCardsMap = make(map[int][]github.ProjectCard)
	}

	for project, columnIDMap := range f.ColumnIDMap {
		columnName, exists := columnIDMap[columnID]
		if exists {
			f.ColumnCardsMap[columnID] = append(
				f.ColumnCardsMap[columnID],
				projectCard,
			)
			f.Column = columnName
			f.Project = project
			return &projectCard, nil
		}
	}
	return nil, fmt.Errorf("Provided column %d does not exist, ColumnIDMap is %v", columnID, f.ColumnIDMap)
}
func (f *FakeClient) DeleteProjectCard(projectCardID int) error {
	if f.ColumnCardsMap == nil {
		return fmt.Errorf("Project card doesn't exist")
	}
	f.Project = ""
	f.Column = ""
	newCards := []github.ProjectCard{}
	oldColumnID := -1
	for column, cards := range f.ColumnCardsMap {
		removalIndex := -1
		for i, existingCard := range cards {
			if existingCard.ContentID == projectCardID {
				oldColumnID = column
				removalIndex = i
				break
			}
		}
		if removalIndex != -1 {
			newCards = cards
			newCards[removalIndex] = newCards[len(newCards)-1]
			newCards = newCards[:len(newCards)-1]
			break
		}
	}
	// Update the old column's list of project cards
	if oldColumnID != -1 {
		f.ColumnCardsMap[oldColumnID] = newCards
	}
	return nil
}
func (f *FakeClient) MoveProjectCard(projectCardID int, newColumnID int) error {
	// Remove project card from old column
	newCards := []github.ProjectCard{}
	oldColumnID := -1
	projectCard := github.ProjectCard{}
	for column, cards := range f.ColumnCardsMap {
		removalIndex := -1
		for i, existingCard := range cards {
			if existingCard.ContentID == projectCardID {
				oldColumnID = column
				removalIndex = i
				projectCard = existingCard
				break
			}
		}
		if removalIndex != -1 {
			newCards = cards
			newCards[removalIndex] = newCards[len(newCards)-1]
			newCards = newCards[:len(newCards)-1]
		}
	}
	if oldColumnID != -1 {
		// Update the old column's list of project cards
		f.ColumnCardsMap[oldColumnID] = newCards
	}

	for project, columnIDMap := range f.ColumnIDMap {
		if columnName, exists := columnIDMap[newColumnID]; exists {
			// Add project card to new column
			f.ColumnCardsMap[newColumnID] = append(
				f.ColumnCardsMap[newColumnID],
				projectCard,
			)
			f.Column = columnName
			f.Project = project
			break
		}
	}

	return nil
}
func (config *InfluxConfig) CreateDatabaseClient() (*InfluxDB, error) {
	client, err := influxdb.NewHTTPClient(influxdb.HTTPConfig{
		Addr:     config.Host,
		Username: config.User,
		Password: config.Password,
	})
	if err != nil {
		return nil, err
	}

	return &InfluxDB{
		client:   client,
		database: config.DB,
	}, nil
}
func (i *InfluxDB) Push(measurement string, tags map[string]string, fields map[string]interface{}, date time.Time) error {
	batch, err := influxdb.NewBatchPoints(influxdb.BatchPointsConfig{
		Database:  i.database,
		Precision: "s",
	})
	if err != nil {
		return err
	}

	pt, err := influxdb.NewPoint(measurement, tags, fields, date)
	if err != nil {
		return err
	}

	batch.AddPoint(pt)

	err = i.client.Write(batch)
	if err != nil {
		return err
	}
	glog.Infof("Sent to influx: %s %+v %+v %s", measurement, tags, fields, date)

	return nil
}
func NewProwJobWithAnnotation(spec prowapi.ProwJobSpec, labels, annotations map[string]string) prowapi.ProwJob {
	return newProwJob(spec, labels, annotations)
}
func NewProwJob(spec prowapi.ProwJobSpec, labels map[string]string) prowapi.ProwJob {
	return newProwJob(spec, labels, nil)
}
func NewPresubmit(pr github.PullRequest, baseSHA string, job config.Presubmit, eventGUID string) prowapi.ProwJob {
	refs := createRefs(pr, baseSHA)
	labels := make(map[string]string)
	for k, v := range job.Labels {
		labels[k] = v
	}
	labels[github.EventGUID] = eventGUID
	return NewProwJob(PresubmitSpec(job, refs), labels)
}
func PresubmitSpec(p config.Presubmit, refs prowapi.Refs) prowapi.ProwJobSpec {
	pjs := specFromJobBase(p.JobBase)
	pjs.Type = prowapi.PresubmitJob
	pjs.Context = p.Context
	pjs.Report = !p.SkipReport
	pjs.RerunCommand = p.RerunCommand
	if p.JenkinsSpec != nil {
		pjs.JenkinsSpec = &prowapi.JenkinsSpec{
			GitHubBranchSourceJob: p.JenkinsSpec.GitHubBranchSourceJob,
		}
	}
	pjs.Refs = completePrimaryRefs(refs, p.JobBase)

	return pjs
}
func PostsubmitSpec(p config.Postsubmit, refs prowapi.Refs) prowapi.ProwJobSpec {
	pjs := specFromJobBase(p.JobBase)
	pjs.Type = prowapi.PostsubmitJob
	pjs.Context = p.Context
	pjs.Report = !p.SkipReport
	pjs.Refs = completePrimaryRefs(refs, p.JobBase)
	if p.JenkinsSpec != nil {
		pjs.JenkinsSpec = &prowapi.JenkinsSpec{
			GitHubBranchSourceJob: p.JenkinsSpec.GitHubBranchSourceJob,
		}
	}

	return pjs
}
func PeriodicSpec(p config.Periodic) prowapi.ProwJobSpec {
	pjs := specFromJobBase(p.JobBase)
	pjs.Type = prowapi.PeriodicJob

	return pjs
}
func BatchSpec(p config.Presubmit, refs prowapi.Refs) prowapi.ProwJobSpec {
	pjs := specFromJobBase(p.JobBase)
	pjs.Type = prowapi.BatchJob
	pjs.Context = p.Context
	pjs.Refs = completePrimaryRefs(refs, p.JobBase)

	return pjs
}
func PartitionActive(pjs []prowapi.ProwJob) (pending, triggered chan prowapi.ProwJob) {
	// Size channels correctly.
	pendingCount, triggeredCount := 0, 0
	for _, pj := range pjs {
		switch pj.Status.State {
		case prowapi.PendingState:
			pendingCount++
		case prowapi.TriggeredState:
			triggeredCount++
		}
	}
	pending = make(chan prowapi.ProwJob, pendingCount)
	triggered = make(chan prowapi.ProwJob, triggeredCount)

	// Partition the jobs into the two separate channels.
	for _, pj := range pjs {
		switch pj.Status.State {
		case prowapi.PendingState:
			pending <- pj
		case prowapi.TriggeredState:
			triggered <- pj
		}
	}
	close(pending)
	close(triggered)
	return pending, triggered
}
func ProwJobFields(pj *prowapi.ProwJob) logrus.Fields {
	fields := make(logrus.Fields)
	fields["name"] = pj.ObjectMeta.Name
	fields["job"] = pj.Spec.Job
	fields["type"] = pj.Spec.Type
	if len(pj.ObjectMeta.Labels[github.EventGUID]) > 0 {
		fields[github.EventGUID] = pj.ObjectMeta.Labels[github.EventGUID]
	}
	if pj.Spec.Refs != nil && len(pj.Spec.Refs.Pulls) == 1 {
		fields[github.PrLogField] = pj.Spec.Refs.Pulls[0].Number
		fields[github.RepoLogField] = pj.Spec.Refs.Repo
		fields[github.OrgLogField] = pj.Spec.Refs.Org
	}
	if pj.Spec.JenkinsSpec != nil {
		fields["github_based_job"] = pj.Spec.JenkinsSpec.GitHubBranchSourceJob
	}

	return fields
}
func ClusterToCtx(cluster string) string {
	if cluster == kube.InClusterContext {
		return kube.DefaultClusterAlias
	}
	return cluster
}
func (pluginHelp *PluginHelp) AddCommand(command Command) {
	pluginHelp.Commands = append(pluginHelp.Commands, command)
}
func (c *FakeProwJobs) Get(name string, options v1.GetOptions) (result *prowjobsv1.ProwJob, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewGetAction(prowjobsResource, c.ns, name), &prowjobsv1.ProwJob{})

	if obj == nil {
		return nil, err
	}
	return obj.(*prowjobsv1.ProwJob), err
}
func (c *FakeProwJobs) Watch(opts v1.ListOptions) (watch.Interface, error) {
	return c.Fake.
		InvokesWatch(testing.NewWatchAction(prowjobsResource, c.ns, opts))

}
func (c *FakeProwJobs) Delete(name string, options *v1.DeleteOptions) error {
	_, err := c.Fake.
		Invokes(testing.NewDeleteAction(prowjobsResource, c.ns, name), &prowjobsv1.ProwJob{})

	return err
}
func (c *FakeProwJobs) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *prowjobsv1.ProwJob, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewPatchSubresourceAction(prowjobsResource, c.ns, name, data, subresources...), &prowjobsv1.ProwJob{})

	if obj == nil {
		return nil, err
	}
	return obj.(*prowjobsv1.ProwJob), err
}
func (t *Tide) MergeMethod(org, repo string) github.PullRequestMergeType {
	name := org + "/" + repo

	v, ok := t.MergeType[name]
	if !ok {
		if ov, found := t.MergeType[org]; found {
			return ov
		}

		return github.MergeMerge
	}

	return v
}
func (tq *TideQuery) Query() string {
	toks := []string{"is:pr", "state:open"}
	for _, o := range tq.Orgs {
		toks = append(toks, fmt.Sprintf("org:\"%s\"", o))
	}
	for _, r := range tq.Repos {
		toks = append(toks, fmt.Sprintf("repo:\"%s\"", r))
	}
	for _, r := range tq.ExcludedRepos {
		toks = append(toks, fmt.Sprintf("-repo:\"%s\"", r))
	}
	for _, b := range tq.ExcludedBranches {
		toks = append(toks, fmt.Sprintf("-base:\"%s\"", b))
	}
	for _, b := range tq.IncludedBranches {
		toks = append(toks, fmt.Sprintf("base:\"%s\"", b))
	}
	for _, l := range tq.Labels {
		toks = append(toks, fmt.Sprintf("label:\"%s\"", l))
	}
	for _, l := range tq.MissingLabels {
		toks = append(toks, fmt.Sprintf("-label:\"%s\"", l))
	}
	if tq.Milestone != "" {
		toks = append(toks, fmt.Sprintf("milestone:\"%s\"", tq.Milestone))
	}
	if tq.ReviewApprovedRequired {
		toks = append(toks, "review:approved")
	}
	return strings.Join(toks, " ")
}
func (tq TideQuery) ForRepo(org, repo string) bool {
	fullName := fmt.Sprintf("%s/%s", org, repo)
	for _, queryOrg := range tq.Orgs {
		if queryOrg != org {
			continue
		}
		// Check for repos excluded from the org.
		for _, excludedRepo := range tq.ExcludedRepos {
			if excludedRepo == fullName {
				return false
			}
		}
		return true
	}
	for _, queryRepo := range tq.Repos {
		if queryRepo == fullName {
			return true
		}
	}
	return false
}
func (tqs TideQueries) OrgExceptionsAndRepos() (map[string]sets.String, sets.String) {
	orgs := make(map[string]sets.String)
	for i := range tqs {
		for _, org := range tqs[i].Orgs {
			applicableRepos := sets.NewString(reposInOrg(org, tqs[i].ExcludedRepos)...)
			if excepts, ok := orgs[org]; !ok {
				// We have not seen this org so the exceptions are just applicable
				// members of 'excludedRepos'.
				orgs[org] = applicableRepos
			} else {
				// We have seen this org so the exceptions are the applicable
				// members of 'excludedRepos' intersected with existing exceptions.
				orgs[org] = excepts.Intersection(applicableRepos)
			}
		}
	}
	repos := sets.NewString()
	for i := range tqs {
		repos.Insert(tqs[i].Repos...)
	}
	// Remove any org exceptions that are explicitly included in a different query.
	reposList := repos.UnsortedList()
	for _, excepts := range orgs {
		excepts.Delete(reposList...)
	}
	return orgs, repos
}
func (tqs TideQueries) QueryMap() *QueryMap {
	return &QueryMap{
		queries: tqs,
		cache:   make(map[string]TideQueries),
	}
}
func (qm *QueryMap) ForRepo(org, repo string) TideQueries {
	res := TideQueries(nil)
	fullName := fmt.Sprintf("%s/%s", org, repo)

	qm.Lock()
	defer qm.Unlock()

	if qs, ok := qm.cache[fullName]; ok {
		return append(res, qs...) // Return a copy.
	}
	// Cache miss. Need to determine relevant queries.

	for _, query := range qm.queries {
		if query.ForRepo(org, repo) {
			res = append(res, query)
		}
	}
	qm.cache[fullName] = res
	return res
}
func (cp *TideContextPolicy) Validate() error {
	if inter := sets.NewString(cp.RequiredContexts...).Intersection(sets.NewString(cp.OptionalContexts...)); inter.Len() > 0 {
		return fmt.Errorf("contexts %s are defined as required and optional", strings.Join(inter.List(), ", "))
	}
	if inter := sets.NewString(cp.RequiredContexts...).Intersection(sets.NewString(cp.RequiredIfPresentContexts...)); inter.Len() > 0 {
		return fmt.Errorf("contexts %s are defined as required and required if present", strings.Join(inter.List(), ", "))
	}
	if inter := sets.NewString(cp.OptionalContexts...).Intersection(sets.NewString(cp.RequiredIfPresentContexts...)); inter.Len() > 0 {
		return fmt.Errorf("contexts %s are defined as optional and required if present", strings.Join(inter.List(), ", "))
	}
	return nil
}
func (c Config) GetTideContextPolicy(org, repo, branch string) (*TideContextPolicy, error) {
	options := parseTideContextPolicyOptions(org, repo, branch, c.Tide.ContextOptions)
	// Adding required and optional contexts from options
	required := sets.NewString(options.RequiredContexts...)
	requiredIfPresent := sets.NewString(options.RequiredIfPresentContexts...)
	optional := sets.NewString(options.OptionalContexts...)

	// automatically generate required and optional entries for Prow Jobs
	prowRequired, prowRequiredIfPresent, prowOptional := BranchRequirements(org, repo, branch, c.Presubmits)
	required.Insert(prowRequired...)
	requiredIfPresent.Insert(prowRequiredIfPresent...)
	optional.Insert(prowOptional...)

	// Using Branch protection configuration
	if options.FromBranchProtection != nil && *options.FromBranchProtection {
		bp, err := c.GetBranchProtection(org, repo, branch)
		if err != nil {
			logrus.WithError(err).Warningf("Error getting branch protection for %s/%s+%s", org, repo, branch)
		} else if bp != nil && bp.Protect != nil && *bp.Protect && bp.RequiredStatusChecks != nil {
			required.Insert(bp.RequiredStatusChecks.Contexts...)
		}
	}

	t := &TideContextPolicy{
		RequiredContexts:          required.List(),
		RequiredIfPresentContexts: requiredIfPresent.List(),
		OptionalContexts:          optional.List(),
		SkipUnknownContexts:       options.SkipUnknownContexts,
	}
	if err := t.Validate(); err != nil {
		return t, err
	}
	return t, nil
}
func (cp *TideContextPolicy) IsOptional(c string) bool {
	if sets.NewString(cp.OptionalContexts...).Has(c) {
		return true
	}
	if sets.NewString(cp.RequiredContexts...).Has(c) {
		return false
	}
	// assume if we're asking that the context is present on the PR
	if sets.NewString(cp.RequiredIfPresentContexts...).Has(c) {
		return false
	}
	if cp.SkipUnknownContexts != nil && *cp.SkipUnknownContexts {
		return true
	}
	return false
}
func (cp *TideContextPolicy) MissingRequiredContexts(contexts []string) []string {
	if len(cp.RequiredContexts) == 0 {
		return nil
	}
	existingContexts := sets.NewString()
	for _, c := range contexts {
		existingContexts.Insert(c)
	}
	var missingContexts []string
	for c := range sets.NewString(cp.RequiredContexts...).Difference(existingContexts) {
		missingContexts = append(missingContexts, c)
	}
	return missingContexts
}
func ValidateWebhook(w http.ResponseWriter, r *http.Request, hmacSecret []byte) (string, string, []byte, bool, int) {
	defer r.Body.Close()

	// Our health check uses GET, so just kick back a 200.
	if r.Method == http.MethodGet {
		return "", "", nil, false, http.StatusOK
	}

	// Header checks: It must be a POST with an event type and a signature.
	if r.Method != http.MethodPost {
		responseHTTPError(w, http.StatusMethodNotAllowed, "405 Method not allowed")
		return "", "", nil, false, http.StatusMethodNotAllowed
	}
	eventType := r.Header.Get("X-GitHub-Event")
	if eventType == "" {
		responseHTTPError(w, http.StatusBadRequest, "400 Bad Request: Missing X-GitHub-Event Header")
		return "", "", nil, false, http.StatusBadRequest
	}
	eventGUID := r.Header.Get("X-GitHub-Delivery")
	if eventGUID == "" {
		responseHTTPError(w, http.StatusBadRequest, "400 Bad Request: Missing X-GitHub-Delivery Header")
		return "", "", nil, false, http.StatusBadRequest
	}
	sig := r.Header.Get("X-Hub-Signature")
	if sig == "" {
		responseHTTPError(w, http.StatusForbidden, "403 Forbidden: Missing X-Hub-Signature")
		return "", "", nil, false, http.StatusForbidden
	}
	contentType := r.Header.Get("content-type")
	if contentType != "application/json" {
		responseHTTPError(w, http.StatusBadRequest, "400 Bad Request: Hook only accepts content-type: application/json - please reconfigure this hook on GitHub")
		return "", "", nil, false, http.StatusBadRequest
	}
	payload, err := ioutil.ReadAll(r.Body)
	if err != nil {
		responseHTTPError(w, http.StatusInternalServerError, "500 Internal Server Error: Failed to read request body")
		return "", "", nil, false, http.StatusInternalServerError
	}
	// Validate the payload with our HMAC secret.
	if !ValidatePayload(payload, sig, hmacSecret) {
		responseHTTPError(w, http.StatusForbidden, "403 Forbidden: Invalid X-Hub-Signature")
		return "", "", nil, false, http.StatusForbidden
	}

	return eventType, eventGUID, payload, true, http.StatusOK
}
func HelpProvider(enabledRepos []string) (*pluginhelp.PluginHelp, error) {
	return &pluginhelp.PluginHelp{
			Description: `The needs-rebase plugin manages the '` + labels.NeedsRebase + `' label by removing it from Pull Requests that are mergeable and adding it to those which are not.
The plugin reacts to commit changes on PRs in addition to periodically scanning all open PRs for any changes to mergeability that could have resulted from changes in other PRs.`,
		},
		nil
}
func HandleEvent(log *logrus.Entry, ghc githubClient, pre *github.PullRequestEvent) error {
	if pre.Action != github.PullRequestActionOpened && pre.Action != github.PullRequestActionSynchronize && pre.Action != github.PullRequestActionReopened {
		return nil
	}

	// Before checking mergeability wait a few seconds to give github a chance to calculate it.
	// This initial delay prevents us from always wasting the first API token.
	sleep(time.Second * 5)

	org := pre.Repo.Owner.Login
	repo := pre.Repo.Name
	number := pre.Number
	sha := pre.PullRequest.Head.SHA

	mergeable, err := ghc.IsMergeable(org, repo, number, sha)
	if err != nil {
		return err
	}
	issueLabels, err := ghc.GetIssueLabels(org, repo, number)
	if err != nil {
		return err
	}
	hasLabel := github.HasLabel(labels.NeedsRebase, issueLabels)

	return takeAction(log, ghc, org, repo, number, pre.PullRequest.User.Login, hasLabel, mergeable)
}
func HandleAll(log *logrus.Entry, ghc githubClient, config *plugins.Configuration) error {
	log.Info("Checking all PRs.")
	orgs, repos := config.EnabledReposForExternalPlugin(PluginName)
	if len(orgs) == 0 && len(repos) == 0 {
		log.Warnf("No repos have been configured for the %s plugin", PluginName)
		return nil
	}
	var buf bytes.Buffer
	fmt.Fprint(&buf, "is:pr is:open")
	for _, org := range orgs {
		fmt.Fprintf(&buf, " org:\"%s\"", org)
	}
	for _, repo := range repos {
		fmt.Fprintf(&buf, " repo:\"%s\"", repo)
	}
	prs, err := search(context.Background(), log, ghc, buf.String())
	if err != nil {
		return err
	}
	log.Infof("Considering %d PRs.", len(prs))

	for _, pr := range prs {
		// Skip PRs that are calculating mergeability. They will be updated by event or next loop.
		if pr.Mergeable == githubql.MergeableStateUnknown {
			continue
		}
		org := string(pr.Repository.Owner.Login)
		repo := string(pr.Repository.Name)
		num := int(pr.Number)
		l := log.WithFields(logrus.Fields{
			"org":  org,
			"repo": repo,
			"pr":   num,
		})
		hasLabel := false
		for _, label := range pr.Labels.Nodes {
			if label.Name == labels.NeedsRebase {
				hasLabel = true
				break
			}
		}
		err := takeAction(
			l,
			ghc,
			org,
			repo,
			num,
			string(pr.Author.Login),
			hasLabel,
			pr.Mergeable == githubql.MergeableStateMergeable,
		)
		if err != nil {
			l.WithError(err).Error("Error handling PR.")
		}
	}
	return nil
}
func NewDryRunProwJobClient(deckURL string) prowv1.ProwJobInterface {
	return &dryRunProwJobClient{
		deckURL: deckURL,
		client:  &http.Client{},
	}
}
func (c *dryRunProwJobClient) Create(*prowapi.ProwJob) (*prowapi.ProwJob, error) {
	return nil, nil
}
func (c *dryRunProwJobClient) Update(*prowapi.ProwJob) (*prowapi.ProwJob, error) {
	return nil, nil
}
func (c *dryRunProwJobClient) UpdateStatus(*prowapi.ProwJob) (*prowapi.ProwJob, error) {
	return nil, nil
}
func (c *dryRunProwJobClient) Delete(name string, options *metav1.DeleteOptions) error {
	return nil
}
func (c *dryRunProwJobClient) DeleteCollection(options *metav1.DeleteOptions, listOptions metav1.ListOptions) error {
	return nil
}
func (c *dryRunProwJobClient) Get(name string, options metav1.GetOptions) (*prowapi.ProwJob, error) {
	return nil, nil
}
func (c *dryRunProwJobClient) List(opts metav1.ListOptions) (*prowapi.ProwJobList, error) {
	var jl prowapi.ProwJobList
	err := c.request("/prowjobs.js", map[string]string{"labelSelector": opts.LabelSelector}, &jl)
	return &jl, err
}
func (c *dryRunProwJobClient) Watch(opts metav1.ListOptions) (watch.Interface, error) {
	return nil, nil
}
func (c *dryRunProwJobClient) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *prowapi.ProwJob, err error) {
	return nil, nil
}
func (c *controller) hasSynced() bool {
	if !c.pjInformer.HasSynced() {
		if c.wait != "prowjobs" {
			c.wait = "prowjobs"
			ns := c.pjNamespace()
			if ns == "" {
				ns = "controllers"
			}
			logrus.Infof("Waiting on prowjobs in %s namespace...", ns)
		}
		return false // still syncing prowjobs
	}
	if !c.prowJobsDone {
		c.prowJobsDone = true
		logrus.Info("Synced prow jobs")
	}
	if c.pipelinesDone == nil {
		c.pipelinesDone = map[string]bool{}
	}
	for n, cfg := range c.pipelines {
		if !cfg.informer.Informer().HasSynced() {
			if c.wait != n {
				c.wait = n
				logrus.Infof("Waiting on %s pipelines...", n)
			}
			return false // still syncing pipelines in at least one cluster
		} else if !c.pipelinesDone[n] {
			c.pipelinesDone[n] = true
			logrus.Infof("Synced %s pipelines", n)
		}
	}
	return true // Everyone is synced
}
func (c *controller) Run(threads int, stop <-chan struct{}) error {
	defer runtime.HandleCrash()
	defer c.workqueue.ShutDown()

	logrus.Info("Starting Pipeline controller")
	logrus.Info("Waiting for informer caches to sync")
	if ok := cache.WaitForCacheSync(stop, c.hasSynced); !ok {
		return fmt.Errorf("failed to wait for caches to sync")
	}

	logrus.Info("Starting workers")
	for i := 0; i < threads; i++ {
		go wait.Until(c.runWorker, time.Second, stop)
	}

	logrus.Info("Started workers")
	<-stop
	logrus.Info("Shutting down workers")
	return nil
}
func (c *controller) runWorker() {
	for {
		key, shutdown := c.workqueue.Get()
		if shutdown {
			return
		}
		func() {
			defer c.workqueue.Done(key)

			if err := reconcile(c, key.(string)); err != nil {
				runtime.HandleError(fmt.Errorf("failed to reconcile %s: %v", key, err))
				return // Do not forget so we retry later.
			}
			c.workqueue.Forget(key)
		}()
	}
}
func fromKey(key string) (string, string, string, error) {
	parts := strings.Split(key, "/")
	if len(parts) != 3 {
		return "", "", "", fmt.Errorf("bad key: %q", key)
	}
	return parts[0], parts[1], parts[2], nil
}
func (c *controller) enqueueKey(ctx string, obj interface{}) {
	switch o := obj.(type) {
	case *prowjobv1.ProwJob:
		ns := o.Spec.Namespace
		if ns == "" {
			ns = o.Namespace
		}
		c.workqueue.AddRateLimited(toKey(ctx, ns, o.Name))
	case *pipelinev1alpha1.PipelineRun:
		c.workqueue.AddRateLimited(toKey(ctx, o.Namespace, o.Name))
	default:
		logrus.Warnf("cannot enqueue unknown type %T: %v", o, obj)
		return
	}
}
func finalState(status prowjobv1.ProwJobState) bool {
	switch status {
	case "", prowjobv1.PendingState, prowjobv1.TriggeredState:
		return false
	}
	return true
}
func description(cond duckv1alpha1.Condition, fallback string) string {
	switch {
	case cond.Message != "":
		return cond.Message
	case cond.Reason != "":
		return cond.Reason
	}
	return fallback
}
func prowJobStatus(ps pipelinev1alpha1.PipelineRunStatus) (prowjobv1.ProwJobState, string) {
	started := ps.StartTime
	finished := ps.CompletionTime
	pcond := ps.GetCondition(duckv1alpha1.ConditionSucceeded)
	if pcond == nil {
		if !finished.IsZero() {
			return prowjobv1.ErrorState, descMissingCondition
		}
		return prowjobv1.TriggeredState, descScheduling
	}
	cond := *pcond
	switch {
	case cond.Status == untypedcorev1.ConditionTrue:
		return prowjobv1.SuccessState, description(cond, descSucceeded)
	case cond.Status == untypedcorev1.ConditionFalse:
		return prowjobv1.FailureState, description(cond, descFailed)
	case started.IsZero():
		return prowjobv1.TriggeredState, description(cond, descInitializing)
	case cond.Status == untypedcorev1.ConditionUnknown, finished.IsZero():
		return prowjobv1.PendingState, description(cond, descRunning)
	}

	logrus.Warnf("Unknown condition %#v", cond)
	return prowjobv1.ErrorState, description(cond, descUnknown) // shouldn't happen
}
func pipelineMeta(pj prowjobv1.ProwJob) metav1.ObjectMeta {
	labels, annotations := decorate.LabelsAndAnnotationsForJob(pj)
	return metav1.ObjectMeta{
		Annotations: annotations,
		Name:        pj.Name,
		Namespace:   pj.Spec.Namespace,
		Labels:      labels,
	}
}
func sourceURL(pj prowjobv1.ProwJob) string {
	if pj.Spec.Refs == nil {
		return ""
	}
	sourceURL := pj.Spec.Refs.CloneURI
	if sourceURL == "" {
		sourceURL = fmt.Sprintf("%s.git", pj.Spec.Refs.RepoLink)
	}
	return sourceURL
}
func makePipelineGitResource(pj prowjobv1.ProwJob) *pipelinev1alpha1.PipelineResource {
	var revision string
	if pj.Spec.Refs != nil {
		if len(pj.Spec.Refs.Pulls) > 0 {
			revision = pj.Spec.Refs.Pulls[0].SHA
		} else {
			revision = pj.Spec.Refs.BaseSHA
		}
	}
	pr := pipelinev1alpha1.PipelineResource{
		ObjectMeta: pipelineMeta(pj),
		Spec: pipelinev1alpha1.PipelineResourceSpec{
			Type: pipelinev1alpha1.PipelineResourceTypeGit,
			Params: []pipelinev1alpha1.Param{
				{
					Name:  "url",
					Value: sourceURL(pj),
				},
				{
					Name:  "revision",
					Value: revision,
				},
			},
		},
	}
	return &pr
}
func makePipelineRun(pj prowjobv1.ProwJob, pr *pipelinev1alpha1.PipelineResource) (*pipelinev1alpha1.PipelineRun, error) {
	if pj.Spec.PipelineRunSpec == nil {
		return nil, errors.New("no PipelineSpec defined")
	}
	p := pipelinev1alpha1.PipelineRun{
		ObjectMeta: pipelineMeta(pj),
		Spec:       *pj.Spec.PipelineRunSpec.DeepCopy(),
	}
	buildID := pj.Status.BuildID
	if buildID == "" {
		return nil, errors.New("empty BuildID in status")
	}
	p.Spec.Params = append(p.Spec.Params, pipelinev1alpha1.Param{
		Name:  "build_id",
		Value: buildID,
	})
	rb := pipelinev1alpha1.PipelineResourceBinding{
		Name: pr.Name,
		ResourceRef: pipelinev1alpha1.PipelineResourceRef{
			Name:       pr.Name,
			APIVersion: pr.APIVersion,
		},
	}
	p.Spec.Resources = append(p.Spec.Resources, rb)

	return &p, nil
}
func matchingConfigs(org, repo, branch, label string, allConfigs []plugins.RequireMatchingLabel) []plugins.RequireMatchingLabel {
	var filtered []plugins.RequireMatchingLabel
	for _, cfg := range allConfigs {
		// Check if the config applies to this issue type.
		if (branch == "" && !cfg.Issues) || (branch != "" && !cfg.PRs) {
			continue
		}
		// Check if the config applies to this 'org[/repo][/branch]'.
		if org != cfg.Org ||
			(cfg.Repo != "" && cfg.Repo != repo) ||
			(cfg.Branch != "" && branch != "" && cfg.Branch != branch) {
			continue
		}
		// If we are reacting to a label event, see if it is relevant.
		if label != "" && !cfg.Re.MatchString(label) {
			continue
		}
		filtered = append(filtered, cfg)
	}
	return filtered
}
func SuggestCodeChange(p lint.Problem) string {
	var suggestion = ""
	for regex, handler := range lintHandlersMap {
		matches := regex.FindStringSubmatch(p.Text)
		suggestion = handler(p, matches)
		if suggestion != "" && suggestion != p.LineText {
			return formatSuggestion(suggestion)
		}
	}
	return ""
}
func ServeExternalPluginHelp(mux *http.ServeMux, log *logrus.Entry, provider ExternalPluginHelpProvider) {
	mux.HandleFunc(
		"/help",
		func(w http.ResponseWriter, r *http.Request) {
			w.Header().Set("Cache-Control", "no-cache")

			serverError := func(action string, err error) {
				log.WithError(err).Errorf("Error %s.", action)
				msg := fmt.Sprintf("500 Internal server error %s: %v", action, err)
				http.Error(w, msg, http.StatusInternalServerError)
			}

			if r.Method != http.MethodPost {
				log.Errorf("Invalid request method: %v.", r.Method)
				http.Error(w, "405 Method not allowed", http.StatusMethodNotAllowed)
				return
			}
			b, err := ioutil.ReadAll(r.Body)
			if err != nil {
				serverError("reading request body", err)
				return
			}
			var enabledRepos []string
			if err := json.Unmarshal(b, &enabledRepos); err != nil {
				serverError("unmarshaling request body", err)
				return
			}
			if provider == nil {
				serverError("generating plugin help", errors.New("help provider is nil"))
				return
			}
			help, err := provider(enabledRepos)
			if err != nil {
				serverError("generating plugin help", err)
				return
			}
			b, err = json.Marshal(help)
			if err != nil {
				serverError("marshaling plugin help", err)
				return
			}

			fmt.Fprint(w, string(b))
		},
	)
}
func (p *protector) protect() {
	bp := p.cfg.BranchProtection

	// Scan the branch-protection configuration
	for orgName := range bp.Orgs {
		org := bp.GetOrg(orgName)
		if err := p.UpdateOrg(orgName, *org); err != nil {
			p.errors.add(fmt.Errorf("update %s: %v", orgName, err))
		}
	}

	// Do not automatically protect tested repositories
	if !bp.ProtectTested {
		return
	}

	// Some repos with presubmits might not be listed in the branch-protection
	for repo := range p.cfg.Presubmits {
		if p.completedRepos[repo] == true {
			continue
		}
		parts := strings.Split(repo, "/")
		if len(parts) != 2 { // TODO(fejta): use a strong type here instead
			p.errors.add(fmt.Errorf("bad presubmit repo: %s", repo))
			continue
		}
		orgName := parts[0]
		repoName := parts[1]
		repo := bp.GetOrg(orgName).GetRepo(repoName)
		if err := p.UpdateRepo(orgName, repoName, *repo); err != nil {
			p.errors.add(fmt.Errorf("update %s/%s: %v", orgName, repoName, err))
		}
	}
}
func (p *protector) UpdateOrg(orgName string, org config.Org) error {
	var repos []string
	if org.Protect != nil {
		// Strongly opinionated org, configure every repo in the org.
		rs, err := p.client.GetRepos(orgName, false)
		if err != nil {
			return fmt.Errorf("list repos: %v", err)
		}
		for _, r := range rs {
			if !r.Archived {
				repos = append(repos, r.Name)
			}
		}
	} else {
		// Unopinionated org, just set explicitly defined repos
		for r := range org.Repos {
			repos = append(repos, r)
		}
	}

	for _, repoName := range repos {
		repo := org.GetRepo(repoName)
		if err := p.UpdateRepo(orgName, repoName, *repo); err != nil {
			return fmt.Errorf("update %s: %v", repoName, err)
		}
	}
	return nil
}
func (p *protector) UpdateRepo(orgName string, repoName string, repo config.Repo) error {
	p.completedRepos[orgName+"/"+repoName] = true

	githubRepo, err := p.client.GetRepo(orgName, repoName)
	if err != nil {
		return fmt.Errorf("could not get repo to check for archival: %v", err)
	}
	if githubRepo.Archived {
		// nothing to do
		return nil
	}

	branches := map[string]github.Branch{}
	for _, onlyProtected := range []bool{false, true} { // put true second so b.Protected is set correctly
		bs, err := p.client.GetBranches(orgName, repoName, onlyProtected)
		if err != nil {
			return fmt.Errorf("list branches: %v", err)
		}
		for _, b := range bs {
			branches[b.Name] = b
		}
	}

	for bn, githubBranch := range branches {
		if branch, err := repo.GetBranch(bn); err != nil {
			return fmt.Errorf("get %s: %v", bn, err)
		} else if err = p.UpdateBranch(orgName, repoName, bn, *branch, githubBranch.Protected); err != nil {
			return fmt.Errorf("update %s from protected=%t: %v", bn, githubBranch.Protected, err)
		}
	}
	return nil
}
func (p *protector) UpdateBranch(orgName, repo string, branchName string, branch config.Branch, protected bool) error {
	bp, err := p.cfg.GetPolicy(orgName, repo, branchName, branch)
	if err != nil {
		return fmt.Errorf("get policy: %v", err)
	}
	if bp == nil || bp.Protect == nil {
		return nil
	}
	if !protected && !*bp.Protect {
		logrus.Infof("%s/%s=%s: already unprotected", orgName, repo, branchName)
		return nil
	}
	var req *github.BranchProtectionRequest
	if *bp.Protect {
		r := makeRequest(*bp)
		req = &r
	}
	p.updates <- requirements{
		Org:     orgName,
		Repo:    repo,
		Branch:  branchName,
		Request: req,
	}
	return nil
}
func (o *Options) LoadConfig(config string) error {
	return json.Unmarshal([]byte(config), o)
}
func (o *Options) Run() error {
	clusterConfig, err := loadClusterConfig()
	if err != nil {
		return fmt.Errorf("failed to load cluster config: %v", err)
	}

	client, err := kubernetes.NewForConfig(clusterConfig)
	if err != nil {
		return err
	}

	prowJobClient, err := kube.NewClientInCluster(o.ProwJobNamespace)
	if err != nil {
		return err
	}

	controller := artifact_uploader.NewController(client.CoreV1(), prowJobClient, o.Options)

	stop := make(chan struct{})
	defer close(stop)
	go controller.Run(o.NumWorkers, stop)

	// Wait forever
	select {}
}
func (a *Agent) Start(paths []string) error {
	secretsMap, err := LoadSecrets(paths)
	if err != nil {
		return err
	}

	a.secretsMap = secretsMap

	// Start one goroutine for each file to monitor and update the secret's values.
	for secretPath := range secretsMap {
		go a.reloadSecret(secretPath)
	}

	return nil
}
func (a *Agent) reloadSecret(secretPath string) {
	var lastModTime time.Time
	logger := logrus.NewEntry(logrus.StandardLogger())

	skips := 0
	for range time.Tick(1 * time.Second) {
		if skips < 600 {
			// Check if the file changed to see if it needs to be re-read.
			secretStat, err := os.Stat(secretPath)
			if err != nil {
				logger.WithField("secret-path", secretPath).
					WithError(err).Error("Error loading secret file.")
				continue
			}

			recentModTime := secretStat.ModTime()
			if !recentModTime.After(lastModTime) {
				skips++
				continue // file hasn't been modified
			}
			lastModTime = recentModTime
		}

		if secretValue, err := LoadSingleSecret(secretPath); err != nil {
			logger.WithField("secret-path: ", secretPath).
				WithError(err).Error("Error loading secret.")
		} else {
			a.setSecret(secretPath, secretValue)
			skips = 0
		}
	}
}
func (a *Agent) GetSecret(secretPath string) []byte {
	a.RLock()
	defer a.RUnlock()
	return a.secretsMap[secretPath]
}
func (a *Agent) setSecret(secretPath string, secretValue []byte) {
	a.Lock()
	defer a.Unlock()
	a.secretsMap[secretPath] = secretValue
}
func (a *Agent) GetTokenGenerator(secretPath string) func() []byte {
	return func() []byte {
		return a.GetSecret(secretPath)
	}
}
func New(maxRecordsPerKey int, opener io.Opener, path string) (*History, error) {
	hist := &History{
		logs:         map[string]*recordLog{},
		logSizeLimit: maxRecordsPerKey,
		opener:       opener,
		path:         path,
	}

	if path != "" {
		// Load existing history from GCS.
		var err error
		start := time.Now()
		hist.logs, err = readHistory(maxRecordsPerKey, hist.opener, hist.path)
		if err != nil {
			return nil, err
		}
		logrus.WithFields(logrus.Fields{
			"duration": time.Since(start).String(),
			"path":     hist.path,
		}).Debugf("Successfully read action history for %d pools.", len(hist.logs))
	}

	return hist, nil
}
func (h *History) Record(poolKey, action, baseSHA, err string, targets []prowapi.Pull) {
	t := now()
	sort.Sort(ByNum(targets))
	h.addRecord(
		poolKey,
		&Record{
			Time:    t,
			Action:  action,
			BaseSHA: baseSHA,
			Target:  targets,
			Err:     err,
		},
	)
}
func (h *History) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	b, err := json.Marshal(h.AllRecords())
	if err != nil {
		logrus.WithError(err).Error("Encoding JSON history.")
		b = []byte("{}")
	}
	if _, err = w.Write(b); err != nil {
		logrus.WithError(err).Error("Writing JSON history response.")
	}
}
func (h *History) Flush() {
	if h.path == "" {
		return
	}
	records := h.AllRecords()
	start := time.Now()
	err := writeHistory(h.opener, h.path, records)
	log := logrus.WithFields(logrus.Fields{
		"duration": time.Since(start).String(),
		"path":     h.path,
	})
	if err != nil {
		log.WithError(err).Error("Error flushing action history to GCS.")
	} else {
		log.Debugf("Successfully flushed action history for %d pools.", len(h.logs))
	}
}
func (h *History) AllRecords() map[string][]*Record {
	h.Lock()
	defer h.Unlock()

	res := make(map[string][]*Record, len(h.logs))
	for key, log := range h.logs {
		res[key] = log.toSlice()
	}
	return res
}
func MakeCommand() *cobra.Command {
	flags := &flags{}
	cmd := &cobra.Command{
		Use:   "download [bucket] [prowjob]",
		Short: "Finds and downloads the coverage profile file from the latest healthy build",
		Long: `Finds and downloads the coverage profile file from the latest healthy build 
stored in given gcs directory.`,
		Run: func(cmd *cobra.Command, args []string) {
			run(flags, cmd, args)
		},
	}
	cmd.Flags().StringVarP(&flags.outputFile, "output", "o", "-", "output file")
	cmd.Flags().StringVarP(&flags.artifactsDirName, "artifactsDir", "a", "artifacts", "artifact directory name in GCS")
	cmd.Flags().StringVarP(&flags.profileName, "profile", "p", "coverage-profile", "code coverage profile file name in GCS")
	return cmd
}
func (c *CommentCounterPlugin) CheckFlags() error {
	for _, pattern := range c.pattern {
		matcher, err := regexp.Compile(pattern)
		if err != nil {
			return err
		}
		c.matcher = append(c.matcher, matcher)
	}
	return nil
}
func (c *CommentCounterPlugin) ReceiveComment(comment sql.Comment) []Point {
	points := []Point{}
	for _, matcher := range c.matcher {
		if matcher.MatchString(comment.Body) {
			points = append(points, Point{
				Values: map[string]interface{}{
					"comment": 1,
				},
				Date: comment.CommentCreatedAt,
			})
		}
	}
	return points
}
func NewController(
	pjclientset clientset.Interface,
	queue workqueue.RateLimitingInterface,
	informer pjinformers.ProwJobInformer,
	reporter reportClient,
	numWorkers int,
	wg *sync.WaitGroup) *Controller {
	return &Controller{
		pjclientset: pjclientset,
		queue:       queue,
		informer:    informer,
		reporter:    reporter,
		numWorkers:  numWorkers,
		wg:          wg,
	}
}
func (c *Controller) Run(stopCh <-chan struct{}) {
	// handle a panic with logging and exiting
	defer utilruntime.HandleCrash()
	// ignore new items in the queue but when all goroutines
	// have completed existing items then shutdown
	defer c.queue.ShutDown()

	logrus.Info("Initiating controller")
	c.informer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			key, err := cache.MetaNamespaceKeyFunc(obj)
			logrus.WithField("prowjob", key).Infof("Add prowjob")
			if err != nil {
				logrus.WithError(err).Error("Cannot get key from object meta")
				return
			}
			c.queue.AddRateLimited(key)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			key, err := cache.MetaNamespaceKeyFunc(newObj)
			logrus.WithField("prowjob", key).Infof("Update prowjob")
			if err != nil {
				logrus.WithError(err).Error("Cannot get key from object meta")
				return
			}
			c.queue.AddRateLimited(key)
		},
	})

	// run the informer to start listing and watching resources
	go c.informer.Informer().Run(stopCh)

	// do the initial synchronization (one time) to populate resources
	if !cache.WaitForCacheSync(stopCh, c.HasSynced) {
		utilruntime.HandleError(fmt.Errorf("Error syncing cache"))
		return
	}
	logrus.Info("Controller.Run: cache sync complete")

	// run the runWorker method every second with a stop channel
	for i := 0; i < c.numWorkers; i++ {
		go wait.Until(c.runWorker, time.Second, stopCh)
	}

	logrus.Infof("Started %d workers", c.numWorkers)
	<-stopCh
	logrus.Info("Shutting down workers")
}
func (c *Controller) runWorker() {
	c.wg.Add(1)
	for c.processNextItem() {
	}
	c.wg.Done()
}
func New() (*LocalGit, *git.Client, error) {
	g, err := exec.LookPath("git")
	if err != nil {
		return nil, nil, err
	}
	t, err := ioutil.TempDir("", "localgit")
	if err != nil {
		return nil, nil, err
	}
	c, err := git.NewClient()
	if err != nil {
		os.RemoveAll(t)
		return nil, nil, err
	}

	getSecret := func() []byte {
		return []byte("")
	}

	c.SetCredentials("", getSecret)

	c.SetRemote(t)
	return &LocalGit{
		Dir: t,
		Git: g,
	}, c, nil
}
func (lg *LocalGit) MakeFakeRepo(org, repo string) error {
	rdir := filepath.Join(lg.Dir, org, repo)
	if err := os.MkdirAll(rdir, os.ModePerm); err != nil {
		return err
	}

	if err := runCmd(lg.Git, rdir, "init"); err != nil {
		return err
	}
	if err := runCmd(lg.Git, rdir, "config", "user.email", "test@test.test"); err != nil {
		return err
	}
	if err := runCmd(lg.Git, rdir, "config", "user.name", "test test"); err != nil {
		return err
	}
	if err := runCmd(lg.Git, rdir, "config", "commit.gpgsign", "false"); err != nil {
		return err
	}
	if err := lg.AddCommit(org, repo, map[string][]byte{"initial": {}}); err != nil {
		return err
	}

	return nil
}
func (lg *LocalGit) AddCommit(org, repo string, files map[string][]byte) error {
	rdir := filepath.Join(lg.Dir, org, repo)
	for f, b := range files {
		path := filepath.Join(rdir, f)
		if err := os.MkdirAll(filepath.Dir(path), os.ModePerm); err != nil {
			return err
		}
		if err := ioutil.WriteFile(path, b, os.ModePerm); err != nil {
			return err
		}
		if err := runCmd(lg.Git, rdir, "add", f); err != nil {
			return err
		}
	}
	return runCmd(lg.Git, rdir, "commit", "-m", "wow")
}
func (lg *LocalGit) CheckoutNewBranch(org, repo, branch string) error {
	rdir := filepath.Join(lg.Dir, org, repo)
	return runCmd(lg.Git, rdir, "checkout", "-b", branch)
}
func (lg *LocalGit) Checkout(org, repo, commitlike string) error {
	rdir := filepath.Join(lg.Dir, org, repo)
	return runCmd(lg.Git, rdir, "checkout", commitlike)
}
func (lg *LocalGit) RevParse(org, repo, commitlike string) (string, error) {
	rdir := filepath.Join(lg.Dir, org, repo)
	return runCmdOutput(lg.Git, rdir, "rev-parse", commitlike)
}
func CleanAll(sess *session.Session, region string) error {
	acct, err := account.GetAccount(sess, regions.Default)
	if err != nil {
		return errors.Wrap(err, "Failed to retrieve account")
	}
	klog.V(1).Infof("Account: %s", acct)

	var regionList []string
	if region == "" {
		regionList, err = regions.GetAll(sess)
		if err != nil {
			return errors.Wrap(err, "Couldn't retrieve list of regions")
		}
	} else {
		regionList = []string{region}
	}
	klog.Infof("Regions: %+v", regionList)

	for _, r := range regionList {
		for _, typ := range RegionalTypeList {
			set, err := typ.ListAll(sess, acct, r)
			if err != nil {
				return errors.Wrapf(err, "Failed to list resources of type %T", typ)
			}
			if err := typ.MarkAndSweep(sess, acct, r, set); err != nil {
				return errors.Wrapf(err, "Couldn't sweep resources of type %T", typ)
			}
		}
	}

	for _, typ := range GlobalTypeList {
		set, err := typ.ListAll(sess, acct, regions.Default)
		if err != nil {
			return errors.Wrapf(err, "Failed to list resources of type %T", typ)
		}
		if err := typ.MarkAndSweep(sess, acct, regions.Default, set); err != nil {
			return errors.Wrapf(err, "Couldn't sweep resources of type %T", typ)
		}
	}

	return nil
}
func optionsForRepo(config *plugins.Configuration, org, repo string) *plugins.Lgtm {
	fullName := fmt.Sprintf("%s/%s", org, repo)
	for i := range config.Lgtm {
		if !strInSlice(org, config.Lgtm[i].Repos) && !strInSlice(fullName, config.Lgtm[i].Repos) {
			continue
		}
		return &config.Lgtm[i]
	}
	return &plugins.Lgtm{}
}
func getChangedFiles(gc githubClient, org, repo string, number int) ([]string, error) {
	changes, err := gc.GetPullRequestChanges(org, repo, number)
	if err != nil {
		return nil, fmt.Errorf("cannot get PR changes for %s/%s#%d", org, repo, number)
	}
	var filenames []string
	for _, change := range changes {
		filenames = append(filenames, change.Filename)
	}
	return filenames, nil
}
func loadReviewers(ro repoowners.RepoOwner, filenames []string) sets.String {
	reviewers := sets.String{}
	for _, filename := range filenames {
		reviewers = reviewers.Union(ro.Approvers(filename)).Union(ro.Reviewers(filename))
	}
	return reviewers
}
func NewController(lastSyncFallback, cookiefilePath string, projects map[string][]string, kc *kube.Client, cfg config.Getter) (*Controller, error) {
	if lastSyncFallback == "" {
		return nil, errors.New("empty lastSyncFallback")
	}

	var lastUpdate time.Time
	if buf, err := ioutil.ReadFile(lastSyncFallback); err == nil {
		unix, err := strconv.ParseInt(string(buf), 10, 64)
		if err != nil {
			return nil, err
		}
		lastUpdate = time.Unix(unix, 0)
	} else if err != nil && !os.IsNotExist(err) {
		return nil, fmt.Errorf("failed to read lastSyncFallback: %v", err)
	} else {
		logrus.Warnf("lastSyncFallback not found: %s", lastSyncFallback)
		lastUpdate = time.Now()
	}

	c, err := client.NewClient(projects)
	if err != nil {
		return nil, err
	}
	c.Start(cookiefilePath)

	return &Controller{
		kc:               kc,
		config:           cfg,
		gc:               c,
		lastUpdate:       lastUpdate,
		lastSyncFallback: lastSyncFallback,
	}, nil
}
func (c *Controller) SaveLastSync(lastSync time.Time) error {
	if c.lastSyncFallback == "" {
		return nil
	}

	lastSyncUnix := strconv.FormatInt(lastSync.Unix(), 10)
	logrus.Infof("Writing last sync: %s", lastSyncUnix)

	tempFile, err := ioutil.TempFile(filepath.Dir(c.lastSyncFallback), "temp")
	if err != nil {
		return err
	}
	defer os.Remove(tempFile.Name())

	err = ioutil.WriteFile(tempFile.Name(), []byte(lastSyncUnix), 0644)
	if err != nil {
		return err
	}

	err = os.Rename(tempFile.Name(), c.lastSyncFallback)
	if err != nil {
		logrus.WithError(err).Info("Rename failed, fallback to copyfile")
		return copyFile(tempFile.Name(), c.lastSyncFallback)
	}
	return nil
}
func (c *Controller) Sync() error {
	syncTime := c.lastUpdate

	for instance, changes := range c.gc.QueryChanges(c.lastUpdate, c.config().Gerrit.RateLimit) {
		for _, change := range changes {
			if err := c.ProcessChange(instance, change); err != nil {
				logrus.WithError(err).Errorf("Failed process change %v", change.CurrentRevision)
			}
			if syncTime.Before(change.Updated.Time) {
				syncTime = change.Updated.Time
			}
		}

		logrus.Infof("Processed %d changes for instance %s", len(changes), instance)
	}

	c.lastUpdate = syncTime
	if err := c.SaveLastSync(syncTime); err != nil {
		logrus.WithError(err).Errorf("last sync %v, cannot save to path %v", syncTime, c.lastSyncFallback)
	}

	return nil
}
func (e *EventCounterPlugin) AddFlags(cmd *cobra.Command) {
	cmd.Flags().StringVar(&e.desc, "event", "", "Match event (eg: `opened`)")
}
func (e *EventCounterPlugin) CheckFlags() error {
	e.matcher = NewEventMatcher(e.desc)
	return nil
}
func (e *EventCounterPlugin) ReceiveIssueEvent(event sql.IssueEvent) []Point {
	var label string
	if event.Label != nil {
		label = *event.Label
	}

	if !e.matcher.Match(event.Event, label) {
		return nil
	}
	return []Point{
		{
			Values: map[string]interface{}{"event": 1},
			Date:   event.EventCreatedAt,
		},
	}
}
func Upload(bucket *storage.BucketHandle, uploadTargets map[string]UploadFunc) error {
	errCh := make(chan error, len(uploadTargets))
	group := &sync.WaitGroup{}
	group.Add(len(uploadTargets))
	for dest, upload := range uploadTargets {
		obj := bucket.Object(dest)
		logrus.WithField("dest", dest).Info("Queued for upload")
		go func(f UploadFunc, obj *storage.ObjectHandle, name string) {
			defer group.Done()
			if err := f(obj); err != nil {
				errCh <- err
			}
			logrus.WithField("dest", name).Info("Finished upload")
		}(upload, obj, dest)
	}
	group.Wait()
	close(errCh)
	if len(errCh) != 0 {
		var uploadErrors []error
		for err := range errCh {
			uploadErrors = append(uploadErrors, err)
		}
		return fmt.Errorf("encountered errors during upload: %v", uploadErrors)
	}

	return nil
}
func FileUploadWithMetadata(file string, metadata map[string]string) UploadFunc {
	return func(obj *storage.ObjectHandle) error {
		reader, err := os.Open(file)
		if err != nil {
			return err
		}

		uploadErr := DataUploadWithMetadata(reader, metadata)(obj)
		closeErr := reader.Close()

		return errorutil.NewAggregate(uploadErr, closeErr)
	}
}
func DataUploadWithMetadata(src io.Reader, metadata map[string]string) UploadFunc {
	return func(obj *storage.ObjectHandle) error {
		writer := obj.NewWriter(context.Background())
		writer.Metadata = metadata
		_, copyErr := io.Copy(writer, src)
		closeErr := writer.Close()

		return errorutil.NewAggregate(copyErr, closeErr)
	}
}
func HasLabel(label string, issueLabels []Label) bool {
	for _, l := range issueLabels {
		if strings.ToLower(l.Name) == strings.ToLower(label) {
			return true
		}
	}
	return false
}
func ImageTooBig(url string) (bool, error) {
	// limit is 10MB
	limit := 10000000
	// try to get the image size from Content-Length header
	resp, err := http.Head(url)
	if err != nil {
		return true, fmt.Errorf("HEAD error: %v", err)
	}
	if sc := resp.StatusCode; sc != http.StatusOK {
		return true, fmt.Errorf("failing %d response", sc)
	}
	size, _ := strconv.Atoi(resp.Header.Get("Content-Length"))
	if size > limit {
		return true, nil
	}
	return false, nil
}
func LevelFromPermissions(permissions RepoPermissions) RepoPermissionLevel {
	if permissions.Admin {
		return Admin
	} else if permissions.Push {
		return Write
	} else if permissions.Pull {
		return Read
	} else {
		return None
	}
}
func PermissionsFromLevel(permission RepoPermissionLevel) RepoPermissions {
	switch permission {
	case None:
		return RepoPermissions{}
	case Read:
		return RepoPermissions{Pull: true}
	case Write:
		return RepoPermissions{Pull: true, Push: true}
	case Admin:
		return RepoPermissions{Pull: true, Push: true, Admin: true}
	default:
		return RepoPermissions{}
	}
}
func newProwJobs(c *ProwV1Client, namespace string) *prowJobs {
	return &prowJobs{
		client: c.RESTClient(),
		ns:     namespace,
	}
}
func (b Blockers) GetApplicable(org, repo, branch string) []Blocker {
	var res []Blocker
	res = append(res, b.Repo[orgRepo{org: org, repo: repo}]...)
	res = append(res, b.Branch[orgRepoBranch{org: org, repo: repo, branch: branch}]...)

	sort.Slice(res, func(i, j int) bool {
		return res[i].Number < res[j].Number
	})
	return res
}
func serve(jc *jenkins.Client) {
	http.Handle("/", gziphandler.GzipHandler(handleLog(jc)))
	http.Handle("/metrics", promhttp.Handler())
	logrus.WithError(http.ListenAndServe(":8080", nil)).Fatal("ListenAndServe returned.")
}
func NewCountPlugin(runner func(Plugin) error) *cobra.Command {
	stateCounter := &StatePlugin{}
	eventCounter := &EventCounterPlugin{}
	commentsAsEvents := NewFakeCommentPluginWrapper(eventCounter)
	commentCounter := &CommentCounterPlugin{}
	authorLoggable := NewMultiplexerPluginWrapper(
		commentsAsEvents,
		commentCounter,
	)
	authorLogged := NewAuthorLoggerPluginWrapper(authorLoggable)
	fullMultiplex := NewMultiplexerPluginWrapper(authorLogged, stateCounter)

	fakeOpen := NewFakeOpenPluginWrapper(fullMultiplex)
	typeFilter := NewTypeFilterWrapperPlugin(fakeOpen)
	authorFilter := NewAuthorFilterPluginWrapper(typeFilter)

	cmd := &cobra.Command{
		Use:   "count",
		Short: "Count events and number of issues in given state, and for how long",
		RunE: func(cmd *cobra.Command, args []string) error {
			if err := eventCounter.CheckFlags(); err != nil {
				return err
			}
			if err := stateCounter.CheckFlags(); err != nil {
				return err
			}
			if err := typeFilter.CheckFlags(); err != nil {
				return err
			}
			if err := commentCounter.CheckFlags(); err != nil {
				return err
			}
			return runner(authorFilter)
		},
	}

	eventCounter.AddFlags(cmd)
	stateCounter.AddFlags(cmd)
	commentCounter.AddFlags(cmd)
	typeFilter.AddFlags(cmd)
	authorFilter.AddFlags(cmd)
	authorLogged.AddFlags(cmd)

	return cmd
}
func (o *FakeCommentPluginWrapper) ReceiveComment(comment sql.Comment) []Point {
	// Create a fake "commented" event for every comment we receive.
	fakeEvent := sql.IssueEvent{
		IssueID:        comment.IssueID,
		Event:          "commented",
		EventCreatedAt: comment.CommentCreatedAt,
		Actor:          &comment.User,
	}

	return append(
		o.plugin.ReceiveComment(comment),
		o.plugin.ReceiveIssueEvent(fakeEvent)...,
	)
}
func updateMetrics(interval time.Duration, diskRoot string) {
	logger := logrus.WithField("sync-loop", "updateMetrics")
	ticker := time.NewTicker(interval)
	for ; true; <-ticker.C {
		logger.Info("tick")
		_, bytesFree, bytesUsed, err := diskutil.GetDiskUsage(diskRoot)
		if err != nil {
			logger.WithError(err).Error("Failed to get disk metrics")
		} else {
			promMetrics.DiskFree.Set(float64(bytesFree) / 1e9)
			promMetrics.DiskUsed.Set(float64(bytesUsed) / 1e9)
			promMetrics.DiskTotal.Set(float64(bytesFree+bytesUsed) / 1e9)
		}
	}
}
func (r *Ranch) LogStatus() {
	resources, err := r.Storage.GetResources()

	if err != nil {
		return
	}

	resJSON, err := json.Marshal(resources)
	if err != nil {
		logrus.WithError(err).Errorf("Fail to marshal Resources. %v", resources)
	}
	logrus.Infof("Current Resources : %v", string(resJSON))
}
func (r *Ranch) SyncConfig(config string) error {
	resources, err := ParseConfig(config)
	if err != nil {
		return err
	}
	if err := r.Storage.SyncResources(resources); err != nil {
		return err
	}
	return nil
}
func (r *Ranch) Metric(rtype string) (common.Metric, error) {
	metric := common.Metric{
		Type:    rtype,
		Current: map[string]int{},
		Owners:  map[string]int{},
	}

	resources, err := r.Storage.GetResources()
	if err != nil {
		logrus.WithError(err).Error("cannot find resources")
		return metric, &ResourceNotFound{rtype}
	}

	for _, res := range resources {
		if res.Type != rtype {
			continue
		}

		if _, ok := metric.Current[res.State]; !ok {
			metric.Current[res.State] = 0
		}

		if _, ok := metric.Owners[res.Owner]; !ok {
			metric.Owners[res.Owner] = 0
		}

		metric.Current[res.State]++
		metric.Owners[res.Owner]++
	}

	if len(metric.Current) == 0 && len(metric.Owners) == 0 {
		return metric, &ResourceNotFound{rtype}
	}

	return metric, nil
}
func FormatURL(dogURL string) (string, error) {
	if dogURL == "" {
		return "", errors.New("empty url")
	}
	src, err := url.ParseRequestURI(dogURL)
	if err != nil {
		return "", fmt.Errorf("invalid url %s: %v", dogURL, err)
	}
	return fmt.Sprintf("[![dog image](%s)](%s)", src, src), nil
}
func runAndSkipJobs(c Client, pr *github.PullRequest, requestedJobs []config.Presubmit, skippedJobs []config.Presubmit, eventGUID string, elideSkippedContexts bool) error {
	if err := validateContextOverlap(requestedJobs, skippedJobs); err != nil {
		c.Logger.WithError(err).Warn("Could not run or skip requested jobs, overlapping contexts.")
		return err
	}
	runErr := RunRequested(c, pr, requestedJobs, eventGUID)
	var skipErr error
	if !elideSkippedContexts {
		skipErr = skipRequested(c, pr, skippedJobs)
	}

	return errorutil.NewAggregate(runErr, skipErr)
}
func validateContextOverlap(toRun, toSkip []config.Presubmit) error {
	requestedContexts := sets.NewString()
	for _, job := range toRun {
		requestedContexts.Insert(job.Context)
	}
	skippedContexts := sets.NewString()
	for _, job := range toSkip {
		skippedContexts.Insert(job.Context)
	}
	if overlap := requestedContexts.Intersection(skippedContexts).List(); len(overlap) > 0 {
		return fmt.Errorf("the following contexts are both triggered and skipped: %s", strings.Join(overlap, ", "))
	}

	return nil
}
func RunRequested(c Client, pr *github.PullRequest, requestedJobs []config.Presubmit, eventGUID string) error {
	baseSHA, err := c.GitHubClient.GetRef(pr.Base.Repo.Owner.Login, pr.Base.Repo.Name, "heads/"+pr.Base.Ref)
	if err != nil {
		return err
	}

	var errors []error
	for _, job := range requestedJobs {
		c.Logger.Infof("Starting %s build.", job.Name)
		pj := pjutil.NewPresubmit(*pr, baseSHA, job, eventGUID)
		c.Logger.WithFields(pjutil.ProwJobFields(&pj)).Info("Creating a new prowjob.")
		if _, err := c.ProwJobClient.Create(&pj); err != nil {
			c.Logger.WithError(err).Error("Failed to create prowjob.")
			errors = append(errors, err)
		}
	}
	return errorutil.NewAggregate(errors...)
}
func skipRequested(c Client, pr *github.PullRequest, skippedJobs []config.Presubmit) error {
	var errors []error
	for _, job := range skippedJobs {
		if job.SkipReport {
			continue
		}
		c.Logger.Infof("Skipping %s build.", job.Name)
		if err := c.GitHubClient.CreateStatus(pr.Base.Repo.Owner.Login, pr.Base.Repo.Name, pr.Head.SHA, skippedStatusFor(job.Context)); err != nil {
			errors = append(errors, err)
		}
	}
	return errorutil.NewAggregate(errors...)
}
func (l LabelEvent) Match(eventName, label string) bool {
	return eventName == "labeled" && label == l.Label
}
func (u UnlabelEvent) Match(eventName, label string) bool {
	return eventName == "unlabeled" && label == u.Label
}
func (o *GitHubOptions) AddFlags(fs *flag.FlagSet) {
	o.addFlags(true, fs)
}
func (o *GitHubOptions) AddFlagsWithoutDefaultGitHubTokenPath(fs *flag.FlagSet) {
	o.addFlags(false, fs)
}
func (o *GitHubOptions) Validate(dryRun bool) error {
	for _, uri := range o.endpoint.Strings() {
		if uri == "" {
			uri = github.DefaultAPIEndpoint
		} else if _, err := url.ParseRequestURI(uri); err != nil {
			return fmt.Errorf("invalid -github-endpoint URI: %q", uri)
		}
	}

	if o.graphqlEndpoint == "" {
		o.graphqlEndpoint = github.DefaultGraphQLEndpoint
	} else if _, err := url.Parse(o.graphqlEndpoint); err != nil {
		return fmt.Errorf("invalid -github-graphql-endpoint URI: %q", o.graphqlEndpoint)
	}

	if o.deprecatedTokenFile != "" {
		o.TokenPath = o.deprecatedTokenFile
		logrus.Error("-github-token-file is deprecated and may be removed anytime after 2019-01-01.  Use -github-token-path instead.")
	}

	if o.TokenPath == "" {
		logrus.Warn("empty -github-token-path, will use anonymous github client")
	}

	return nil
}
func (o *GitHubOptions) GitHubClientWithLogFields(secretAgent *secret.Agent, dryRun bool, fields logrus.Fields) (client *github.Client, err error) {
	var generator *func() []byte
	if o.TokenPath == "" {
		generatorFunc := func() []byte {
			return []byte{}
		}
		generator = &generatorFunc
	} else {
		if secretAgent == nil {
			return nil, fmt.Errorf("cannot store token from %q without a secret agent", o.TokenPath)
		}
		generatorFunc := secretAgent.GetTokenGenerator(o.TokenPath)
		generator = &generatorFunc
	}

	if dryRun {
		return github.NewDryRunClientWithFields(fields, *generator, o.graphqlEndpoint, o.endpoint.Strings()...), nil
	}
	return github.NewClientWithFields(fields, *generator, o.graphqlEndpoint, o.endpoint.Strings()...), nil
}
func (o *GitHubOptions) GitHubClient(secretAgent *secret.Agent, dryRun bool) (client *github.Client, err error) {
	return o.GitHubClientWithLogFields(secretAgent, dryRun, logrus.Fields{})
}
func (o *GitHubOptions) GitClient(secretAgent *secret.Agent, dryRun bool) (client *git.Client, err error) {
	client, err = git.NewClient()
	if err != nil {
		return nil, err
	}

	// We must capture the value of client here to prevent issues related
	// to the use of named return values when an error is encountered.
	// Without this, we risk a nil pointer dereference.
	defer func(client *git.Client) {
		if err != nil {
			client.Clean()
		}
	}(client)

	// Get the bot's name in order to set credentials for the Git client.
	githubClient, err := o.GitHubClient(secretAgent, dryRun)
	if err != nil {
		return nil, fmt.Errorf("error getting GitHub client: %v", err)
	}
	botName, err := githubClient.BotName()
	if err != nil {
		return nil, fmt.Errorf("error getting bot name: %v", err)
	}
	client.SetCredentials(botName, secretAgent.GetTokenGenerator(o.TokenPath))

	return client, nil
}
func toMap(g *calculation.CoverageList) map[string]calculation.Coverage {
	m := make(map[string]calculation.Coverage)
	for _, cov := range g.Group {
		m[cov.Name] = cov
	}
	return m
}
func findChanges(baseList *calculation.CoverageList, newList *calculation.CoverageList) []*coverageChange {
	var changes []*coverageChange
	baseFilesMap := toMap(baseList)
	for _, newCov := range newList.Group {
		baseCov, ok := baseFilesMap[newCov.Name]
		var baseRatio float32
		if !ok {
			baseRatio = -1
		} else {
			baseRatio = baseCov.Ratio()
		}
		newRatio := newCov.Ratio()
		if isChangeSignificant(baseRatio, newRatio) {
			changes = append(changes, &coverageChange{
				name:      newCov.Name,
				baseRatio: baseRatio,
				newRatio:  newRatio,
			})
		}
	}
	return changes
}
func (config *MySQLConfig) CreateDatabase() (*gorm.DB, error) {
	db, err := gorm.Open("mysql", config.getDSN(""))
	if err != nil {
		return nil, err
	}

	db.Exec(fmt.Sprintf("CREATE DATABASE IF NOT EXISTS %v;", config.Db))
	db.Close()

	db, err = gorm.Open("mysql", config.getDSN(config.Db))
	err = db.AutoMigrate(&Assignee{}, &Issue{}, &IssueEvent{}, &Label{}, &Comment{}).Error
	if err != nil {
		return nil, err
	}

	return db, nil
}
func (c *Client) ShouldReport(pj *v1.ProwJob) bool {

	if !pj.Spec.Report {
		// Respect report field
		return false
	}

	if pj.Spec.Type != v1.PresubmitJob && pj.Spec.Type != v1.PostsubmitJob {
		// Report presubmit and postsubmit github jobs for github reporter
		return false
	}

	if c.reportAgent != "" && pj.Spec.Agent != c.reportAgent {
		// Only report for specified agent
		return false
	}

	return true
}
func (c *Client) Report(pj *v1.ProwJob) ([]*v1.ProwJob, error) {
	// TODO(krzyzacy): ditch ReportTemplate, and we can drop reference to config.Getter
	return []*v1.ProwJob{pj}, report.Report(c.gc, c.config().Plank.ReportTemplate, *pj, c.config().GitHubReporter.JobTypesToReport)
}
func (s *Set) MarkComplete() int {
	var gone []string
	for key := range s.firstSeen {
		if !s.marked[key] {
			gone = append(gone, key)
		}
	}

	for _, key := range gone {
		klog.V(1).Infof("%s: deleted since last run", key)
		delete(s.firstSeen, key)
	}

	if len(s.swept) > 0 {
		klog.Errorf("%d resources swept: %v", len(s.swept), s.swept)
	}

	return len(s.swept)
}
func NewJobAgent(kc serviceClusterClient, plClients map[string]PodLogClient, cfg config.Getter) *JobAgent {
	return &JobAgent{
		kc:     kc,
		pkcs:   plClients,
		config: cfg,
	}
}
func (ja *JobAgent) Start() {
	ja.tryUpdate()
	go func() {
		t := time.Tick(period)
		for range t {
			ja.tryUpdate()
		}
	}()
}
func (ja *JobAgent) Jobs() []Job {
	ja.mut.Lock()
	defer ja.mut.Unlock()
	res := make([]Job, len(ja.jobs))
	copy(res, ja.jobs)
	return res
}
func (ja *JobAgent) ProwJobs() []prowapi.ProwJob {
	ja.mut.Lock()
	defer ja.mut.Unlock()
	res := make([]prowapi.ProwJob, len(ja.prowJobs))
	copy(res, ja.prowJobs)
	return res
}
func (ja *JobAgent) GetProwJob(job, id string) (prowapi.ProwJob, error) {
	if ja == nil {
		return prowapi.ProwJob{}, fmt.Errorf("Prow job agent doesn't exist (are you running locally?)")
	}
	var j prowapi.ProwJob
	ja.mut.Lock()
	idMap, ok := ja.jobsIDMap[job]
	if ok {
		j, ok = idMap[id]
	}
	ja.mut.Unlock()
	if !ok {
		return prowapi.ProwJob{}, errProwjobNotFound
	}
	return j, nil
}
func (ja *JobAgent) GetJobLog(job, id string) ([]byte, error) {
	j, err := ja.GetProwJob(job, id)
	if err != nil {
		return nil, fmt.Errorf("error getting prowjob: %v", err)
	}
	if j.Spec.Agent == prowapi.KubernetesAgent {
		client, ok := ja.pkcs[j.ClusterAlias()]
		if !ok {
			return nil, fmt.Errorf("cannot get logs for prowjob %q with agent %q: unknown cluster alias %q", j.ObjectMeta.Name, j.Spec.Agent, j.ClusterAlias())
		}
		return client.GetLogs(j.Status.PodName, &coreapi.PodLogOptions{Container: kube.TestContainerName})
	}
	for _, agentToTmpl := range ja.config().Deck.ExternalAgentLogs {
		if agentToTmpl.Agent != string(j.Spec.Agent) {
			continue
		}
		if !agentToTmpl.Selector.Matches(labels.Set(j.ObjectMeta.Labels)) {
			continue
		}
		var b bytes.Buffer
		if err := agentToTmpl.URLTemplate.Execute(&b, &j); err != nil {
			return nil, fmt.Errorf("cannot execute URL template for prowjob %q with agent %q: %v", j.ObjectMeta.Name, j.Spec.Agent, err)
		}
		resp, err := http.Get(b.String())
		if err != nil {
			return nil, err
		}
		defer resp.Body.Close()
		return ioutil.ReadAll(resp.Body)

	}
	return nil, fmt.Errorf("cannot get logs for prowjob %q with agent %q: the agent is missing from the prow config file", j.ObjectMeta.Name, j.Spec.Agent)
}
func unionStrings(parent, child []string) []string {
	if child == nil {
		return parent
	}
	if parent == nil {
		return child
	}
	s := sets.NewString(parent...)
	s.Insert(child...)
	return s.List()
}
func (p Policy) Apply(child Policy) Policy {
	return Policy{
		Protect:                    selectBool(p.Protect, child.Protect),
		RequiredStatusChecks:       mergeContextPolicy(p.RequiredStatusChecks, child.RequiredStatusChecks),
		Admins:                     selectBool(p.Admins, child.Admins),
		Restrictions:               mergeRestrictions(p.Restrictions, child.Restrictions),
		RequiredPullRequestReviews: mergeReviewPolicy(p.RequiredPullRequestReviews, child.RequiredPullRequestReviews),
	}
}
func (bp BranchProtection) GetOrg(name string) *Org {
	o, ok := bp.Orgs[name]
	if ok {
		o.Policy = bp.Apply(o.Policy)
	} else {
		o.Policy = bp.Policy
	}
	return &o
}
func (o Org) GetRepo(name string) *Repo {
	r, ok := o.Repos[name]
	if ok {
		r.Policy = o.Apply(r.Policy)
	} else {
		r.Policy = o.Policy
	}
	return &r
}
func (r Repo) GetBranch(name string) (*Branch, error) {
	b, ok := r.Branches[name]
	if ok {
		b.Policy = r.Apply(b.Policy)
		if b.Protect == nil {
			return nil, errors.New("defined branch policies must set protect")
		}
	} else {
		b.Policy = r.Policy
	}
	return &b, nil
}
func (c *Config) GetPolicy(org, repo, branch string, b Branch) (*Policy, error) {
	policy := b.Policy

	// Automatically require contexts from prow which must always be present
	if prowContexts, _, _ := BranchRequirements(org, repo, branch, c.Presubmits); len(prowContexts) > 0 {
		// Error if protection is disabled
		if policy.Protect != nil && !*policy.Protect {
			return nil, fmt.Errorf("required prow jobs require branch protection")
		}
		ps := Policy{
			RequiredStatusChecks: &ContextPolicy{
				Contexts: prowContexts,
			},
		}
		// Require protection by default if ProtectTested is true
		if c.BranchProtection.ProtectTested {
			yes := true
			ps.Protect = &yes
		}
		policy = policy.Apply(ps)
	}

	if policy.Protect != nil && !*policy.Protect {
		// Ensure that protection is false => no protection settings
		var old *bool
		old, policy.Protect = policy.Protect, old
		switch {
		case policy.defined() && c.BranchProtection.AllowDisabledPolicies:
			logrus.Warnf("%s/%s=%s defines a policy but has protect: false", org, repo, branch)
			policy = Policy{
				Protect: policy.Protect,
			}
		case policy.defined():
			return nil, fmt.Errorf("%s/%s=%s defines a policy, which requires protect: true", org, repo, branch)
		}
		policy.Protect = old
	}

	if !policy.defined() {
		return nil, nil
	}
	return &policy, nil
}
func UpdateIssueEvents(issueID int, db *gorm.DB, client ClientInterface) {
	latest, err := findLatestEvent(issueID, db, client.RepositoryName())
	if err != nil {
		glog.Error("Failed to find last event: ", err)
		return
	}
	c := make(chan *github.IssueEvent, 500)

	go client.FetchIssueEvents(issueID, latest, c)
	for event := range c {
		eventOrm, err := NewIssueEvent(event, issueID, client.RepositoryName())
		if err != nil {
			glog.Error("Failed to create issue-event", err)
		}
		db.Create(eventOrm)
	}
}
func (c *controller) enqueueKey(ctx string, obj interface{}) {
	switch o := obj.(type) {
	case *prowjobv1.ProwJob:
		c.workqueue.AddRateLimited(toKey(ctx, o.Spec.Namespace, o.Name))
	case *buildv1alpha1.Build:
		c.workqueue.AddRateLimited(toKey(ctx, o.Namespace, o.Name))
	default:
		logrus.Warnf("cannot enqueue unknown type %T: %v", o, obj)
		return
	}
}
func prowJobStatus(bs buildv1alpha1.BuildStatus) (prowjobv1.ProwJobState, string) {
	started := bs.StartTime
	finished := bs.CompletionTime
	pcond := bs.GetCondition(buildv1alpha1.BuildSucceeded)
	if pcond == nil {
		if !finished.IsZero() {
			return prowjobv1.ErrorState, descMissingCondition
		}
		return prowjobv1.TriggeredState, descScheduling
	}
	cond := *pcond
	switch {
	case cond.Status == coreapi.ConditionTrue:
		return prowjobv1.SuccessState, description(cond, descSucceeded)
	case cond.Status == coreapi.ConditionFalse:
		return prowjobv1.FailureState, description(cond, descFailed)
	case started.IsZero():
		return prowjobv1.TriggeredState, description(cond, descInitializing)
	case cond.Status == coreapi.ConditionUnknown, finished.IsZero():
		return prowjobv1.PendingState, description(cond, descRunning)
	}
	logrus.Warnf("Unknown condition %#v", cond)
	return prowjobv1.ErrorState, description(cond, descUnknown) // shouldn't happen
}
func buildEnv(pj prowjobv1.ProwJob, buildID string) (map[string]string, error) {
	return downwardapi.EnvForSpec(downwardapi.NewJobSpec(pj.Spec, buildID, pj.Name))
}
func defaultArguments(t *buildv1alpha1.TemplateInstantiationSpec, rawEnv map[string]string) {
	keys := sets.String{}
	for _, arg := range t.Arguments {
		keys.Insert(arg.Name)
	}
	for _, k := range sets.StringKeySet(rawEnv).List() { // deterministic ordering
		if keys.Has(k) {
			continue
		}
		t.Arguments = append(t.Arguments, buildv1alpha1.ArgumentSpec{Name: k, Value: rawEnv[k]})
	}
}
func defaultEnv(c *coreapi.Container, rawEnv map[string]string) {
	keys := sets.String{}
	for _, arg := range c.Env {
		keys.Insert(arg.Name)
	}
	for _, k := range sets.StringKeySet(rawEnv).List() { // deterministic ordering
		if keys.Has(k) {
			continue
		}
		c.Env = append(c.Env, coreapi.EnvVar{Name: k, Value: rawEnv[k]})
	}
}
func injectSource(b *buildv1alpha1.Build, pj prowjobv1.ProwJob) (bool, error) {
	if b.Spec.Source != nil {
		return false, nil
	}
	srcContainer, refs, cloneVolumes, err := decorate.CloneRefs(pj, codeMount, logMount)
	if err != nil {
		return false, fmt.Errorf("clone source error: %v", err)
	}
	if srcContainer == nil {
		return false, nil
	} else {
		srcContainer.Name = "" // knative-build requirement
	}

	b.Spec.Source = &buildv1alpha1.SourceSpec{
		Custom: srcContainer,
	}
	b.Spec.Volumes = append(b.Spec.Volumes, cloneVolumes...)

	wd := workDir(refs[0])
	// Inject correct working directory
	for i := range b.Spec.Steps {
		if b.Spec.Steps[i].WorkingDir != "" {
			continue
		}
		b.Spec.Steps[i].WorkingDir = wd.Value
	}
	if b.Spec.Template != nil {
		// Best we can do for a template is to set WORKDIR
		b.Spec.Template.Arguments = append(b.Spec.Template.Arguments, wd)
	}

	return true, nil
}
func injectedSteps(encodedJobSpec string, dc prowjobv1.DecorationConfig, injectedSource bool, toolsMount coreapi.VolumeMount, entries []wrapper.Options) ([]coreapi.Container, *coreapi.Container, *coreapi.Volume, error) {
	gcsVol, gcsMount, gcsOptions := decorate.GCSOptions(dc)

	sidecar, err := decorate.Sidecar(dc.UtilityImages.Sidecar, gcsOptions, gcsMount, logMount, encodedJobSpec, decorate.RequirePassingEntries, entries...)
	if err != nil {
		return nil, nil, nil, fmt.Errorf("inject sidecar: %v", err)
	}

	var cloneLogMount *coreapi.VolumeMount
	if injectedSource {
		cloneLogMount = &logMount
	}
	initUpload, err := decorate.InitUpload(dc.UtilityImages.InitUpload, gcsOptions, gcsMount, cloneLogMount, encodedJobSpec)
	if err != nil {
		return nil, nil, nil, fmt.Errorf("inject initupload: %v", err)
	}

	placer := decorate.PlaceEntrypoint(dc.UtilityImages.Entrypoint, toolsMount)

	return []coreapi.Container{placer, *initUpload}, sidecar, &gcsVol, nil
}
func determineTimeout(spec *buildv1alpha1.BuildSpec, dc *prowjobv1.DecorationConfig, defaultTimeout time.Duration) time.Duration {
	switch {
	case spec.Timeout != nil:
		return spec.Timeout.Duration
	case dc != nil && dc.Timeout.Duration > 0:
		return dc.Timeout.Duration
	default:
		return defaultTimeout
	}
}
func makeBuild(pj prowjobv1.ProwJob, defaultTimeout time.Duration) (*buildv1alpha1.Build, error) {
	if pj.Spec.BuildSpec == nil {
		return nil, errors.New("nil BuildSpec in spec")
	}
	buildID := pj.Status.BuildID
	if buildID == "" {
		return nil, errors.New("empty BuildID in status")
	}
	b := buildv1alpha1.Build{
		ObjectMeta: buildMeta(pj),
		Spec:       *pj.Spec.BuildSpec.DeepCopy(),
	}
	rawEnv, err := buildEnv(pj, buildID)
	if err != nil {
		return nil, fmt.Errorf("environment error: %v", err)
	}
	injectEnvironment(&b, rawEnv)
	injectedSource, err := injectSource(&b, pj)
	if err != nil {
		return nil, fmt.Errorf("inject source: %v", err)
	}
	injectTimeout(&b.Spec, pj.Spec.DecorationConfig, defaultTimeout)
	if pj.Spec.DecorationConfig != nil {
		encodedJobSpec := rawEnv[downwardapi.JobSpecEnv]
		err = decorateBuild(&b.Spec, encodedJobSpec, *pj.Spec.DecorationConfig, injectedSource)
		if err != nil {
			return nil, fmt.Errorf("decorate build: %v", err)
		}
	}

	return &b, nil
}
func newLabels(issueID int, gLabels []github.Label, repository string) ([]sql.Label, error) {
	labels := []sql.Label{}
	repository = strings.ToLower(repository)

	for _, label := range gLabels {
		if label.Name == nil {
			return nil, fmt.Errorf("Label is missing name field")
		}
		labels = append(labels, sql.Label{
			IssueID:    strconv.Itoa(issueID),
			Name:       *label.Name,
			Repository: repository,
		})
	}

	return labels, nil
}
func newAssignees(issueID int, gAssignees []*github.User, repository string) ([]sql.Assignee, error) {
	assignees := []sql.Assignee{}
	repository = strings.ToLower(repository)

	for _, assignee := range gAssignees {
		if assignee != nil && assignee.Login == nil {
			return nil, fmt.Errorf("Assignee is missing Login field")
		}
		assignees = append(assignees, sql.Assignee{
			IssueID:    strconv.Itoa(issueID),
			Name:       *assignee.Login,
			Repository: repository,
		})
	}

	return assignees, nil
}
func NewIssueComment(issueID int, gComment *github.IssueComment, repository string) (*sql.Comment, error) {
	if gComment.ID == nil ||
		gComment.Body == nil ||
		gComment.CreatedAt == nil ||
		gComment.UpdatedAt == nil {
		return nil, fmt.Errorf("IssueComment is missing mandatory field: %s", gComment)
	}

	var login string
	if gComment.User != nil && gComment.User.Login != nil {
		login = *gComment.User.Login
	}

	return &sql.Comment{
		ID:               itoa(*gComment.ID),
		IssueID:          strconv.Itoa(issueID),
		Body:             *gComment.Body,
		User:             login,
		CommentCreatedAt: *gComment.CreatedAt,
		CommentUpdatedAt: *gComment.UpdatedAt,
		PullRequest:      false,
		Repository:       strings.ToLower(repository),
	}, nil
}
func messageFilter(lastUpdate time.Time, change client.ChangeInfo, presubmits []config.Presubmit) (pjutil.Filter, error) {
	var filters []pjutil.Filter
	currentRevision := change.Revisions[change.CurrentRevision].Number
	for _, message := range change.Messages {
		messageTime := message.Date.Time
		if message.RevisionNumber != currentRevision || !messageTime.After(lastUpdate) {
			continue
		}

		if !pjutil.TestAllRe.MatchString(message.Message) {
			for _, presubmit := range presubmits {
				if presubmit.TriggerMatches(message.Message) {
					logrus.Infof("Change %d: Comment %s matches triggering regex, for %s.", change.Number, message.Message, presubmit.Name)
					filters = append(filters, pjutil.CommandFilter(message.Message))
				}
			}
		} else {
			filters = append(filters, pjutil.TestAllFilter())
		}

	}
	return pjutil.AggregateFilter(filters), nil
}
func (jb *Build) IsSuccess() bool {
	return jb.Result != nil && *jb.Result == success
}
func (jb *Build) IsFailure() bool {
	return jb.Result != nil && (*jb.Result == failure || *jb.Result == unstable)
}
func (jb *Build) IsAborted() bool {
	return jb.Result != nil && *jb.Result == aborted
}
func (jb *Build) ProwJobID() string {
	for _, action := range jb.Actions {
		for _, p := range action.Parameters {
			if p.Name == prowJobID {
				value, ok := p.Value.(string)
				if !ok {
					logrus.Errorf("Cannot determine %s value for %#v", p.Name, jb)
					continue
				}
				return value
			}
		}
	}
	return ""
}
func (jb *Build) BuildID() string {
	var buildID string
	hasProwJobID := false
	for _, action := range jb.Actions {
		for _, p := range action.Parameters {
			hasProwJobID = hasProwJobID || p.Name == prowJobID
			if p.Name == statusBuildID {
				value, ok := p.Value.(string)
				if !ok {
					logrus.Errorf("Cannot determine %s value for %#v", p.Name, jb)
					continue
				}
				buildID = value
			}
		}
	}

	if !hasProwJobID {
		return ""
	}
	return buildID
}
func (c *Client) CrumbRequest() error {
	if c.authConfig.csrfToken != "" && c.authConfig.csrfRequestField != "" {
		return nil
	}
	c.logger.Debug("CrumbRequest")
	data, err := c.GetSkipMetrics("/crumbIssuer/api/json")
	if err != nil {
		return err
	}
	crumbResp := struct {
		Crumb             string `json:"crumb"`
		CrumbRequestField string `json:"crumbRequestField"`
	}{}
	if err := json.Unmarshal(data, &crumbResp); err != nil {
		return fmt.Errorf("cannot unmarshal crumb response: %v", err)
	}
	c.authConfig.csrfToken = crumbResp.Crumb
	c.authConfig.csrfRequestField = crumbResp.CrumbRequestField
	return nil
}
func (c *Client) measure(method, path string, code int, start time.Time) {
	if c.metrics == nil {
		return
	}
	c.metrics.RequestLatency.WithLabelValues(method, path).Observe(time.Since(start).Seconds())
	c.metrics.Requests.WithLabelValues(method, path, fmt.Sprintf("%d", code)).Inc()
}
func (c *Client) GetSkipMetrics(path string) ([]byte, error) {
	resp, err := c.request(http.MethodGet, path, nil, false)
	if err != nil {
		return nil, err
	}
	return readResp(resp)
}
func (c *Client) Get(path string) ([]byte, error) {
	resp, err := c.request(http.MethodGet, path, nil, true)
	if err != nil {
		return nil, err
	}
	return readResp(resp)
}
func (c *Client) request(method, path string, params url.Values, measure bool) (*http.Response, error) {
	var resp *http.Response
	var err error
	backoff := retryDelay

	urlPath := fmt.Sprintf("%s%s", c.baseURL, path)
	if params != nil {
		urlPath = fmt.Sprintf("%s?%s", urlPath, params.Encode())
	}

	start := time.Now()
	for retries := 0; retries < maxRetries; retries++ {
		resp, err = c.doRequest(method, urlPath)
		if err == nil && resp.StatusCode < 500 {
			break
		} else if err == nil && retries+1 < maxRetries {
			resp.Body.Close()
		}
		// Capture the retry in a metric.
		if measure && c.metrics != nil {
			c.metrics.RequestRetries.Inc()
		}
		time.Sleep(backoff)
		backoff *= 2
	}
	if measure && resp != nil {
		c.measure(method, path, resp.StatusCode, start)
	}
	return resp, err
}
func (c *Client) doRequest(method, path string) (*http.Response, error) {
	req, err := http.NewRequest(method, path, nil)
	if err != nil {
		return nil, err
	}
	if c.authConfig != nil {
		if c.authConfig.Basic != nil {
			req.SetBasicAuth(c.authConfig.Basic.User, string(c.authConfig.Basic.GetToken()))
		}
		if c.authConfig.BearerToken != nil {
			req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.authConfig.BearerToken.GetToken()))
		}
		if c.authConfig.CSRFProtect && c.authConfig.csrfRequestField != "" && c.authConfig.csrfToken != "" {
			req.Header.Set(c.authConfig.csrfRequestField, c.authConfig.csrfToken)
		}
	}
	return c.client.Do(req)
}
func getJobName(spec *prowapi.ProwJobSpec) string {
	if spec.JenkinsSpec != nil && spec.JenkinsSpec.GitHubBranchSourceJob && spec.Refs != nil {
		if len(spec.Refs.Pulls) > 0 {
			return fmt.Sprintf("%s/view/change-requests/job/PR-%d", spec.Job, spec.Refs.Pulls[0].Number)
		}

		return fmt.Sprintf("%s/job/%s", spec.Job, spec.Refs.BaseRef)
	}

	return spec.Job
}
func getBuildPath(spec *prowapi.ProwJobSpec) string {
	jenkinsJobName := getJobName(spec)
	jenkinsPath := fmt.Sprintf("/job/%s/build", jenkinsJobName)

	return jenkinsPath
}
func (c *Client) GetJobInfo(spec *prowapi.ProwJobSpec) (*JobInfo, error) {
	path := getJobInfoPath(spec)
	c.logger.Debugf("getJobInfoPath: %s", path)

	data, err := c.Get(path)

	if err != nil {
		c.logger.Errorf("Failed to get job info: %v", err)
		return nil, err
	}

	var jobInfo JobInfo

	if err := json.Unmarshal(data, &jobInfo); err != nil {
		return nil, fmt.Errorf("Cannot unmarshal job info from API: %v", err)
	}

	c.logger.Tracef("JobInfo: %+v", jobInfo)

	return &jobInfo, nil
}
func (c *Client) JobParameterized(jobInfo *JobInfo) bool {
	for _, prop := range jobInfo.Property {
		if prop.ParameterDefinitions != nil && len(prop.ParameterDefinitions) > 0 {
			return true
		}
	}

	return false
}
func (c *Client) EnsureBuildableJob(spec *prowapi.ProwJobSpec) error {
	var jobInfo *JobInfo

	// wait at most 20 seconds for the job to appear
	getJobInfoBackoff := wait.Backoff{
		Duration: time.Duration(10) * time.Second,
		Factor:   1,
		Jitter:   0,
		Steps:    2,
	}

	getJobErr := wait.ExponentialBackoff(getJobInfoBackoff, func() (bool, error) {
		var jobErr error
		jobInfo, jobErr = c.GetJobInfo(spec)

		if jobErr != nil && !strings.Contains(strings.ToLower(jobErr.Error()), "404 not found") {
			return false, jobErr
		}

		return jobInfo != nil, nil
	})

	if getJobErr != nil {
		return fmt.Errorf("Job %v does not exist", spec.Job)
	}

	isParameterized := c.JobParameterized(jobInfo)

	c.logger.Tracef("JobHasParameters: %v", isParameterized)

	if isParameterized || len(jobInfo.Builds) > 0 {
		return nil
	}

	buildErr := c.LaunchBuild(spec, nil)

	if buildErr != nil {
		return buildErr
	}

	backoff := wait.Backoff{
		Duration: time.Duration(5) * time.Second,
		Factor:   1,
		Jitter:   1,
		Steps:    10,
	}

	return wait.ExponentialBackoff(backoff, func() (bool, error) {
		c.logger.Debugf("Waiting for job %v to become parameterized", spec.Job)

		jobInfo, _ := c.GetJobInfo(spec)
		isParameterized := false

		if jobInfo != nil {
			isParameterized = c.JobParameterized(jobInfo)

			if isParameterized && jobInfo.LastBuild != nil {
				c.logger.Debugf("Job %v is now parameterized, aborting the build", spec.Job)
				err := c.Abort(getJobName(spec), jobInfo.LastBuild)

				if err != nil {
					c.logger.Infof("Couldn't abort build #%v for job %v: %v", jobInfo.LastBuild.Number, spec.Job, err)
				}
			}
		}

		// don't stop on (possibly) intermittent errors
		return isParameterized, nil
	})
}
func (c *Client) LaunchBuild(spec *prowapi.ProwJobSpec, params url.Values) error {
	var path string

	if params != nil {
		path = getBuildWithParametersPath(spec)
	} else {
		path = getBuildPath(spec)
	}

	c.logger.Debugf("getBuildPath/getBuildWithParametersPath: %s", path)

	resp, err := c.request(http.MethodPost, path, params, true)

	if err != nil {
		return err
	}

	defer resp.Body.Close()

	if resp.StatusCode != 201 {
		return fmt.Errorf("response not 201: %s", resp.Status)
	}

	return nil
}
func (c *Client) Build(pj *prowapi.ProwJob, buildID string) error {
	c.logger.WithFields(pjutil.ProwJobFields(pj)).Info("Build")
	return c.BuildFromSpec(&pj.Spec, buildID, pj.ObjectMeta.Name)
}
func (c *Client) BuildFromSpec(spec *prowapi.ProwJobSpec, buildID, prowJobID string) error {
	if c.dryRun {
		return nil
	}
	env, err := downwardapi.EnvForSpec(downwardapi.NewJobSpec(*spec, buildID, prowJobID))
	if err != nil {
		return err
	}
	params := url.Values{}
	for key, value := range env {
		params.Set(key, value)
	}

	if err := c.EnsureBuildableJob(spec); err != nil {
		return fmt.Errorf("Job %v cannot be build: %v", spec.Job, err)
	}

	return c.LaunchBuild(spec, params)
}
func (c *Client) GetEnqueuedBuilds(jobs []BuildQueryParams) (map[string]Build, error) {
	c.logger.Debug("GetEnqueuedBuilds")

	data, err := c.Get("/queue/api/json?tree=items[task[name],actions[parameters[name,value]]]")
	if err != nil {
		return nil, fmt.Errorf("cannot list builds from the queue: %v", err)
	}
	page := struct {
		QueuedBuilds []Build `json:"items"`
	}{}
	if err := json.Unmarshal(data, &page); err != nil {
		return nil, fmt.Errorf("cannot unmarshal builds from the queue: %v", err)
	}
	jenkinsBuilds := make(map[string]Build)
	for _, jb := range page.QueuedBuilds {
		prowJobID := jb.ProwJobID()
		// Ignore builds with missing buildID parameters.
		if prowJobID == "" {
			continue
		}
		// Ignore builds for jobs we didn't ask for.
		var exists bool
		for _, job := range jobs {
			if prowJobID == job.ProwJobID {
				exists = true
				break
			}
		}
		if !exists {
			continue
		}
		jb.enqueued = true
		jenkinsBuilds[prowJobID] = jb
	}
	return jenkinsBuilds, nil
}
func (c *Client) Abort(job string, build *Build) error {
	c.logger.Debugf("Abort(%v %v)", job, build.Number)
	if c.dryRun {
		return nil
	}
	resp, err := c.request(http.MethodPost, fmt.Sprintf("/job/%s/%d/stop", job, build.Number), nil, false)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		return fmt.Errorf("response not 2XX: %s", resp.Status)
	}
	return nil
}
func PresubmitToJobSpec(pre config.Presubmit) *downwardapi.JobSpec {
	return &downwardapi.JobSpec{
		Type: prowapi.PresubmitJob,
		Job:  pre.Name,
	}
}
func PostsubmitToJobSpec(post config.Postsubmit) *downwardapi.JobSpec {
	return &downwardapi.JobSpec{
		Type: prowapi.PostsubmitJob,
		Job:  post.Name,
	}
}
func PeriodicToJobSpec(periodic config.Periodic) *downwardapi.JobSpec {
	return &downwardapi.JobSpec{
		Type: prowapi.PeriodicJob,
		Job:  periodic.Name,
	}
}
func GetBuildID(name, totURL string) (string, error) {
	if totURL == "" {
		return node.Generate().String(), nil
	}
	var err error
	url, err := url.Parse(totURL)
	if err != nil {
		return "", fmt.Errorf("invalid tot url: %v", err)
	}
	url.Path = path.Join(url.Path, "vend", name)
	sleepDuration := 100 * time.Millisecond
	for retries := 0; retries < 10; retries++ {
		if retries > 0 {
			sleep(sleepDuration)
			sleepDuration = sleepDuration * 2
		}
		var resp *http.Response
		resp, err = http.Get(url.String())
		if err != nil {
			continue
		}
		defer resp.Body.Close()
		if resp.StatusCode != 200 {
			err = fmt.Errorf("got unexpected response from tot: %v", resp.Status)
			continue
		}
		var buf []byte
		buf, err = ioutil.ReadAll(resp.Body)
		if err == nil {
			return string(buf), nil
		}
		return "", err
	}
	return "", err
}
func listGcsObjects(ctx context.Context, client *storage.Client, bucketName, prefix, delim string) (
	[]string, error) {

	var objects []string
	it := client.Bucket(bucketName).Objects(ctx, &storage.Query{
		Prefix:    prefix,
		Delimiter: delim,
	})

	for {
		attrs, err := it.Next()
		if err == iterator.Done {
			break
		}
		if err != nil {
			return objects, fmt.Errorf("error iterating: %v", err)
		}

		if attrs.Prefix != "" {
			objects = append(objects, path.Base(attrs.Prefix))
		}
	}
	logrus.Info("end of listGcsObjects(...)")
	return objects, nil
}
func FindBaseProfile(ctx context.Context, client *storage.Client, bucket, prowJobName, artifactsDirName,
	covProfileName string) ([]byte, error) {

	dirOfJob := path.Join("logs", prowJobName)

	strBuilds, err := listGcsObjects(ctx, client, bucket, dirOfJob+"/", "/")
	if err != nil {
		return nil, fmt.Errorf("error listing gcs objects: %v", err)
	}

	builds := sortBuilds(strBuilds)
	profilePath := ""
	for _, build := range builds {
		buildDirPath := path.Join(dirOfJob, strconv.Itoa(build))
		dirOfStatusJSON := path.Join(buildDirPath, statusJSON)

		statusText, err := readGcsObject(ctx, client, bucket, dirOfStatusJSON)
		if err != nil {
			logrus.Infof("Cannot read finished.json (%s) in bucket '%s'", dirOfStatusJSON, bucket)
		} else if isBuildSucceeded(statusText) {
			artifactsDirPath := path.Join(buildDirPath, artifactsDirName)
			profilePath = path.Join(artifactsDirPath, covProfileName)
			break
		}
	}
	if profilePath == "" {
		return nil, fmt.Errorf("no healthy build found for job '%s' in bucket '%s'; total # builds = %v", dirOfJob, bucket, len(builds))
	}
	return readGcsObject(ctx, client, bucket, profilePath)
}
func sortBuilds(strBuilds []string) []int {
	var res []int
	for _, buildStr := range strBuilds {
		num, err := strconv.Atoi(buildStr)
		if err != nil {
			logrus.Infof("Non-int build number found: '%s'", buildStr)
		} else {
			res = append(res, num)
		}
	}
	sort.Sort(sort.Reverse(sort.IntSlice(res)))
	return res
}
func GetAll(sess *session.Session) ([]string, error) {
	var regions []string
	svc := ec2.New(sess, &aws.Config{Region: aws.String(Default)})
	resp, err := svc.DescribeRegions(nil)
	if err != nil {
		return nil, err
	}
	for _, region := range resp.Regions {
		regions = append(regions, *region.RegionName)
	}
	return regions, nil
}
func NewEventClient(ghc githubClient, log *logrus.Entry, org, repo string, number int) *EventClient {
	return &EventClient{
		org:    org,
		repo:   repo,
		number: number,

		ghc: ghc,
		log: log,
	}
}
func (c *EventClient) PruneComments(shouldPrune func(github.IssueComment) bool) {
	c.once.Do(func() {
		botName, err := c.ghc.BotName()
		if err != nil {
			c.log.WithError(err).Error("failed to get the bot's name. Pruning will consider all comments.")
		}
		comments, err := c.ghc.ListIssueComments(c.org, c.repo, c.number)
		if err != nil {
			c.log.WithError(err).Errorf("failed to list comments for %s/%s#%d", c.org, c.repo, c.number)
		}
		if botName != "" {
			for _, comment := range comments {
				if comment.User.Login == botName {
					c.comments = append(c.comments, comment)
				}
			}
		}
	})

	c.lock.Lock()
	defer c.lock.Unlock()

	var remaining []github.IssueComment
	for _, comment := range c.comments {
		removed := false
		if shouldPrune(comment) {
			if err := c.ghc.DeleteComment(c.org, c.repo, comment.ID); err != nil {
				c.log.WithError(err).Errorf("failed to delete stale comment with ID '%d'", comment.ID)
			} else {
				removed = true
			}
		}
		if !removed {
			remaining = append(remaining, comment)
		}
	}
	c.comments = remaining
}
func FormatResponse(to, message, reason string) string {
	format := `@%s: %s

<details>

%s

%s
</details>`

	return fmt.Sprintf(format, to, message, reason, AboutThisBotWithoutCommands)
}
func FormatSimpleResponse(to, message string) string {
	format := `@%s: %s

<details>

%s
</details>`

	return fmt.Sprintf(format, to, message, AboutThisBotWithoutCommands)
}
func FormatICResponse(ic github.IssueComment, s string) string {
	return FormatResponseRaw(ic.Body, ic.HTMLURL, ic.User.Login, s)
}
func FormatResponseRaw(body, bodyURL, login, reply string) string {
	format := `In response to [this](%s):

%s
`
	// Quote the user's comment by prepending ">" to each line.
	var quoted []string
	for _, l := range strings.Split(body, "\n") {
		quoted = append(quoted, ">"+l)
	}
	return FormatResponse(login, reply, fmt.Sprintf(format, bodyURL, strings.Join(quoted, "\n")))
}
func (o *Options) Validate() error {
	if o.gcsPath.String() != "" {
		o.Bucket = o.gcsPath.Bucket()
		o.PathPrefix = o.gcsPath.Object()
	}

	if !o.DryRun {
		if o.Bucket == "" {
			return errors.New("GCS upload was requested no GCS bucket was provided")
		}

		if o.GcsCredentialsFile == "" {
			return errors.New("GCS upload was requested but no GCS credentials file was provided")
		}
	}

	return o.GCSConfiguration.Validate()
}
func Encode(options Options) (string, error) {
	encoded, err := json.Marshal(options)
	return string(encoded), err
}
func RegisterIssueHandler(name string, fn IssueHandler, help HelpProvider) {
	pluginHelp[name] = help
	issueHandlers[name] = fn
}
func RegisterIssueCommentHandler(name string, fn IssueCommentHandler, help HelpProvider) {
	pluginHelp[name] = help
	issueCommentHandlers[name] = fn
}
func RegisterPullRequestHandler(name string, fn PullRequestHandler, help HelpProvider) {
	pluginHelp[name] = help
	pullRequestHandlers[name] = fn
}
func RegisterStatusEventHandler(name string, fn StatusEventHandler, help HelpProvider) {
	pluginHelp[name] = help
	statusEventHandlers[name] = fn
}
func RegisterPushEventHandler(name string, fn PushEventHandler, help HelpProvider) {
	pluginHelp[name] = help
	pushEventHandlers[name] = fn
}
func RegisterReviewEventHandler(name string, fn ReviewEventHandler, help HelpProvider) {
	pluginHelp[name] = help
	reviewEventHandlers[name] = fn
}
func RegisterReviewCommentEventHandler(name string, fn ReviewCommentEventHandler, help HelpProvider) {
	pluginHelp[name] = help
	reviewCommentEventHandlers[name] = fn
}
func RegisterGenericCommentHandler(name string, fn GenericCommentHandler, help HelpProvider) {
	pluginHelp[name] = help
	genericCommentHandlers[name] = fn
}
func NewAgent(configAgent *config.Agent, pluginConfigAgent *ConfigAgent, clientAgent *ClientAgent, logger *logrus.Entry) Agent {
	prowConfig := configAgent.Config()
	pluginConfig := pluginConfigAgent.Config()
	return Agent{
		GitHubClient:     clientAgent.GitHubClient,
		KubernetesClient: clientAgent.KubernetesClient,
		ProwJobClient:    clientAgent.ProwJobClient,
		GitClient:        clientAgent.GitClient,
		SlackClient:      clientAgent.SlackClient,
		OwnersClient:     clientAgent.OwnersClient,
		Config:           prowConfig,
		PluginConfig:     pluginConfig,
		Logger:           logger,
	}
}
func (a *Agent) InitializeCommentPruner(org, repo string, pr int) {
	a.commentPruner = commentpruner.NewEventClient(
		a.GitHubClient, a.Logger.WithField("client", "commentpruner"),
		org, repo, pr,
	)
}
func (a *Agent) CommentPruner() (*commentpruner.EventClient, error) {
	if a.commentPruner == nil {
		return nil, errors.New("comment pruner client never initialized")
	}
	return a.commentPruner, nil
}
func (pa *ConfigAgent) Load(path string) error {
	b, err := ioutil.ReadFile(path)
	if err != nil {
		return err
	}
	np := &Configuration{}
	if err := yaml.Unmarshal(b, np); err != nil {
		return err
	}
	if err := np.Validate(); err != nil {
		return err
	}

	pa.Set(np)
	return nil
}
func (pa *ConfigAgent) Config() *Configuration {
	pa.mut.Lock()
	defer pa.mut.Unlock()
	return pa.configuration
}
func (pa *ConfigAgent) Set(pc *Configuration) {
	pa.mut.Lock()
	defer pa.mut.Unlock()
	pa.configuration = pc
}
func (pa *ConfigAgent) Start(path string) error {
	if err := pa.Load(path); err != nil {
		return err
	}
	ticker := time.Tick(1 * time.Minute)
	go func() {
		for range ticker {
			if err := pa.Load(path); err != nil {
				logrus.WithField("path", path).WithError(err).Error("Error loading plugin config.")
			}
		}
	}()
	return nil
}
func (pa *ConfigAgent) GenericCommentHandlers(owner, repo string) map[string]GenericCommentHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]GenericCommentHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := genericCommentHandlers[p]; ok {
			hs[p] = h
		}
	}
	return hs
}
func (pa *ConfigAgent) IssueHandlers(owner, repo string) map[string]IssueHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]IssueHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := issueHandlers[p]; ok {
			hs[p] = h
		}
	}
	return hs
}
func (pa *ConfigAgent) IssueCommentHandlers(owner, repo string) map[string]IssueCommentHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]IssueCommentHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := issueCommentHandlers[p]; ok {
			hs[p] = h
		}
	}

	return hs
}
func (pa *ConfigAgent) PullRequestHandlers(owner, repo string) map[string]PullRequestHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]PullRequestHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := pullRequestHandlers[p]; ok {
			hs[p] = h
		}
	}

	return hs
}
func (pa *ConfigAgent) ReviewEventHandlers(owner, repo string) map[string]ReviewEventHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]ReviewEventHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := reviewEventHandlers[p]; ok {
			hs[p] = h
		}
	}

	return hs
}
func (pa *ConfigAgent) ReviewCommentEventHandlers(owner, repo string) map[string]ReviewCommentEventHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]ReviewCommentEventHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := reviewCommentEventHandlers[p]; ok {
			hs[p] = h
		}
	}

	return hs
}
func (pa *ConfigAgent) StatusEventHandlers(owner, repo string) map[string]StatusEventHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]StatusEventHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := statusEventHandlers[p]; ok {
			hs[p] = h
		}
	}

	return hs
}
func (pa *ConfigAgent) PushEventHandlers(owner, repo string) map[string]PushEventHandler {
	pa.mut.Lock()
	defer pa.mut.Unlock()

	hs := map[string]PushEventHandler{}
	for _, p := range pa.getPlugins(owner, repo) {
		if h, ok := pushEventHandlers[p]; ok {
			hs[p] = h
		}
	}

	return hs
}
func EventsForPlugin(name string) []string {
	var events []string
	if _, ok := issueHandlers[name]; ok {
		events = append(events, "issue")
	}
	if _, ok := issueCommentHandlers[name]; ok {
		events = append(events, "issue_comment")
	}
	if _, ok := pullRequestHandlers[name]; ok {
		events = append(events, "pull_request")
	}
	if _, ok := pushEventHandlers[name]; ok {
		events = append(events, "push")
	}
	if _, ok := reviewEventHandlers[name]; ok {
		events = append(events, "pull_request_review")
	}
	if _, ok := reviewCommentEventHandlers[name]; ok {
		events = append(events, "pull_request_review_comment")
	}
	if _, ok := statusEventHandlers[name]; ok {
		events = append(events, "status")
	}
	if _, ok := genericCommentHandlers[name]; ok {
		events = append(events, "GenericCommentEvent (any event for user text)")
	}
	return events
}
func insertLink(started *gcs.Started, viewURL string) (bool, error) {
	if started.Metadata == nil {
		started.Metadata = metadata.Metadata{}
	}
	meta := started.Metadata
	var changed bool
	top, present := meta.String(resultstoreKey)
	if !present || top == nil || *top != viewURL {
		changed = true
		meta[resultstoreKey] = viewURL
	}
	links, present := meta.Meta(linksKey)
	if present && links == nil {
		return false, fmt.Errorf("metadata.links is not a Metadata value: %v", meta[linksKey])
	}
	if links == nil {
		links = &metadata.Metadata{}
		changed = true
	}
	resultstoreMeta, present := links.Meta(resultstoreKey)
	if present && resultstoreMeta == nil {
		return false, fmt.Errorf("metadata.links.resultstore is not a Metadata value: %v", (*links)[resultstoreKey])
	}
	if resultstoreMeta == nil {
		resultstoreMeta = &metadata.Metadata{}
		changed = true
	}
	val, present := resultstoreMeta.String(urlKey)
	if present && val == nil {
		return false, fmt.Errorf("metadata.links.resultstore.url is not a string value: %v", (*resultstoreMeta)[urlKey])
	}
	if !changed && val != nil && *val == viewURL {
		return false, nil
	}

	(*resultstoreMeta)[urlKey] = viewURL
	(*links)[resultstoreKey] = *resultstoreMeta
	meta[linksKey] = *links
	return true, nil
}
func HelpProvider(enabledRepos []string) (*pluginhelp.PluginHelp, error) {
	pluginHelp := &pluginhelp.PluginHelp{
		Description: `The cherrypick plugin is used for cherrypicking PRs across branches. For every successful cherrypick invocation a new PR is opened against the target branch and assigned to the requester. If the parent PR contains a release note, it is copied to the cherrypick PR.`,
	}
	pluginHelp.AddCommand(pluginhelp.Command{
		Usage:       "/cherrypick [branch]",
		Description: "Cherrypick a PR to a different branch. This command works both in merged PRs (the cherrypick PR is opened immediately) and open PRs (the cherrypick PR opens as soon as the original PR merges).",
		Featured:    true,
		// depends on how the cherrypick server runs; needs auth by default (--allow-all=false)
		WhoCanUse: "Members of the trusted organization for the repo.",
		Examples:  []string{"/cherrypick release-3.9"},
	})
	return pluginHelp, nil
}
func (s *Server) getPatch(org, repo, targetBranch string, num int) (string, error) {
	patch, err := s.ghc.GetPullRequestPatch(org, repo, num)
	if err != nil {
		return "", err
	}
	localPath := fmt.Sprintf("/tmp/%s_%s_%d_%s.patch", org, repo, num, normalize(targetBranch))
	out, err := os.Create(localPath)
	if err != nil {
		return "", err
	}
	defer out.Close()
	if _, err := io.Copy(out, bytes.NewBuffer(patch)); err != nil {
		return "", err
	}
	return localPath, nil
}
func releaseNoteFromParentPR(body string) string {
	potentialMatch := releaseNoteRe.FindStringSubmatch(body)
	if potentialMatch == nil {
		return ""
	}
	return fmt.Sprintf("```release-note\n%s\n```", strings.TrimSpace(potentialMatch[1]))
}
func ValidatePayload(payload []byte, sig string, key []byte) bool {
	if !strings.HasPrefix(sig, "sha1=") {
		return false
	}
	sig = sig[5:]
	sb, err := hex.DecodeString(sig)
	if err != nil {
		return false
	}
	mac := hmac.New(sha1.New, key)
	mac.Write(payload)
	expected := mac.Sum(nil)
	return hmac.Equal(sb, expected)
}
func PayloadSignature(payload []byte, key []byte) string {
	mac := hmac.New(sha1.New, key)
	mac.Write(payload)
	sum := mac.Sum(nil)
	return "sha1=" + hex.EncodeToString(sum)
}
func updateString(have, want *string) bool {
	switch {
	case have == nil:
		panic("have must be non-nil")
	case want == nil:
		return false // do not care what we have
	case *have == *want:
		return false // already have it
	}
	*have = *want // update value
	return true
}
func updateBool(have, want *bool) bool {
	switch {
	case have == nil:
		panic("have must not be nil")
	case want == nil:
		return false // do not care what we have
	case *have == *want:
		return false //already have it
	}
	*have = *want // update value
	return true
}
func configureOrgMeta(client orgMetadataClient, orgName string, want org.Metadata) error {
	cur, err := client.GetOrg(orgName)
	if err != nil {
		return fmt.Errorf("failed to get %s metadata: %v", orgName, err)
	}
	change := false
	change = updateString(&cur.BillingEmail, want.BillingEmail) || change
	change = updateString(&cur.Company, want.Company) || change
	change = updateString(&cur.Email, want.Email) || change
	change = updateString(&cur.Name, want.Name) || change
	change = updateString(&cur.Description, want.Description) || change
	change = updateString(&cur.Location, want.Location) || change
	if want.DefaultRepositoryPermission != nil {
		w := string(*want.DefaultRepositoryPermission)
		change = updateString(&cur.DefaultRepositoryPermission, &w)
	}
	change = updateBool(&cur.HasOrganizationProjects, want.HasOrganizationProjects) || change
	change = updateBool(&cur.HasRepositoryProjects, want.HasRepositoryProjects) || change
	change = updateBool(&cur.MembersCanCreateRepositories, want.MembersCanCreateRepositories) || change
	if change {
		if _, err := client.EditOrg(orgName, *cur); err != nil {
			return fmt.Errorf("failed to edit %s metadata: %v", orgName, err)
		}
	}
	return nil
}
func configureTeamRepos(client teamRepoClient, githubTeams map[string]github.Team, name, orgName string, team org.Team) error {
	gt, ok := githubTeams[name]
	if !ok { // configureTeams is buggy if this is the case
		return fmt.Errorf("%s not found in id list", name)
	}

	want := team.Repos
	have := map[string]github.RepoPermissionLevel{}
	repos, err := client.ListTeamRepos(gt.ID)
	if err != nil {
		return fmt.Errorf("failed to list team %d(%s) repos: %v", gt.ID, name, err)
	}
	for _, repo := range repos {
		have[repo.Name] = github.LevelFromPermissions(repo.Permissions)
	}

	actions := map[string]github.RepoPermissionLevel{}
	for wantRepo, wantPermission := range want {
		if havePermission, haveRepo := have[wantRepo]; haveRepo && havePermission == wantPermission {
			// nothing to do
			continue
		}
		// create or update this permission
		actions[wantRepo] = wantPermission
	}

	for haveRepo := range have {
		if _, wantRepo := want[haveRepo]; !wantRepo {
			// should remove these permissions
			actions[haveRepo] = github.None
		}
	}

	var updateErrors []error
	for repo, permission := range actions {
		var err error
		if permission == github.None {
			err = client.RemoveTeamRepo(gt.ID, orgName, repo)
		} else {
			err = client.UpdateTeamRepo(gt.ID, orgName, repo, permission)
		}
		if err != nil {
			updateErrors = append(updateErrors, fmt.Errorf("failed to update team %d(%s) permissions on repo %s to %s: %v", gt.ID, name, repo, permission, err))
		}
	}

	return errorutil.NewAggregate(updateErrors...)
}
func (c *Client) ShouldReport(pj *prowapi.ProwJob) bool {
	pubSubMap := findLabels(pj, PubSubProjectLabel, PubSubTopicLabel)
	return pubSubMap[PubSubProjectLabel] != "" && pubSubMap[PubSubTopicLabel] != ""
}
func (o Options) Run(spec *downwardapi.JobSpec, extra map[string]gcs.UploadFunc) error {
	uploadTargets := o.assembleTargets(spec, extra)

	if !o.DryRun {
		ctx := context.Background()
		gcsClient, err := storage.NewClient(ctx, option.WithCredentialsFile(o.GcsCredentialsFile))
		if err != nil {
			return fmt.Errorf("could not connect to GCS: %v", err)
		}

		if err := gcs.Upload(gcsClient.Bucket(o.Bucket), uploadTargets); err != nil {
			return fmt.Errorf("failed to upload to GCS: %v", err)
		}
	} else {
		for destination := range uploadTargets {
			logrus.WithField("dest", destination).Info("Would upload")
		}
	}

	logrus.Info("Finished upload to GCS")
	return nil
}
func (d *DefaultFieldsFormatter) Format(entry *logrus.Entry) ([]byte, error) {
	data := make(logrus.Fields, len(entry.Data)+len(d.DefaultFields))
	for k, v := range d.DefaultFields {
		data[k] = v
	}
	for k, v := range entry.Data {
		data[k] = v
	}
	return d.WrappedFormatter.Format(&logrus.Entry{
		Logger:  entry.Logger,
		Data:    data,
		Time:    entry.Time,
		Level:   entry.Level,
		Message: entry.Message,
	})
}
func (issue *Issue) FindLabels(regex *regexp.Regexp) []Label {
	labels := []Label{}

	for _, label := range issue.Labels {
		if regex.MatchString(label.Name) {
			labels = append(labels, label)
		}
	}

	return labels
}
func (o *Options) AddFlags(flags *flag.FlagSet) {
	flags.StringVar(&o.Log, "clone-log", "", "Path to the clone records log")
	o.Options.AddFlags(flags)
}
func NewAgent(config *config.GitHubOAuthConfig, logger *logrus.Entry) *Agent {
	return &Agent{
		gc:     config,
		logger: logger,
	}
}
func (ga *Agent) HandleLogin(client OAuthClient) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		stateToken := xsrftoken.Generate(ga.gc.ClientSecret, "", "")
		state := hex.EncodeToString([]byte(stateToken))
		oauthSession, err := ga.gc.CookieStore.New(r, oauthSessionCookie)
		oauthSession.Options.Secure = true
		oauthSession.Options.HttpOnly = true
		if err != nil {
			ga.serverError(w, "Creating new OAuth session", err)
			return
		}
		oauthSession.Options.MaxAge = 10 * 60
		oauthSession.Values[stateKey] = state

		if err := oauthSession.Save(r, w); err != nil {
			ga.serverError(w, "Save oauth session", err)
			return
		}

		redirectURL := client.AuthCodeURL(state, oauth2.ApprovalForce, oauth2.AccessTypeOnline)
		http.Redirect(w, r, redirectURL, http.StatusFound)
	}
}
func (ga *Agent) HandleLogout(client OAuthClient) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		accessTokenSession, err := ga.gc.CookieStore.Get(r, tokenSession)
		if err != nil {
			ga.serverError(w, "get cookie", err)
			return
		}
		// Clear session
		accessTokenSession.Options.MaxAge = -1
		if err := accessTokenSession.Save(r, w); err != nil {
			ga.serverError(w, "Save invalidated session on log out", err)
			return
		}
		loginCookie, err := r.Cookie(loginSession)
		if err == nil {
			loginCookie.MaxAge = -1
			loginCookie.Expires = time.Now().Add(-time.Hour * 24)
			http.SetCookie(w, loginCookie)
		}
		http.Redirect(w, r, ga.gc.FinalRedirectURL, http.StatusFound)
	}
}
func (ga *Agent) serverError(w http.ResponseWriter, action string, err error) {
	ga.logger.WithError(err).Errorf("Error %s.", action)
	msg := fmt.Sprintf("500 Internal server error %s: %v", action, err)
	http.Error(w, msg, http.StatusInternalServerError)
}
func (in *ResourcesConfigObject) FromItem(i common.Item) {
	c, err := common.ItemToResourcesConfig(i)
	if err == nil {
		in.fromConfig(c)
	}
}
func (in *ResourcesConfigCollection) GetItems() []Object {
	var items []Object
	for _, i := range in.Items {
		items = append(items, i)
	}
	return items
}
func (in *ResourcesConfigCollection) SetItems(objects []Object) {
	var items []*ResourcesConfigObject
	for _, b := range objects {
		items = append(items, b.(*ResourcesConfigObject))
	}
	in.Items = items
}
func (l *RepoPermissionLevel) UnmarshalText(text []byte) error {
	v := RepoPermissionLevel(text)
	if _, ok := repoPermissionLevels[v]; !ok {
		return fmt.Errorf("bad repo permission: %s not in %v", v, repoPermissionLevels)
	}
	*l = v
	return nil
}
func (i Issue) IsAssignee(login string) bool {
	for _, assignee := range i.Assignees {
		if NormLogin(login) == NormLogin(assignee.Login) {
			return true
		}
	}
	return false
}
func (i Issue) IsAuthor(login string) bool {
	return NormLogin(i.User.Login) == NormLogin(login)
}
func (i Issue) HasLabel(labelToFind string) bool {
	for _, label := range i.Labels {
		if strings.ToLower(label.Name) == strings.ToLower(labelToFind) {
			return true
		}
	}
	return false
}
func (pe PushEvent) Branch() string {
	ref := strings.TrimPrefix(pe.Ref, "refs/heads/") // if Ref is a branch
	ref = strings.TrimPrefix(ref, "refs/tags/")      // if Ref is a tag
	return ref
}
func truncate(in string) string {
	const (
		half = (maxLen - len(elide)) / 2
	)
	if len(in) <= maxLen {
		return in
	}
	return in[:half] + elide + in[len(in)-half:]
}
func reportStatus(ghc GitHubClient, pj prowapi.ProwJob) error {
	refs := pj.Spec.Refs
	if pj.Spec.Report {
		contextState, err := prowjobStateToGitHubStatus(pj.Status.State)
		if err != nil {
			return err
		}
		sha := refs.BaseSHA
		if len(refs.Pulls) > 0 {
			sha = refs.Pulls[0].SHA
		}
		if err := ghc.CreateStatus(refs.Org, refs.Repo, sha, github.Status{
			State:       contextState,
			Description: truncate(pj.Status.Description),
			Context:     pj.Spec.Context, // consider truncating this too
			TargetURL:   pj.Status.URL,
		}); err != nil {
			return err
		}
	}
	return nil
}
func parseIssueComments(pj prowapi.ProwJob, botName string, ics []github.IssueComment) ([]int, []string, int) {
	var delete []int
	var previousComments []int
	var latestComment int
	var entries []string
	// First accumulate result entries and comment IDs
	for _, ic := range ics {
		if ic.User.Login != botName {
			continue
		}
		// Old report comments started with the context. Delete them.
		// TODO(spxtr): Delete this check a few weeks after this merges.
		if strings.HasPrefix(ic.Body, pj.Spec.Context) {
			delete = append(delete, ic.ID)
		}
		if !strings.Contains(ic.Body, commentTag) {
			continue
		}
		if latestComment != 0 {
			previousComments = append(previousComments, latestComment)
		}
		latestComment = ic.ID
		var tracking bool
		for _, line := range strings.Split(ic.Body, "\n") {
			line = strings.TrimSpace(line)
			if strings.HasPrefix(line, "---") {
				tracking = true
			} else if len(line) == 0 {
				tracking = false
			} else if tracking {
				entries = append(entries, line)
			}
		}
	}
	var newEntries []string
	// Next decide which entries to keep.
	for i := range entries {
		keep := true
		f1 := strings.Split(entries[i], " | ")
		for j := range entries {
			if i == j {
				continue
			}
			f2 := strings.Split(entries[j], " | ")
			// Use the newer results if there are multiple.
			if j > i && f2[0] == f1[0] {
				keep = false
			}
		}
		// Use the current result if there is an old one.
		if pj.Spec.Context == f1[0] {
			keep = false
		}
		if keep {
			newEntries = append(newEntries, entries[i])
		}
	}
	var createNewComment bool
	if string(pj.Status.State) == github.StatusFailure {
		newEntries = append(newEntries, createEntry(pj))
		createNewComment = true
	}
	delete = append(delete, previousComments...)
	if (createNewComment || len(newEntries) == 0) && latestComment != 0 {
		delete = append(delete, latestComment)
		latestComment = 0
	}
	return delete, newEntries, latestComment
}
func createComment(reportTemplate *template.Template, pj prowapi.ProwJob, entries []string) (string, error) {
	plural := ""
	if len(entries) > 1 {
		plural = "s"
	}
	var b bytes.Buffer
	if reportTemplate != nil {
		if err := reportTemplate.Execute(&b, &pj); err != nil {
			return "", err
		}
	}
	lines := []string{
		fmt.Sprintf("@%s: The following test%s **failed**, say `/retest` to rerun them all:", pj.Spec.Refs.Pulls[0].Author, plural),
		"",
		"Test name | Commit | Details | Rerun command",
		"--- | --- | --- | ---",
	}
	lines = append(lines, entries...)
	if reportTemplate != nil {
		lines = append(lines, "", b.String())
	}
	lines = append(lines, []string{
		"",
		"<details>",
		"",
		plugins.AboutThisBot,
		"</details>",
		commentTag,
	}...)
	return strings.Join(lines, "\n"), nil
}
func (lens Lens) Config() lenses.LensConfig {
	return lenses.LensConfig{
		Name:     name,
		Title:    title,
		Priority: priority,
	}
}
func (lens Lens) Callback(artifacts []lenses.Artifact, resourceDir string, data string) string {
	return ""
}
func FormatRecord(record Record) string {
	output := bytes.Buffer{}
	if record.Failed {
		fmt.Fprintln(&output, "# FAILED!")
	}
	fmt.Fprintf(&output, "# Cloning %s/%s at %s", record.Refs.Org, record.Refs.Repo, record.Refs.BaseRef)
	if record.Refs.BaseSHA != "" {
		fmt.Fprintf(&output, "(%s)", record.Refs.BaseSHA)
	}
	output.WriteString("\n")
	if len(record.Refs.Pulls) > 0 {
		output.WriteString("# Checking out pulls:\n")
		for _, pull := range record.Refs.Pulls {
			fmt.Fprintf(&output, "#\t%d", pull.Number)
			if pull.SHA != "" {
				fmt.Fprintf(&output, "(%s)", pull.SHA)
			}
			fmt.Fprint(&output, "\n")
		}
	}
	for _, command := range record.Commands {
		fmt.Fprintf(&output, "$ %s\n", command.Command)
		fmt.Fprint(&output, command.Output)
		if command.Error != "" {
			fmt.Fprintf(&output, "# Error: %s\n", command.Error)
		}
	}

	return output.String()
}
func (c *Client) Namespace(ns string) *Client {
	nc := *c
	nc.namespace = ns
	return &nc
}
func NewFakeClient(deckURL string) *Client {
	return &Client{
		namespace: "default",
		deckURL:   deckURL,
		client:    &http.Client{},
		fake:      true,
	}
}
func NewClientInCluster(namespace string) (*Client, error) {
	tokenFile := "/var/run/secrets/kubernetes.io/serviceaccount/token"
	token, err := ioutil.ReadFile(tokenFile)
	if err != nil {
		return nil, err
	}

	rootCAFile := "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
	certData, err := ioutil.ReadFile(rootCAFile)
	if err != nil {
		return nil, err
	}

	cp := x509.NewCertPool()
	cp.AppendCertsFromPEM(certData)

	tr := &http.Transport{
		TLSClientConfig: &tls.Config{
			MinVersion: tls.VersionTLS12,
			RootCAs:    cp,
		},
	}
	return &Client{
		logger:    logrus.WithField("client", "kube"),
		baseURL:   inClusterBaseURL,
		client:    &http.Client{Transport: tr, Timeout: requestTimeout},
		token:     string(token),
		namespace: namespace,
	}, nil
}
func NewClientFromFile(clusterPath, namespace string) (*Client, error) {
	data, err := ioutil.ReadFile(clusterPath)
	if err != nil {
		return nil, err
	}
	var c Cluster
	if err := yaml.Unmarshal(data, &c); err != nil {
		return nil, err
	}
	return NewClient(&c, namespace)
}
func NewClient(c *Cluster, namespace string) (*Client, error) {
	// Relies on json encoding/decoding []byte as base64
	// https://golang.org/pkg/encoding/json/#Marshal
	cc := c.ClientCertificate
	ck := c.ClientKey
	ca := c.ClusterCACertificate

	cert, err := tls.X509KeyPair(cc, ck)
	if err != nil {
		return nil, err
	}

	cp := x509.NewCertPool()
	cp.AppendCertsFromPEM(ca)

	tr := &http.Transport{
		TLSClientConfig: &tls.Config{
			MinVersion:   tls.VersionTLS12,
			Certificates: []tls.Certificate{cert},
			RootCAs:      cp,
		},
	}
	return &Client{
		logger:    logrus.WithField("client", "kube"),
		baseURL:   c.Endpoint,
		client:    &http.Client{Transport: tr, Timeout: requestTimeout},
		namespace: namespace,
	}, nil
}
func (c *Client) ReplaceConfigMap(name string, config ConfigMap) (ConfigMap, error) {
	c.log("ReplaceConfigMap", name)
	namespace := c.namespace
	if config.Namespace != "" {
		namespace = config.Namespace
	}
	var retConfigMap ConfigMap
	err := c.request(&request{
		method:      http.MethodPut,
		path:        fmt.Sprintf("/api/v1/namespaces/%s/configmaps/%s", namespace, name),
		requestBody: &config,
	}, &retConfigMap)

	return retConfigMap, err
}
func GetDiskUsage(path string) (percentBlocksFree float64, bytesFree, bytesUsed uint64, err error) {
	var stat syscall.Statfs_t
	err = syscall.Statfs(path, &stat)
	if err != nil {
		return 0, 0, 0, err
	}
	percentBlocksFree = float64(stat.Bfree) / float64(stat.Blocks) * 100
	bytesFree = stat.Bfree * uint64(stat.Bsize)
	bytesUsed = (stat.Blocks - stat.Bfree) * uint64(stat.Bsize)
	return percentBlocksFree, bytesFree, bytesUsed, nil
}
func GetATime(path string, defaultTime time.Time) time.Time {
	at, err := atime.Stat(path)
	if err != nil {
		log.WithError(err).Errorf("Could not get atime for %s", path)
		return defaultTime
	}
	return at
}
func RegisterLens(lens Lens) error {
	config := lens.Config()
	_, ok := lensReg[config.Name]
	if ok {
		return fmt.Errorf("viewer already registered with name %s", config.Name)
	}

	if config.Title == "" {
		return errors.New("empty title field in view metadata")
	}
	if config.Priority < 0 {
		return errors.New("priority must be >=0")
	}
	lensReg[config.Name] = lens
	logrus.Infof("Spyglass registered viewer %s with title %s.", config.Name, config.Title)
	return nil
}
func GetLens(name string) (Lens, error) {
	lens, ok := lensReg[name]
	if !ok {
		return nil, ErrInvalidLensName
	}
	return lens, nil
}
func LastNLines(a Artifact, n int64) ([]string, error) {
	// 300B, a reasonable log line length, probably a bit more scalable than a hard-coded value
	return LastNLinesChunked(a, n, 300*n+1)
}
func NewClient(tokenGenerator func() []byte) *Client {
	return &Client{
		logger:         logrus.WithField("client", "slack"),
		tokenGenerator: tokenGenerator,
	}
}
func (sl *Client) WriteMessage(text, channel string) error {
	sl.log("WriteMessage", text, channel)
	if sl.fake {
		return nil
	}

	var uv = sl.urlValues()
	uv.Add("channel", channel)
	uv.Add("text", text)

	_, err := sl.postMessage(chatPostMessage, uv)
	return err
}
func (NATGateway) MarkAndSweep(sess *session.Session, acct string, region string, set *Set) error {
	svc := ec2.New(sess, &aws.Config{Region: aws.String(region)})

	inp := &ec2.DescribeNatGatewaysInput{}
	if err := svc.DescribeNatGatewaysPages(inp, func(page *ec2.DescribeNatGatewaysOutput, _ bool) bool {
		for _, gw := range page.NatGateways {
			g := &natGateway{
				Account: acct,
				Region:  region,
				ID:      *gw.NatGatewayId,
			}

			if set.Mark(g) {
				inp := &ec2.DeleteNatGatewayInput{NatGatewayId: gw.NatGatewayId}
				if _, err := svc.DeleteNatGateway(inp); err != nil {
					klog.Warningf("%v: delete failed: %v", g.ARN(), err)
				}
			}
		}
		return true
	}); err != nil {
		return err
	}

	return nil
}
func (NATGateway) ListAll(sess *session.Session, acct, region string) (*Set, error) {
	svc := ec2.New(sess, &aws.Config{Region: aws.String(region)})
	set := NewSet(0)
	inp := &ec2.DescribeNatGatewaysInput{}

	err := svc.DescribeNatGatewaysPages(inp, func(page *ec2.DescribeNatGatewaysOutput, _ bool) bool {
		for _, gw := range page.NatGateways {
			now := time.Now()
			arn := natGateway{
				Account: acct,
				Region:  region,
				ID:      *gw.NatGatewayId,
			}.ARN()

			set.firstSeen[arn] = now
		}

		return true
	})

	return set, errors.Wrapf(err, "couldn't describe nat gateways for %q in %q", acct, region)
}
func NewClient(owner string, url string) *Client {

	client := &Client{
		url:     url,
		owner:   owner,
		storage: storage.NewMemoryStorage(),
	}

	// Configure the dialer to attempt three additional times to establish
	// a connection after a failed dial attempt. The dialer should wait 10
	// seconds between each attempt.
	client.Dialer.RetryCount = 3
	client.Dialer.RetrySleep = time.Second * 10

	// Configure the dialer and HTTP client transport to mimic the configuration
	// of the http.DefaultTransport with the exception that the Dialer's Dial
	// and DialContext functions are assigned to the client transport.
	//
	// See https://golang.org/pkg/net/http/#RoundTripper for the values
	// values used for the http.DefaultTransport.
	client.Dialer.Timeout = 30 * time.Second
	client.Dialer.KeepAlive = 30 * time.Second
	client.Dialer.DualStack = true
	client.http.Transport = &http.Transport{
		Proxy:                 http.ProxyFromEnvironment,
		Dial:                  client.Dialer.Dial,
		DialContext:           client.Dialer.DialContext,
		MaxIdleConns:          100,
		IdleConnTimeout:       90 * time.Second,
		TLSHandshakeTimeout:   10 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
	}

	return client
}
func (c *Client) Acquire(rtype, state, dest string) (*common.Resource, error) {
	r, err := c.acquire(rtype, state, dest)
	if err != nil {
		return nil, err
	}
	c.lock.Lock()
	defer c.lock.Unlock()
	if r != nil {
		c.storage.Add(*r)
	}

	return r, nil
}
func (c *Client) AcquireWait(ctx context.Context, rtype, state, dest string) (*common.Resource, error) {
	if ctx == nil {
		return nil, ErrContextRequired
	}
	// Try to acquire the resource until available or the context is
	// cancelled or its deadline exceeded.
	for {
		r, err := c.Acquire(rtype, state, dest)
		if err != nil {
			if err == ErrAlreadyInUse || err == ErrNotFound {
				select {
				case <-ctx.Done():
					return nil, err
				case <-time.After(3 * time.Second):
					continue
				}
			}
			return nil, err
		}
		return r, nil
	}
}
func (c *Client) AcquireByState(state, dest string, names []string) ([]common.Resource, error) {
	resources, err := c.acquireByState(state, dest, names)
	if err != nil {
		return nil, err
	}
	c.lock.Lock()
	defer c.lock.Unlock()
	for _, r := range resources {
		c.storage.Add(r)
	}
	return resources, nil
}
func (c *Client) ReleaseAll(dest string) error {
	c.lock.Lock()
	defer c.lock.Unlock()
	resources, err := c.storage.List()
	if err != nil {
		return err
	}
	if len(resources) == 0 {
		return fmt.Errorf("no holding resource")
	}
	var allErrors error
	for _, r := range resources {
		c.storage.Delete(r.GetName())
		err := c.release(r.GetName(), dest)
		if err != nil {
			allErrors = multierror.Append(allErrors, err)
		}
	}
	return allErrors
}
func (c *Client) ReleaseOne(name, dest string) error {
	c.lock.Lock()
	defer c.lock.Unlock()

	if _, err := c.storage.Get(name); err != nil {
		return fmt.Errorf("no resource name %v", name)
	}
	c.storage.Delete(name)
	if err := c.release(name, dest); err != nil {
		return err
	}
	return nil
}
func (c *Client) UpdateAll(state string) error {
	c.lock.Lock()
	defer c.lock.Unlock()

	resources, err := c.storage.List()
	if err != nil {
		return err
	}
	if len(resources) == 0 {
		return fmt.Errorf("no holding resource")
	}
	var allErrors error
	for _, r := range resources {
		if err := c.update(r.GetName(), state, nil); err != nil {
			allErrors = multierror.Append(allErrors, err)
			continue
		}
		if err := c.updateLocalResource(r, state, nil); err != nil {
			allErrors = multierror.Append(allErrors, err)
		}
	}
	return allErrors
}
func (c *Client) SyncAll() error {
	c.lock.Lock()
	defer c.lock.Unlock()

	resources, err := c.storage.List()
	if err != nil {
		return err
	}
	if len(resources) == 0 {
		logrus.Info("no resource to sync")
		return nil
	}
	var allErrors error
	for _, i := range resources {
		r, err := common.ItemToResource(i)
		if err != nil {
			allErrors = multierror.Append(allErrors, err)
			continue
		}
		if err := c.update(r.Name, r.State, nil); err != nil {
			allErrors = multierror.Append(allErrors, err)
			continue
		}
		if err := c.storage.Update(r); err != nil {
			allErrors = multierror.Append(allErrors, err)
		}
	}
	return allErrors
}
func (c *Client) UpdateOne(name, state string, userData *common.UserData) error {
	c.lock.Lock()
	defer c.lock.Unlock()

	r, err := c.storage.Get(name)
	if err != nil {
		return fmt.Errorf("no resource name %v", name)
	}
	if err := c.update(r.GetName(), state, userData); err != nil {
		return err
	}
	return c.updateLocalResource(r, state, userData)
}
func (c *Client) Metric(rtype string) (common.Metric, error) {
	return c.metric(rtype)
}
func (c *Client) HasResource() bool {
	resources, _ := c.storage.List()
	return len(resources) > 0
}
func (d *DialerWithRetry) DialContext(ctx context.Context, network, address string) (net.Conn, error) {
	// Always bump the retry count by 1 in order to equal the actual number of
	// attempts. For example, if a retry count of 2 is specified, the intent
	// is for three attempts -- the initial attempt with two retries in case
	// the initial attempt times out.
	count := d.RetryCount + 1
	sleep := d.RetrySleep
	i := uint(0)
	for {
		conn, err := d.Dialer.DialContext(ctx, network, address)
		if err != nil {
			if isDialErrorRetriable(err) {
				if i < count-1 {
					select {
					case <-time.After(sleep):
						i++
						continue
					case <-ctx.Done():
						return nil, err
					}
				}
			}
			return nil, err
		}
		return conn, nil
	}
}
func NewDashboardAgent(repos []string, config *config.GitHubOAuthConfig, log *logrus.Entry) *DashboardAgent {
	return &DashboardAgent{
		repos: repos,
		goac:  config,
		log:   log,
	}
}
func (da *DashboardAgent) QueryPullRequests(ctx context.Context, ghc githubClient, query string) ([]PullRequest, error) {
	var prs []PullRequest
	vars := map[string]interface{}{
		"query":        (githubql.String)(query),
		"searchCursor": (*githubql.String)(nil),
	}
	var totalCost int
	var remaining int
	for {
		sq := searchQuery{}
		if err := ghc.Query(ctx, &sq, vars); err != nil {
			return nil, err
		}
		totalCost += int(sq.RateLimit.Cost)
		remaining = int(sq.RateLimit.Remaining)
		for _, n := range sq.Search.Nodes {
			prs = append(prs, n.PullRequest)
		}
		if !sq.Search.PageInfo.HasNextPage {
			break
		}
		vars["searchCursor"] = githubql.NewString(sq.Search.PageInfo.EndCursor)
	}
	da.log.Infof("Search for query \"%s\" cost %d point(s). %d remaining.", query, totalCost, remaining)
	return prs, nil
}
func (da *DashboardAgent) GetHeadContexts(ghc githubClient, pr PullRequest) ([]Context, error) {
	org := string(pr.Repository.Owner.Login)
	repo := string(pr.Repository.Name)
	combined, err := ghc.GetCombinedStatus(org, repo, string(pr.HeadRefOID))
	if err != nil {
		return nil, fmt.Errorf("failed to get the combined status: %v", err)
	}
	contexts := make([]Context, 0, len(combined.Statuses))
	for _, status := range combined.Statuses {
		contexts = append(
			contexts,
			Context{
				Context:     status.Context,
				Description: status.Description,
				State:       strings.ToUpper(status.State),
			},
		)
	}
	return contexts, nil
}
func (da *DashboardAgent) ConstructSearchQuery(login string) string {
	tokens := []string{"is:pr", "state:open", "author:" + login}
	for i := range da.repos {
		tokens = append(tokens, fmt.Sprintf("repo:\"%s\"", da.repos[i]))
	}
	return strings.Join(tokens, " ")
}
func NewBundledStates(description string) BundledStates {
	return BundledStates{
		description: description,
		states:      map[string]State{},
	}
}
func (b BundledStates) ReceiveEvent(ID string, eventName, label string, t time.Time) bool {
	state, ok := b.states[ID]
	if !ok {
		state = NewState(b.description)
	}
	state, changed := state.ReceiveEvent(eventName, label, t)
	b.states[ID] = state
	return changed
}
func (b BundledStates) ages(t time.Time) map[string]time.Duration {
	ages := map[string]time.Duration{}

	for id, state := range b.states {
		if !state.Active() {
			continue
		}
		ages[id] = state.Age(t)
	}
	return ages
}
func (b BundledStates) Percentile(t time.Time, percentile int) time.Duration {
	if percentile > 100 || percentile <= 0 {
		panic(fmt.Errorf("percentile %d is out of scope", percentile))
	}

	ages := []time.Duration{}
	for _, age := range b.ages(t) {
		ages = append(ages, age)
	}

	if len(ages) == 0 {
		return 0
	}

	sort.Sort(ByDuration(ages))

	index := int(math.Ceil(float64(percentile)*float64(len(ages))/100) - 1)
	if index >= len(ages) {
		panic(fmt.Errorf("Index is out of range: %d/%d", index, len(ages)))
	}
	return ages[index]
}
func NewMetrics() *Metrics {
	return &Metrics{
		ClientMetrics: &ClientMetrics{
			Requests:       requests,
			RequestRetries: requestRetries,
			RequestLatency: requestLatency,
		},
		ResyncPeriod: resyncPeriod,
	}
}
func NewDiskCache(delegate http.RoundTripper, cacheDir string, cacheSizeGB, maxConcurrency int) http.RoundTripper {
	return NewFromCache(delegate, diskcache.NewWithDiskv(
		diskv.New(diskv.Options{
			BasePath:     path.Join(cacheDir, "data"),
			TempDir:      path.Join(cacheDir, "temp"),
			CacheSizeMax: uint64(cacheSizeGB) * uint64(1000000000), // convert G to B
		})),
		maxConcurrency,
	)
}
func NewMemCache(delegate http.RoundTripper, maxConcurrency int) http.RoundTripper {
	return NewFromCache(delegate, httpcache.NewMemoryCache(), maxConcurrency)
}
func NewFromCache(delegate http.RoundTripper, cache httpcache.Cache, maxConcurrency int) http.RoundTripper {
	cacheTransport := httpcache.NewTransport(cache)
	cacheTransport.Transport = newThrottlingTransport(maxConcurrency, upstreamTransport{delegate: delegate})
	return &requestCoalescer{
		keys:     make(map[string]*responseWaiter),
		delegate: cacheTransport,
	}
}
func (c *Clientset) ProwV1() prowv1.ProwV1Interface {
	return &fakeprowv1.FakeProwV1{Fake: &c.Fake}
}
func (c *Clientset) Prow() prowv1.ProwV1Interface {
	return &fakeprowv1.FakeProwV1{Fake: &c.Fake}
}
func NewOwners(log *logrus.Entry, filenames []string, r Repo, s int64) Owners {
	return Owners{filenames: filenames, repo: r, seed: s, log: log}
}
func (o Owners) GetApprovers() map[string]sets.String {
	ownersToApprovers := map[string]sets.String{}

	for fn := range o.GetOwnersSet() {
		ownersToApprovers[fn] = o.repo.Approvers(fn)
	}

	return ownersToApprovers
}
func (o Owners) GetAllPotentialApprovers() []string {
	approversOnly := []string{}
	for _, approverList := range o.GetLeafApprovers() {
		for approver := range approverList {
			approversOnly = append(approversOnly, approver)
		}
	}
	sort.Strings(approversOnly)
	if len(approversOnly) == 0 {
		o.log.Debug("No potential approvers exist. Does the repo have OWNERS files?")
	}
	return approversOnly
}
func (o Owners) GetReverseMap(approvers map[string]sets.String) map[string]sets.String {
	approverOwnersfiles := map[string]sets.String{}
	for ownersFile, approvers := range approvers {
		for approver := range approvers {
			if _, ok := approverOwnersfiles[approver]; ok {
				approverOwnersfiles[approver].Insert(ownersFile)
			} else {
				approverOwnersfiles[approver] = sets.NewString(ownersFile)
			}
		}
	}
	return approverOwnersfiles
}
func (o Owners) temporaryUnapprovedFiles(approvers sets.String) sets.String {
	ap := NewApprovers(o)
	for approver := range approvers {
		ap.AddApprover(approver, "", false)
	}
	return ap.UnapprovedFiles()
}
func (o Owners) KeepCoveringApprovers(reverseMap map[string]sets.String, knownApprovers sets.String, potentialApprovers []string) sets.String {
	if len(potentialApprovers) == 0 {
		o.log.Debug("No potential approvers exist to filter for relevance. Does this repo have OWNERS files?")
	}
	keptApprovers := sets.NewString()

	unapproved := o.temporaryUnapprovedFiles(knownApprovers)

	for _, suggestedApprover := range o.GetSuggestedApprovers(reverseMap, potentialApprovers).List() {
		if reverseMap[suggestedApprover].Intersection(unapproved).Len() != 0 {
			keptApprovers.Insert(suggestedApprover)
		}
	}

	return keptApprovers
}
func (o Owners) GetSuggestedApprovers(reverseMap map[string]sets.String, potentialApprovers []string) sets.String {
	ap := NewApprovers(o)
	for !ap.RequirementsMet() {
		newApprover := findMostCoveringApprover(potentialApprovers, reverseMap, ap.UnapprovedFiles())
		if newApprover == "" {
			o.log.Warnf("Couldn't find/suggest approvers for each files. Unapproved: %q", ap.UnapprovedFiles().List())
			return ap.GetCurrentApproversSet()
		}
		ap.AddApprover(newApprover, "", false)
	}

	return ap.GetCurrentApproversSet()
}
func (o Owners) GetOwnersSet() sets.String {
	owners := sets.NewString()
	for _, fn := range o.filenames {
		owners.Insert(o.repo.FindApproverOwnersForFile(fn))
	}
	o.removeSubdirs(owners)
	return owners
}
func (o Owners) GetShuffledApprovers() []string {
	approversList := o.GetAllPotentialApprovers()
	order := rand.New(rand.NewSource(o.seed)).Perm(len(approversList))
	people := make([]string, 0, len(approversList))
	for _, i := range order {
		people = append(people, approversList[i])
	}
	return people
}
func (a Approval) String() string {
	return fmt.Sprintf(
		`*<a href="%s" title="%s">%s</a>*`,
		a.Reference,
		a.How,
		a.Login,
	)
}
func IntersectSetsCase(one, other sets.String) sets.String {
	lower := sets.NewString()
	for item := range other {
		lower.Insert(strings.ToLower(item))
	}

	intersection := sets.NewString()
	for item := range one {
		if lower.Has(strings.ToLower(item)) {
			intersection.Insert(item)
		}
	}
	return intersection
}
func NewApprovers(owners Owners) Approvers {
	return Approvers{
		owners:    owners,
		approvers: map[string]Approval{},
		assignees: sets.NewString(),

		ManuallyApproved: func() bool {
			return false
		},
	}
}
func (ap *Approvers) AddLGTMer(login, reference string, noIssue bool) {
	if ap.shouldNotOverrideApproval(login, noIssue) {
		return
	}
	ap.approvers[strings.ToLower(login)] = Approval{
		Login:     login,
		How:       "LGTM",
		Reference: reference,
		NoIssue:   noIssue,
	}
}
func (ap *Approvers) RemoveApprover(login string) {
	delete(ap.approvers, strings.ToLower(login))
}
func (ap *Approvers) AddAssignees(logins ...string) {
	for _, login := range logins {
		ap.assignees.Insert(strings.ToLower(login))
	}
}
func (ap Approvers) GetCurrentApproversSetCased() sets.String {
	currentApprovers := sets.NewString()

	for _, approval := range ap.approvers {
		currentApprovers.Insert(approval.Login)
	}

	return currentApprovers
}
func (ap Approvers) GetFilesApprovers() map[string]sets.String {
	filesApprovers := map[string]sets.String{}
	currentApprovers := ap.GetCurrentApproversSetCased()
	for fn, potentialApprovers := range ap.owners.GetApprovers() {
		// The order of parameter matters here:
		// - currentApprovers is the list of github handles that have approved
		// - potentialApprovers is the list of handles in the OWNER
		// files (lower case).
		//
		// We want to keep the syntax of the github handle
		// rather than the potential mis-cased username found in
		// the OWNERS file, that's why it's the first parameter.
		filesApprovers[fn] = IntersectSetsCase(currentApprovers, potentialApprovers)
	}

	return filesApprovers
}
func (ap Approvers) NoIssueApprovers() map[string]Approval {
	nia := map[string]Approval{}
	reverseMap := ap.owners.GetReverseMap(ap.owners.GetApprovers())

	for login, approver := range ap.approvers {
		if !approver.NoIssue {
			continue
		}

		if len(reverseMap[login]) == 0 {
			continue
		}

		nia[login] = approver
	}

	return nia
}
func (ap Approvers) UnapprovedFiles() sets.String {
	unapproved := sets.NewString()
	for fn, approvers := range ap.GetFilesApprovers() {
		if len(approvers) == 0 {
			unapproved.Insert(fn)
		}
	}
	return unapproved
}
func (ap Approvers) GetFiles(baseURL *url.URL, branch string) []File {
	allOwnersFiles := []File{}
	filesApprovers := ap.GetFilesApprovers()
	for _, file := range ap.owners.GetOwnersSet().List() {
		if len(filesApprovers[file]) == 0 {
			allOwnersFiles = append(allOwnersFiles, UnapprovedFile{
				baseURL:  baseURL,
				filepath: file,
				branch:   branch,
			})
		} else {
			allOwnersFiles = append(allOwnersFiles, ApprovedFile{
				baseURL:   baseURL,
				filepath:  file,
				approvers: filesApprovers[file],
				branch:    branch,
			})
		}
	}

	return allOwnersFiles
}
func (ap Approvers) IsApproved() bool {
	reqsMet := ap.RequirementsMet()
	if !reqsMet && ap.ManuallyApproved() {
		return true
	}
	return reqsMet
}
func (ap Approvers) ListApprovals() []Approval {
	approvals := []Approval{}

	for _, approver := range ap.GetCurrentApproversSet().List() {
		approvals = append(approvals, ap.approvers[approver])
	}

	return approvals
}
func (ap Approvers) ListNoIssueApprovals() []Approval {
	approvals := []Approval{}

	for _, approver := range ap.GetNoIssueApproversSet().List() {
		approvals = append(approvals, ap.approvers[approver])
	}

	return approvals
}
func GenerateTemplate(templ, name string, data interface{}) (string, error) {
	buf := bytes.NewBufferString("")
	if messageTempl, err := template.New(name).Parse(templ); err != nil {
		return "", fmt.Errorf("failed to parse template for %s: %v", name, err)
	} else if err := messageTempl.Execute(buf, data); err != nil {
		return "", fmt.Errorf("failed to execute template for %s: %v", name, err)
	}
	return buf.String(), nil
}
func writeTemplate(templatePath string, outputPath string, data interface{}) error {
	// set up template
	funcMap := template.FuncMap{
		"anchor": func(input string) string {
			return strings.Replace(input, ":", " ", -1)
		},
	}
	t, err := template.New(filepath.Base(templatePath)).Funcs(funcMap).ParseFiles(templatePath)
	if err != nil {
		return err
	}

	// ensure output path exists
	if !pathExists(outputPath) {
		_, err = os.Create(outputPath)
		if err != nil {
			return err
		}
	}

	// open file at output path and truncate
	f, err := os.OpenFile(outputPath, os.O_RDWR, 0644)
	if err != nil {
		return err
	}
	defer f.Close()
	f.Truncate(0)

	// render template to output path
	err = t.Execute(f, data)
	if err != nil {
		return err
	}

	return nil
}
func (c Configuration) Labels() []Label {
	var labelarrays [][]Label
	labelarrays = append(labelarrays, c.Default.Labels)
	for _, repo := range c.Repos {
		labelarrays = append(labelarrays, repo.Labels)
	}

	labelmap := make(map[string]Label)
	for _, labels := range labelarrays {
		for _, l := range labels {
			name := strings.ToLower(l.Name)
			if _, ok := labelmap[name]; !ok {
				labelmap[name] = l
			}
		}
	}

	var labels []Label
	for _, label := range labelmap {
		labels = append(labels, label)
	}
	sort.Slice(labels, func(i, j int) bool { return labels[i].Name < labels[j].Name })
	return labels
}
func LabelsForTarget(labels []Label, target LabelTarget) (filteredLabels []Label) {
	for _, label := range labels {
		if target == label.Target {
			filteredLabels = append(filteredLabels, label)
		}
	}
	// We also sort to make nice tables
	sort.Slice(filteredLabels, func(i, j int) bool { return filteredLabels[i].Name < filteredLabels[j].Name })
	return
}
func LoadConfig(path string, orgs string) (*Configuration, error) {
	if path == "" {
		return nil, errors.New("empty path")
	}
	var c Configuration
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}
	if err = yaml.Unmarshal(data, &c); err != nil {
		return nil, err
	}
	if err = c.validate(orgs); err != nil { // Ensure no dups
		return nil, err
	}
	return &c, nil
}
func loadLabels(gc client, org string, repos []string) (*RepoLabels, error) {
	repoChan := make(chan string, len(repos))
	for _, repo := range repos {
		repoChan <- repo
	}
	close(repoChan)

	wg := sync.WaitGroup{}
	wg.Add(maxConcurrentWorkers)
	labels := make(chan RepoLabels, len(repos))
	errChan := make(chan error, len(repos))
	for i := 0; i < maxConcurrentWorkers; i++ {
		go func(repositories <-chan string) {
			defer wg.Done()
			for repository := range repositories {
				logrus.WithField("org", org).WithField("repo", repository).Info("Listing labels for repo")
				repoLabels, err := gc.GetRepoLabels(org, repository)
				if err != nil {
					logrus.WithField("org", org).WithField("repo", repository).Error("Failed listing labels for repo")
					errChan <- err
				}
				labels <- RepoLabels{repository: repoLabels}
			}
		}(repoChan)
	}

	wg.Wait()
	close(labels)
	close(errChan)

	rl := RepoLabels{}
	for data := range labels {
		for repo, repoLabels := range data {
			rl[repo] = repoLabels
		}
	}

	var overallErr error
	if len(errChan) > 0 {
		var listErrs []error
		for listErr := range errChan {
			listErrs = append(listErrs, listErr)
		}
		overallErr = fmt.Errorf("failed to list labels: %v", listErrs)
	}

	return &rl, overallErr
}
func kill(repo string, label Label) Update {
	logrus.WithField("repo", repo).WithField("label", label.Name).Info("kill")
	return Update{Why: "dead", Current: &label, repo: repo}
}
func create(repo string, label Label) Update {
	logrus.WithField("repo", repo).WithField("label", label.Name).Info("create")
	return Update{Why: "missing", Wanted: &label, repo: repo}
}
func classifyLabels(labels []Label, required, archaic, dead map[string]Label, now time.Time, parent *Label) (map[string]Label, map[string]Label, map[string]Label) {
	newRequired := copyLabelMap(required)
	newArchaic := copyLabelMap(archaic)
	newDead := copyLabelMap(dead)
	for i, l := range labels {
		first := parent
		if first == nil {
			first = &labels[i]
		}
		lower := strings.ToLower(l.Name)
		switch {
		case parent == nil && l.DeleteAfter == nil: // Live label
			newRequired[lower] = l
		case l.DeleteAfter != nil && now.After(*l.DeleteAfter):
			newDead[lower] = l
		case parent != nil:
			l.parent = parent
			newArchaic[lower] = l
		}
		newRequired, newArchaic, newDead = classifyLabels(l.Previously, newRequired, newArchaic, newDead, now, first)
	}
	return newRequired, newArchaic, newDead
}
func linkify(text string) string {
	// swap space with dash
	link := strings.Replace(text, " ", "-", -1)
	// discard some special characters
	discard, _ := regexp.Compile("[,/]")
	link = discard.ReplaceAllString(link, "")
	// lowercase
	return strings.ToLower(link)
}
func NewCache(diskRoot string) *Cache {
	return &Cache{
		diskRoot: strings.TrimSuffix(diskRoot, string(os.PathListSeparator)),
	}
}
func (c *Cache) KeyToPath(key string) string {
	return filepath.Join(c.diskRoot, key)
}
func ensureDir(dir string) error {
	if exists(dir) {
		return nil
	}
	return os.MkdirAll(dir, os.FileMode(0744))
}
func (c *Cache) Put(key string, content io.Reader, contentSHA256 string) error {
	// make sure directory exists
	path := c.KeyToPath(key)
	dir := filepath.Dir(path)
	err := ensureDir(dir)
	if err != nil {
		logrus.WithError(err).Errorf("error ensuring directory '%s' exists", dir)
	}

	// create a temp file to get the content on disk
	temp, err := ioutil.TempFile(dir, "temp-put")
	if err != nil {
		return fmt.Errorf("failed to create cache entry: %v", err)
	}

	// fast path copying when not hashing content,s
	if contentSHA256 == "" {
		_, err = io.Copy(temp, content)
		if err != nil {
			removeTemp(temp.Name())
			return fmt.Errorf("failed to copy into cache entry: %v", err)
		}

	} else {
		hasher := sha256.New()
		_, err = io.Copy(io.MultiWriter(temp, hasher), content)
		if err != nil {
			removeTemp(temp.Name())
			return fmt.Errorf("failed to copy into cache entry: %v", err)
		}
		actualContentSHA256 := hex.EncodeToString(hasher.Sum(nil))
		if actualContentSHA256 != contentSHA256 {
			removeTemp(temp.Name())
			return fmt.Errorf(
				"hashes did not match for '%s', given: '%s' actual: '%s",
				key, contentSHA256, actualContentSHA256)
		}
	}

	// move the content to the key location
	err = temp.Sync()
	if err != nil {
		removeTemp(temp.Name())
		return fmt.Errorf("failed to sync cache entry: %v", err)
	}
	temp.Close()
	err = os.Rename(temp.Name(), path)
	if err != nil {
		removeTemp(temp.Name())
		return fmt.Errorf("failed to insert contents into cache: %v", err)
	}
	return nil
}
func (c *Cache) Get(key string, readHandler ReadHandler) error {
	path := c.KeyToPath(key)
	f, err := os.Open(path)
	if err != nil {
		if os.IsNotExist(err) {
			return readHandler(false, nil)
		}
		return fmt.Errorf("failed to get key: %v", err)
	}
	return readHandler(true, f)
}
func (c *Cache) Delete(key string) error {
	return os.Remove(c.KeyToPath(key))
}
func NewGCSArtifact(ctx context.Context, handle artifactHandle, link string, path string, sizeLimit int64) *GCSArtifact {
	return &GCSArtifact{
		handle:    handle,
		link:      link,
		path:      path,
		sizeLimit: sizeLimit,
		ctx:       ctx,
	}
}
func (a *GCSArtifact) Size() (int64, error) {
	attrs, err := a.handle.Attrs(a.ctx)
	if err != nil {
		return 0, fmt.Errorf("error getting gcs attributes for artifact: %v", err)
	}
	return attrs.Size, nil
}
func (a *GCSArtifact) ReadAll() ([]byte, error) {
	size, err := a.Size()
	if err != nil {
		return nil, fmt.Errorf("error getting artifact size: %v", err)
	}
	if size > a.sizeLimit {
		return nil, lenses.ErrFileTooLarge
	}
	reader, err := a.handle.NewReader(a.ctx)
	if err != nil {
		return nil, fmt.Errorf("error getting artifact reader: %v", err)
	}
	defer reader.Close()
	p, err := ioutil.ReadAll(reader)
	if err != nil {
		return nil, fmt.Errorf("error reading all from artifact: %v", err)
	}
	return p, nil
}
func (a *GCSArtifact) ReadTail(n int64) ([]byte, error) {
	gzipped, err := a.gzipped()
	if err != nil {
		return nil, fmt.Errorf("error checking artifact for gzip compression: %v", err)
	}
	if gzipped {
		return nil, lenses.ErrGzipOffsetRead
	}
	size, err := a.Size()
	if err != nil {
		return nil, fmt.Errorf("error getting artifact size: %v", err)
	}
	var offset int64
	if n >= size {
		offset = 0
	} else {
		offset = size - n
	}
	reader, err := a.handle.NewRangeReader(a.ctx, offset, -1)
	defer reader.Close()
	if err != nil && err != io.EOF {
		return nil, fmt.Errorf("error getting artifact reader: %v", err)
	}
	read, err := ioutil.ReadAll(reader)
	if err != nil {
		return nil, fmt.Errorf("error reading all from artiact: %v", err)
	}
	return read, nil
}
func (a *GCSArtifact) gzipped() (bool, error) {
	attrs, err := a.handle.Attrs(a.ctx)
	if err != nil {
		return false, fmt.Errorf("error getting gcs attributes for artifact: %v", err)
	}
	return attrs.ContentEncoding == "gzip", nil
}
func optionsForRepo(config *plugins.Configuration, org, repo string) *plugins.Welcome {
	fullName := fmt.Sprintf("%s/%s", org, repo)

	// First search for repo config
	for _, c := range config.Welcome {
		if !strInSlice(fullName, c.Repos) {
			continue
		}
		return &c
	}

	// If you don't find anything, loop again looking for an org config
	for _, c := range config.Welcome {
		if !strInSlice(org, c.Repos) {
			continue
		}
		return &c
	}

	// Return an empty config, and default to defaultWelcomeMessage
	return &plugins.Welcome{}
}
func (s *prowJobLister) List(selector labels.Selector) (ret []*v1.ProwJob, err error) {
	err = cache.ListAll(s.indexer, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.ProwJob))
	})
	return ret, err
}
func (s *prowJobLister) ProwJobs(namespace string) ProwJobNamespaceLister {
	return prowJobNamespaceLister{indexer: s.indexer, namespace: namespace}
}
func (s prowJobNamespaceLister) List(selector labels.Selector) (ret []*v1.ProwJob, err error) {
	err = cache.ListAllByNamespace(s.indexer, s.namespace, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.ProwJob))
	})
	return ret, err
}
func (br Brancher) RunsAgainstAllBranch() bool {
	return len(br.SkipBranches) == 0 && len(br.Branches) == 0
}
func (br Brancher) Intersects(other Brancher) bool {
	if br.RunsAgainstAllBranch() || other.RunsAgainstAllBranch() {
		return true
	}
	if len(br.Branches) > 0 {
		baseBranches := sets.NewString(br.Branches...)
		if len(other.Branches) > 0 {
			otherBranches := sets.NewString(other.Branches...)
			if baseBranches.Intersection(otherBranches).Len() > 0 {
				return true
			}
			return false
		}

		// Actually test our branches against the other brancher - if there are regex skip lists, simple comparison
		// is insufficient.
		for _, b := range baseBranches.List() {
			if other.ShouldRun(b) {
				return true
			}
		}
		return false
	}
	if len(other.Branches) == 0 {
		// There can only be one Brancher with skip_branches.
		return true
	}
	return other.Intersects(br)
}
func (cm RegexpChangeMatcher) ShouldRun(changes ChangedFilesProvider) (determined bool, shouldRun bool, err error) {
	if cm.CouldRun() {
		changeList, err := changes()
		if err != nil {
			return true, false, err
		}
		return true, cm.RunsAgainstChanges(changeList), nil
	}
	return false, false, nil
}
func (cm RegexpChangeMatcher) RunsAgainstChanges(changes []string) bool {
	for _, change := range changes {
		if cm.reChanges.MatchString(change) {
			return true
		}
	}
	return false
}
func (ps Postsubmit) CouldRun(baseRef string) bool {
	return ps.Brancher.ShouldRun(baseRef)
}
func (ps Postsubmit) ShouldRun(baseRef string, changes ChangedFilesProvider) (bool, error) {
	if !ps.CouldRun(baseRef) {
		return false, nil
	}
	if determined, shouldRun, err := ps.RegexpChangeMatcher.ShouldRun(changes); err != nil {
		return false, err
	} else if determined {
		return shouldRun, nil
	}
	// Postsubmits default to always run
	return true, nil
}
func (ps Presubmit) CouldRun(baseRef string) bool {
	return ps.Brancher.ShouldRun(baseRef)
}
func (ps Presubmit) ShouldRun(baseRef string, changes ChangedFilesProvider, forced, defaults bool) (bool, error) {
	if !ps.CouldRun(baseRef) {
		return false, nil
	}
	if ps.AlwaysRun {
		return true, nil
	}
	if forced {
		return true, nil
	}
	if determined, shouldRun, err := ps.RegexpChangeMatcher.ShouldRun(changes); err != nil {
		return false, err
	} else if determined {
		return shouldRun, nil
	}
	return defaults, nil
}
func (c *JobConfig) GetPresubmit(repo, jobName string) *Presubmit {
	presubmits := c.AllPresubmits([]string{repo})
	for i := range presubmits {
		ps := presubmits[i]
		if ps.Name == jobName {
			return &ps
		}
	}
	return nil
}
func (c *JobConfig) SetPresubmits(jobs map[string][]Presubmit) error {
	nj := map[string][]Presubmit{}
	for k, v := range jobs {
		nj[k] = make([]Presubmit, len(v))
		copy(nj[k], v)
		if err := SetPresubmitRegexes(nj[k]); err != nil {
			return err
		}
	}
	c.Presubmits = nj
	return nil
}
func (c *JobConfig) SetPostsubmits(jobs map[string][]Postsubmit) error {
	nj := map[string][]Postsubmit{}
	for k, v := range jobs {
		nj[k] = make([]Postsubmit, len(v))
		copy(nj[k], v)
		if err := SetPostsubmitRegexes(nj[k]); err != nil {
			return err
		}
	}
	c.Postsubmits = nj
	return nil
}
func (c *JobConfig) AllPresubmits(repos []string) []Presubmit {
	var res []Presubmit

	for repo, v := range c.Presubmits {
		if len(repos) == 0 {
			res = append(res, v...)
		} else {
			for _, r := range repos {
				if r == repo {
					res = append(res, v...)
					break
				}
			}
		}
	}

	return res
}
func (c *JobConfig) AllPostsubmits(repos []string) []Postsubmit {
	var res []Postsubmit

	for repo, v := range c.Postsubmits {
		if len(repos) == 0 {
			res = append(res, v...)
		} else {
			for _, r := range repos {
				if r == repo {
					res = append(res, v...)
					break
				}
			}
		}
	}

	return res
}
func (c *JobConfig) AllPeriodics() []Periodic {
	var listPeriodic func(ps []Periodic) []Periodic
	listPeriodic = func(ps []Periodic) []Periodic {
		var res []Periodic
		for _, p := range ps {
			res = append(res, p)
		}
		return res
	}

	return listPeriodic(c.Periodics)
}
func ClearCompiledRegexes(presubmits []Presubmit) {
	for i := range presubmits {
		presubmits[i].re = nil
		presubmits[i].Brancher.re = nil
		presubmits[i].Brancher.reSkip = nil
		presubmits[i].RegexpChangeMatcher.reChanges = nil
	}
}
func (s *SimpleConfig) Empty() bool {
	return len(s.Approvers) == 0 && len(s.Reviewers) == 0 && len(s.RequiredReviewers) == 0 && len(s.Labels) == 0
}
func NewClient(
	gc *git.Client,
	ghc *github.Client,
	mdYAMLEnabled func(org, repo string) bool,
	skipCollaborators func(org, repo string) bool,
	ownersDirBlacklist func() prowConf.OwnersDirBlacklist,
) *Client {
	return &Client{
		git:    gc,
		ghc:    ghc,
		logger: logrus.WithField("client", "repoowners"),
		cache:  make(map[string]cacheEntry),

		mdYAMLEnabled:      mdYAMLEnabled,
		skipCollaborators:  skipCollaborators,
		ownersDirBlacklist: ownersDirBlacklist,
	}
}
func (a RepoAliases) ExpandAlias(alias string) sets.String {
	if a == nil {
		return nil
	}
	return a[github.NormLogin(alias)]
}
func (a RepoAliases) ExpandAliases(logins sets.String) sets.String {
	if a == nil {
		return logins
	}
	// Make logins a copy of the original set to avoid modifying the original.
	logins = logins.Union(nil)
	for _, login := range logins.List() {
		if expanded := a.ExpandAlias(login); len(expanded) > 0 {
			logins.Delete(login)
			logins = logins.Union(expanded)
		}
	}
	return logins
}
func ParseFullConfig(b []byte) (FullConfig, error) {
	full := new(FullConfig)
	err := yaml.Unmarshal(b, full)
	return *full, err
}
func ParseSimpleConfig(b []byte) (SimpleConfig, error) {
	simple := new(SimpleConfig)
	err := yaml.Unmarshal(b, simple)
	return *simple, err
}
func decodeOwnersMdConfig(path string, config *SimpleConfig) error {
	fileBytes, err := ioutil.ReadFile(path)
	if err != nil {
		return err
	}
	// Parse the yaml header from the top of the file.  Will return an empty string if regex does not match.
	meta := mdStructuredHeaderRegex.FindString(string(fileBytes))

	// Unmarshal the yaml header into the config
	return yaml.Unmarshal([]byte(meta), &config)
}
func findOwnersForFile(log *logrus.Entry, path string, ownerMap map[string]map[*regexp.Regexp]sets.String) string {
	d := path

	for ; d != baseDirConvention; d = canonicalize(filepath.Dir(d)) {
		relative, err := filepath.Rel(d, path)
		if err != nil {
			log.WithError(err).WithField("path", path).Errorf("Unable to find relative path between %q and path.", d)
			return ""
		}
		for re, n := range ownerMap[d] {
			if re != nil && !re.MatchString(relative) {
				continue
			}
			if len(n) != 0 {
				return d
			}
		}
	}
	return ""
}
func (o *RepoOwners) FindApproverOwnersForFile(path string) string {
	return findOwnersForFile(o.log, path, o.approvers)
}
func (o *RepoOwners) FindReviewersOwnersForFile(path string) string {
	return findOwnersForFile(o.log, path, o.reviewers)
}
func (o *RepoOwners) FindLabelsForFile(path string) sets.String {
	return o.entriesForFile(path, o.labels, false)
}
func (o *RepoOwners) IsNoParentOwners(path string) bool {
	return o.options[path].NoParentOwners
}
func (c *Coverage) Ratio() float32 {
	if c.NumAllStmts == 0 {
		return 1
	}
	return float32(c.NumCoveredStmts) / float32(c.NumAllStmts)
}
func (pe *PeriodicProwJobEvent) FromPayload(data []byte) error {
	if err := json.Unmarshal(data, pe); err != nil {
		return err
	}
	return nil
}
func (pe *PeriodicProwJobEvent) ToMessage() (*pubsub.Message, error) {
	data, err := json.Marshal(pe)
	if err != nil {
		return nil, err
	}
	message := pubsub.Message{
		Data: data,
		Attributes: map[string]string{
			prowEventType: periodicProwJobEvent,
		},
	}
	return &message, nil
}
func (p *Privacy) UnmarshalText(text []byte) error {
	v := Privacy(text)
	if _, ok := privacySettings[v]; !ok {
		return fmt.Errorf("bad privacy setting: %s", v)
	}
	*p = v
	return nil
}
func compileApplicableBlockades(org, repo string, log *logrus.Entry, blockades []plugins.Blockade) []blockade {
	if len(blockades) == 0 {
		return nil
	}

	orgRepo := fmt.Sprintf("%s/%s", org, repo)
	var compiled []blockade
	for _, raw := range blockades {
		// Only consider blockades that apply to this repo.
		if !stringInSlice(org, raw.Repos) && !stringInSlice(orgRepo, raw.Repos) {
			continue
		}
		b := blockade{}
		for _, str := range raw.BlockRegexps {
			if reg, err := regexp.Compile(str); err != nil {
				log.WithError(err).Errorf("Failed to compile the blockade regexp '%s'.", str)
			} else {
				b.blockRegexps = append(b.blockRegexps, reg)
			}
		}
		if len(b.blockRegexps) == 0 {
			continue
		}
		if raw.Explanation == "" {
			b.explanation = "Files are protected"
		} else {
			b.explanation = raw.Explanation
		}
		for _, str := range raw.ExceptionRegexps {
			if reg, err := regexp.Compile(str); err != nil {
				log.WithError(err).Errorf("Failed to compile the blockade regexp '%s'.", str)
			} else {
				b.exceptionRegexps = append(b.exceptionRegexps, reg)
			}
		}
		compiled = append(compiled, b)
	}
	return compiled
}
func calculateBlocks(changes []github.PullRequestChange, blockades []blockade) summary {
	sum := make(summary)
	for _, change := range changes {
		for _, b := range blockades {
			if b.isBlocked(change.Filename) {
				sum[b.explanation] = append(sum[b.explanation], change)
			}
		}
	}
	return sum
}
func MergeMultipleProfiles(profiles [][]*cover.Profile) ([]*cover.Profile, error) {
	if len(profiles) < 1 {
		return nil, errors.New("can't merge zero profiles")
	}
	result := profiles[0]
	for _, profile := range profiles[1:] {
		var err error
		if result, err = MergeProfiles(result, profile); err != nil {
			return nil, err
		}
	}
	return result, nil
}
func (o *Options) AddFlags(fs *flag.FlagSet) {
	fs.StringVar(&o.ProcessLog, "process-log", "", "path to the log where stdout and stderr are streamed for the process we execute")
	fs.StringVar(&o.MarkerFile, "marker-file", "", "file we write the return code of the process we execute once it has finished running")
	fs.StringVar(&o.MetadataFile, "metadata-file", "", "path to the metadata file generated from the job")
}
func (c *Controller) processNextItem() bool {
	key, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(key)

	workItem := key.(item)

	prowJob, err := c.prowJobClient.GetProwJob(workItem.prowJobId)
	if err != nil {
		c.handleErr(err, workItem)
		return true
	}
	spec := downwardapi.NewJobSpec(prowJob.Spec, prowJob.Status.BuildID, prowJob.Name)

	result := c.client.Pods(workItem.namespace).GetLogs(workItem.podName, &api.PodLogOptions{Container: workItem.containerName}).Do()
	if err := result.Error(); err != nil {
		c.handleErr(err, workItem)
		return true
	}

	// error is checked above
	log, _ := result.Raw()
	var target string
	if workItem.podName == workItem.prowJobId {
		target = path.Join(ContainerLogDir, fmt.Sprintf("%s.txt", workItem.containerName))
	} else {
		target = path.Join(ContainerLogDir, workItem.podName, fmt.Sprintf("%s.txt", workItem.containerName))
	}
	data := gcs.DataUpload(bytes.NewReader(log))
	if err := c.gcsConfig.Run(&spec, map[string]gcs.UploadFunc{target: data}); err != nil {
		c.handleErr(err, workItem)
		return true
	}
	c.queue.Forget(key)
	return true
}
func (c *Controller) handleErr(err error, key item) {
	if c.queue.NumRequeues(key) < 5 {
		glog.Infof("Error uploading logs for container %v in pod %v: %v", key.containerName, key.podName, err)
		c.queue.AddRateLimited(key)
		return
	}

	c.queue.Forget(key)
	glog.Infof("Giving up on upload of logs for container %v in pod %v: %v", key.containerName, key.podName, err)
}
func AggregateFilter(filters []Filter) Filter {
	return func(presubmit config.Presubmit) (bool, bool, bool) {
		for _, filter := range filters {
			if shouldRun, forced, defaults := filter(presubmit); shouldRun {
				return shouldRun, forced, defaults
			}
		}
		return false, false, false
	}
}
func FilterPresubmits(filter Filter, changes config.ChangedFilesProvider, branch string, presubmits []config.Presubmit, logger *logrus.Entry) ([]config.Presubmit, []config.Presubmit, error) {

	var toTrigger []config.Presubmit
	var toSkip []config.Presubmit
	for _, presubmit := range presubmits {
		matches, forced, defaults := filter(presubmit)
		if !matches {
			continue
		}
		shouldRun, err := presubmit.ShouldRun(branch, changes, forced, defaults)
		if err != nil {
			return nil, nil, err
		}
		if shouldRun {
			toTrigger = append(toTrigger, presubmit)
		} else {
			toSkip = append(toSkip, presubmit)
		}
	}

	logger.WithFields(logrus.Fields{"to-trigger": toTrigger, "to-skip": toSkip}).Debugf("Filtered %d jobs, found %d to trigger and %d to skip.", len(presubmits), len(toTrigger), len(toSkip))
	return toTrigger, toSkip, nil
}
func MakeCommand() *cobra.Command {
	flags := &flags{}
	cmd := &cobra.Command{
		Use:   "filter [file]",
		Short: "Filters a Go coverage file.",
		Long:  `Filters a Go coverage file, removing entries that do not match the given flags.`,
		Run: func(cmd *cobra.Command, args []string) {
			run(flags, cmd, args)
		},
	}
	cmd.Flags().StringVarP(&flags.OutputFile, "output", "o", "-", "output file")
	cmd.Flags().StringSliceVar(&flags.IncludePaths, "include-path", nil, "If specified at least once, only files with paths matching one of these regexes are included.")
	cmd.Flags().StringSliceVar(&flags.ExcludePaths, "exclude-path", nil, "Files with paths matching one of these regexes are excluded. Can be used repeatedly.")
	return cmd
}
func (t *EventTimeHeap) Push(x interface{}) {
	*t = append(*t, x.(sql.IssueEvent))
}
func (t *EventTimeHeap) Pop() interface{} {
	old := *t
	n := len(old)
	x := old[n-1]
	*t = old[0 : n-1]
	return x
}
func NewFakeOpenPluginWrapper(plugin Plugin) *FakeOpenPluginWrapper {
	return &FakeOpenPluginWrapper{
		plugin:      plugin,
		alreadyOpen: map[string]bool{},
	}
}
func (o *FakeOpenPluginWrapper) ReceiveIssue(issue sql.Issue) []Point {
	if _, ok := o.alreadyOpen[issue.ID]; !ok {
		// Create/Add fake "opened" events
		heap.Push(&o.openEvents, sql.IssueEvent{
			Event:          "opened",
			IssueID:        issue.ID,
			Actor:          &issue.User,
			EventCreatedAt: issue.IssueCreatedAt,
		})
		o.alreadyOpen[issue.ID] = true
	}

	return o.plugin.ReceiveIssue(issue)
}
func (o *Options) Validate() error {
	if o.SrcRoot == "" {
		return errors.New("no source root specified")
	}

	if o.Log == "" {
		return errors.New("no log file specified")
	}

	if len(o.GitRefs) == 0 {
		return errors.New("no refs specified to clone")
	}

	seen := map[string]sets.String{}
	for _, ref := range o.GitRefs {
		if _, seenOrg := seen[ref.Org]; seenOrg {
			if seen[ref.Org].Has(ref.Repo) {
				return errors.New("sync config for %s/%s provided more than once")
			}
			seen[ref.Org].Insert(ref.Repo)
		} else {
			seen[ref.Org] = sets.NewString(ref.Repo)
		}
	}

	return nil
}
func (o *Options) Complete(args []string) {
	o.GitRefs = o.refs.gitRefs
	o.KeyFiles = o.keys.data

	for _, ref := range o.GitRefs {
		alias, err := o.clonePath.Execute(OrgRepo{Org: ref.Org, Repo: ref.Repo})
		if err != nil {
			panic(err)
		}
		ref.PathAlias = alias

		alias, err = o.cloneURI.Execute(OrgRepo{Org: ref.Org, Repo: ref.Repo})
		if err != nil {
			panic(err)
		}
		ref.CloneURI = alias
	}
}
func (a *orgRepoFormat) Set(value string) error {
	templ, err := template.New("format").Parse(value)
	if err != nil {
		return err
	}
	a.raw = value
	a.format = templ
	return nil
}
func ensure(binary, install string) error {
	if _, err := exec.LookPath(binary); err != nil {
		return fmt.Errorf("%s: %s", binary, install)
	}
	return nil
}
func output(args ...string) (string, error) {
	cmd := exec.Command(args[0], args[1:]...)
	cmd.Stderr = os.Stderr
	cmd.Stdin = os.Stdin
	b, err := cmd.Output()
	return strings.TrimSpace(string(b)), err
}
func projects(max int) ([]string, error) {
	out, err := output("gcloud", "projects", "list", fmt.Sprintf("--limit=%d", max), "--format=value(project_id)")
	if err != nil {
		return nil, err
	}
	return strings.Split(out, "\n"), nil
}
func selectProject(choice string) (string, error) {
	fmt.Print("Getting active GCP account...")
	who, err := currentAccount()
	if err != nil {
		logrus.Warn("Run gcloud auth login to initialize gcloud")
		return "", err
	}
	fmt.Println(who)

	var projs []string

	if choice == "" {
		fmt.Printf("Projects available to %s:", who)
		fmt.Println()
		const max = 20
		projs, err = projects(max)
		for _, proj := range projs {
			fmt.Println("  *", proj)
		}
		if err != nil {
			return "", fmt.Errorf("list projects: %v", err)
		}
		if len(projs) == 0 {
			fmt.Println("Create a project at https://console.cloud.google.com/")
			return "", errors.New("no projects")
		}
		if len(projs) == max {
			fmt.Println("  ... Wow, that is a lot of projects!")
			fmt.Println("Type the name of any project, including ones not in this truncated list")
		}

		def, err := currentProject()
		if err != nil {
			return "", fmt.Errorf("get current project: %v", err)
		}
		fmt.Printf("Select project [%s]: ", def)
		fmt.Scanln(&choice)

		// use default project
		if choice == "" {
			return def, nil
		}
	}

	// is this a project from the list?
	for _, p := range projs {
		if p == choice {
			return choice, nil
		}
	}

	fmt.Printf("Ensuring %s has access to %s...", who, choice)
	fmt.Println()

	// no, make sure user has access to it
	if err = exec.Command("gcloud", "projects", "describe", choice).Run(); err != nil {
		return "", fmt.Errorf("%s cannot describe project: %v", who, err)
	}

	return choice, nil
}
func createCluster(proj, choice string) (*cluster, error) {
	const def = "prow"
	if choice == "" {
		fmt.Printf("Cluster name [%s]: ", def)
		fmt.Scanln(&choice)
		if choice == "" {
			choice = def
		}
	}

	cmd := exec.Command("gcloud", "container", "clusters", "create", choice)
	cmd.Stdin = os.Stdin
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Run(); err != nil {
		return nil, fmt.Errorf("create cluster: %v", err)
	}

	out, err := output("gcloud", "container", "clusters", "describe", choice, "--format=value(name,zone)")
	if err != nil {
		return nil, fmt.Errorf("describe cluster: %v", err)
	}
	parts := strings.Split(out, "\t")
	if len(parts) != 2 {
		return nil, fmt.Errorf("bad describe cluster output: %s", out)
	}

	return &cluster{name: parts[0], zone: parts[1], project: proj}, nil
}
func createContext(co contextOptions) (string, error) {
	proj, err := selectProject(co.project)
	if err != nil {
		logrus.Info("Run gcloud auth login to initialize gcloud")
		return "", fmt.Errorf("get current project: %v", err)
	}

	fmt.Printf("Existing GKE clusters in %s:", proj)
	fmt.Println()
	clusters, err := currentClusters(proj)
	if err != nil {
		return "", fmt.Errorf("list %s clusters: %v", proj, err)
	}
	for name := range clusters {
		fmt.Println("  *", name)
	}
	if len(clusters) == 0 {
		fmt.Println("  No clusters")
	}
	var choice string
	create := co.create
	reuse := co.reuse
	switch {
	case create != "" && reuse != "":
		return "", errors.New("Cannot use both --create and --reuse")
	case create != "":
		fmt.Println("Creating new " + create + " cluster...")
		choice = "new"
	case reuse != "":
		fmt.Println("Reusing existing " + reuse + " cluster...")
		choice = reuse
	default:
		fmt.Print("Get credentials for existing cluster or [create new]: ")
		fmt.Scanln(&choice)
	}

	if choice == "" || choice == "new" {
		cluster, err := createCluster(proj, create)
		if err != nil {
			return "", fmt.Errorf("create cluster in %s: %v", proj, err)
		}
		return cluster.context(), nil
	}

	cluster, ok := clusters[choice]
	if !ok {
		return "", fmt.Errorf("cluster not found: %s", choice)
	}
	cmd := exec.Command("gcloud", "container", "clusters", "get-credentials", cluster.name, "--project="+cluster.project, "--zone="+cluster.zone)
	cmd.Stdin = os.Stdin
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Run(); err != nil {
		return "", fmt.Errorf("get credentials: %v", err)
	}
	return cluster.context(), nil
}
func contextConfig() (clientcmd.ClientConfigLoader, *clientcmdapi.Config, error) {
	if err := ensureKubectl(); err != nil {
		fmt.Println("Prow's tackler requires kubectl, please install:")
		fmt.Println("  *", err)
		if gerr := ensureGcloud(); gerr != nil {
			fmt.Println("  *", gerr)
		}
		return nil, nil, errors.New("missing kubectl")
	}

	l := clientcmd.NewDefaultClientConfigLoadingRules()
	c, err := l.Load()
	return l, c, err
}
func selectContext(co contextOptions) (string, error) {
	fmt.Println("Existing kubernetes contexts:")
	// get cluster context
	_, cfg, err := contextConfig()
	if err != nil {
		logrus.WithError(err).Fatal("Failed to load ~/.kube/config from any obvious location")
	}
	// list contexts and ask to user to choose a context
	options := map[int]string{}

	var ctxs []string
	for ctx := range cfg.Contexts {
		ctxs = append(ctxs, ctx)
	}
	sort.Strings(ctxs)
	for idx, ctx := range ctxs {
		options[idx] = ctx
		if ctx == cfg.CurrentContext {
			fmt.Printf("* %d: %s (current)", idx, ctx)
		} else {
			fmt.Printf("  %d: %s", idx, ctx)
		}
		fmt.Println()
	}
	fmt.Println()
	choice := co.context
	switch {
	case choice != "":
		fmt.Println("Reuse " + choice + " context...")
	case co.create != "" || co.reuse != "":
		choice = "create"
		fmt.Println("Create new context...")
	default:
		fmt.Print("Choose context or [create new]: ")
		fmt.Scanln(&choice)
	}

	if choice == "create" || choice == "" || choice == "create new" || choice == "new" {
		ctx, err := createContext(co)
		if err != nil {
			return "", fmt.Errorf("create context: %v", err)
		}
		return ctx, nil
	}

	if _, ok := cfg.Contexts[choice]; ok {
		return choice, nil
	}

	idx, err := strconv.Atoi(choice)
	if err != nil {
		return "", fmt.Errorf("invalid context: %q", choice)
	}

	if ctx, ok := options[idx]; ok {
		return ctx, nil
	}

	return "", fmt.Errorf("invalid index: %d", idx)
}
func applyCreate(ctx string, args ...string) error {
	create := exec.Command("kubectl", append([]string{"--dry-run=true", "--output=yaml", "create"}, args...)...)
	create.Stderr = os.Stderr
	obj, err := create.StdoutPipe()
	if err != nil {
		return fmt.Errorf("rolebinding pipe: %v", err)
	}

	if err := create.Start(); err != nil {
		return fmt.Errorf("start create: %v", err)
	}
	if err := apply(ctx, obj); err != nil {
		return fmt.Errorf("apply: %v", err)
	}
	if err := create.Wait(); err != nil {
		return fmt.Errorf("create: %v", err)
	}
	return nil
}
func determineSkippedPresubmits(toTrigger, toSkipSuperset []config.Presubmit, logger *logrus.Entry) []config.Presubmit {
	triggeredContexts := sets.NewString()
	for _, presubmit := range toTrigger {
		triggeredContexts.Insert(presubmit.Context)
	}
	var toSkip []config.Presubmit
	for _, presubmit := range toSkipSuperset {
		if triggeredContexts.Has(presubmit.Context) {
			logger.WithFields(logrus.Fields{"context": presubmit.Context, "job": presubmit.Name}).Debug("Not skipping job as context will be created by a triggered job.")
			continue
		}
		toSkip = append(toSkip, presubmit)
	}
	return toSkip
}
func Dispatch(plugin plugins.Plugin, DB *InfluxDB, issues chan sql.Issue, eventsCommentsChannel chan interface{}) {
	for {
		var points []plugins.Point
		select {
		case issue, ok := <-issues:
			if !ok {
				return
			}
			points = plugin.ReceiveIssue(issue)
		case event, ok := <-eventsCommentsChannel:
			if !ok {
				return
			}
			switch event := event.(type) {
			case sql.IssueEvent:
				points = plugin.ReceiveIssueEvent(event)
			case sql.Comment:
				points = plugin.ReceiveComment(event)
			default:
				glog.Fatal("Received invalid object: ", event)
			}
		}

		for _, point := range points {
			if err := DB.Push(point.Tags, point.Values, point.Date); err != nil {
				glog.Fatal("Failed to push point: ", err)
			}
		}
	}
}
func (c *Client) CreateIssue(org, repo, title, body string, labels, assignees []string) (*github.Issue, error) {
	glog.Infof("CreateIssue(dry=%t) Title:%q, Labels:%q, Assignees:%q\n", c.dryRun, title, labels, assignees)
	if c.dryRun {
		return nil, nil
	}

	issue := &github.IssueRequest{
		Title: &title,
		Body:  &body,
	}
	if len(labels) > 0 {
		issue.Labels = &labels
	}
	if len(assignees) > 0 {
		issue.Assignees = &assignees
	}

	var result *github.Issue
	_, err := c.retry(
		fmt.Sprintf("creating issue '%s'", title),
		func() (*github.Response, error) {
			var resp *github.Response
			var err error
			result, resp, err = c.issueService.Create(context.Background(), org, repo, issue)
			return resp, err
		},
	)
	return result, err
}
func (c *Client) CreateStatus(owner, repo, ref string, status *github.RepoStatus) (*github.RepoStatus, error) {
	glog.Infof("CreateStatus(dry=%t) ref:%s: %s:%s", c.dryRun, ref, *status.Context, *status.State)
	if c.dryRun {
		return nil, nil
	}
	var result *github.RepoStatus
	msg := fmt.Sprintf("creating status for ref '%s'", ref)
	_, err := c.retry(msg, func() (*github.Response, error) {
		var resp *github.Response
		var err error
		result, resp, err = c.repoService.CreateStatus(context.Background(), owner, repo, ref, status)
		return resp, err
	})
	return result, err
}
func (c *Client) ForEachPR(owner, repo string, opts *github.PullRequestListOptions, continueOnError bool, mungePR PRMungeFunc) error {
	var lastPage int
	// Munge each page as we get it (or in other words, wait until we are ready to munge the next
	// page of issues before getting it). We use depaginate to make the calls, but don't care about
	// the slice it returns since we consume the pages as we go.
	_, err := c.depaginate(
		"processing PRs",
		&opts.ListOptions,
		func() ([]interface{}, *github.Response, error) {
			list, resp, err := c.prService.List(context.Background(), owner, repo, opts)
			if err == nil {
				for _, pr := range list {
					if pr == nil {
						glog.Errorln("Received a nil PR from go-github while listing PRs. Skipping...")
					}
					if mungeErr := mungePR(pr); mungeErr != nil {
						if pr.Number == nil {
							mungeErr = fmt.Errorf("error munging pull request with nil Number field: %v", mungeErr)
						} else {
							mungeErr = fmt.Errorf("error munging pull request #%d: %v", *pr.Number, mungeErr)
						}
						if !continueOnError {
							return nil, resp, &retryAbort{mungeErr}
						}
						glog.Errorf("%v\n", mungeErr)
					}
				}
				if resp.LastPage > 0 {
					lastPage = resp.LastPage
				}
				glog.Infof("ForEachPR processed page %d/%d\n", opts.ListOptions.Page, lastPage)
			}
			return nil, resp, err
		},
	)
	return err
}
func (c *Client) GetCollaborators(org, repo string) ([]*github.User, error) {
	opts := &github.ListCollaboratorsOptions{}
	collaborators, err := c.depaginate(
		fmt.Sprintf("getting collaborators for '%s/%s'", org, repo),
		&opts.ListOptions,
		func() ([]interface{}, *github.Response, error) {
			page, resp, err := c.repoService.ListCollaborators(context.Background(), org, repo, opts)

			var interfaceList []interface{}
			if err == nil {
				interfaceList = make([]interface{}, 0, len(page))
				for _, user := range page {
					interfaceList = append(interfaceList, user)
				}
			}
			return interfaceList, resp, err
		},
	)

	result := make([]*github.User, 0, len(collaborators))
	for _, user := range collaborators {
		result = append(result, user.(*github.User))
	}
	return result, err
}
func (c *Client) GetCombinedStatus(owner, repo, ref string) (*github.CombinedStatus, error) {
	var result *github.CombinedStatus
	listOpts := &github.ListOptions{}

	statuses, err := c.depaginate(
		fmt.Sprintf("getting combined status for ref '%s'", ref),
		listOpts,
		func() ([]interface{}, *github.Response, error) {
			combined, resp, err := c.repoService.GetCombinedStatus(
				context.Background(),
				owner,
				repo,
				ref,
				listOpts,
			)
			if result == nil {
				result = combined
			}

			var interfaceList []interface{}
			if err == nil {
				interfaceList = make([]interface{}, 0, len(combined.Statuses))
				for _, status := range combined.Statuses {
					interfaceList = append(interfaceList, status)
				}
			}
			return interfaceList, resp, err
		},
	)

	if result != nil {
		result.Statuses = make([]github.RepoStatus, 0, len(statuses))
		for _, status := range statuses {
			result.Statuses = append(result.Statuses, status.(github.RepoStatus))
		}
	}

	return result, err
}
func (c *Client) GetIssues(org, repo string, opts *github.IssueListByRepoOptions) ([]*github.Issue, error) {
	issues, err := c.depaginate(
		fmt.Sprintf("getting issues from '%s/%s'", org, repo),
		&opts.ListOptions,
		func() ([]interface{}, *github.Response, error) {
			page, resp, err := c.issueService.ListByRepo(context.Background(), org, repo, opts)

			var interfaceList []interface{}
			if err == nil {
				interfaceList = make([]interface{}, 0, len(page))
				for _, issue := range page {
					interfaceList = append(interfaceList, issue)
				}
			}
			return interfaceList, resp, err
		},
	)

	result := make([]*github.Issue, 0, len(issues))
	for _, issue := range issues {
		result = append(result, issue.(*github.Issue))
	}
	return result, err
}
func (c *Client) GetRepoLabels(org, repo string) ([]*github.Label, error) {
	opts := &github.ListOptions{}
	labels, err := c.depaginate(
		fmt.Sprintf("getting valid labels for '%s/%s'", org, repo),
		opts,
		func() ([]interface{}, *github.Response, error) {
			page, resp, err := c.issueService.ListLabels(context.Background(), org, repo, opts)

			var interfaceList []interface{}
			if err == nil {
				interfaceList = make([]interface{}, 0, len(page))
				for _, label := range page {
					interfaceList = append(interfaceList, label)
				}
			}
			return interfaceList, resp, err
		},
	)

	result := make([]*github.Label, 0, len(labels))
	for _, label := range labels {
		result = append(result, label.(*github.Label))
	}
	return result, err
}
func (c *Client) GetUser(login string) (*github.User, error) {
	var result *github.User
	_, err := c.retry(
		fmt.Sprintf("getting user '%s'", login),
		func() (*github.Response, error) {
			var resp *github.Response
			var err error
			result, resp, err = c.userService.Get(context.Background(), login)
			return resp, err
		},
	)
	return result, err
}
func checkConfigValidity() error {
	glog.Info("Verifying if a valid config has been provided through the flags")
	if *nodeName == "" {
		return fmt.Errorf("Flag --node-name has its value unspecified")
	}
	if *gcsPath == "" {
		return fmt.Errorf("Flag --gcs-path has its value unspecified")
	}
	if _, err := os.Stat(*gcloudAuthFilePath); err != nil {
		return fmt.Errorf("Could not find the gcloud service account file: %v", err)
	} else {
		glog.Infof("Running gcloud auth activate-service-account --key-file=%s\n", *gcloudAuthFilePath)
		cmd := exec.Command("gcloud", "auth", "activate-service-account", "--key-file="+*gcloudAuthFilePath)
		var stderr, stdout bytes.Buffer
		cmd.Stderr, cmd.Stdout = &stderr, &stdout
		err = cmd.Run()
		glog.Infof("Stdout:\n%s\n", stdout.String())
		glog.Infof("Stderr:\n%s\n", stderr.String())
		if err != nil {
			return fmt.Errorf("Failed to activate gcloud service account: %v", err)
		}
	}
	return nil
}
func createSystemdLogfile(service string, outputMode string, outputDir string) error {
	// Generate the journalctl command.
	journalCmdArgs := []string{fmt.Sprintf("--output=%v", outputMode), "-D", *journalPath}
	if service == "kern" {
		journalCmdArgs = append(journalCmdArgs, "-k")
	} else {
		journalCmdArgs = append(journalCmdArgs, "-u", fmt.Sprintf("%v.service", service))
	}
	cmd := exec.Command("journalctl", journalCmdArgs...)

	// Run the command and record the output to a file.
	output, err := cmd.Output()
	if err != nil {
		return fmt.Errorf("Journalctl command for '%v' service failed: %v", service, err)
	}
	logfile := filepath.Join(outputDir, service+".log")
	if err := ioutil.WriteFile(logfile, output, 0444); err != nil {
		return fmt.Errorf("Writing to file of journalctl logs for '%v' service failed: %v", service, err)
	}
	return nil
}
func createFullSystemdLogfile(outputDir string) error {
	cmd := exec.Command("journalctl", "--output=short-precise", "-D", *journalPath)
	// Run the command and record the output to a file.
	output, err := cmd.Output()
	if err != nil {
		return fmt.Errorf("Journalctl command failed: %v", err)
	}
	logfile := filepath.Join(outputDir, "systemd.log")
	if err := ioutil.WriteFile(logfile, output, 0444); err != nil {
		return fmt.Errorf("Writing full journalctl logs to file failed: %v", err)
	}
	return nil
}
func createSystemdLogfiles(outputDir string) {
	services := append(systemdServices, nodeSystemdServices...)
	for _, service := range services {
		if err := createSystemdLogfile(service, "cat", outputDir); err != nil {
			glog.Warningf("Failed to record journalctl logs: %v", err)
		}
	}
	// Service logs specific to VM setup.
	for _, service := range systemdSetupServices {
		if err := createSystemdLogfile(service, "short-precise", outputDir); err != nil {
			glog.Warningf("Failed to record journalctl logs: %v", err)
		}
	}
	if *dumpSystemdJournal {
		if err := createFullSystemdLogfile(outputDir); err != nil {
			glog.Warningf("Failed to record journalctl logs: %v", err)
		}
	}
}
func prepareLogfiles(logDir string) {
	glog.Info("Preparing logfiles relevant to this node")
	logfiles := nodeLogs[:]

	switch *cloudProvider {
	case "gce", "gke":
		logfiles = append(logfiles, gceLogs...)
	case "aws":
		logfiles = append(logfiles, awsLogs...)
	default:
		glog.Errorf("Unknown cloud provider '%v' provided, skipping any provider specific logs", *cloudProvider)
	}

	// Grab kubemark logs too, if asked for.
	if *enableHollowNodeLogs {
		logfiles = append(logfiles, kubemarkLogs...)
	}

	// Select system/service specific logs.
	if _, err := os.Stat("/workspace/etc/systemd/journald.conf"); err == nil {
		glog.Info("Journalctl found on host. Collecting systemd logs")
		createSystemdLogfiles(logDir)
	} else {
		glog.Infof("Journalctl not found on host (%v). Collecting supervisord logs instead", err)
		logfiles = append(logfiles, kernelLog)
		logfiles = append(logfiles, initdLogs...)
		logfiles = append(logfiles, supervisordLogs...)
	}

	// Copy all the logfiles that exist, to logDir.
	for _, logfile := range logfiles {
		logfileFullPath := filepath.Join(localLogPath, logfile+".log*") // Append .log* to copy rotated logs too.
		cmd := exec.Command("/bin/sh", "-c", fmt.Sprintf("cp %v %v", logfileFullPath, logDir))
		if err := cmd.Run(); err != nil {
			glog.Warningf("Failed to copy any logfiles with pattern '%v': %v", logfileFullPath, err)
		}
	}
}
func writeSuccessMarkerFile() error {
	markerFilePath := *gcsPath + "/logexported-nodes-registry/" + *nodeName + ".txt"
	cmd := exec.Command("gsutil", "-q", "cp", "-a", "public-read", "-", markerFilePath)
	stdin, err := cmd.StdinPipe()
	if err != nil {
		return fmt.Errorf("Failed to get stdin pipe to write marker file: %v", err)
	}
	io.WriteString(stdin, "")
	stdin.Close()
	if err = cmd.Run(); err != nil {
		return fmt.Errorf("Failed to write marker file to GCS: %v", err)
	}
	return nil
}
func MakeCommand() *cobra.Command {
	flags := &flags{}
	cmd := &cobra.Command{
		Use:   "junit [profile]",
		Short: "Summarize coverage profile and produce the result in junit xml format.",
		Long: `Summarize coverage profile and produce the result in junit xml format.
Summary done at per-file and per-package level. Any coverage below coverage-threshold will be marked
with a <failure> tag in the xml produced.`,
		Run: func(cmd *cobra.Command, args []string) {
			run(flags, cmd, args)
		},
	}
	cmd.Flags().StringVarP(&flags.outputFile, "output", "o", "-", "output file")
	cmd.Flags().Float32VarP(&flags.threshold, "threshold", "t", .8, "code coverage threshold")
	return cmd
}
func warnDeprecated(last *time.Time, freq time.Duration, msg string) {
	// have we warned within the last freq?
	warnLock.RLock()
	fresh := time.Now().Sub(*last) <= freq
	warnLock.RUnlock()
	if fresh { // we've warned recently
		return
	}
	// Warning is stale, will we win the race to warn?
	warnLock.Lock()
	defer warnLock.Unlock()
	now := time.Now()           // Recalculate now, we might wait awhile for the lock
	if now.Sub(*last) <= freq { // Nope, we lost
		return
	}
	*last = now
	logrus.Warn(msg)
}
func (r RequireMatchingLabel) Describe() string {
	str := &strings.Builder{}
	fmt.Fprintf(str, "Applies the '%s' label ", r.MissingLabel)
	if r.MissingComment == "" {
		fmt.Fprint(str, "to ")
	} else {
		fmt.Fprint(str, "and comments on ")
	}

	if r.Issues {
		fmt.Fprint(str, "Issues ")
		if r.PRs {
			fmt.Fprint(str, "and ")
		}
	}
	if r.PRs {
		if r.Branch != "" {
			fmt.Fprintf(str, "'%s' branch ", r.Branch)
		}
		fmt.Fprint(str, "PRs ")
	}

	if r.Repo == "" {
		fmt.Fprintf(str, "in the '%s' GitHub org ", r.Org)
	} else {
		fmt.Fprintf(str, "in the '%s/%s' GitHub repo ", r.Org, r.Repo)
	}
	fmt.Fprintf(str, "that have no labels matching the regular expression '%s'.", r.Regexp)
	return str.String()
}
func (c *Configuration) TriggerFor(org, repo string) Trigger {
	for _, tr := range c.Triggers {
		for _, r := range tr.Repos {
			if r == org || r == fmt.Sprintf("%s/%s", org, repo) {
				return tr
			}
		}
	}
	return Trigger{}
}
func (c *Configuration) EnabledReposForPlugin(plugin string) (orgs, repos []string) {
	for repo, plugins := range c.Plugins {
		found := false
		for _, candidate := range plugins {
			if candidate == plugin {
				found = true
				break
			}
		}
		if found {
			if strings.Contains(repo, "/") {
				repos = append(repos, repo)
			} else {
				orgs = append(orgs, repo)
			}
		}
	}
	return
}
func (c *Configuration) EnabledReposForExternalPlugin(plugin string) (orgs, repos []string) {
	for repo, plugins := range c.ExternalPlugins {
		found := false
		for _, candidate := range plugins {
			if candidate.Name == plugin {
				found = true
				break
			}
		}
		if found {
			if strings.Contains(repo, "/") {
				repos = append(repos, repo)
			} else {
				orgs = append(orgs, repo)
			}
		}
	}
	return
}
func (c *ConfigUpdater) SetDefaults() {
	if len(c.Maps) == 0 {
		cf := c.ConfigFile
		if cf == "" {
			cf = "prow/config.yaml"
		} else {
			logrus.Warnf(`config_file is deprecated, please switch to "maps": {"%s": "config"} before July 2018`, cf)
		}
		pf := c.PluginFile
		if pf == "" {
			pf = "prow/plugins.yaml"
		} else {
			logrus.Warnf(`plugin_file is deprecated, please switch to "maps": {"%s": "plugins"} before July 2018`, pf)
		}
		c.Maps = map[string]ConfigMapSpec{
			cf: {
				Name: "config",
			},
			pf: {
				Name: "plugins",
			},
		}
	}

	for name, spec := range c.Maps {
		spec.Namespaces = append([]string{spec.Namespace}, spec.AdditionalNamespaces...)
		c.Maps[name] = spec
	}
}
func validatePlugins(plugins map[string][]string) error {
	var errors []string
	for _, configuration := range plugins {
		for _, plugin := range configuration {
			if _, ok := pluginHelp[plugin]; !ok {
				errors = append(errors, fmt.Sprintf("unknown plugin: %s", plugin))
			}
		}
	}
	for repo, repoConfig := range plugins {
		if strings.Contains(repo, "/") {
			org := strings.Split(repo, "/")[0]
			if dupes := findDuplicatedPluginConfig(repoConfig, plugins[org]); len(dupes) > 0 {
				errors = append(errors, fmt.Sprintf("plugins %v are duplicated for %s and %s", dupes, repo, org))
			}
		}
	}

	if len(errors) > 0 {
		return fmt.Errorf("invalid plugin configuration:\n\t%v", strings.Join(errors, "\n\t"))
	}
	return nil
}
func (c *Client) ShouldReport(pj *v1.ProwJob) bool {

	if pj.Status.State == v1.TriggeredState || pj.Status.State == v1.PendingState {
		// not done yet
		logrus.WithField("prowjob", pj.ObjectMeta.Name).Info("PJ not finished")
		return false
	}

	if pj.Status.State == v1.AbortedState {
		// aborted (new patchset)
		logrus.WithField("prowjob", pj.ObjectMeta.Name).Info("PJ aborted")
		return false
	}

	// has gerrit metadata (scheduled by gerrit adapter)
	if pj.ObjectMeta.Annotations[client.GerritID] == "" ||
		pj.ObjectMeta.Annotations[client.GerritInstance] == "" ||
		pj.ObjectMeta.Labels[client.GerritRevision] == "" {
		logrus.WithField("prowjob", pj.ObjectMeta.Name).Info("Not a gerrit job")
		return false
	}

	// Only report when all jobs of the same type on the same revision finished
	selector := labels.Set{
		client.GerritRevision: pj.ObjectMeta.Labels[client.GerritRevision],
		kube.ProwJobTypeLabel: pj.ObjectMeta.Labels[kube.ProwJobTypeLabel],
	}

	if pj.ObjectMeta.Labels[client.GerritReportLabel] == "" {
		// Shouldn't happen, adapter should already have defaulted to Code-Review
		logrus.Errorf("Gerrit report label not set for job %s", pj.Spec.Job)
	} else {
		selector[client.GerritReportLabel] = pj.ObjectMeta.Labels[client.GerritReportLabel]
	}

	pjs, err := c.lister.List(selector.AsSelector())
	if err != nil {
		logrus.WithError(err).Errorf("Cannot list prowjob with selector %v", selector)
		return false
	}

	for _, pjob := range pjs {
		if pjob.Status.State == v1.TriggeredState || pjob.Status.State == v1.PendingState {
			// other jobs with same label are still running on this revision, skip report
			logrus.WithField("prowjob", pjob.ObjectMeta.Name).Info("Other jobs with same label are still running on this revision")
			return false
		}
	}

	return true
}
func Run(refs prowapi.Refs, dir, gitUserName, gitUserEmail, cookiePath string, env []string) Record {
	logrus.WithFields(logrus.Fields{"refs": refs}).Info("Cloning refs")
	record := Record{Refs: refs}

	// This function runs the provided commands in order, logging them as they run,
	// aborting early and returning if any command fails.
	runCommands := func(commands []cloneCommand) error {
		for _, command := range commands {
			formattedCommand, output, err := command.run()
			logrus.WithFields(logrus.Fields{"command": formattedCommand, "output": output, "error": err}).Info("Ran command")
			message := ""
			if err != nil {
				message = err.Error()
				record.Failed = true
			}
			record.Commands = append(record.Commands, Command{Command: formattedCommand, Output: output, Error: message})
			if err != nil {
				return err
			}
		}
		return nil
	}

	g := gitCtxForRefs(refs, dir, env)
	if err := runCommands(g.commandsForBaseRef(refs, gitUserName, gitUserEmail, cookiePath)); err != nil {
		return record
	}

	timestamp, err := g.gitHeadTimestamp()
	if err != nil {
		timestamp = int(time.Now().Unix())
	}
	if err := runCommands(g.commandsForPullRefs(refs, timestamp)); err != nil {
		return record
	}

	finalSHA, err := g.gitRevParse()
	if err != nil {
		logrus.WithError(err).Warnf("Cannot resolve finalSHA for ref %#v", refs)
	} else {
		record.FinalSHA = finalSHA
	}

	return record
}
func PathForRefs(baseDir string, refs prowapi.Refs) string {
	var clonePath string
	if refs.PathAlias != "" {
		clonePath = refs.PathAlias
	} else {
		clonePath = fmt.Sprintf("github.com/%s/%s", refs.Org, refs.Repo)
	}
	return fmt.Sprintf("%s/src/%s", baseDir, clonePath)
}
func gitCtxForRefs(refs prowapi.Refs, baseDir string, env []string) gitCtx {
	g := gitCtx{
		cloneDir:      PathForRefs(baseDir, refs),
		env:           env,
		repositoryURI: fmt.Sprintf("https://github.com/%s/%s.git", refs.Org, refs.Repo),
	}
	if refs.CloneURI != "" {
		g.repositoryURI = refs.CloneURI
	}
	return g
}
func (g *gitCtx) commandsForBaseRef(refs prowapi.Refs, gitUserName, gitUserEmail, cookiePath string) []cloneCommand {
	commands := []cloneCommand{{dir: "/", env: g.env, command: "mkdir", args: []string{"-p", g.cloneDir}}}

	commands = append(commands, g.gitCommand("init"))
	if gitUserName != "" {
		commands = append(commands, g.gitCommand("config", "user.name", gitUserName))
	}
	if gitUserEmail != "" {
		commands = append(commands, g.gitCommand("config", "user.email", gitUserEmail))
	}
	if cookiePath != "" {
		commands = append(commands, g.gitCommand("config", "http.cookiefile", cookiePath))
	}
	commands = append(commands, g.gitCommand("fetch", g.repositoryURI, "--tags", "--prune"))
	commands = append(commands, g.gitCommand("fetch", g.repositoryURI, refs.BaseRef))

	var target string
	if refs.BaseSHA != "" {
		target = refs.BaseSHA
	} else {
		target = "FETCH_HEAD"
	}
	// we need to be "on" the target branch after the sync
	// so we need to set the branch to point to the base ref,
	// but we cannot update a branch we are on, so in case we
	// are on the branch we are syncing, we check out the SHA
	// first and reset the branch second, then check out the
	// branch we just reset to be in the correct final state
	commands = append(commands, g.gitCommand("checkout", target))
	commands = append(commands, g.gitCommand("branch", "--force", refs.BaseRef, target))
	commands = append(commands, g.gitCommand("checkout", refs.BaseRef))

	return commands
}
func gitTimestampEnvs(timestamp int) []string {
	return []string{
		fmt.Sprintf("GIT_AUTHOR_DATE=%d", timestamp),
		fmt.Sprintf("GIT_COMMITTER_DATE=%d", timestamp),
	}
}
func (g *gitCtx) gitRevParse() (string, error) {
	gitRevParseCommand := g.gitCommand("rev-parse", "HEAD")
	_, commit, err := gitRevParseCommand.run()
	if err != nil {
		logrus.WithError(err).Error("git rev-parse HEAD failed!")
		return "", err
	}
	return strings.TrimSpace(commit), nil
}
func (g *gitCtx) commandsForPullRefs(refs prowapi.Refs, fakeTimestamp int) []cloneCommand {
	var commands []cloneCommand
	for _, prRef := range refs.Pulls {
		ref := fmt.Sprintf("pull/%d/head", prRef.Number)
		if prRef.Ref != "" {
			ref = prRef.Ref
		}
		commands = append(commands, g.gitCommand("fetch", g.repositoryURI, ref))
		var prCheckout string
		if prRef.SHA != "" {
			prCheckout = prRef.SHA
		} else {
			prCheckout = "FETCH_HEAD"
		}
		fakeTimestamp++
		gitMergeCommand := g.gitCommand("merge", "--no-ff", prCheckout)
		gitMergeCommand.env = append(gitMergeCommand.env, gitTimestampEnvs(fakeTimestamp)...)
		commands = append(commands, gitMergeCommand)
	}

	// unless the user specifically asks us not to, init submodules
	if !refs.SkipSubmodules {
		commands = append(commands, g.gitCommand("submodule", "update", "--init", "--recursive"))
	}

	return commands
}
func ProduceCovList(profiles []*cover.Profile) *CoverageList {
	covList := newCoverageList("summary")
	for _, prof := range profiles {
		covList.Group = append(covList.Group, summarizeBlocks(prof))
	}
	return covList
}
func popRandom(set sets.String) string {
	list := set.List()
	sort.Strings(list)
	sel := list[rand.Intn(len(list))]
	set.Delete(sel)
	return sel
}
func (o *ExperimentalKubernetesOptions) resolve(dryRun bool) (err error) {
	if o.resolved {
		return nil
	}

	o.dryRun = dryRun
	if dryRun {
		return nil
	}

	clusterConfigs, err := kube.LoadClusterConfigs(o.kubeconfig, o.buildCluster)
	if err != nil {
		return fmt.Errorf("load --kubeconfig=%q --build-cluster=%q configs: %v", o.kubeconfig, o.buildCluster, err)
	}
	clients := map[string]kubernetes.Interface{}
	for context, config := range clusterConfigs {
		client, err := kubernetes.NewForConfig(&config)
		if err != nil {
			return fmt.Errorf("create %s kubernetes client: %v", context, err)
		}
		clients[context] = client
	}

	localCfg := clusterConfigs[kube.InClusterContext]
	pjClient, err := prow.NewForConfig(&localCfg)
	if err != nil {
		return err
	}

	o.prowJobClientset = pjClient
	o.kubernetesClientsByContext = clients
	o.resolved = true

	return nil
}
func (o *ExperimentalKubernetesOptions) ProwJobClientset(namespace string, dryRun bool) (prowJobClientset prow.Interface, err error) {
	if err := o.resolve(dryRun); err != nil {
		return nil, err
	}

	if o.dryRun {
		return nil, errors.New("no dry-run prowjob clientset is supported in dry-run mode")
	}

	return o.prowJobClientset, nil
}
func (o *ExperimentalKubernetesOptions) ProwJobClient(namespace string, dryRun bool) (prowJobClient prowv1.ProwJobInterface, err error) {
	if err := o.resolve(dryRun); err != nil {
		return nil, err
	}

	if o.dryRun {
		return kube.NewDryRunProwJobClient(o.DeckURI), nil
	}

	return o.prowJobClientset.ProwV1().ProwJobs(namespace), nil
}
func (o *ExperimentalKubernetesOptions) InfrastructureClusterClient(dryRun bool) (kubernetesClient kubernetes.Interface, err error) {
	if err := o.resolve(dryRun); err != nil {
		return nil, err
	}

	if o.dryRun {
		return nil, errors.New("no dry-run kubernetes client is supported in dry-run mode")
	}

	return o.kubernetesClientsByContext[kube.InClusterContext], nil
}
func (o *ExperimentalKubernetesOptions) BuildClusterClients(namespace string, dryRun bool) (buildClusterClients map[string]corev1.PodInterface, err error) {
	if err := o.resolve(dryRun); err != nil {
		return nil, err
	}

	if o.dryRun {
		return nil, errors.New("no dry-run pod client is supported for build clusters in dry-run mode")
	}

	buildClients := map[string]corev1.PodInterface{}
	for context, client := range o.kubernetesClientsByContext {
		buildClients[context] = client.CoreV1().Pods(namespace)
	}
	return buildClients, nil
}
func (a *ActiveState) Age(t time.Time) time.Duration {
	return t.Sub(a.startTime)
}
func (a *ActiveState) ReceiveEvent(eventName, label string, t time.Time) (State, bool) {
	if a.exit.Match(eventName, label) {
		return &InactiveState{
			entry: a.exit.Opposite(),
		}, true
	}
	return a, false
}
func (i *InactiveState) ReceiveEvent(eventName, label string, t time.Time) (State, bool) {
	if i.entry.Match(eventName, label) {
		return &ActiveState{
			startTime: t,
			exit:      i.entry.Opposite(),
		}, true
	}
	return i, false
}
func (m *MultiState) Active() bool {
	for _, state := range m.states {
		if !state.Active() {
			return false
		}
	}
	return true
}
func (m *MultiState) Age(t time.Time) time.Duration {
	minAge := time.Duration(1<<63 - 1)
	for _, state := range m.states {
		stateAge := state.Age(t)
		if stateAge < minAge {
			minAge = stateAge
		}
	}
	return minAge
}
func (m *MultiState) ReceiveEvent(eventName, label string, t time.Time) (State, bool) {
	oneChanged := false
	for i := range m.states {
		state, changed := m.states[i].ReceiveEvent(eventName, label, t)
		if changed {
			oneChanged = true
		}
		m.states[i] = state

	}
	return m, oneChanged
}
func (v *version) ProwJobs() ProwJobInformer {
	return &prowJobInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}
}
func ItemToResourcesConfig(i Item) (ResourcesConfig, error) {
	conf, ok := i.(ResourcesConfig)
	if !ok {
		return ResourcesConfig{}, fmt.Errorf("cannot construct Resource from received object %v", i)
	}
	return conf, nil
}
func (t TypeToResources) Copy() TypeToResources {
	n := TypeToResources{}
	for k, v := range t {
		n[k] = v
	}
	return n
}
func MakeCommand() *cobra.Command {
	flags := &flags{}
	cmd := &cobra.Command{
		Use:   "aggregate [files...]",
		Short: "Aggregates multiple Go coverage files.",
		Long: `Given multiple Go coverage files from identical binaries recorded in
"count" or "atomic" mode, produces a new Go coverage file in the same mode
that counts how many of those coverage profiles hit a block at least once.`,
		Run: func(cmd *cobra.Command, args []string) {
			run(flags, cmd, args)
		},
	}
	cmd.Flags().StringVarP(&flags.OutputFile, "output", "o", "-", "output file")
	return cmd
}
func (c *Controller) incrementNumPendingJobs(job string) {
	c.lock.Lock()
	defer c.lock.Unlock()
	c.pendingJobs[job]++
}
func (c *Controller) setPreviousReportState(pj prowapi.ProwJob) error {
	// fetch latest before replace
	latestPJ, err := c.kc.GetProwJob(pj.ObjectMeta.Name)
	if err != nil {
		return err
	}

	if latestPJ.Status.PrevReportStates == nil {
		latestPJ.Status.PrevReportStates = map[string]prowapi.ProwJobState{}
	}
	latestPJ.Status.PrevReportStates[reporter.GitHubReporterName] = latestPJ.Status.State
	_, err = c.kc.ReplaceProwJob(latestPJ.ObjectMeta.Name, latestPJ)
	return err
}
func (c *Controller) SyncMetrics() {
	c.pjLock.RLock()
	defer c.pjLock.RUnlock()
	kube.GatherProwJobMetrics(c.pjs)
}
func DumpProfile(profiles []*cover.Profile, writer io.Writer) error {
	if len(profiles) == 0 {
		return errors.New("can't write an empty profile")
	}
	if _, err := io.WriteString(writer, "mode: "+profiles[0].Mode+"\n"); err != nil {
		return err
	}
	for _, profile := range profiles {
		for _, block := range profile.Blocks {
			if _, err := fmt.Fprintf(writer, "%s:%d.%d,%d.%d %d %d\n", profile.FileName, block.StartLine, block.StartCol, block.EndLine, block.EndCol, block.NumStmt, block.Count); err != nil {
				return err
			}
		}
	}
	return nil
}
func blocksEqual(a cover.ProfileBlock, b cover.ProfileBlock) bool {
	return a.StartCol == b.StartCol && a.StartLine == b.StartLine &&
		a.EndCol == b.EndCol && a.EndLine == b.EndLine && a.NumStmt == b.NumStmt
}
func NewProwJobInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer {
	return NewFilteredProwJobInformer(client, namespace, resyncPeriod, indexers, nil)
}
func NewFilteredProwJobInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.ProwV1().ProwJobs(namespace).List(options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.ProwV1().ProwJobs(namespace).Watch(options)
			},
		},
		&prowjobsv1.ProwJob{},
		resyncPeriod,
		indexers,
	)
}
func New(ja *jobs.JobAgent, cfg config.Getter, c *storage.Client, ctx context.Context) *Spyglass {
	return &Spyglass{
		JobAgent:              ja,
		config:                cfg,
		PodLogArtifactFetcher: NewPodLogArtifactFetcher(ja),
		GCSArtifactFetcher:    NewGCSArtifactFetcher(c),
		testgrid: &TestGrid{
			conf:   cfg,
			client: c,
			ctx:    ctx,
		},
	}
}
func (s *Spyglass) Lenses(matchCache map[string][]string) []lenses.Lens {
	ls := []lenses.Lens{}
	for lensName, matches := range matchCache {
		if len(matches) == 0 {
			continue
		}
		lens, err := lenses.GetLens(lensName)
		if err != nil {
			logrus.WithField("lensName", lens).WithError(err).Error("Could not find artifact lens")
		} else {
			ls = append(ls, lens)
		}
	}
	// Make sure lenses are rendered in order by ascending priority
	sort.Slice(ls, func(i, j int) bool {
		iconf := ls[i].Config()
		jconf := ls[j].Config()
		iname := iconf.Name
		jname := jconf.Name
		pi := iconf.Priority
		pj := jconf.Priority
		if pi == pj {
			return iname < jname
		}
		return pi < pj
	})
	return ls
}
func (s *Spyglass) JobPath(src string) (string, error) {
	src = strings.TrimSuffix(src, "/")
	keyType, key, err := splitSrc(src)
	if err != nil {
		return "", fmt.Errorf("error parsing src: %v", src)
	}
	split := strings.Split(key, "/")
	switch keyType {
	case gcsKeyType:
		if len(split) < 4 {
			return "", fmt.Errorf("invalid key %s: expected <bucket-name>/<log-type>/.../<job-name>/<build-id>", key)
		}
		// see https://github.com/kubernetes/test-infra/tree/master/gubernator
		bktName := split[0]
		logType := split[1]
		jobName := split[len(split)-2]
		if logType == gcs.NonPRLogs {
			return path.Dir(key), nil
		} else if logType == gcs.PRLogs {
			return path.Join(bktName, gcs.PRLogs, "directory", jobName), nil
		}
		return "", fmt.Errorf("unrecognized GCS key: %s", key)
	case prowKeyType:
		if len(split) < 2 {
			return "", fmt.Errorf("invalid key %s: expected <job-name>/<build-id>", key)
		}
		jobName := split[0]
		buildID := split[1]
		job, err := s.jobAgent.GetProwJob(jobName, buildID)
		if err != nil {
			return "", fmt.Errorf("failed to get prow job from src %q: %v", key, err)
		}
		if job.Spec.DecorationConfig == nil {
			return "", fmt.Errorf("failed to locate GCS upload bucket for %s: job is undecorated", jobName)
		}
		if job.Spec.DecorationConfig.GCSConfiguration == nil {
			return "", fmt.Errorf("failed to locate GCS upload bucket for %s: missing GCS configuration", jobName)
		}
		bktName := job.Spec.DecorationConfig.GCSConfiguration.Bucket
		if job.Spec.Type == prowapi.PresubmitJob {
			return path.Join(bktName, gcs.PRLogs, "directory", jobName), nil
		}
		return path.Join(bktName, gcs.NonPRLogs, jobName), nil
	default:
		return "", fmt.Errorf("unrecognized key type for src: %v", src)
	}
}
func (s *Spyglass) RunPath(src string) (string, error) {
	src = strings.TrimSuffix(src, "/")
	keyType, key, err := splitSrc(src)
	if err != nil {
		return "", fmt.Errorf("error parsing src: %v", src)
	}
	switch keyType {
	case gcsKeyType:
		return key, nil
	case prowKeyType:
		return s.prowToGCS(key)
	default:
		return "", fmt.Errorf("unrecognized key type for src: %v", src)
	}
}
func (sg *Spyglass) ExtraLinks(src string) ([]ExtraLink, error) {
	artifacts, err := sg.FetchArtifacts(src, "", 1000000, []string{"started.json"})
	// Failing to find started.json is okay, just return nothing quietly.
	if err != nil || len(artifacts) == 0 {
		logrus.WithError(err).Debugf("Failed to find started.json while looking for extra links.")
		return nil, nil
	}
	// Failing to read an artifact we already know to exist shouldn't happen, so that's an error.
	content, err := artifacts[0].ReadAll()
	if err != nil {
		return nil, err
	}
	// Being unable to parse a successfully fetched started.json correctly is also an error.
	started := metadata.Started{}
	if err := json.Unmarshal(content, &started); err != nil {
		return nil, err
	}
	// Not having any links is fine.
	links, ok := started.Metadata.Meta("links")
	if !ok {
		return nil, nil
	}
	extraLinks := make([]ExtraLink, 0, len(*links))
	for _, name := range links.Keys() {
		m, ok := links.Meta(name)
		if !ok {
			// This should never happen, because Keys() should only return valid Metas.
			logrus.Debugf("Got bad link key %q from %s, but that should be impossible.", name, artifacts[0].CanonicalLink())
			continue
		}
		s := m.Strings()
		link := ExtraLink{
			Name:        name,
			URL:         s["url"],
			Description: s["description"],
		}
		if link.URL == "" || link.Name == "" {
			continue
		}
		extraLinks = append(extraLinks, link)
	}
	return extraLinks, nil
}
func (s *Server) needDemux(eventType, srcRepo string) []plugins.ExternalPlugin {
	var matching []plugins.ExternalPlugin
	srcOrg := strings.Split(srcRepo, "/")[0]

	for repo, plugins := range s.Plugins.Config().ExternalPlugins {
		// Make sure the repositories match
		if repo != srcRepo && repo != srcOrg {
			continue
		}

		// Make sure the events match
		for _, p := range plugins {
			if len(p.Events) == 0 {
				matching = append(matching, p)
			} else {
				for _, et := range p.Events {
					if et != eventType {
						continue
					}
					matching = append(matching, p)
					break
				}
			}
		}
	}
	return matching
}
func (s *Server) demuxExternal(l *logrus.Entry, externalPlugins []plugins.ExternalPlugin, payload []byte, h http.Header) {
	h.Set("User-Agent", "ProwHook")
	for _, p := range externalPlugins {
		s.wg.Add(1)
		go func(p plugins.ExternalPlugin) {
			defer s.wg.Done()
			if err := s.dispatch(p.Endpoint, payload, h); err != nil {
				l.WithError(err).WithField("external-plugin", p.Name).Error("Error dispatching event to external plugin.")
			} else {
				l.WithField("external-plugin", p.Name).Info("Dispatched event to external plugin")
			}
		}(p)
	}
}
func (s *Server) dispatch(endpoint string, payload []byte, h http.Header) error {
	req, err := http.NewRequest(http.MethodPost, endpoint, bytes.NewBuffer(payload))
	if err != nil {
		return err
	}
	req.Header = h
	resp, err := s.do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	rb, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return err
	}
	if resp.StatusCode < 200 || resp.StatusCode > 299 {
		return fmt.Errorf("response has status %q and body %q", resp.Status, string(rb))
	}
	return nil
}
func (s *StatePlugin) AddFlags(cmd *cobra.Command) {
	cmd.Flags().StringVar(&s.desc, "state", "", "Description of the state (eg: `opened,!merged,labeled:cool`)")
	cmd.Flags().IntSliceVar(&s.percentiles, "percentiles", []int{}, "Age percentiles for state")
}
func (s *StatePlugin) CheckFlags() error {
	s.states = NewBundledStates(s.desc)
	return nil
}
func (s *StatePlugin) ReceiveIssueEvent(event sql.IssueEvent) []Point {
	label := ""
	if event.Label != nil {
		label = *event.Label
	}

	if !s.states.ReceiveEvent(event.IssueID, event.Event, label, event.EventCreatedAt) {
		return nil
	}

	total, sum := s.states.Total(event.EventCreatedAt)
	values := map[string]interface{}{
		"count": total,
		"sum":   int(sum),
	}
	for _, percentile := range s.percentiles {
		values[fmt.Sprintf("%d%%", percentile)] = int(s.states.Percentile(event.EventCreatedAt, percentile))
	}

	return []Point{
		{
			Values: values,
			Date:   event.EventCreatedAt,
		},
	}
}
func Load(prowConfig, jobConfig string) (c *Config, err error) {
	// we never want config loading to take down the prow components
	defer func() {
		if r := recover(); r != nil {
			c, err = nil, fmt.Errorf("panic loading config: %v", r)
		}
	}()
	c, err = loadConfig(prowConfig, jobConfig)
	if err != nil {
		return nil, err
	}
	if err := c.finalizeJobConfig(); err != nil {
		return nil, err
	}
	if err := c.validateComponentConfig(); err != nil {
		return nil, err
	}
	if err := c.validateJobConfig(); err != nil {
		return nil, err
	}
	return c, nil
}
func loadConfig(prowConfig, jobConfig string) (*Config, error) {
	stat, err := os.Stat(prowConfig)
	if err != nil {
		return nil, err
	}

	if stat.IsDir() {
		return nil, fmt.Errorf("prowConfig cannot be a dir - %s", prowConfig)
	}

	var nc Config
	if err := yamlToConfig(prowConfig, &nc); err != nil {
		return nil, err
	}
	if err := parseProwConfig(&nc); err != nil {
		return nil, err
	}

	// TODO(krzyzacy): temporary allow empty jobconfig
	//                 also temporary allow job config in prow config
	if jobConfig == "" {
		return &nc, nil
	}

	stat, err = os.Stat(jobConfig)
	if err != nil {
		return nil, err
	}

	if !stat.IsDir() {
		// still support a single file
		var jc JobConfig
		if err := yamlToConfig(jobConfig, &jc); err != nil {
			return nil, err
		}
		if err := nc.mergeJobConfig(jc); err != nil {
			return nil, err
		}
		return &nc, nil
	}

	// we need to ensure all config files have unique basenames,
	// since updateconfig plugin will use basename as a key in the configmap
	uniqueBasenames := sets.String{}

	err = filepath.Walk(jobConfig, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			logrus.WithError(err).Errorf("walking path %q.", path)
			// bad file should not stop us from parsing the directory
			return nil
		}

		if strings.HasPrefix(info.Name(), "..") {
			// kubernetes volumes also include files we
			// should not look be looking into for keys
			if info.IsDir() {
				return filepath.SkipDir
			}
			return nil
		}

		if filepath.Ext(path) != ".yaml" && filepath.Ext(path) != ".yml" {
			return nil
		}

		if info.IsDir() {
			return nil
		}

		base := filepath.Base(path)
		if uniqueBasenames.Has(base) {
			return fmt.Errorf("duplicated basename is not allowed: %s", base)
		}
		uniqueBasenames.Insert(base)

		var subConfig JobConfig
		if err := yamlToConfig(path, &subConfig); err != nil {
			return err
		}
		return nc.mergeJobConfig(subConfig)
	})

	if err != nil {
		return nil, err
	}

	return &nc, nil
}
func yamlToConfig(path string, nc interface{}) error {
	b, err := ReadFileMaybeGZIP(path)
	if err != nil {
		return fmt.Errorf("error reading %s: %v", path, err)
	}
	if err := yaml.Unmarshal(b, nc); err != nil {
		return fmt.Errorf("error unmarshaling %s: %v", path, err)
	}
	var jc *JobConfig
	switch v := nc.(type) {
	case *JobConfig:
		jc = v
	case *Config:
		jc = &v.JobConfig
	}
	for rep := range jc.Presubmits {
		var fix func(*Presubmit)
		fix = func(job *Presubmit) {
			job.SourcePath = path
		}
		for i := range jc.Presubmits[rep] {
			fix(&jc.Presubmits[rep][i])
		}
	}
	for rep := range jc.Postsubmits {
		var fix func(*Postsubmit)
		fix = func(job *Postsubmit) {
			job.SourcePath = path
		}
		for i := range jc.Postsubmits[rep] {
			fix(&jc.Postsubmits[rep][i])
		}
	}

	var fix func(*Periodic)
	fix = func(job *Periodic) {
		job.SourcePath = path
	}
	for i := range jc.Periodics {
		fix(&jc.Periodics[i])
	}
	return nil
}
func ReadFileMaybeGZIP(path string) ([]byte, error) {
	b, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}
	// check if file contains gzip header: http://www.zlib.org/rfc-gzip.html
	if !bytes.HasPrefix(b, []byte("\x1F\x8B")) {
		// go ahead and return the contents if not gzipped
		return b, nil
	}
	// otherwise decode
	gzipReader, err := gzip.NewReader(bytes.NewBuffer(b))
	if err != nil {
		return nil, err
	}
	return ioutil.ReadAll(gzipReader)
}
func (c *Config) finalizeJobConfig() error {
	if c.decorationRequested() {
		if c.Plank.DefaultDecorationConfig == nil {
			return errors.New("no default decoration config provided for plank")
		}
		if c.Plank.DefaultDecorationConfig.UtilityImages == nil {
			return errors.New("no default decoration image pull specs provided for plank")
		}
		if c.Plank.DefaultDecorationConfig.GCSConfiguration == nil {
			return errors.New("no default GCS decoration config provided for plank")
		}
		if c.Plank.DefaultDecorationConfig.GCSCredentialsSecret == "" {
			return errors.New("no default GCS credentials secret provided for plank")
		}

		for _, vs := range c.Presubmits {
			for i := range vs {
				setPresubmitDecorationDefaults(c, &vs[i])
			}
		}

		for _, js := range c.Postsubmits {
			for i := range js {
				setPostsubmitDecorationDefaults(c, &js[i])
			}
		}

		for i := range c.Periodics {
			setPeriodicDecorationDefaults(c, &c.Periodics[i])
		}
	}

	// Ensure that regexes are valid and set defaults.
	for _, vs := range c.Presubmits {
		c.defaultPresubmitFields(vs)
		if err := SetPresubmitRegexes(vs); err != nil {
			return fmt.Errorf("could not set regex: %v", err)
		}
	}
	for _, js := range c.Postsubmits {
		c.defaultPostsubmitFields(js)
		if err := SetPostsubmitRegexes(js); err != nil {
			return fmt.Errorf("could not set regex: %v", err)
		}
	}

	c.defaultPeriodicFields(c.Periodics)

	for _, v := range c.AllPresubmits(nil) {
		if err := resolvePresets(v.Name, v.Labels, v.Spec, v.BuildSpec, c.Presets); err != nil {
			return err
		}
	}

	for _, v := range c.AllPostsubmits(nil) {
		if err := resolvePresets(v.Name, v.Labels, v.Spec, v.BuildSpec, c.Presets); err != nil {
			return err
		}
	}

	for _, v := range c.AllPeriodics() {
		if err := resolvePresets(v.Name, v.Labels, v.Spec, v.BuildSpec, c.Presets); err != nil {
			return err
		}
	}

	return nil
}
func (c *Config) validateComponentConfig() error {
	if c.Plank.JobURLPrefix != "" && c.Plank.JobURLPrefixConfig["*"] != "" {
		return errors.New(`Planks job_url_prefix must be unset when job_url_prefix_config["*"] is set. The former is deprecated, use the latter`)
	}
	for k, v := range c.Plank.JobURLPrefixConfig {
		if _, err := url.Parse(v); err != nil {
			return fmt.Errorf(`Invalid value for Planks job_url_prefix_config["%s"]: %v`, k, err)
		}
	}

	if c.SlackReporter != nil {
		if err := c.SlackReporter.DefaultAndValidate(); err != nil {
			return fmt.Errorf("failed to validate slackreporter config: %v", err)
		}
	}
	return nil
}
func ConfigPath(value string) string {

	if value != "" {
		return value
	}
	logrus.Warningf("defaulting to %s until 15 July 2019, please migrate", DefaultConfigPath)
	return DefaultConfigPath
}
func ValidateController(c *Controller) error {
	urlTmpl, err := template.New("JobURL").Parse(c.JobURLTemplateString)
	if err != nil {
		return fmt.Errorf("parsing template: %v", err)
	}
	c.JobURLTemplate = urlTmpl

	reportTmpl, err := template.New("Report").Parse(c.ReportTemplateString)
	if err != nil {
		return fmt.Errorf("parsing template: %v", err)
	}
	c.ReportTemplate = reportTmpl
	if c.MaxConcurrency < 0 {
		return fmt.Errorf("controller has invalid max_concurrency (%d), it needs to be a non-negative number", c.MaxConcurrency)
	}
	if c.MaxGoroutines == 0 {
		c.MaxGoroutines = 20
	}
	if c.MaxGoroutines <= 0 {
		return fmt.Errorf("controller has invalid max_goroutines (%d), it needs to be a positive number", c.MaxGoroutines)
	}
	return nil
}
func (c *ProwConfig) defaultJobBase(base *JobBase) {
	if base.Agent == "" { // Use kubernetes by default
		base.Agent = string(prowapi.KubernetesAgent)
	}
	if base.Namespace == nil || *base.Namespace == "" {
		s := c.PodNamespace
		base.Namespace = &s
	}
	if base.Cluster == "" {
		base.Cluster = kube.DefaultClusterAlias
	}
}
func SetPresubmitRegexes(js []Presubmit) error {
	for i, j := range js {
		if re, err := regexp.Compile(j.Trigger); err == nil {
			js[i].re = re
		} else {
			return fmt.Errorf("could not compile trigger regex for %s: %v", j.Name, err)
		}
		if !js[i].re.MatchString(j.RerunCommand) {
			return fmt.Errorf("for job %s, rerun command \"%s\" does not match trigger \"%s\"", j.Name, j.RerunCommand, j.Trigger)
		}
		b, err := setBrancherRegexes(j.Brancher)
		if err != nil {
			return fmt.Errorf("could not set branch regexes for %s: %v", j.Name, err)
		}
		js[i].Brancher = b

		c, err := setChangeRegexes(j.RegexpChangeMatcher)
		if err != nil {
			return fmt.Errorf("could not set change regexes for %s: %v", j.Name, err)
		}
		js[i].RegexpChangeMatcher = c
	}
	return nil
}
func setBrancherRegexes(br Brancher) (Brancher, error) {
	if len(br.Branches) > 0 {
		if re, err := regexp.Compile(strings.Join(br.Branches, `|`)); err == nil {
			br.re = re
		} else {
			return br, fmt.Errorf("could not compile positive branch regex: %v", err)
		}
	}
	if len(br.SkipBranches) > 0 {
		if re, err := regexp.Compile(strings.Join(br.SkipBranches, `|`)); err == nil {
			br.reSkip = re
		} else {
			return br, fmt.Errorf("could not compile negative branch regex: %v", err)
		}
	}
	return br, nil
}
func SetPostsubmitRegexes(ps []Postsubmit) error {
	for i, j := range ps {
		b, err := setBrancherRegexes(j.Brancher)
		if err != nil {
			return fmt.Errorf("could not set branch regexes for %s: %v", j.Name, err)
		}
		ps[i].Brancher = b
		c, err := setChangeRegexes(j.RegexpChangeMatcher)
		if err != nil {
			return fmt.Errorf("could not set change regexes for %s: %v", j.Name, err)
		}
		ps[i].RegexpChangeMatcher = c
	}
	return nil
}
func (lens Lens) Body(artifacts []lenses.Artifact, resourceDir string, data string) string {
	var buf bytes.Buffer
	type MetadataViewData struct {
		Status       string
		StartTime    time.Time
		FinishedTime time.Time
		Elapsed      time.Duration
		Metadata     map[string]string
	}
	metadataViewData := MetadataViewData{Status: "Pending"}
	started := gcs.Started{}
	finished := gcs.Finished{}
	for _, a := range artifacts {
		read, err := a.ReadAll()
		if err != nil {
			logrus.WithError(err).Error("Failed reading from artifact.")
		}
		if a.JobPath() == "started.json" {
			if err = json.Unmarshal(read, &started); err != nil {
				logrus.WithError(err).Error("Error unmarshaling started.json")
			}
			metadataViewData.StartTime = time.Unix(started.Timestamp, 0)
		} else if a.JobPath() == "finished.json" {
			if err = json.Unmarshal(read, &finished); err != nil {
				logrus.WithError(err).Error("Error unmarshaling finished.json")
			}
			if finished.Timestamp != nil {
				metadataViewData.FinishedTime = time.Unix(*finished.Timestamp, 0)
			}
			metadataViewData.Status = finished.Result
		}
	}

	if !metadataViewData.StartTime.IsZero() {
		if metadataViewData.FinishedTime.IsZero() {
			metadataViewData.Elapsed = time.Now().Sub(metadataViewData.StartTime)
		} else {
			metadataViewData.Elapsed =
				metadataViewData.FinishedTime.Sub(metadataViewData.StartTime)
		}
		metadataViewData.Elapsed = metadataViewData.Elapsed.Round(time.Second)
	}

	metadataViewData.Metadata = map[string]string{"node": started.Node}

	metadatas := []metadata.Metadata{started.Metadata, finished.Metadata}
	for _, m := range metadatas {
		for k, v := range m {
			if s, ok := v.(string); ok && v != "" {
				metadataViewData.Metadata[k] = s
			}
		}
	}

	metadataTemplate, err := template.ParseFiles(filepath.Join(resourceDir, "template.html"))
	if err != nil {
		return fmt.Sprintf("Failed to load template: %v", err)
	}

	if err := metadataTemplate.ExecuteTemplate(&buf, "body", metadataViewData); err != nil {
		logrus.WithError(err).Error("Error executing template.")
	}
	return buf.String()
}
func NewBoskosHandler(r *ranch.Ranch) *http.ServeMux {
	mux := http.NewServeMux()
	mux.Handle("/", handleDefault(r))
	mux.Handle("/acquire", handleAcquire(r))
	mux.Handle("/acquirebystate", handleAcquireByState(r))
	mux.Handle("/release", handleRelease(r))
	mux.Handle("/reset", handleReset(r))
	mux.Handle("/update", handleUpdate(r))
	mux.Handle("/metric", handleMetric(r))
	return mux
}
func ErrorToStatus(err error) int {
	switch err.(type) {
	default:
		return http.StatusInternalServerError
	case *ranch.OwnerNotMatch:
		return http.StatusUnauthorized
	case *ranch.ResourceNotFound:
		return http.StatusNotFound
	case *ranch.ResourceTypeNotFound:
		return http.StatusNotFound
	case *ranch.StateNotMatch:
		return http.StatusConflict
	}
}
func DumpProfile(destination string, profile []*cover.Profile) error {
	var output io.Writer
	if destination == "-" {
		output = os.Stdout
	} else {
		f, err := os.Create(destination)
		if err != nil {
			return fmt.Errorf("failed to open %s: %v", destination, err)
		}
		defer f.Close()
		output = f
	}
	err := cov.DumpProfile(profile, output)
	if err != nil {
		return fmt.Errorf("failed to dump profile: %v", err)
	}
	return nil
}
func LoadProfile(origin string) ([]*cover.Profile, error) {
	filename := origin
	if origin == "-" {
		// Annoyingly, ParseProfiles only accepts a filename, so we have to write the bytes to disk
		// so it can read them back.
		// We could probably also just give it /dev/stdin, but that'll break on Windows.
		tf, err := ioutil.TempFile("", "")
		if err != nil {
			return nil, fmt.Errorf("failed to create temp file: %v", err)
		}
		defer tf.Close()
		defer os.Remove(tf.Name())
		if _, err := io.Copy(tf, os.Stdin); err != nil {
			return nil, fmt.Errorf("failed to copy stdin to temp file: %v", err)
		}
		filename = tf.Name()
	}
	return cover.ParseProfiles(filename)
}
func NewClient() (*Client, error) {
	g, err := exec.LookPath("git")
	if err != nil {
		return nil, err
	}
	t, err := ioutil.TempDir("", "git")
	if err != nil {
		return nil, err
	}
	return &Client{
		logger:    logrus.WithField("client", "git"),
		dir:       t,
		git:       g,
		base:      fmt.Sprintf("https://%s", github),
		repoLocks: make(map[string]*sync.Mutex),
	}, nil
}
func (c *Client) SetCredentials(user string, tokenGenerator func() []byte) {
	c.credLock.Lock()
	defer c.credLock.Unlock()
	c.user = user
	c.tokenGenerator = tokenGenerator
}
func (r *Repo) Checkout(commitlike string) error {
	r.logger.Infof("Checkout %s.", commitlike)
	co := r.gitCommand("checkout", commitlike)
	if b, err := co.CombinedOutput(); err != nil {
		return fmt.Errorf("error checking out %s: %v. output: %s", commitlike, err, string(b))
	}
	return nil
}
func (r *Repo) CheckoutNewBranch(branch string) error {
	r.logger.Infof("Create and checkout %s.", branch)
	co := r.gitCommand("checkout", "-b", branch)
	if b, err := co.CombinedOutput(); err != nil {
		return fmt.Errorf("error checking out %s: %v. output: %s", branch, err, string(b))
	}
	return nil
}
func (r *Repo) Merge(commitlike string) (bool, error) {
	r.logger.Infof("Merging %s.", commitlike)
	co := r.gitCommand("merge", "--no-ff", "--no-stat", "-m merge", commitlike)

	b, err := co.CombinedOutput()
	if err == nil {
		return true, nil
	}
	r.logger.WithError(err).Infof("Merge failed with output: %s", string(b))

	if b, err := r.gitCommand("merge", "--abort").CombinedOutput(); err != nil {
		return false, fmt.Errorf("error aborting merge for commitlike %s: %v. output: %s", commitlike, err, string(b))
	}

	return false, nil
}
func (r *Repo) CheckoutPullRequest(number int) error {
	r.logger.Infof("Fetching and checking out %s#%d.", r.repo, number)
	if b, err := retryCmd(r.logger, r.Dir, r.git, "fetch", r.base+"/"+r.repo, fmt.Sprintf("pull/%d/head:pull%d", number, number)); err != nil {
		return fmt.Errorf("git fetch failed for PR %d: %v. output: %s", number, err, string(b))
	}
	co := r.gitCommand("checkout", fmt.Sprintf("pull%d", number))
	if b, err := co.CombinedOutput(); err != nil {
		return fmt.Errorf("git checkout failed for PR %d: %v. output: %s", number, err, string(b))
	}
	return nil
}
func (r *Repo) Config(key, value string) error {
	r.logger.Infof("Running git config %s %s", key, value)
	if b, err := r.gitCommand("config", key, value).CombinedOutput(); err != nil {
		return fmt.Errorf("git config %s %s failed: %v. output: %s", key, value, err, string(b))
	}
	return nil
}
func retryCmd(l *logrus.Entry, dir, cmd string, arg ...string) ([]byte, error) {
	var b []byte
	var err error
	sleepyTime := time.Second
	for i := 0; i < 3; i++ {
		c := exec.Command(cmd, arg...)
		c.Dir = dir
		b, err = c.CombinedOutput()
		if err != nil {
			l.Warningf("Running %s %v returned error %v with output %s.", cmd, arg, err, string(b))
			time.Sleep(sleepyTime)
			sleepyTime *= 2
			continue
		}
		break
	}
	return b, err
}
func LabelsAndAnnotationsForSpec(spec prowapi.ProwJobSpec, extraLabels, extraAnnotations map[string]string) (map[string]string, map[string]string) {
	jobNameForLabel := spec.Job
	if len(jobNameForLabel) > validation.LabelValueMaxLength {
		// TODO(fejta): consider truncating middle rather than end.
		jobNameForLabel = strings.TrimRight(spec.Job[:validation.LabelValueMaxLength], ".-")
		logrus.WithFields(logrus.Fields{
			"job":       spec.Job,
			"key":       kube.ProwJobAnnotation,
			"value":     spec.Job,
			"truncated": jobNameForLabel,
		}).Info("Cannot use full job name, will truncate.")
	}
	labels := map[string]string{
		kube.CreatedByProw:     "true",
		kube.ProwJobTypeLabel:  string(spec.Type),
		kube.ProwJobAnnotation: jobNameForLabel,
	}
	if spec.Type != prowapi.PeriodicJob && spec.Refs != nil {
		labels[kube.OrgLabel] = spec.Refs.Org
		labels[kube.RepoLabel] = spec.Refs.Repo
		if len(spec.Refs.Pulls) > 0 {
			labels[kube.PullLabel] = strconv.Itoa(spec.Refs.Pulls[0].Number)
		}
	}

	for k, v := range extraLabels {
		labels[k] = v
	}

	// let's validate labels
	for key, value := range labels {
		if errs := validation.IsValidLabelValue(value); len(errs) > 0 {
			// try to use basename of a path, if path contains invalid //
			base := filepath.Base(value)
			if errs := validation.IsValidLabelValue(base); len(errs) == 0 {
				labels[key] = base
				continue
			}
			logrus.WithFields(logrus.Fields{
				"key":    key,
				"value":  value,
				"errors": errs,
			}).Warn("Removing invalid label")
			delete(labels, key)
		}
	}

	annotations := map[string]string{
		kube.ProwJobAnnotation: spec.Job,
	}
	for k, v := range extraAnnotations {
		annotations[k] = v
	}

	return labels, annotations
}
func ProwJobToPod(pj prowapi.ProwJob, buildID string) (*coreapi.Pod, error) {
	if pj.Spec.PodSpec == nil {
		return nil, fmt.Errorf("prowjob %q lacks a pod spec", pj.Name)
	}

	rawEnv, err := downwardapi.EnvForSpec(downwardapi.NewJobSpec(pj.Spec, buildID, pj.Name))
	if err != nil {
		return nil, err
	}

	spec := pj.Spec.PodSpec.DeepCopy()
	spec.RestartPolicy = "Never"
	spec.Containers[0].Name = kube.TestContainerName

	// if the user has not provided a serviceaccount to use or explicitly
	// requested mounting the default token, we treat the unset value as
	// false, while kubernetes treats it as true if it is unset because
	// it was added in v1.6
	if spec.AutomountServiceAccountToken == nil && spec.ServiceAccountName == "" {
		myFalse := false
		spec.AutomountServiceAccountToken = &myFalse
	}

	if pj.Spec.DecorationConfig == nil {
		spec.Containers[0].Env = append(spec.Containers[0].Env, kubeEnv(rawEnv)...)
	} else {
		if err := decorate(spec, &pj, rawEnv); err != nil {
			return nil, fmt.Errorf("error decorating podspec: %v", err)
		}
	}

	podLabels, annotations := LabelsAndAnnotationsForJob(pj)
	return &coreapi.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:        pj.ObjectMeta.Name,
			Labels:      podLabels,
			Annotations: annotations,
		},
		Spec: *spec,
	}, nil
}
func CloneLogPath(logMount coreapi.VolumeMount) string {
	return filepath.Join(logMount.MountPath, cloneLogPath)
}
func cloneEnv(opt clonerefs.Options) ([]coreapi.EnvVar, error) {
	// TODO(fejta): use flags
	cloneConfigEnv, err := clonerefs.Encode(opt)
	if err != nil {
		return nil, err
	}
	return kubeEnv(map[string]string{clonerefs.JSONConfigEnvVar: cloneConfigEnv}), nil
}
func sshVolume(secret string) (coreapi.Volume, coreapi.VolumeMount) {
	var sshKeyMode int32 = 0400 // this is octal, so symbolic ref is `u+r`
	name := strings.Join([]string{"ssh-keys", secret}, "-")
	mountPath := path.Join("/secrets/ssh", secret)
	v := coreapi.Volume{
		Name: name,
		VolumeSource: coreapi.VolumeSource{
			Secret: &coreapi.SecretVolumeSource{
				SecretName:  secret,
				DefaultMode: &sshKeyMode,
			},
		},
	}

	vm := coreapi.VolumeMount{
		Name:      name,
		MountPath: mountPath,
		ReadOnly:  true,
	}

	return v, vm
}
func InjectEntrypoint(c *coreapi.Container, timeout, gracePeriod time.Duration, prefix, previousMarker string, exitZero bool, log, tools coreapi.VolumeMount) (*wrapper.Options, error) {
	wrapperOptions := &wrapper.Options{
		Args:         append(c.Command, c.Args...),
		ProcessLog:   processLog(log, prefix),
		MarkerFile:   markerFile(log, prefix),
		MetadataFile: metadataFile(log, prefix),
	}
	// TODO(fejta): use flags
	entrypointConfigEnv, err := entrypoint.Encode(entrypoint.Options{
		ArtifactDir:    artifactsDir(log),
		GracePeriod:    gracePeriod,
		Options:        wrapperOptions,
		Timeout:        timeout,
		AlwaysZero:     exitZero,
		PreviousMarker: previousMarker,
	})
	if err != nil {
		return nil, err
	}

	c.Command = []string{entrypointLocation(tools)}
	c.Args = nil
	c.Env = append(c.Env, kubeEnv(map[string]string{entrypoint.JSONConfigEnvVar: entrypointConfigEnv})...)
	c.VolumeMounts = append(c.VolumeMounts, log, tools)
	return wrapperOptions, nil
}
func PlaceEntrypoint(image string, toolsMount coreapi.VolumeMount) coreapi.Container {
	return coreapi.Container{
		Name:         "place-entrypoint",
		Image:        image,
		Command:      []string{"/bin/cp"},
		Args:         []string{"/entrypoint", entrypointLocation(toolsMount)},
		VolumeMounts: []coreapi.VolumeMount{toolsMount},
	}
}
func kubeEnv(environment map[string]string) []coreapi.EnvVar {
	var keys []string
	for key := range environment {
		keys = append(keys, key)
	}
	sort.Strings(keys)

	var kubeEnvironment []coreapi.EnvVar
	for _, key := range keys {
		kubeEnvironment = append(kubeEnvironment, coreapi.EnvVar{
			Name:  key,
			Value: environment[key],
		})
	}

	return kubeEnvironment
}
func (o *KubernetesOptions) Client(namespace string, dryRun bool) (*kube.Client, error) {
	if dryRun {
		return kube.NewFakeClient(o.DeckURI), nil
	}

	if o.cluster == "" {
		return kube.NewClientInCluster(namespace)
	}

	return kube.NewClientFromFile(o.cluster, namespace)
}
func handle(gc githubClient, le *logrus.Entry, e *event) error {
	needsLabel := e.draft || titleRegex.MatchString(e.title)

	if needsLabel && !e.hasLabel {
		if err := gc.AddLabel(e.org, e.repo, e.number, labels.WorkInProgress); err != nil {
			le.Warnf("error while adding Label %q: %v", labels.WorkInProgress, err)
			return err
		}
	} else if !needsLabel && e.hasLabel {
		if err := gc.RemoveLabel(e.org, e.repo, e.number, labels.WorkInProgress); err != nil {
			le.Warnf("error while removing Label %q: %v", labels.WorkInProgress, err)
			return err
		}
	}
	return nil
}
func SendHook(address, eventType string, payload, hmac []byte) error {
	req, err := http.NewRequest(http.MethodPost, address, bytes.NewBuffer(payload))
	if err != nil {
		return err
	}
	req.Header.Set("X-GitHub-Event", eventType)
	req.Header.Set("X-GitHub-Delivery", "GUID")
	req.Header.Set("X-Hub-Signature", github.PayloadSignature(payload, hmac))
	req.Header.Set("content-type", "application/json")

	c := &http.Client{}
	resp, err := c.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	rb, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return err
	}
	if resp.StatusCode != 200 {
		return fmt.Errorf("response from hook has status %d and body %s", resp.StatusCode, string(bytes.TrimSpace(rb)))
	}
	return nil
}
func janitorClean(resource *common.Resource, flags []string) error {
	args := append([]string{fmt.Sprintf("--%s=%s", format(resource.Type), resource.Name)}, flags...)
	logrus.Infof("executing janitor: %s %s", *janitorPath, strings.Join(args, " "))
	cmd := exec.Command(*janitorPath, args...)
	b, err := cmd.CombinedOutput()
	if err != nil {
		logrus.WithError(err).Errorf("failed to clean up project %s, error info: %s", resource.Name, string(b))
	} else {
		logrus.Tracef("output from janitor: %s", string(b))
		logrus.Infof("successfully cleaned up resource %s", resource.Name)
	}
	return err
}
func janitor(c boskosClient, buffer <-chan *common.Resource, fn clean, flags []string) {
	for {
		resource := <-buffer

		dest := common.Free
		if err := fn(resource, flags); err != nil {
			logrus.WithError(err).Errorf("%s failed!", *janitorPath)
			dest = common.Dirty
		}

		if err := c.ReleaseOne(resource.Name, dest); err != nil {
			logrus.WithError(err).Error("boskos release failed!")
		}
	}
}
func (s *PullServer) Run(ctx context.Context) error {
	configEvent := make(chan config.Delta, 2)
	s.Subscriber.ConfigAgent.Subscribe(configEvent)

	var err error
	defer func() {
		if err != nil {
			logrus.WithError(ctx.Err()).Error("Pull server shutting down")
		}
		logrus.Warn("Pull server shutting down")
	}()
	currentConfig := s.Subscriber.ConfigAgent.Config().PubSubSubscriptions
	errGroup, derivedCtx, err := s.handlePulls(ctx, currentConfig)
	if err != nil {
		return err
	}

	for {
		select {
		// Parent context. Shutdown
		case <-ctx.Done():
			return ctx.Err()
		// Current thread context, it may be failing already
		case <-derivedCtx.Done():
			err = errGroup.Wait()
			return err
		// Checking for update config
		case event := <-configEvent:
			newConfig := event.After.PubSubSubscriptions
			logrus.Info("Received new config")
			if !reflect.DeepEqual(currentConfig, newConfig) {
				logrus.Warn("New config found, reloading pull Server")
				// Making sure the current thread finishes before starting a new one.
				errGroup.Wait()
				// Starting a new thread with new config
				errGroup, derivedCtx, err = s.handlePulls(ctx, newConfig)
				if err != nil {
					return err
				}
				currentConfig = newConfig
			}
		}
	}
}
func specToStarted(spec *downwardapi.JobSpec, mainRefSHA string) gcs.Started {
	started := gcs.Started{
		Timestamp:   time.Now().Unix(),
		RepoVersion: downwardapi.GetRevisionFromSpec(spec),
	}

	if mainRefSHA != "" {
		started.RepoVersion = mainRefSHA
	}

	// TODO(fejta): VM name

	if spec.Refs != nil && len(spec.Refs.Pulls) > 0 {
		started.Pull = strconv.Itoa(spec.Refs.Pulls[0].Number)
	}

	started.Repos = map[string]string{}

	if spec.Refs != nil {
		started.Repos[spec.Refs.Org+"/"+spec.Refs.Repo] = spec.Refs.String()
	}
	for _, ref := range spec.ExtraRefs {
		started.Repos[ref.Org+"/"+ref.Repo] = ref.String()
	}

	return started
}
func (o Options) Run() error {
	spec, err := downwardapi.ResolveSpecFromEnv()
	if err != nil {
		return fmt.Errorf("could not resolve job spec: %v", err)
	}

	uploadTargets := map[string]gcs.UploadFunc{}

	var failed bool
	var mainRefSHA string
	if o.Log != "" {
		if failed, mainRefSHA, err = processCloneLog(o.Log, uploadTargets); err != nil {
			return err
		}
	}

	started := specToStarted(spec, mainRefSHA)

	startedData, err := json.Marshal(&started)
	if err != nil {
		return fmt.Errorf("could not marshal starting data: %v", err)
	}

	uploadTargets["started.json"] = gcs.DataUpload(bytes.NewReader(startedData))

	if err := o.Options.Run(spec, uploadTargets); err != nil {
		return fmt.Errorf("failed to upload to GCS: %v", err)
	}

	if failed {
		return errors.New("cloning the appropriate refs failed")
	}

	return nil
}
func hasPRChanged(pr github.PullRequestEvent) bool {
	switch pr.Action {
	case github.PullRequestActionOpened:
		return true
	case github.PullRequestActionReopened:
		return true
	case github.PullRequestActionSynchronize:
		return true
	default:
		return false
	}
}
func UpdateIssues(db *gorm.DB, client ClientInterface) {
	latest, err := findLatestIssueUpdate(db, client.RepositoryName())
	if err != nil {
		glog.Error("Failed to find last issue update: ", err)
		return
	}
	c := make(chan *github.Issue, 200)

	go client.FetchIssues(latest, c)
	for issue := range c {
		issueOrm, err := NewIssue(issue, client.RepositoryName())
		if err != nil {
			glog.Error("Can't create issue:", err)
			continue
		}
		if db.Create(issueOrm).Error != nil {
			// We assume record already exists. Let's
			// update. First we need to delete labels and
			// assignees, as they are just concatenated
			// otherwise.
			db.Delete(sql.Label{},
				"issue_id = ? AND repository = ?",
				issueOrm.ID, client.RepositoryName())
			db.Delete(sql.Assignee{},
				"issue_id = ? AND repository = ?",
				issueOrm.ID, client.RepositoryName())

			if err := db.Save(issueOrm).Error; err != nil {
				glog.Error("Failed to update database issue: ", err)
			}
		}

		// Issue is updated, find if we have new comments
		UpdateComments(*issue.Number, issueOrm.IsPR, db, client)
		// and find if we have new events
		UpdateIssueEvents(*issue.Number, db, client)
	}
}
func handleReviewEvent(pc plugins.Agent, re github.ReviewEvent) error {
	return handleReview(
		pc.Logger,
		pc.GitHubClient,
		pc.OwnersClient,
		pc.Config.GitHubOptions,
		pc.PluginConfig,
		&re,
	)
}
func findAssociatedIssue(body, org string) (int, error) {
	associatedIssueRegex, err := regexp.Compile(fmt.Sprintf(associatedIssueRegexFormat, org))
	if err != nil {
		return 0, err
	}
	match := associatedIssueRegex.FindStringSubmatch(body)
	if len(match) == 0 {
		return 0, nil
	}
	v, err := strconv.Atoi(match[1])
	if err != nil {
		return 0, err
	}
	return v, nil
}
func optionsForRepo(config *plugins.Configuration, org, repo string) *plugins.Approve {
	fullName := fmt.Sprintf("%s/%s", org, repo)

	a := func() *plugins.Approve {
		// First search for repo config
		for _, c := range config.Approve {
			if !strInSlice(fullName, c.Repos) {
				continue
			}
			return &c
		}

		// If you don't find anything, loop again looking for an org config
		for _, c := range config.Approve {
			if !strInSlice(org, c.Repos) {
				continue
			}
			return &c
		}

		// Return an empty config, and use plugin defaults
		return &plugins.Approve{}
	}()
	if a.DeprecatedImplicitSelfApprove == nil && a.RequireSelfApproval == nil && config.UseDeprecatedSelfApprove {
		no := false
		a.DeprecatedImplicitSelfApprove = &no
	}
	if a.DeprecatedReviewActsAsApprove == nil && a.IgnoreReviewState == nil && config.UseDeprecatedReviewApprove {
		no := false
		a.DeprecatedReviewActsAsApprove = &no
	}
	return a
}
func localOnlyMain(cfg config.Getter, o options, mux *http.ServeMux) *http.ServeMux {
	mux.Handle("/github-login", gziphandler.GzipHandler(handleSimpleTemplate(o, cfg, "github-login.html", nil)))

	if o.spyglass {
		initSpyglass(cfg, o, mux, nil)
	}

	return mux
}
func (covList *CoverageList) summarize() {
	covList.NumCoveredStmts = 0
	covList.NumAllStmts = 0
	for _, item := range covList.Group {
		covList.NumCoveredStmts += item.NumCoveredStmts
		covList.NumAllStmts += item.NumAllStmts
	}
}
func (covList *CoverageList) Subset(prefix string) *CoverageList {
	s := newCoverageList("Filtered Summary")
	for _, c := range covList.Group {
		if strings.HasPrefix(c.Name, prefix) {
			covList.Group = append(covList.Group, c)
		}
	}
	return s
}
func (covList CoverageList) ListDirectories() []string {
	dirSet := map[string]bool{}
	for _, cov := range covList.Group {
		dirSet[path.Dir(cov.Name)] = true
	}
	var result []string
	for key := range dirSet {
		result = append(result, key)
	}
	return result
}
func readRequest(r io.Reader, contentType string) (*admissionapi.AdmissionRequest, error) {
	if contentType != contentTypeJSON {
		return nil, fmt.Errorf("Content-Type=%s, expected %s", contentType, contentTypeJSON)
	}

	// Can we read the body?
	if r == nil {
		return nil, fmt.Errorf("no body")
	}
	body, err := ioutil.ReadAll(r)
	if err != nil {
		return nil, fmt.Errorf("read body: %v", err)
	}

	// Can we convert the body into an AdmissionReview?
	var ar admissionapi.AdmissionReview
	deserializer := codecs.UniversalDeserializer()
	if _, _, err := deserializer.Decode(body, nil, &ar); err != nil {
		return nil, fmt.Errorf("decode body: %v", err)
	}
	return ar.Request, nil
}
func handle(w http.ResponseWriter, r *http.Request) {
	req, err := readRequest(r.Body, r.Header.Get("Content-Type"))
	if err != nil {
		logrus.WithError(err).Error("read")
	}

	if err := writeResponse(*req, w, onlyUpdateStatus); err != nil {
		logrus.WithError(err).Error("write")
	}
}
func writeResponse(ar admissionapi.AdmissionRequest, w io.Writer, decide decider) error {
	response, err := decide(ar)
	if err != nil {
		logrus.WithError(err).Error("failed decision")
		response = &admissionapi.AdmissionResponse{
			Result: &meta.Status{
				Message: err.Error(),
			},
		}
	}
	var result admissionapi.AdmissionReview
	result.Response = response
	result.Response.UID = ar.UID
	out, err := json.Marshal(result)
	if err != nil {
		return fmt.Errorf("encode response: %v", err)
	}
	if _, err := w.Write(out); err != nil {
		return fmt.Errorf("write response: %v", err)
	}
	return nil
}
func onlyUpdateStatus(req admissionapi.AdmissionRequest) (*admissionapi.AdmissionResponse, error) {
	logger := logrus.WithFields(logrus.Fields{
		"resource":    req.Resource,
		"subresource": req.SubResource,
		"name":        req.Name,
		"namespace":   req.Namespace,
		"operation":   req.Operation,
	})

	// Does this only update status?
	if req.SubResource == "status" {
		logrus.Info("accept status update")
		return &allow, nil
	}

	// Otherwise, do the specs match?
	var new prowjobv1.ProwJob
	if _, _, err := codecs.UniversalDeserializer().Decode(req.Object.Raw, nil, &new); err != nil {
		return nil, fmt.Errorf("decode new: %v", err)
	}
	var old prowjobv1.ProwJob
	if _, _, err := codecs.UniversalDeserializer().Decode(req.OldObject.Raw, nil, &old); err != nil {
		return nil, fmt.Errorf("decode old: %v", err)
	}
	if equality.Semantic.DeepEqual(old.Spec, new.Spec) {
		logrus.Info("accept update with equivalent spec")
		return &allow, nil // yes
	}
	logger.Info("reject") // no
	return &reject, nil
}
func convertSuiteMeta(suiteMeta gcs.SuitesMeta) resultstore.Suite {
	out := resultstore.Suite{
		Name: path.Base(suiteMeta.Path),
		Files: []resultstore.File{
			{
				ContentType: "text/xml",
				ID:          resultstore.UUID(),
				URL:         suiteMeta.Path, // ensure the junit.xml file appears in artifacts list
			},
		},
	}
	for _, suite := range suiteMeta.Suites.Suites {
		child := resultstore.Suite{
			Name:     suite.Name,
			Duration: dur(suite.Time),
		}
		switch {
		case suite.Failures > 0 && suite.Tests >= suite.Failures:
			child.Failures = append(child.Failures, resultstore.Failure{
				Message: fmt.Sprintf("%d out of %d tests failed (%.1f%% passing)", suite.Failures, suite.Tests, float64(suite.Tests-suite.Failures)*100.0/float64(suite.Tests)),
			})
		case suite.Failures > 0:
			child.Failures = append(child.Failures, resultstore.Failure{
				Message: fmt.Sprintf("%d tests failed", suite.Failures),
			})
		}
		for _, result := range suite.Results {
			name, tags := stripTags(result.Name)
			class := result.ClassName
			if class == "" {
				class = strings.Join(tags, " ")
			} else {
				class += " " + strings.Join(tags, " ")
			}
			c := resultstore.Case{
				Name:     name,
				Class:    class,
				Duration: dur(result.Time),
				Result:   resultstore.Completed,
			}
			const max = 5000 // truncate messages to this length
			msg := result.Message(max)
			switch {
			case result.Failure != nil:
				// failing tests have a completed result with an error
				if msg == "" {
					msg = "unknown failure"
				}
				c.Failures = append(c.Failures, resultstore.Failure{
					Message: msg,
				})
			case result.Skipped != nil:
				c.Result = resultstore.Skipped
				if msg != "" { // skipped results do not require an error, but may.
					c.Errors = append(c.Errors, resultstore.Error{
						Message: msg,
					})
				}
			}
			child.Cases = append(child.Cases, c)
			if c.Duration > child.Duration {
				child.Duration = c.Duration
			}
		}
		if child.Duration > out.Duration {
			// Assume suites run in parallel, so choose max
			out.Duration = child.Duration
		}
		out.Suites = append(out.Suites, child)
	}
	return out
}
func NewHealth() *Health {
	healthMux := http.NewServeMux()
	healthMux.HandleFunc("/healthz", func(w http.ResponseWriter, r *http.Request) { fmt.Fprint(w, "OK") })
	go func() {
		logrus.WithError(http.ListenAndServe(":"+strconv.Itoa(healthPort), healthMux)).Fatal("ListenAndServe returned.")
	}()
	return &Health{
		healthMux: healthMux,
	}
}
func (h *Health) ServeReady() {
	h.healthMux.HandleFunc("/healthz/ready", func(w http.ResponseWriter, r *http.Request) { fmt.Fprint(w, "OK") })
}
func NewController(ghcSync, ghcStatus *github.Client, prowJobClient prowv1.ProwJobInterface, cfg config.Getter, gc *git.Client, maxRecordsPerPool int, opener io.Opener, historyURI, statusURI string, logger *logrus.Entry) (*Controller, error) {
	if logger == nil {
		logger = logrus.NewEntry(logrus.StandardLogger())
	}
	hist, err := history.New(maxRecordsPerPool, opener, historyURI)
	if err != nil {
		return nil, fmt.Errorf("error initializing history client from %q: %v", historyURI, err)
	}
	sc := &statusController{
		logger:         logger.WithField("controller", "status-update"),
		ghc:            ghcStatus,
		config:         cfg,
		newPoolPending: make(chan bool, 1),
		shutDown:       make(chan bool),
		opener:         opener,
		path:           statusURI,
	}
	go sc.run()
	return &Controller{
		logger:        logger.WithField("controller", "sync"),
		ghc:           ghcSync,
		prowJobClient: prowJobClient,
		config:        cfg,
		gc:            gc,
		sc:            sc,
		changedFiles: &changedFilesAgent{
			ghc:             ghcSync,
			nextChangeCache: make(map[changeCacheKey][]string),
		},
		History: hist,
	}, nil
}
func newExpectedContext(c string) Context {
	return Context{
		Context:     githubql.String(c),
		State:       githubql.StatusStateExpected,
		Description: githubql.String(""),
	}
}
func contextsToStrings(contexts []Context) []string {
	var names []string
	for _, c := range contexts {
		names = append(names, string(c.Context))
	}
	return names
}
func (c *Controller) filterSubpools(goroutines int, raw map[string]*subpool) map[string]*subpool {
	filtered := make(map[string]*subpool)
	var lock sync.Mutex

	subpoolsInParallel(
		goroutines,
		raw,
		func(sp *subpool) {
			if err := c.initSubpoolData(sp); err != nil {
				sp.log.WithError(err).Error("Error initializing subpool.")
				return
			}
			key := poolKey(sp.org, sp.repo, sp.branch)
			if spFiltered := filterSubpool(c.ghc, sp); spFiltered != nil {
				sp.log.WithField("key", key).WithField("pool", spFiltered).Debug("filtered sub-pool")

				lock.Lock()
				filtered[key] = spFiltered
				lock.Unlock()
			} else {
				sp.log.WithField("key", key).WithField("pool", spFiltered).Debug("filtering sub-pool removed all PRs")
			}
		},
	)
	return filtered
}
func filterSubpool(ghc githubClient, sp *subpool) *subpool {
	var toKeep []PullRequest
	for _, pr := range sp.prs {
		if !filterPR(ghc, sp, &pr) {
			toKeep = append(toKeep, pr)
		}
	}
	if len(toKeep) == 0 {
		return nil
	}
	sp.prs = toKeep
	return sp
}
func poolPRMap(subpoolMap map[string]*subpool) map[string]PullRequest {
	prs := make(map[string]PullRequest)
	for _, sp := range subpoolMap {
		for _, pr := range sp.prs {
			prs[prKey(&pr)] = pr
		}
	}
	return prs
}
func unsuccessfulContexts(contexts []Context, cc contextChecker, log *logrus.Entry) []Context {
	var failed []Context
	for _, ctx := range contexts {
		if string(ctx.Context) == statusContext {
			continue
		}
		if cc.IsOptional(string(ctx.Context)) {
			continue
		}
		if ctx.State != githubql.StatusStateSuccess {
			failed = append(failed, ctx)
		}
	}
	for _, c := range cc.MissingRequiredContexts(contextsToStrings(contexts)) {
		failed = append(failed, newExpectedContext(c))
	}

	log.Debugf("from %d total contexts (%v) found %d failing contexts: %v", len(contexts), contextsToStrings(contexts), len(failed), contextsToStrings(failed))
	return failed
}
func accumulate(presubmits map[int][]config.Presubmit, prs []PullRequest, pjs []prowapi.ProwJob, log *logrus.Entry) (successes, pendings, nones []PullRequest) {
	for _, pr := range prs {
		// Accumulate the best result for each job.
		psStates := make(map[string]simpleState)
		for _, pj := range pjs {
			if pj.Spec.Type != prowapi.PresubmitJob {
				continue
			}
			if pj.Spec.Refs.Pulls[0].Number != int(pr.Number) {
				continue
			}
			if pj.Spec.Refs.Pulls[0].SHA != string(pr.HeadRefOID) {
				continue
			}

			name := pj.Spec.Context
			oldState := psStates[name]
			newState := toSimpleState(pj.Status.State)
			if oldState == failureState || oldState == "" {
				psStates[name] = newState
			} else if oldState == pendingState && newState == successState {
				psStates[name] = successState
			}
		}
		// The overall result is the worst of the best.
		overallState := successState
		for _, ps := range presubmits[int(pr.Number)] {
			if s, ok := psStates[ps.Context]; !ok {
				overallState = failureState
				log.WithFields(pr.logFields()).Debugf("missing presubmit %s", ps.Context)
				break
			} else if s == failureState {
				overallState = failureState
				log.WithFields(pr.logFields()).Debugf("presubmit %s not passing", ps.Context)
				break
			} else if s == pendingState {
				log.WithFields(pr.logFields()).Debugf("presubmit %s pending", ps.Context)
				overallState = pendingState
			}
		}
		if overallState == successState {
			successes = append(successes, pr)
		} else if overallState == pendingState {
			pendings = append(pendings, pr)
		} else {
			nones = append(nones, pr)
		}
	}
	return
}
func tryMerge(mergeFunc func() error) (bool, error) {
	var err error
	const maxRetries = 3
	backoff := time.Second * 4
	for retry := 0; retry < maxRetries; retry++ {
		if err = mergeFunc(); err == nil {
			// Successful merge!
			return true, nil
		}
		// TODO: Add a config option to abort batches if a PR in the batch
		// cannot be merged for any reason. This would skip merging
		// not just the changed PR, but also the other PRs in the batch.
		// This shouldn't be the default behavior as merging batches is high
		// priority and this is unlikely to be problematic.
		// Note: We would also need to be able to roll back any merges for the
		// batch that were already successfully completed before the failure.
		// Ref: https://github.com/kubernetes/test-infra/issues/10621
		if _, ok := err.(github.ModifiedHeadError); ok {
			// This is a possible source of incorrect behavior. If someone
			// modifies their PR as we try to merge it in a batch then we
			// end up in an untested state. This is unlikely to cause any
			// real problems.
			return true, fmt.Errorf("PR was modified: %v", err)
		} else if _, ok = err.(github.UnmergablePRBaseChangedError); ok {
			//  complained that the base branch was modified. This is a
			// strange error because the API doesn't even allow the request to
			// specify the base branch sha, only the head sha.
			// We suspect that github is complaining because we are making the
			// merge requests too rapidly and it cannot recompute mergability
			// in time. https://github.com/kubernetes/test-infra/issues/5171
			// We handle this by sleeping for a few seconds before trying to
			// merge again.
			err = fmt.Errorf("base branch was modified: %v", err)
			if retry+1 < maxRetries {
				sleep(backoff)
				backoff *= 2
			}
		} else if _, ok = err.(github.UnauthorizedToPushError); ok {
			// GitHub let us know that the token used cannot push to the branch.
			// Even if the robot is set up to have write access to the repo, an
			// overzealous branch protection setting will not allow the robot to
			// push to a specific branch.
			// We won't be able to merge the other PRs.
			return false, fmt.Errorf("branch needs to be configured to allow this robot to push: %v", err)
		} else if _, ok = err.(github.MergeCommitsForbiddenError); ok {
			// GitHub let us know that the merge method configured for this repo
			// is not allowed by other repo settings, so we should let the admins
			// know that the configuration needs to be updated.
			// We won't be able to merge the other PRs.
			return false, fmt.Errorf("Tide needs to be configured to use the 'rebase' merge method for this repo or the repo needs to allow merge commits: %v", err)
		} else if _, ok = err.(github.UnmergablePRError); ok {
			return true, fmt.Errorf("PR is unmergable. Do the Tide merge requirements match the GitHub settings for the repo? %v", err)
		} else {
			return true, err
		}
	}
	// We ran out of retries. Return the last transient error.
	return true, err
}
func (c *changedFilesAgent) prChanges(pr *PullRequest) config.ChangedFilesProvider {
	return func() ([]string, error) {
		cacheKey := changeCacheKey{
			org:    string(pr.Repository.Owner.Login),
			repo:   string(pr.Repository.Name),
			number: int(pr.Number),
			sha:    string(pr.HeadRefOID),
		}

		c.RLock()
		changedFiles, ok := c.changeCache[cacheKey]
		if ok {
			c.RUnlock()
			c.Lock()
			c.nextChangeCache[cacheKey] = changedFiles
			c.Unlock()
			return changedFiles, nil
		}
		if changedFiles, ok = c.nextChangeCache[cacheKey]; ok {
			c.RUnlock()
			return changedFiles, nil
		}
		c.RUnlock()

		// We need to query the changes from GitHub.
		changes, err := c.ghc.GetPullRequestChanges(
			string(pr.Repository.Owner.Login),
			string(pr.Repository.Name),
			int(pr.Number),
		)
		if err != nil {
			return nil, fmt.Errorf("error getting PR changes for #%d: %v", int(pr.Number), err)
		}
		changedFiles = make([]string, 0, len(changes))
		for _, change := range changes {
			changedFiles = append(changedFiles, change.Filename)
		}

		c.Lock()
		c.nextChangeCache[cacheKey] = changedFiles
		c.Unlock()
		return changedFiles, nil
	}
}
func (c *changedFilesAgent) prune() {
	c.Lock()
	defer c.Unlock()
	c.changeCache = c.nextChangeCache
	c.nextChangeCache = make(map[changeCacheKey][]string)
}
func (c *Controller) dividePool(pool map[string]PullRequest, pjs []prowapi.ProwJob) (map[string]*subpool, error) {
	sps := make(map[string]*subpool)
	for _, pr := range pool {
		org := string(pr.Repository.Owner.Login)
		repo := string(pr.Repository.Name)
		branch := string(pr.BaseRef.Name)
		branchRef := string(pr.BaseRef.Prefix) + string(pr.BaseRef.Name)
		fn := poolKey(org, repo, branch)
		if sps[fn] == nil {
			sha, err := c.ghc.GetRef(org, repo, strings.TrimPrefix(branchRef, "refs/"))
			if err != nil {
				return nil, err
			}
			sps[fn] = &subpool{
				log: c.logger.WithFields(logrus.Fields{
					"org":      org,
					"repo":     repo,
					"branch":   branch,
					"base-sha": sha,
				}),
				org:    org,
				repo:   repo,
				branch: branch,
				sha:    sha,
			}
		}
		sps[fn].prs = append(sps[fn].prs, pr)
	}
	for _, pj := range pjs {
		if pj.Spec.Type != prowapi.PresubmitJob && pj.Spec.Type != prowapi.BatchJob {
			continue
		}
		fn := poolKey(pj.Spec.Refs.Org, pj.Spec.Refs.Repo, pj.Spec.Refs.BaseRef)
		if sps[fn] == nil || pj.Spec.Refs.BaseSHA != sps[fn].sha {
			continue
		}
		sps[fn].pjs = append(sps[fn].pjs, pj)
	}
	return sps, nil
}
func AggregateProfiles(profiles [][]*cover.Profile) ([]*cover.Profile, error) {
	setProfiles := make([][]*cover.Profile, 0, len(profiles))
	for _, p := range profiles {
		c := countToBoolean(p)
		setProfiles = append(setProfiles, c)
	}
	aggregateProfiles, err := MergeMultipleProfiles(setProfiles)
	if err != nil {
		return nil, err
	}
	return aggregateProfiles, nil
}
func countToBoolean(profile []*cover.Profile) []*cover.Profile {
	setProfile := make([]*cover.Profile, 0, len(profile))
	for _, p := range profile {
		pc := deepCopyProfile(*p)
		for i := range pc.Blocks {
			if pc.Blocks[i].Count > 0 {
				pc.Blocks[i].Count = 1
			}
		}
		setProfile = append(setProfile, &pc)
	}
	return setProfile
}
func NewStorage(r storage.PersistenceLayer, storage string) (*Storage, error) {
	s := &Storage{
		resources: r,
	}

	if storage != "" {
		var data struct {
			Resources []common.Resource
		}
		buf, err := ioutil.ReadFile(storage)
		if err == nil {
			logrus.Infof("Current state: %s.", string(buf))
			err = json.Unmarshal(buf, &data)
			if err != nil {
				return nil, err
			}
		} else if !os.IsNotExist(err) {
			return nil, err
		}

		logrus.Info("Before adding resource loop")
		for _, res := range data.Resources {
			if err := s.AddResource(res); err != nil {
				logrus.WithError(err).Errorf("Failed Adding Resources: %s - %s.", res.Name, res.State)
			}
			logrus.Infof("Successfully Added Resources: %s - %s.", res.Name, res.State)
		}
	}
	return s, nil
}
func (s *Storage) AddResource(resource common.Resource) error {
	return s.resources.Add(resource)
}
func (s *Storage) DeleteResource(name string) error {
	return s.resources.Delete(name)
}
func (s *Storage) UpdateResource(resource common.Resource) error {
	return s.resources.Update(resource)
}
func (s *Storage) GetResource(name string) (common.Resource, error) {
	i, err := s.resources.Get(name)
	if err != nil {
		return common.Resource{}, err
	}
	var res common.Resource
	res, err = common.ItemToResource(i)
	if err != nil {
		return common.Resource{}, err
	}
	return res, nil
}
func (s *Storage) GetResources() ([]common.Resource, error) {
	var resources []common.Resource
	items, err := s.resources.List()
	if err != nil {
		return resources, err
	}
	for _, i := range items {
		var res common.Resource
		res, err = common.ItemToResource(i)
		if err != nil {
			return nil, err
		}
		resources = append(resources, res)
	}
	sort.Stable(common.ResourceByUpdateTime(resources))
	return resources, nil
}
func (s *Storage) SyncResources(data []common.Resource) error {
	s.resourcesLock.Lock()
	defer s.resourcesLock.Unlock()

	resources, err := s.GetResources()
	if err != nil {
		logrus.WithError(err).Error("cannot find resources")
		return err
	}

	var finalError error

	// delete non-exist resource
	valid := 0
	for _, res := range resources {
		// If currently busy, yield deletion to later cycles.
		if res.Owner != "" {
			resources[valid] = res
			valid++
			continue
		}
		toDelete := true
		for _, newRes := range data {
			if res.Name == newRes.Name {
				resources[valid] = res
				valid++
				toDelete = false
				break
			}
		}
		if toDelete {
			logrus.Infof("Deleting resource %s", res.Name)
			if err := s.DeleteResource(res.Name); err != nil {
				finalError = multierror.Append(finalError, err)
				logrus.WithError(err).Errorf("unable to delete resource %s", res.Name)
			}
		}
	}
	resources = resources[:valid]

	// add new resource
	for _, p := range data {
		found := false
		for idx := range resources {
			exist := resources[idx]
			if p.Name == exist.Name {
				found = true
				logrus.Infof("Keeping resource %s", p.Name)
				break
			}
		}

		if !found {
			if p.State == "" {
				p.State = common.Free
			}
			logrus.Infof("Adding resource %s", p.Name)
			resources = append(resources, p)
			if err := s.AddResource(p); err != nil {
				logrus.WithError(err).Errorf("unable to add resource %s", p.Name)
				finalError = multierror.Append(finalError, err)
			}
		}
	}
	return finalError
}
func ParseConfig(configPath string) ([]common.Resource, error) {
	file, err := ioutil.ReadFile(configPath)
	if err != nil {
		return nil, err
	}

	var data common.BoskosConfig
	err = yaml.Unmarshal(file, &data)
	if err != nil {
		return nil, err
	}

	var resources []common.Resource
	for _, entry := range data.Resources {
		resources = append(resources, common.NewResourcesFromConfig(entry)...)
	}
	return resources, nil
}
func problemsInFiles(r *git.Repo, files map[string]string) (map[string][]string, error) {
	problems := make(map[string][]string)
	for f := range files {
		src, err := ioutil.ReadFile(filepath.Join(r.Dir, f))
		if err != nil {
			return nil, err
		}
		// This is modeled after the logic from buildifier:
		// https://github.com/bazelbuild/buildtools/blob/8818289/buildifier/buildifier.go#L261
		content, err := build.Parse(f, src)
		if err != nil {
			return nil, fmt.Errorf("parsing as Bazel file %v", err)
		}
		beforeRewrite := build.Format(content)
		var info build.RewriteInfo
		build.Rewrite(content, &info)
		ndata := build.Format(content)
		if !bytes.Equal(src, ndata) && !bytes.Equal(src, beforeRewrite) {
			// TODO(mattmoor): This always seems to be empty?
			problems[f] = uniqProblems(info.Log)
		}
	}
	return problems, nil
}
func NewPodLogArtifact(jobName string, buildID string, sizeLimit int64, ja jobAgent) (*PodLogArtifact, error) {
	if jobName == "" {
		return nil, errInsufficientJobInfo
	}
	if buildID == "" {
		return nil, errInsufficientJobInfo
	}
	if sizeLimit < 0 {
		return nil, errInvalidSizeLimit
	}
	return &PodLogArtifact{
		name:      jobName,
		buildID:   buildID,
		sizeLimit: sizeLimit,
		jobAgent:  ja,
	}, nil
}
func (a *PodLogArtifact) CanonicalLink() string {
	q := url.Values{
		"job": []string{a.name},
		"id":  []string{a.buildID},
	}
	u := url.URL{
		Path:     "/log",
		RawQuery: q.Encode(),
	}
	return u.String()
}
func (a *PodLogArtifact) ReadAt(p []byte, off int64) (n int, err error) {
	logs, err := a.jobAgent.GetJobLog(a.name, a.buildID)
	if err != nil {
		return 0, fmt.Errorf("error getting pod log: %v", err)
	}
	r := bytes.NewReader(logs)
	readBytes, err := r.ReadAt(p, off)
	if err == io.EOF {
		return readBytes, io.EOF
	}
	if err != nil {
		return 0, fmt.Errorf("error reading pod logs: %v", err)
	}
	return readBytes, nil
}
func (a *PodLogArtifact) ReadAll() ([]byte, error) {
	size, err := a.Size()
	if err != nil {
		return nil, fmt.Errorf("error getting pod log size: %v", err)
	}
	if size > a.sizeLimit {
		return nil, lenses.ErrFileTooLarge
	}
	logs, err := a.jobAgent.GetJobLog(a.name, a.buildID)
	if err != nil {
		return nil, fmt.Errorf("error getting pod log: %v", err)
	}
	return logs, nil
}
func (a *PodLogArtifact) ReadAtMost(n int64) ([]byte, error) {
	logs, err := a.jobAgent.GetJobLog(a.name, a.buildID)
	if err != nil {
		return nil, fmt.Errorf("error getting pod log: %v", err)
	}
	reader := bytes.NewReader(logs)
	var byteCount int64
	var p []byte
	for byteCount < n {
		b, err := reader.ReadByte()
		if err == io.EOF {
			return p, io.EOF
		}
		if err != nil {
			return nil, fmt.Errorf("error reading pod log: %v", err)
		}
		p = append(p, b)
		byteCount++
	}
	return p, nil
}
func (a *PodLogArtifact) ReadTail(n int64) ([]byte, error) {
	logs, err := a.jobAgent.GetJobLog(a.name, a.buildID)
	if err != nil {
		return nil, fmt.Errorf("error getting pod log tail: %v", err)
	}
	size := int64(len(logs))
	var off int64
	if n > size {
		off = 0
	} else {
		off = size - n
	}
	p := make([]byte, n)
	readBytes, err := bytes.NewReader(logs).ReadAt(p, off)
	if err != nil && err != io.EOF {
		return nil, fmt.Errorf("error reading pod log tail: %v", err)
	}
	return p[:readBytes], nil
}
func newProblems(cs []github.ReviewComment, ps map[string]map[int]lint.Problem) map[string]map[int]lint.Problem {
	// Make a copy, then remove the old elements.
	res := make(map[string]map[int]lint.Problem)
	for f, ls := range ps {
		res[f] = make(map[int]lint.Problem)
		for l, p := range ls {
			res[f][l] = p
		}
	}
	for _, c := range cs {
		if c.Position == nil {
			continue
		}
		if !strings.Contains(c.Body, commentTag) {
			continue
		}
		delete(res[c.Path], *c.Position)
	}
	return res
}
func problemsInFiles(r *git.Repo, files map[string]string) (map[string]map[int]lint.Problem, []github.DraftReviewComment) {
	problems := make(map[string]map[int]lint.Problem)
	var lintErrorComments []github.DraftReviewComment
	l := new(lint.Linter)
	for f, patch := range files {
		problems[f] = make(map[int]lint.Problem)
		src, err := ioutil.ReadFile(filepath.Join(r.Dir, f))
		if err != nil {
			lintErrorComments = append(lintErrorComments, github.DraftReviewComment{
				Path: f,
				Body: fmt.Sprintf("%v", err),
			})
		}
		ps, err := l.Lint(f, src)
		if err != nil {
			// Get error line by parsing the error message
			errLineIndexStart := strings.LastIndex(err.Error(), f) + len(f)
			reNumber := regexp.MustCompile(`:([0-9]+):`)
			matches := reNumber.FindStringSubmatch(err.Error()[errLineIndexStart:])
			newComment := github.DraftReviewComment{
				Path: f,
				Body: err.Error(),
			}
			if len(matches) > 1 {
				errLineString := matches[1]
				errLine, errAtoi := strconv.Atoi(errLineString)
				if errAtoi == nil {
					newComment.Position = errLine
				}
				// Trim error message to after the line and column numbers
				reTrimError := regexp.MustCompile(`(:[0-9]+:[0-9]+: )`)
				matches = reTrimError.FindStringSubmatch(err.Error())
				if len(matches) > 0 {
					newComment.Body = err.Error()[len(matches[0])+errLineIndexStart:]
				}
			}
			lintErrorComments = append(lintErrorComments, newComment)
		}
		al, err := AddedLines(patch)
		if err != nil {
			lintErrorComments = append(lintErrorComments,
				github.DraftReviewComment{
					Path: f,
					Body: fmt.Sprintf("computing added lines in %s: %v", f, err),
				})
		}
		for _, p := range ps {
			if pl, ok := al[p.Position.Line]; ok {
				problems[f][pl] = p
			}
		}
	}
	return problems, lintErrorComments
}
func undoPreset(preset *config.Preset, labels map[string]string, pod *coreapi.PodSpec) {
	// skip presets that do not match the job labels
	for l, v := range preset.Labels {
		if v2, ok := labels[l]; !ok || v2 != v {
			return
		}
	}

	// collect up preset created keys
	removeEnvNames := sets.NewString()
	for _, e1 := range preset.Env {
		removeEnvNames.Insert(e1.Name)
	}
	removeVolumeNames := sets.NewString()
	for _, volume := range preset.Volumes {
		removeVolumeNames.Insert(volume.Name)
	}
	removeVolumeMountNames := sets.NewString()
	for _, volumeMount := range preset.VolumeMounts {
		removeVolumeMountNames.Insert(volumeMount.Name)
	}

	// remove volumes from spec
	filteredVolumes := []coreapi.Volume{}
	for _, volume := range pod.Volumes {
		if !removeVolumeNames.Has(volume.Name) {
			filteredVolumes = append(filteredVolumes, volume)
		}
	}
	pod.Volumes = filteredVolumes

	// remove env and volume mounts from containers
	for i := range pod.Containers {
		filteredEnv := []coreapi.EnvVar{}
		for _, env := range pod.Containers[i].Env {
			if !removeEnvNames.Has(env.Name) {
				filteredEnv = append(filteredEnv, env)
			}
		}
		pod.Containers[i].Env = filteredEnv

		filteredVolumeMounts := []coreapi.VolumeMount{}
		for _, mount := range pod.Containers[i].VolumeMounts {
			if !removeVolumeMountNames.Has(mount.Name) {
				filteredVolumeMounts = append(filteredVolumeMounts, mount)
			}
		}
		pod.Containers[i].VolumeMounts = filteredVolumeMounts
	}
}
func undoPresubmitPresets(presets []config.Preset, presubmit *config.Presubmit) {
	if presubmit.Spec == nil {
		return
	}
	for _, preset := range presets {
		undoPreset(&preset, presubmit.Labels, presubmit.Spec)
	}
}
func yamlBytesStripNulls(yamlBytes []byte) []byte {
	nullRE := regexp.MustCompile("(?m)[\n]+^[^\n]+: null$")
	return nullRE.ReplaceAll(yamlBytes, []byte{})
}
func monitorDiskAndEvict(
	c *diskcache.Cache,
	interval time.Duration,
	minPercentBlocksFree, evictUntilPercentBlocksFree float64,
) {
	diskRoot := c.DiskRoot()
	// forever check if usage is past thresholds and evict
	ticker := time.NewTicker(interval)
	for ; true; <-ticker.C {
		blocksFree, _, _, err := diskutil.GetDiskUsage(diskRoot)
		if err != nil {
			logrus.WithError(err).Error("Failed to get disk usage!")
			continue
		}
		logger := logrus.WithFields(logrus.Fields{
			"sync-loop":   "MonitorDiskAndEvict",
			"blocks-free": blocksFree,
		})
		logger.Info("tick")
		// if we are past the threshold, start evicting
		if blocksFree < minPercentBlocksFree {
			logger.Warn("Eviction triggered")
			// get all cache entries and sort by lastaccess
			// so we can pop entries until we have evicted enough
			files := c.GetEntries()
			sort.Slice(files, func(i, j int) bool {
				return files[i].LastAccess.Before(files[j].LastAccess)
			})
			// evict until we pass the safe threshold so we don't thrash at the eviction trigger
			for blocksFree < evictUntilPercentBlocksFree {
				if len(files) < 1 {
					logger.Fatal("Failed to find entries to evict!")
				}
				// pop entry and delete
				var entry diskcache.EntryInfo
				entry, files = files[0], files[1:]
				err = c.Delete(c.PathToKey(entry.Path))
				if err != nil {
					logger.WithError(err).Errorf("Error deleting entry at path: %v", entry.Path)
				} else {
					promMetrics.FilesEvicted.Inc()
					promMetrics.LastEvictedAccessAge.Set(time.Now().Sub(entry.LastAccess).Hours())
				}
				// get new disk usage
				blocksFree, _, _, err = diskutil.GetDiskUsage(diskRoot)
				logger = logrus.WithFields(logrus.Fields{
					"sync-loop":   "MonitorDiskAndEvict",
					"blocks-free": blocksFree,
				})
				if err != nil {
					logrus.WithError(err).Error("Failed to get disk usage!")
					continue
				}
			}
			logger.Info("Done evicting")
		}
	}
}
func (c *orgRepoConfig) difference(c2 *orgRepoConfig) *orgRepoConfig {
	res := &orgRepoConfig{
		orgExceptions: make(map[string]sets.String),
		repos:         sets.NewString().Union(c.repos),
	}
	for org, excepts1 := range c.orgExceptions {
		if excepts2, ok := c2.orgExceptions[org]; ok {
			res.repos.Insert(excepts2.Difference(excepts1).UnsortedList()...)
		} else {
			excepts := sets.NewString().Union(excepts1)
			// Add any applicable repos in repos2 to excepts
			for _, repo := range c2.repos.UnsortedList() {
				if parts := strings.SplitN(repo, "/", 2); len(parts) == 2 && parts[0] == org {
					excepts.Insert(repo)
				}
			}
			res.orgExceptions[org] = excepts
		}
	}

	res.repos = res.repos.Difference(c2.repos)

	for _, repo := range res.repos.UnsortedList() {
		if parts := strings.SplitN(repo, "/", 2); len(parts) == 2 {
			if excepts2, ok := c2.orgExceptions[parts[0]]; ok && !excepts2.Has(repo) {
				res.repos.Delete(repo)
			}
		}
	}
	return res
}
func (c *orgRepoConfig) union(c2 *orgRepoConfig) *orgRepoConfig {
	res := &orgRepoConfig{
		orgExceptions: make(map[string]sets.String),
		repos:         sets.NewString(),
	}

	for org, excepts1 := range c.orgExceptions {
		// keep only items in both blacklists that are not in the
		// explicit repo whitelists for the other configuration;
		// we know from how the orgRepoConfigs are constructed that
		// a org blacklist won't intersect it's own repo whitelist
		pruned := excepts1.Difference(c2.repos)
		if excepts2, ok := c2.orgExceptions[org]; ok {
			res.orgExceptions[org] = pruned.Intersection(excepts2.Difference(c.repos))
		} else {
			res.orgExceptions[org] = pruned
		}
	}

	for org, excepts2 := range c2.orgExceptions {
		// update any blacklists not previously updated
		if _, exists := res.orgExceptions[org]; !exists {
			res.orgExceptions[org] = excepts2.Difference(c.repos)
		}
	}

	// we need to prune out repos in the whitelists which are
	// covered by an org already; we know from above that no
	// org blacklist in the result will contain a repo whitelist
	for _, repo := range c.repos.Union(c2.repos).UnsortedList() {
		parts := strings.SplitN(repo, "/", 2)
		if len(parts) != 2 {
			logrus.Warnf("org/repo %q is formatted incorrectly", repo)
			continue
		}
		if _, exists := res.orgExceptions[parts[0]]; !exists {
			res.repos.Insert(repo)
		}
	}
	return res
}
func clearStaleComments(gc githubClient, log *logrus.Entry, pr *github.PullRequestEvent, prLabels sets.String, comments []github.IssueComment) error {
	// If the PR must follow the process and hasn't yet completed the process, don't remove comments.
	if prMustFollowRelNoteProcess(gc, log, pr, prLabels, false) && !releaseNoteAlreadyAdded(prLabels) {
		return nil
	}
	botName, err := gc.BotName()
	if err != nil {
		return err
	}
	return gc.DeleteStaleComments(
		pr.Repo.Owner.Login,
		pr.Repo.Name,
		pr.Number,
		comments,
		func(c github.IssueComment) bool { // isStale function
			return c.User.Login == botName &&
				(strings.Contains(c.Body, releaseNoteBody) ||
					strings.Contains(c.Body, parentReleaseNoteBody))
		},
	)
}
func determineReleaseNoteLabel(body string) string {
	composedReleaseNote := strings.ToLower(strings.TrimSpace(getReleaseNote(body)))

	if composedReleaseNote == "" {
		return ReleaseNoteLabelNeeded
	}
	if noneRe.MatchString(composedReleaseNote) {
		return releaseNoteNone
	}
	if strings.Contains(composedReleaseNote, actionRequiredNote) {
		return releaseNoteActionRequired
	}
	return releaseNote
}
func getReleaseNote(body string) string {
	potentialMatch := noteMatcherRE.FindStringSubmatch(body)
	if potentialMatch == nil {
		return ""
	}
	return strings.TrimSpace(potentialMatch[1])
}
func NewClient(boskosClient boskosClient) *Client {
	return &Client{
		basic:     boskosClient,
		resources: map[string]common.Resource{},
	}
}
func (c *Client) Acquire(rtype, state, dest string) (*common.Resource, error) {
	var resourcesToRelease []common.Resource
	releaseOnFailure := func() {
		for _, r := range resourcesToRelease {
			if err := c.basic.ReleaseOne(r.Name, common.Dirty); err != nil {
				logrus.WithError(err).Warningf("failed to release resource %s", r.Name)
			}
		}
	}
	res, err := c.basic.Acquire(rtype, state, dest)
	if err != nil {
		return nil, err
	}
	var leasedResources common.LeasedResources
	if err = res.UserData.Extract(LeasedResources, &leasedResources); err != nil {
		if _, ok := err.(*common.UserDataNotFound); !ok {
			logrus.WithError(err).Errorf("cannot parse %s from User Data", LeasedResources)
			return nil, err
		}
	}
	resourcesToRelease = append(resourcesToRelease, *res)
	resources, err := c.basic.AcquireByState(res.Name, dest, leasedResources)
	if err != nil {
		releaseOnFailure()
		return nil, err
	}
	resourcesToRelease = append(resourcesToRelease, resources...)
	c.updateResource(*res)
	return res, nil
}
func (c *Client) ReleaseOne(name, dest string) (allErrors error) {
	res, err := c.getResource(name)
	if err != nil {
		allErrors = err
		return
	}
	resourceNames := []string{name}
	var leasedResources common.LeasedResources
	if err := res.UserData.Extract(LeasedResources, &leasedResources); err != nil {
		if _, ok := err.(*common.UserDataNotFound); !ok {
			logrus.WithError(err).Errorf("cannot parse %s from User Data", LeasedResources)
			allErrors = multierror.Append(allErrors, err)
			if err := c.basic.ReleaseOne(name, dest); err != nil {
				logrus.WithError(err).Warningf("failed to release resource %s", name)
				allErrors = multierror.Append(allErrors, err)
			}
			return
		}
	}
	resourceNames = append(resourceNames, leasedResources...)
	for _, n := range resourceNames {
		if err := c.basic.ReleaseOne(n, dest); err != nil {
			logrus.WithError(err).Warningf("failed to release resource %s", n)
			allErrors = multierror.Append(allErrors, err)
		}
	}
	c.deleteResource(name)
	return
}
func (c *Client) UpdateAll(state string) error {
	return c.basic.UpdateAll(state)
}
func GetGitHubClient(token string) *github.Client {
	return github.NewClient(
		oauth2.NewClient(
			oauth2.NoContext,
			oauth2.StaticTokenSource(&oauth2.Token{AccessToken: token}),
		),
	)
}
func GetUsername(client *github.Client) (string, error) {
	user, _, err := client.Users.Get(context.Background(), "")
	if err != nil {
		return "", err
	}
	if user.Login == nil {
		return "", errors.New("Users.Get(\"\") returned empty login")
	}

	return *user.Login, nil
}
func CreateTokenHandler(tokenStream io.Reader, influxdb *InfluxDB) (*TokenHandler, error) {
	token, err := ioutil.ReadAll(tokenStream)
	if err != nil {
		return nil, err
	}
	client := GetGitHubClient(strings.TrimSpace(string(token)))
	login, err := GetUsername(client) // Get user name for token
	if err != nil {
		return nil, err
	}

	return &TokenHandler{
		gClient:  client,
		login:    login,
		influxdb: influxdb,
	}, nil
}
func CreateTokenHandlers(tokenFiles []string, influxdb *InfluxDB) ([]TokenHandler, error) {
	tokens := []TokenHandler{}
	for _, tokenFile := range tokenFiles {
		f, err := os.Open(tokenFile)
		if err != nil {
			return nil, fmt.Errorf("Can't open token-file (%s): %s", tokenFile, err)
		}
		token, err := CreateTokenHandler(f, influxdb)
		if err != nil {
			return nil, fmt.Errorf("Failed to create token (%s): %s", tokenFile, err)
		}
		tokens = append(tokens, *token)
	}
	return tokens, nil
}
func (i *jobIndentifier) String() string {
	return fmt.Sprintf("%s %s/%s#%d", i.job, i.organization, i.repository, i.pullRequest)
}
func TerminateOlderPresubmitJobs(pjc prowClient, log *logrus.Entry, pjs []prowapi.ProwJob,
	cleanup ProwJobResourcesCleanup) error {
	dupes := map[jobIndentifier]int{}
	for i, pj := range pjs {
		if pj.Complete() || pj.Spec.Type != prowapi.PresubmitJob {
			continue
		}

		ji := jobIndentifier{
			job:          pj.Spec.Job,
			organization: pj.Spec.Refs.Org,
			repository:   pj.Spec.Refs.Repo,
			pullRequest:  pj.Spec.Refs.Pulls[0].Number,
		}
		prev, ok := dupes[ji]
		if !ok {
			dupes[ji] = i
			continue
		}
		cancelIndex := i
		if (&pjs[prev].Status.StartTime).Before(&pj.Status.StartTime) {
			cancelIndex = prev
			dupes[ji] = i
		}
		toCancel := pjs[cancelIndex]

		// TODO cancel the prow job before cleaning up its resources and make this system
		// independent.
		// See this discussion for more details:  https://github.com/kubernetes/test-infra/pull/11451#discussion_r263523932
		if err := cleanup(toCancel); err != nil {
			log.WithError(err).WithFields(ProwJobFields(&toCancel)).Warn("Cannot clean up job resources")
		}

		toCancel.SetComplete()
		prevState := toCancel.Status.State
		toCancel.Status.State = prowapi.AbortedState
		log.WithFields(ProwJobFields(&toCancel)).
			WithField("from", prevState).
			WithField("to", toCancel.Status.State).Info("Transitioning states")

		npj, err := pjc.ReplaceProwJob(toCancel.ObjectMeta.Name, toCancel)
		if err != nil {
			return err
		}
		pjs[cancelIndex] = npj
	}

	return nil
}
func PushMetrics(component, endpoint string, interval time.Duration) {
	sig := make(chan os.Signal, 1)
	signal.Notify(sig, os.Interrupt, syscall.SIGTERM)

	for {
		select {
		case <-time.Tick(interval):
			if err := push.FromGatherer(component, push.HostnameGroupingKey(), endpoint, prometheus.DefaultGatherer); err != nil {
				logrus.WithField("component", component).WithError(err).Error("Failed to push metrics.")
			}
		case <-sig:
			logrus.WithField("component", component).Infof("Metrics pusher shutting down...")
			return
		}
	}
}
func RateLimiter(controllerName string) workqueue.RateLimitingInterface {
	rl := workqueue.NewMaxOfRateLimiter(
		workqueue.NewItemExponentialFailureRateLimiter(5*time.Millisecond, 120*time.Second),
		&workqueue.BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(1000), 50000)},
	)
	return workqueue.NewNamedRateLimitingQueue(rl, controllerName)
}
func checkExistingStatus(gc gitHubClient, l *logrus.Entry, org, repo, sha string) (string, error) {
	statuses, err := gc.ListStatuses(org, repo, sha)
	if err != nil {
		return "", fmt.Errorf("error listing pull request statuses: %v", err)
	}

	existingStatus := ""
	for _, status := range statuses {
		if status.Context != dcoContextName {
			continue
		}
		existingStatus = status.State
		break
	}
	l.Debugf("Existing DCO status context status is %q", existingStatus)
	return existingStatus, nil
}
func checkExistingLabels(gc gitHubClient, l *logrus.Entry, org, repo string, number int) (hasYesLabel, hasNoLabel bool, err error) {
	labels, err := gc.GetIssueLabels(org, repo, number)
	if err != nil {
		return false, false, fmt.Errorf("error getting pull request labels: %v", err)
	}

	for _, l := range labels {
		if l.Name == dcoYesLabel {
			hasYesLabel = true
		}
		if l.Name == dcoNoLabel {
			hasNoLabel = true
		}
	}

	return hasYesLabel, hasNoLabel, nil
}
func handle(gc gitHubClient, cp commentPruner, log *logrus.Entry, org, repo string, pr github.PullRequest, addComment bool) error {
	l := log.WithField("pr", pr.Number)

	commitsMissingDCO, err := checkCommitMessages(gc, l, org, repo, pr.Number)
	if err != nil {
		l.WithError(err).Infof("Error running DCO check against commits in PR")
		return err
	}

	existingStatus, err := checkExistingStatus(gc, l, org, repo, pr.Head.SHA)
	if err != nil {
		l.WithError(err).Infof("Error checking existing PR status")
		return err
	}

	hasYesLabel, hasNoLabel, err := checkExistingLabels(gc, l, org, repo, pr.Number)
	if err != nil {
		l.WithError(err).Infof("Error checking existing PR labels")
		return err
	}

	return takeAction(gc, cp, l, org, repo, pr, commitsMissingDCO, existingStatus, hasYesLabel, hasNoLabel, addComment)
}
func MarkdownSHAList(org, repo string, list []github.GitCommit) string {
	lines := make([]string, len(list))
	lineFmt := "- [%s](https://github.com/%s/%s/commits/%s) %s"
	for i, commit := range list {
		if commit.SHA == "" {
			continue
		}
		// if we somehow encounter a SHA that's less than 7 characters, we will
		// just use it as is.
		shortSHA := commit.SHA
		if len(shortSHA) > 7 {
			shortSHA = shortSHA[:7]
		}

		// get the first line of the commit
		message := strings.Split(commit.Message, "\n")[0]

		lines[i] = fmt.Sprintf(lineFmt, shortSHA, org, repo, commit.SHA, message)
	}
	return strings.Join(lines, "\n")
}
func PathForSpec(spec *downwardapi.JobSpec, pathSegment RepoPathBuilder) string {
	switch spec.Type {
	case prowapi.PeriodicJob, prowapi.PostsubmitJob:
		return path.Join(NonPRLogs, spec.Job, spec.BuildID)
	case prowapi.PresubmitJob:
		return path.Join(PRLogs, "pull", pathSegment(spec.Refs.Org, spec.Refs.Repo), strconv.Itoa(spec.Refs.Pulls[0].Number), spec.Job, spec.BuildID)
	case prowapi.BatchJob:
		return path.Join(PRLogs, "pull", "batch", spec.Job, spec.BuildID)
	default:
		logrus.Fatalf("unknown job spec type: %v", spec.Type)
	}
	return ""
}
func AliasForSpec(spec *downwardapi.JobSpec) string {
	switch spec.Type {
	case prowapi.PeriodicJob, prowapi.PostsubmitJob, prowapi.BatchJob:
		return ""
	case prowapi.PresubmitJob:
		return path.Join(PRLogs, "directory", spec.Job, fmt.Sprintf("%s.txt", spec.BuildID))
	default:
		logrus.Fatalf("unknown job spec type: %v", spec.Type)
	}
	return ""
}
func RootForSpec(spec *downwardapi.JobSpec) string {
	switch spec.Type {
	case prowapi.PeriodicJob, prowapi.PostsubmitJob:
		return path.Join(NonPRLogs, spec.Job)
	case prowapi.PresubmitJob, prowapi.BatchJob:
		return path.Join(PRLogs, "directory", spec.Job)
	default:
		logrus.Errorf("unknown job spec type: %v", spec.Type)
	}
	return ""
}
func NewSingleDefaultRepoPathBuilder(defaultOrg, defaultRepo string) RepoPathBuilder {
	return func(org, repo string) string {
		if org == defaultOrg && repo == defaultRepo {
			return ""
		}
		// handle gerrit repo
		repo = strings.Replace(repo, "/", "_", -1)
		return fmt.Sprintf("%s_%s", org, repo)
	}
}
func NewExplicitRepoPathBuilder() RepoPathBuilder {
	return func(org, repo string) string {
		// handle gerrit repo
		repo = strings.Replace(repo, "/", "_", -1)
		return fmt.Sprintf("%s_%s", org, repo)
	}
}
func RegisterSourceOrDie(name string, src IssueSource) {
	if _, ok := sources[name]; ok {
		glog.Fatalf("Cannot register an IssueSource with name %q, already exists!", name)
	}
	sources[name] = src
	glog.Infof("Registered issue source '%s'.", name)
}
func (c *IssueCreator) CreateAndSync() {
	var err error
	if err = c.initialize(); err != nil {
		glog.Fatalf("Error initializing IssueCreator: %v.", err)
	}
	glog.Info("IssueCreator initialization complete.")

	for srcName, src := range sources {
		glog.Infof("Generating issues from source: %s.", srcName)
		var issues []Issue
		if issues, err = src.Issues(c); err != nil {
			glog.Errorf("Error generating issues. Source: %s Msg: %v.", srcName, err)
			continue
		}

		// Note: We assume that no issues made by this bot with ID's matching issues generated by
		// sources will be created while this code is creating issues. If this is a possibility then
		// this loop should be updated to fetch recently changed issues from github after every issue
		// sync that results in an issue being created.
		glog.Infof("Syncing issues from source: %s.", srcName)
		created := 0
		for _, issue := range issues {
			if c.sync(issue) {
				created++
			}
		}
		glog.Infof(
			"Created issues for %d of the %d issues synced from source: %s.",
			created,
			len(issues),
			srcName,
		)
	}
}
func (c *IssueCreator) loadCache() error {
	user, err := c.client.GetUser("")
	if err != nil {
		return fmt.Errorf("failed to fetch the User struct for the current authenticated user. errmsg: %v", err)
	}
	if user == nil {
		return fmt.Errorf("received a nil User struct pointer when trying to look up the currently authenticated user")
	}
	if user.Login == nil {
		return fmt.Errorf("the user struct for the currently authenticated user does not specify a login")
	}
	c.authorName = *user.Login

	// Try to get the list of valid labels for the repo.
	if validLabels, err := c.client.GetRepoLabels(c.org, c.project); err != nil {
		c.validLabels = nil
		glog.Errorf("Failed to retrieve the list of valid labels for repo '%s/%s'. Allowing all labels. errmsg: %v\n", c.org, c.project, err)
	} else {
		c.validLabels = make([]string, 0, len(validLabels))
		for _, label := range validLabels {
			if label.Name != nil && *label.Name != "" {
				c.validLabels = append(c.validLabels, *label.Name)
			}
		}
	}
	// Try to get the valid collaborators for the repo.
	if collaborators, err := c.client.GetCollaborators(c.org, c.project); err != nil {
		c.Collaborators = nil
		glog.Errorf("Failed to retrieve the list of valid collaborators for repo '%s/%s'. Allowing all assignees. errmsg: %v\n", c.org, c.project, err)
	} else {
		c.Collaborators = make([]string, 0, len(collaborators))
		for _, user := range collaborators {
			if user.Login != nil && *user.Login != "" {
				c.Collaborators = append(c.Collaborators, strings.ToLower(*user.Login))
			}
		}
	}

	// Populate the issue cache (allIssues).
	issues, err := c.client.GetIssues(
		c.org,
		c.project,
		&github.IssueListByRepoOptions{
			State:   "all",
			Creator: c.authorName,
		},
	)
	if err != nil {
		return fmt.Errorf("failed to refresh the list of all issues created by %s in repo '%s/%s'. errmsg: %v", c.authorName, c.org, c.project, err)
	}
	if len(issues) == 0 {
		glog.Warningf("IssueCreator found no issues in the repo '%s/%s' authored by '%s'.\n", c.org, c.project, c.authorName)
	}
	c.allIssues = make(map[int]*github.Issue)
	for _, i := range issues {
		c.allIssues[*i.Number] = i
	}
	return nil
}
func setIntersect(a, b []string) (filtered, removed []string) {
	for _, elemA := range a {
		found := false
		for _, elemB := range b {
			if elemA == elemB {
				found = true
				break
			}
		}
		if found {
			filtered = append(filtered, elemA)
		} else {
			removed = append(removed, elemA)
		}
	}
	return
}
func (c *IssueCreator) sync(issue Issue) bool {
	// First look for existing issues with this ID.
	id := issue.ID()
	var closedIssues []*github.Issue
	for _, i := range c.allIssues {
		if strings.Contains(*i.Body, id) {
			switch *i.State {
			case "open":
				//if an open issue is found with the ID then the issue is already synced
				return false
			case "closed":
				closedIssues = append(closedIssues, i)
			default:
				glog.Errorf("Unrecognized issue state '%s' for issue #%d. Ignoring this issue.\n", *i.State, *i.Number)
			}
		}
	}
	// No open issues exist for the ID.
	body := issue.Body(closedIssues)
	if body == "" {
		// Issue indicated that it should not be synced.
		glog.Infof("Issue aborted sync by providing \"\" (empty) body. ID: %s.", id)
		return false
	}
	if !strings.Contains(body, id) {
		glog.Fatalf("Programmer error: The following body text does not contain id '%s'.\n%s\n", id, body)
	}

	title := issue.Title()
	owners := issue.Owners()
	if c.Collaborators != nil {
		var removedOwners []string
		owners, removedOwners = setIntersect(owners, c.Collaborators)
		if len(removedOwners) > 0 {
			glog.Errorf("Filtered the following invalid assignees from issue %q: %q.", title, removedOwners)
		}
	}

	labels := issue.Labels()
	if prio, ok := issue.Priority(); ok {
		labels = append(labels, "priority/"+prio)
	}
	if c.validLabels != nil {
		var removedLabels []string
		labels, removedLabels = setIntersect(labels, c.validLabels)
		if len(removedLabels) > 0 {
			glog.Errorf("Filtered the following invalid labels from issue %q: %q.", title, removedLabels)
		}
	}

	glog.Infof("Create Issue: %q Assigned to: %q\n", title, owners)
	if c.dryRun {
		return true
	}

	created, err := c.client.CreateIssue(c.org, c.project, title, body, labels, owners)
	if err != nil {
		glog.Errorf("Failed to create a new github issue for issue ID '%s'.\n", id)
		return false
	}
	c.allIssues[*created.Number] = created
	return true
}
func GetAWSCreds(r *common.Resource) (credentials.Value, error) {
	val := credentials.Value{}

	if r.Type != ResourceType {
		return val, fmt.Errorf("Wanted resource of type %q, got %q", ResourceType, r.Type)
	}

	accessKey, ok := r.UserData.Map.Load(UserDataAccessIDKey)
	if !ok {
		return val, errors.New("No Access Key ID in UserData")
	}
	secretKey, ok := r.UserData.Map.Load(UserDataSecretAccessKey)
	if !ok {
		return val, errors.New("No Secret Access Key in UserData")
	}

	val.AccessKeyID = accessKey.(string)
	val.SecretAccessKey = secretKey.(string)

	return val, nil

}
func stopper() chan struct{} {
	stop := make(chan struct{})
	c := make(chan os.Signal, 2)
	signal.Notify(c, os.Interrupt, syscall.SIGTERM)
	go func() {
		<-c
		logrus.Warn("Interrupt received, attempting clean shutdown...")
		close(stop)
		<-c
		logrus.Error("Second interrupt received, force exiting...")
		os.Exit(1)
	}()
	return stop
}
func newPipelineConfig(cfg rest.Config, stop chan struct{}) (*pipelineConfig, error) {
	bc, err := pipelineset.NewForConfig(&cfg)
	if err != nil {
		return nil, err
	}

	// Ensure the pipeline CRD is deployed
	// TODO(fejta): probably a better way to do this
	if _, err := bc.TektonV1alpha1().PipelineRuns("").List(metav1.ListOptions{Limit: 1}); err != nil {
		return nil, err
	}

	// Assume watches receive updates, but resync every 30m in case something wonky happens
	bif := pipelineinfo.NewSharedInformerFactory(bc, 30*time.Minute)
	bif.Tekton().V1alpha1().PipelineRuns().Lister()
	go bif.Start(stop)
	return &pipelineConfig{
		client:   bc,
		informer: bif.Tekton().V1alpha1().PipelineRuns(),
	}, nil
}
func (o *KubernetesClientOptions) KubeClient() (kubernetes.Interface, error) {
	return kube.GetKubernetesClient(o.masterURL, o.kubeConfig)
}
func (o *KubernetesClientOptions) ProwJobClient() (versioned.Interface, error) {
	return kube.GetProwJobClient(o.masterURL, o.kubeConfig)
}
func (bucket gcsBucket) resolveSymLink(symLink string) (string, error) {
	data, err := bucket.readObject(symLink)
	if err != nil {
		return "", fmt.Errorf("failed to read %s: %v", symLink, err)
	}
	// strip gs://<bucket-name> from global address `u`
	u := string(data)
	return prefixRe.ReplaceAllString(u, ""), nil
}
func readJSON(bucket storageBucket, key string, data interface{}) error {
	rawData, err := bucket.readObject(key)
	if err != nil {
		return fmt.Errorf("failed to read %s: %v", key, err)
	}
	err = json.Unmarshal(rawData, &data)
	if err != nil {
		return fmt.Errorf("failed to parse %s: %v", key, err)
	}
	return nil
}
func (bucket gcsBucket) listSubDirs(prefix string) ([]string, error) {
	if !strings.HasSuffix(prefix, "/") {
		prefix += "/"
	}
	dirs := []string{}
	it := bucket.Objects(context.Background(), &storage.Query{
		Prefix:    prefix,
		Delimiter: "/",
	})
	for {
		attrs, err := it.Next()
		if err == iterator.Done {
			break
		}
		if err != nil {
			return dirs, err
		}
		if attrs.Prefix != "" {
			dirs = append(dirs, attrs.Prefix)
		}
	}
	return dirs, nil
}
func (bucket gcsBucket) listAll(prefix string) ([]string, error) {
	keys := []string{}
	it := bucket.Objects(context.Background(), &storage.Query{
		Prefix: prefix,
	})
	for {
		attrs, err := it.Next()
		if err == iterator.Done {
			break
		}
		if err != nil {
			return keys, err
		}
		keys = append(keys, attrs.Name)
	}
	return keys, nil
}
func (bucket gcsBucket) listBuildIDs(root string) ([]int64, error) {
	ids := []int64{}
	if strings.HasPrefix(root, logsPrefix) {
		dirs, err := bucket.listSubDirs(root)
		if err != nil {
			return ids, fmt.Errorf("failed to list GCS directories: %v", err)
		}
		for _, dir := range dirs {
			i, err := strconv.ParseInt(path.Base(dir), 10, 64)
			if err == nil {
				ids = append(ids, i)
			} else {
				logrus.Warningf("unrecognized directory name (expected int64): %s", dir)
			}
		}
	} else {
		keys, err := bucket.listAll(root)
		if err != nil {
			return ids, fmt.Errorf("failed to list GCS keys: %v", err)
		}
		for _, key := range keys {
			matches := linkRe.FindStringSubmatch(key)
			if len(matches) == 2 {
				i, err := strconv.ParseInt(matches[1], 10, 64)
				if err == nil {
					ids = append(ids, i)
				} else {
					logrus.Warningf("unrecognized file name (expected <int64>.txt): %s", key)
				}
			}
		}
	}
	return ids, nil
}
func FilterProfilePaths(profile []*cover.Profile, paths []string, include bool) ([]*cover.Profile, error) {
	parenPaths := make([]string, len(paths))
	for i, path := range paths {
		parenPaths[i] = "(" + path + ")"
	}
	joined := strings.Join(parenPaths, "|")
	re, err := regexp.Compile(joined)
	if err != nil {
		return nil, err
	}
	result := make([]*cover.Profile, 0, len(profile))
	for _, p := range profile {
		if re.MatchString(p.FileName) == include {
			result = append(result, p)
		}
	}
	return result, nil
}
func LoadSecrets(paths []string) (map[string][]byte, error) {
	secretsMap := make(map[string][]byte, len(paths))

	for _, path := range paths {
		secretValue, err := LoadSingleSecret(path)
		if err != nil {
			return nil, err
		}
		secretsMap[path] = secretValue
	}
	return secretsMap, nil
}
func LoadSingleSecret(path string) ([]byte, error) {
	b, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("error reading %s: %v", path, err)
	}
	return bytes.TrimSpace(b), nil
}
func (b *Bool) Set(s string) error {
	v, err := strconv.ParseBool(s)
	if err != nil {
		return err
	}
	b.Explicit = true
	b.Value = v
	return nil
}
func NewOpener(ctx context.Context, creds string) (Opener, error) {
	var options []option.ClientOption
	if creds != "" {
		options = append(options, option.WithCredentialsFile(creds))
	}
	client, err := storage.NewClient(ctx, options...)
	if err != nil {
		if creds != "" {
			return nil, err
		}
		logrus.WithError(err).Debug("Cannot load application default gcp credentials")
		client = nil
	}
	return opener{gcs: client}, nil
}
func IsNotExist(err error) bool {
	return os.IsNotExist(err) || err == storage.ErrObjectNotExist
}
func LogClose(c io.Closer) {
	if err := c.Close(); err != nil {
		logrus.WithError(err).Error("Failed to close")
	}
}
func (o opener) Writer(ctx context.Context, path string) (io.WriteCloser, error) {
	g, err := o.openGCS(path)
	if err != nil {
		return nil, fmt.Errorf("bad gcs path: %v", err)
	}
	if g == nil {
		return os.Create(path)
	}
	return g.NewWriter(ctx), nil
}
func (gac *GitHubOAuthConfig) InitGitHubOAuthConfig(cookie *sessions.CookieStore) {
	gob.Register(&oauth2.Token{})
	gac.CookieStore = cookie
}
func deltaDisplayed(change *coverageChange) string {
	if change.baseRatio < 0 {
		return ""
	}
	return fmt.Sprintf("%.1f", (change.newRatio-change.baseRatio)*100)
}
func makeTable(baseCovList, newCovList *calculation.CoverageList, coverageThreshold float32) (string, bool) {
	var rows []string
	isCoverageLow := false
	for _, change := range findChanges(baseCovList, newCovList) {
		filePath := change.name
		rows = append(rows, fmt.Sprintf("%s | %s | %s | %s",
			filePath,
			formatPercentage(change.baseRatio),
			formatPercentage(change.newRatio),
			deltaDisplayed(change)))

		if change.newRatio < coverageThreshold {
			isCoverageLow = true
		}
	}
	return strings.Join(rows, "\n"), isCoverageLow
}
func ContentForGitHubPost(baseProfiles, newProfiles []*cover.Profile, jobName string, coverageThreshold float32) (
	string, bool) {

	rows := []string{
		"The following is the code coverage report",
		fmt.Sprintf("Say `/test %s` to re-run this coverage report", jobName),
		"",
		"File | Old Coverage | New Coverage | Delta",
		"---- |:------------:|:------------:|:-----:",
	}

	table, isCoverageLow := makeTable(calculation.ProduceCovList(baseProfiles), calculation.ProduceCovList(newProfiles), coverageThreshold)

	if table == "" {
		return "", false
	}

	rows = append(rows, table)
	rows = append(rows, "")

	return strings.Join(rows, "\n"), isCoverageLow
}
func (client *Client) AddFlags(cmd *cobra.Command) {
	cmd.PersistentFlags().StringVar(&client.Token, "token", "",
		"The OAuth Token to use for requests.")
	cmd.PersistentFlags().StringVar(&client.TokenFile, "token-file", "",
		"The file containing the OAuth Token to use for requests.")
	cmd.PersistentFlags().StringVar(&client.Org, "organization", "",
		"The github organization to scan")
	cmd.PersistentFlags().StringVar(&client.Project, "project", "",
		"The github project to scan")
}
func (client *Client) CheckFlags() error {
	if client.Org == "" {
		return fmt.Errorf("organization flag must be set")
	}
	client.Org = strings.ToLower(client.Org)

	if client.Project == "" {
		return fmt.Errorf("project flag must be set")
	}
	client.Project = strings.ToLower(client.Project)

	return nil
}
func (client *Client) getGitHubClient() (*github.Client, error) {
	if client.githubClient != nil {
		return client.githubClient, nil
	}
	token := client.Token
	if len(token) == 0 && len(client.TokenFile) != 0 {
		data, err := ioutil.ReadFile(client.TokenFile)
		if err != nil {
			return nil, err
		}
		token = strings.TrimSpace(string(data))
	}

	if len(token) > 0 {
		ts := oauth2.StaticTokenSource(&oauth2.Token{AccessToken: token})
		tc := oauth2.NewClient(oauth2.NoContext, ts)
		client.githubClient = github.NewClient(tc)
	} else {
		client.githubClient = github.NewClient(nil)
	}
	return client.githubClient, nil
}
func (client *Client) limitsCheckAndWait() {
	var sleep time.Duration
	githubClient, err := client.getGitHubClient()
	if err != nil {
		glog.Error("Failed to get RateLimits: ", err)
		sleep = time.Minute
	} else {
		limits, _, err := githubClient.RateLimits(context.Background())
		if err != nil {
			glog.Error("Failed to get RateLimits:", err)
			sleep = time.Minute
		}
		if limits != nil && limits.Core != nil && limits.Core.Remaining < tokenLimit {
			sleep = limits.Core.Reset.Sub(time.Now())
			glog.Warning("RateLimits: reached. Sleeping for ", sleep)
		}
	}

	time.Sleep(sleep)
}
func (client *Client) FetchIssues(latest time.Time, c chan *github.Issue) {
	opt := &github.IssueListByRepoOptions{Since: latest, Sort: "updated", State: "all", Direction: "asc"}

	githubClient, err := client.getGitHubClient()
	if err != nil {
		close(c)
		glog.Error(err)
		return
	}

	count := 0
	for {
		client.limitsCheckAndWait()

		issues, resp, err := githubClient.Issues.ListByRepo(
			context.Background(),
			client.Org,
			client.Project,
			opt,
		)
		if err != nil {
			close(c)
			glog.Error(err)
			return
		}

		for _, issue := range issues {
			c <- issue
			count++
		}

		if resp.NextPage == 0 {
			break
		}
		opt.ListOptions.Page = resp.NextPage
	}

	glog.Infof("Fetched %d issues updated issue since %v.", count, latest)
	close(c)
}
func hasID(events []*github.IssueEvent, id int) bool {
	for _, event := range events {
		if *event.ID == int64(id) {
			return true
		}
	}
	return false
}
func (client *Client) FetchIssueEvents(issueID int, latest *int, c chan *github.IssueEvent) {
	opt := &github.ListOptions{PerPage: 100}

	githubClient, err := client.getGitHubClient()
	if err != nil {
		close(c)
		glog.Error(err)
		return
	}

	count := 0
	for {
		client.limitsCheckAndWait()

		events, resp, err := githubClient.Issues.ListIssueEvents(
			context.Background(),
			client.Org,
			client.Project,
			issueID,
			opt,
		)
		if err != nil {
			glog.Errorf("ListIssueEvents failed: %s. Retrying...", err)
			time.Sleep(time.Second)
			continue
		}

		for _, event := range events {
			c <- event
			count++
		}
		if resp.NextPage == 0 || (latest != nil && hasID(events, *latest)) {
			break
		}
		opt.Page = resp.NextPage
	}

	glog.Infof("Fetched %d events.", count)
	close(c)
}
func isPRChanged(pe github.PullRequestEvent) bool {
	switch pe.Action {
	case github.PullRequestActionOpened:
		return true
	case github.PullRequestActionReopened:
		return true
	case github.PullRequestActionSynchronize:
		return true
	case github.PullRequestActionEdited:
		return true
	default:
		return false
	}
}
func NewFetcher(repository string) *Fetcher {
	return &Fetcher{
		IssuesChannel:         make(chan sql.Issue, 100),
		EventsCommentsChannel: make(chan interface{}, 100),
		repository:            repository,
	}
}
func (f *Fetcher) fetchRecentIssues(db *gorm.DB) error {
	glog.Infof("Fetching issues updated after %s", f.lastIssue)

	var issues []sql.Issue
	query := db.
		Where("issue_updated_at >= ?", f.lastIssue).
		Where("repository = ?", f.repository).
		Order("issue_updated_at").
		Preload("Labels").
		Find(&issues)
	if query.Error != nil {
		return query.Error
	}

	count := len(issues)
	for _, issue := range issues {
		f.IssuesChannel <- issue
		f.lastIssue = issue.IssueUpdatedAt
	}
	glog.Infof("Found and pushed %d updated/new issues", count)

	return nil
}
func (f *Fetcher) fetchRecentEventsAndComments(db *gorm.DB) error {
	glog.Infof("Fetching issue-events with id bigger than %s", f.lastEvent)
	glog.Infof("Fetching comments with id bigger than %s", f.lastComment)

	eventRows, err := db.
		Model(sql.IssueEvent{}).
		Where("repository = ?", f.repository).
		Where("event_created_at > ?", f.lastEvent).
		Order("event_created_at asc").
		Rows()
	if err != nil {
		return fmt.Errorf("Failed to query events from database: %s", err)
	}

	commentRows, err := db.
		Model(sql.Comment{}).
		Where("repository = ?", f.repository).
		Where("comment_created_at > ?", f.lastComment).
		Order("comment_created_at asc").
		Rows()
	if err != nil {
		return fmt.Errorf("Failed to query comments from database: %s", err)
	}

	count := 0
	comment := &sql.Comment{}
	if commentRows.Next() {
		db.ScanRows(commentRows, comment)
	} else {
		comment = nil
	}
	event := &sql.IssueEvent{}
	if eventRows.Next() {
		db.ScanRows(eventRows, event)
	} else {
		event = nil
	}

	for event != nil || comment != nil {
		if event == nil || (comment != nil && comment.CommentCreatedAt.Before(event.EventCreatedAt)) {
			f.EventsCommentsChannel <- *comment
			f.lastComment = comment.CommentCreatedAt
			if commentRows.Next() {
				db.ScanRows(commentRows, comment)
			} else {
				comment = nil
			}
		} else {
			f.EventsCommentsChannel <- *event
			f.lastEvent = event.EventCreatedAt
			if eventRows.Next() {
				db.ScanRows(eventRows, event)
			} else {
				event = nil
			}
		}
		count++
	}

	glog.Infof("Found and pushed %d new events/comments", count)

	return nil
}
func (f *Fetcher) Fetch(db *gorm.DB) error {
	if err := f.fetchRecentIssues(db); err != nil {
		return err
	}
	if err := f.fetchRecentEventsAndComments(db); err != nil {
		return err
	}
	return nil
}
func (fjr *FlakyJobReporter) Issues(c *creator.IssueCreator) ([]creator.Issue, error) {
	fjr.creator = c
	json, err := ReadHTTP(fjr.flakyJobDataURL)
	if err != nil {
		return nil, err
	}

	flakyJobs, err := fjr.parseFlakyJobs(json)
	if err != nil {
		return nil, err
	}

	count := fjr.syncCount
	if len(flakyJobs) < count {
		count = len(flakyJobs)
	}
	issues := make([]creator.Issue, 0, count)
	for _, fj := range flakyJobs[0:count] {
		issues = append(issues, fj)
	}

	return issues, nil
}
func (fj *FlakyJob) Title() string {
	return fmt.Sprintf("%s flaked %d times in the past week", fj.Name, *fj.FlakeCount)
}
func (fj *FlakyJob) Labels() []string {
	labels := []string{"kind/flake"}
	// get sig labels
	for sig := range fj.reporter.creator.TestsSIGs(fj.TestsSorted()) {
		labels = append(labels, "sig/"+sig)
	}
	return labels
}
func ReadHTTP(url string) ([]byte, error) {
	var err error
	retryDelay := time.Duration(2) * time.Second
	for retryCount := 0; retryCount < 5; retryCount++ {
		if retryCount > 0 {
			time.Sleep(retryDelay)
			retryDelay *= time.Duration(2)
		}

		resp, err := http.Get(url)
		if resp != nil && resp.StatusCode >= 500 {
			// Retry on this type of error.
			continue
		}
		if err != nil {
			return nil, err
		}
		defer resp.Body.Close()

		body, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			continue
		}
		return body, nil
	}
	return nil, fmt.Errorf("ran out of retries reading from '%s'. Last error was %v", url, err)
}
func (l linesByTimestamp) String() string {
	sort.Sort(l)

	var log string
	for i, line := range l {
		switch i {
		case len(l) - 1:
			log += string(line.actual)
		default:
			// buf.ReadBytes('\n') does not remove the newline
			log += fmt.Sprintf("%s,\n", strings.TrimSuffix(string(line.actual), "\n"))
		}
	}

	return fmt.Sprintf("[%s]", log)
}
func NewJobSpec(spec prowapi.ProwJobSpec, buildID, prowJobID string) JobSpec {
	return JobSpec{
		Type:      spec.Type,
		Job:       spec.Job,
		BuildID:   buildID,
		ProwJobID: prowJobID,
		Refs:      spec.Refs,
		ExtraRefs: spec.ExtraRefs,
		agent:     spec.Agent,
	}
}
func ResolveSpecFromEnv() (*JobSpec, error) {
	specEnv, ok := os.LookupEnv(JobSpecEnv)
	if !ok {
		return nil, fmt.Errorf("$%s unset", JobSpecEnv)
	}

	spec := &JobSpec{}
	if err := json.Unmarshal([]byte(specEnv), spec); err != nil {
		return nil, fmt.Errorf("malformed $%s: %v", JobSpecEnv, err)
	}

	return spec, nil
}
func EnvForSpec(spec JobSpec) (map[string]string, error) {
	env := map[string]string{
		jobNameEnv:   spec.Job,
		buildIDEnv:   spec.BuildID,
		prowJobIDEnv: spec.ProwJobID,
		jobTypeEnv:   string(spec.Type),
	}

	// for backwards compatibility, we provide the build ID
	// in both $BUILD_ID and $BUILD_NUMBER for Prow agents
	// and in both $buildId and $BUILD_NUMBER for Jenkins
	if spec.agent == prowapi.KubernetesAgent {
		env[prowBuildIDEnv] = spec.BuildID
	}

	raw, err := json.Marshal(spec)
	if err != nil {
		return env, fmt.Errorf("failed to marshal job spec: %v", err)
	}
	env[JobSpecEnv] = string(raw)

	if spec.Type == prowapi.PeriodicJob {
		return env, nil
	}

	env[repoOwnerEnv] = spec.Refs.Org
	env[repoNameEnv] = spec.Refs.Repo
	env[pullBaseRefEnv] = spec.Refs.BaseRef
	env[pullBaseShaEnv] = spec.Refs.BaseSHA
	env[pullRefsEnv] = spec.Refs.String()

	if spec.Type == prowapi.PostsubmitJob || spec.Type == prowapi.BatchJob {
		return env, nil
	}

	env[pullNumberEnv] = strconv.Itoa(spec.Refs.Pulls[0].Number)
	env[pullPullShaEnv] = spec.Refs.Pulls[0].SHA
	return env, nil
}
func EnvForType(jobType prowapi.ProwJobType) []string {
	baseEnv := []string{jobNameEnv, JobSpecEnv, jobTypeEnv, prowJobIDEnv, buildIDEnv, prowBuildIDEnv}
	refsEnv := []string{repoOwnerEnv, repoNameEnv, pullBaseRefEnv, pullBaseShaEnv, pullRefsEnv}
	pullEnv := []string{pullNumberEnv, pullPullShaEnv}

	switch jobType {
	case prowapi.PeriodicJob:
		return baseEnv
	case prowapi.PostsubmitJob, prowapi.BatchJob:
		return append(baseEnv, refsEnv...)
	case prowapi.PresubmitJob:
		return append(append(baseEnv, refsEnv...), pullEnv...)
	default:
		return []string{}
	}
}
func getRevisionFromRef(refs *prowapi.Refs) string {
	if len(refs.Pulls) > 0 {
		return refs.Pulls[0].SHA
	}

	if refs.BaseSHA != "" {
		return refs.BaseSHA
	}

	return refs.BaseRef
}
func GetRevisionFromSpec(spec *JobSpec) string {
	if spec.Refs != nil {
		return getRevisionFromRef(spec.Refs)
	} else if len(spec.ExtraRefs) > 0 {
		return getRevisionFromRef(&spec.ExtraRefs[0])
	}
	return ""
}
func helpProvider(config *plugins.Configuration, enabledRepos []string) (*pluginhelp.PluginHelp, error) {
	// Only the Description field is specified because this plugin is not triggered with commands and is not configurable.
	return &pluginhelp.PluginHelp{
		Description: fmt.Sprintf("The merge commit blocker plugin adds the %s label to pull requests that contain merge commits", labels.MergeCommits),
	}, nil
}
func (g *Group) load(r io.Reader) ([]string, error) {
	var repoPaths []string
	s := bufio.NewScanner(r)
	for s.Scan() {
		l := strings.TrimSpace(s.Text())
		if l == "" || l[0] == '#' {
			// Ignore comments and empty lines.
			continue
		}

		fs := strings.Fields(l)
		if len(fs) != 2 {
			return repoPaths, &ParseError{line: l}
		}

		switch fs[0] {
		case "prefix", "path-prefix":
			g.PathPrefixes[fs[1]] = true
		case "file-prefix":
			g.FilePrefixes[fs[1]] = true
		case "file-name":
			g.FileNames[fs[1]] = true
		case "path":
			g.FileNames[fs[1]] = true
		case "paths-from-repo":
			// Despite the name, this command actually requires a file
			// of paths from the _same_ repo in which the .generated_files
			// config lives.
			repoPaths = append(repoPaths, fs[1])
		default:
			return repoPaths, &ParseError{line: l}
		}
	}

	if err := s.Err(); err != nil {
		return repoPaths, err
	}

	return repoPaths, nil
}
func (g *Group) loadPaths(r io.Reader) error {
	s := bufio.NewScanner(r)

	for s.Scan() {
		l := strings.TrimSpace(s.Text())
		if l == "" || l[0] == '#' {
			// Ignore comments and empty lines.
			continue
		}

		g.Paths[l] = true
	}

	if err := s.Err(); err != nil {
		return fmt.Errorf("scan error: %v", err)
	}

	return nil
}
func (g *Group) Match(path string) bool {
	if g.Paths[path] {
		return true
	}

	for prefix := range g.PathPrefixes {
		if strings.HasPrefix(path, prefix) {
			return true
		}
	}

	base := filepath.Base(path)

	if g.FileNames[base] {
		return true
	}

	for prefix := range g.FilePrefixes {
		if strings.HasPrefix(base, prefix) {
			return true
		}
	}

	return false
}
func (config *InfluxConfig) CreateDatabase(tags map[string]string, measurement string) (*InfluxDB, error) {
	client, err := influxdb.NewHTTPClient(influxdb.HTTPConfig{
		Addr:     config.Host,
		Username: config.User,
		Password: config.Password,
	})
	if err != nil {
		return nil, err
	}

	err = dropSeries(client, measurement, config.DB, tags)
	if err != nil {
		return nil, err
	}

	bp, err := influxdb.NewBatchPoints(influxdb.BatchPointsConfig{
		Database:  config.DB,
		Precision: "s",
	})
	if err != nil {
		return nil, err
	}

	return &InfluxDB{
		client:      client,
		database:    config.DB,
		batch:       bp,
		tags:        tags,
		measurement: measurement,
	}, err
}
func mergeTags(defaultTags, extraTags map[string]string) map[string]string {
	newTags := map[string]string{}

	for k, v := range defaultTags {
		newTags[k] = v
	}
	for k, v := range extraTags {
		newTags[k] = v
	}

	return newTags
}
func tagsToWhere(tags map[string]string) string {
	if len(tags) == 0 {
		return ""
	}

	sortedKeys := []string{}
	for k := range tags {
		sortedKeys = append(sortedKeys, k)
	}
	sort.Strings(sortedKeys)

	conditions := []string{}
	for _, key := range sortedKeys {
		conditions = append(conditions, fmt.Sprintf(`"%s" = '%v'`, key, tags[key]))
	}
	return "WHERE " + strings.Join(conditions, " AND ")
}
func (i *InfluxDB) Push(tags map[string]string, fields map[string]interface{}, date time.Time) error {
	pt, err := influxdb.NewPoint(i.measurement, mergeTags(i.tags, tags), fields, date)
	if err != nil {
		return err
	}

	i.batch.AddPoint(pt)
	i.batchSize++

	return nil
}
func (af *PodLogArtifactFetcher) artifact(jobName, buildID string, sizeLimit int64) (lenses.Artifact, error) {

	podLog, err := NewPodLogArtifact(jobName, buildID, sizeLimit, af.jobAgent)
	if err != nil {
		return nil, fmt.Errorf("Error accessing pod log from given source: %v", err)
	}
	return podLog, nil
}
func serve() {
	http.Handle("/metrics", promhttp.Handler())
	logrus.WithError(http.ListenAndServe(":8080", nil)).Fatal("ListenAndServe returned.")
}
func gather(c *plank.Controller) {
	tick := time.Tick(30 * time.Second)
	sig := make(chan os.Signal, 1)
	signal.Notify(sig, os.Interrupt, syscall.SIGTERM)

	for {
		select {
		case <-tick:
			start := time.Now()
			c.SyncMetrics()
			logrus.WithField("metrics-duration", fmt.Sprintf("%v", time.Since(start))).Debug("Metrics synced")
		case <-sig:
			logrus.Debug("Plank gatherer is shutting down...")
			return
		}
	}
}
func makeRequest(policy branchprotection.Policy) github.BranchProtectionRequest {
	return github.BranchProtectionRequest{
		EnforceAdmins:              makeAdmins(policy.Admins),
		RequiredPullRequestReviews: makeReviews(policy.RequiredPullRequestReviews),
		RequiredStatusChecks:       makeChecks(policy.RequiredStatusChecks),
		Restrictions:               makeRestrictions(policy.Restrictions),
	}

}
func makeReviews(rp *branchprotection.ReviewPolicy) *github.RequiredPullRequestReviews {
	switch {
	case rp == nil:
		return nil
	case rp.Approvals == nil:
		logrus.Warn("WARNING: required_pull_request_reviews policy does not specify required_approving_review_count, disabling")
		return nil
	case *rp.Approvals == 0:
		return nil
	}
	rprr := github.RequiredPullRequestReviews{
		DismissStaleReviews:          makeBool(rp.DismissStale),
		RequireCodeOwnerReviews:      makeBool(rp.RequireOwners),
		RequiredApprovingReviewCount: *rp.Approvals,
	}
	if rp.DismissalRestrictions != nil {
		rprr.DismissalRestrictions = *makeRestrictions(rp.DismissalRestrictions)
	}
	return &rprr
}
func (lens Lens) Header(artifacts []lenses.Artifact, resourceDir string) string {
	return executeTemplate(resourceDir, "header", BuildLogsView{})
}
func (lens Lens) Callback(artifacts []lenses.Artifact, resourceDir string, data string) string {
	var request LineRequest
	err := json.Unmarshal([]byte(data), &request)
	if err != nil {
		return "failed to unmarshal request"
	}
	artifact, ok := artifactByName(artifacts, request.Artifact)
	if !ok {
		return "no artifact named " + request.Artifact
	}

	var lines []string
	if request.Offset == 0 && request.Length == -1 {
		lines, err = logLinesAll(artifact)
	} else {
		lines, err = logLines(artifact, request.Offset, request.Length)
	}
	if err != nil {
		return fmt.Sprintf("failed to retrieve log lines: %v", err)
	}

	logLines := highlightLines(lines, request.StartLine)
	return executeTemplate(resourceDir, "line group", logLines)
}
func logLinesAll(artifact lenses.Artifact) ([]string, error) {
	read, err := artifact.ReadAll()
	if err != nil {
		return nil, fmt.Errorf("failed to read log %q: %v", artifact.JobPath(), err)
	}
	logLines := strings.Split(string(read), "\n")

	return logLines, nil
}
func executeTemplate(resourceDir, templateName string, data interface{}) string {
	t := template.New("template.html")
	_, err := t.ParseFiles(filepath.Join(resourceDir, "template.html"))
	if err != nil {
		return fmt.Sprintf("Failed to load template: %v", err)
	}
	var buf bytes.Buffer
	if err := t.ExecuteTemplate(&buf, templateName, data); err != nil {
		logrus.WithError(err).Error("Error executing template.")
	}
	return buf.String()
}
func (in *ResourceObject) DeepCopyObject() runtime.Object {
	if c := in.deepCopy(); c != nil {
		return c
	}
	return nil
}
func (in *ResourceObject) FromItem(i common.Item) {
	r, err := common.ItemToResource(i)
	if err == nil {
		in.fromResource(r)
	}
}
func (in *ResourceCollection) SetItems(objects []Object) {
	var items []*ResourceObject
	for _, b := range objects {
		items = append(items, b.(*ResourceObject))
	}
	in.Items = items
}
func (in *ResourceCollection) DeepCopyObject() runtime.Object {
	if c := in.deepCopy(); c != nil {
		return c
	}
	return nil
}
func useContext(o options, ctx string) error {
	_, cmd := command("kubectl", "config", "use-context", ctx)
	return cmd.Run()
}
func currentContext(o options) (string, error) {
	_, cmd := command("kubectl", "config", "current-context")
	b, err := cmd.Output()
	return strings.TrimSpace(string(b)), err
}
func command(bin string, args ...string) ([]string, *exec.Cmd) {
	cmd := exec.Command(bin, args...)
	cmd.Stderr = os.Stderr
	return append([]string{bin}, args...), cmd
}
func describeCluster(o options) (*describe, error) {
	if o.account != "" {
		act, err := getAccount()
		if err != nil {
			return nil, fmt.Errorf("get current account: %v", err)
		}
		defer setAccount(act)
		if err = setAccount(o.account); err != nil {
			return nil, fmt.Errorf("set account %s: %v", o.account, err)
		}
	}
	args, cmd := command(
		"gcloud", "container", "clusters", "describe", o.cluster,
		"--project", o.project,
		"--zone", o.zone,
		"--format=yaml",
	)
	data, err := cmd.Output()
	if err != nil {
		return nil, fmt.Errorf("%s: %v", strings.Join(args, " "), err)
	}
	var d describe
	if yaml.Unmarshal(data, &d); err != nil {
		return nil, fmt.Errorf("unmarshal gcloud: %v", err)
	}

	if d.Endpoint == "" {
		return nil, errors.New("empty endpoint")
	}
	if len(d.Auth.ClusterCACertificate) == 0 {
		return nil, errors.New("empty clusterCaCertificate")
	}

	if len(d.Auth.ClientKey) == 0 {
		return nil, errors.New("empty clientKey, consider running with --get-client-cert")
	}
	if len(d.Auth.ClientCertificate) == 0 {
		return nil, errors.New("empty clientCertificate, consider running with --get-client-cert")
	}

	return &d, nil
}
func (ss *strslice) Set(value string) error {
	*ss = append(*ss, value)
	return nil
}
func parseXML(body []byte, object string) (*gcsDir, error) {
	dir := new(gcsDir)
	if err := xml.Unmarshal(body, &dir); err != nil {
		return nil, err
	}
	// We think this is a dir if the object is "/" (just the bucket) or if we
	// find any Contents or CommonPrefixes.
	isDir := object == "/" || len(dir.Contents)+len(dir.CommonPrefixes) > 0
	selfIndex := -1
	for i := range dir.Contents {
		rec := &dir.Contents[i]
		name := strings.TrimPrefix(rec.Name, object)
		if name == "" {
			selfIndex = i
			continue
		}
		rec.Name = name
		if strings.HasSuffix(name, "/") {
			rec.isDir = true
		}
	}

	for i := range dir.CommonPrefixes {
		cp := &dir.CommonPrefixes[i]
		cp.Prefix = strings.TrimPrefix(cp.Prefix, object)
	}

	if !isDir {
		return nil, nil
	}

	if selfIndex >= 0 {
		// Strip out the record that indicates this object.
		dir.Contents = append(dir.Contents[:selfIndex], dir.Contents[selfIndex+1:]...)
	}
	return dir, nil
}
func (dir *gcsDir) Render(out http.ResponseWriter, inPath string) {
	htmlPageHeader(out, dir.Name)

	if !strings.HasSuffix(inPath, "/") {
		inPath += "/"
	}

	htmlContentHeader(out, dir.Name, inPath)

	if dir.NextMarker != "" {
		htmlNextButton(out, gcsPath+inPath, dir.NextMarker)
	}

	htmlGridHeader(out)
	if parent := dirname(inPath); parent != "" {
		url := gcsPath + parent
		htmlGridItem(out, iconBack, url, "..", "-", "-")
	}
	for i := range dir.CommonPrefixes {
		dir.CommonPrefixes[i].Render(out, inPath)
	}
	for i := range dir.Contents {
		dir.Contents[i].Render(out, inPath)
	}

	if dir.NextMarker != "" {
		htmlNextButton(out, gcsPath+inPath, dir.NextMarker)
	}

	htmlContentFooter(out)

	htmlPageFooter(out)
}
func (rec *Record) Render(out http.ResponseWriter, inPath string) {
	mtime := "<unknown>"
	ts, err := time.Parse(time.RFC3339, rec.MTime)
	if err == nil {
		mtime = ts.Format("02 Jan 2006 15:04:05")
	}
	var url, size string
	if rec.isDir {
		url = gcsPath + inPath + rec.Name
		size = "-"
	} else {
		url = gcsBaseURL + inPath + rec.Name
		size = fmt.Sprintf("%v", rec.Size)
	}
	htmlGridItem(out, iconFile, url, rec.Name, size, mtime)
}
func (pfx *Prefix) Render(out http.ResponseWriter, inPath string) {
	url := gcsPath + inPath + pfx.Prefix
	htmlGridItem(out, iconDir, url, pfx.Prefix, "-", "-")
}
func (tl txnLogger) Printf(fmt string, args ...interface{}) {
	args = append([]interface{}{tl.nonce}, args...)
	log.Printf("[txn-%s] "+fmt, args...)
}
func NewClient(instances map[string][]string) (*Client, error) {
	c := &Client{
		handlers: map[string]*gerritInstanceHandler{},
	}
	for instance := range instances {
		gc, err := gerrit.NewClient(instance, nil)
		if err != nil {
			return nil, err
		}

		c.handlers[instance] = &gerritInstanceHandler{
			instance:       instance,
			projects:       instances[instance],
			authService:    gc.Authentication,
			accountService: gc.Accounts,
			changeService:  gc.Changes,
			projectService: gc.Projects,
		}
	}

	return c, nil
}
func (c *Client) SetReview(instance, id, revision, message string, labels map[string]string) error {
	h, ok := c.handlers[instance]
	if !ok {
		return fmt.Errorf("not activated gerrit instance: %s", instance)
	}

	if _, _, err := h.changeService.SetReview(id, revision, &gerrit.ReviewInput{
		Message: message,
		Labels:  labels,
	}); err != nil {
		return fmt.Errorf("cannot comment to gerrit: %v", err)
	}

	return nil
}
func (c *Client) GetBranchRevision(instance, project, branch string) (string, error) {
	h, ok := c.handlers[instance]
	if !ok {
		return "", fmt.Errorf("not activated gerrit instance: %s", instance)
	}

	res, _, err := h.projectService.GetBranch(project, branch)
	if err != nil {
		return "", err
	}

	return res.Revision, nil
}
func (h *gerritInstanceHandler) queryAllChanges(lastUpdate time.Time, rateLimit int) []gerrit.ChangeInfo {
	result := []gerrit.ChangeInfo{}
	for _, project := range h.projects {
		changes, err := h.queryChangesForProject(project, lastUpdate, rateLimit)
		if err != nil {
			// don't halt on error from one project, log & continue
			logrus.WithError(err).Errorf("fail to query changes for project %s", project)
			continue
		}
		result = append(result, changes...)
	}

	return result
}
func NewTypeFilterWrapperPlugin(plugin Plugin) *TypeFilterWrapperPlugin {
	return &TypeFilterWrapperPlugin{
		plugin: plugin,
		pass:   map[string]bool{},
	}
}
func (t *TypeFilterWrapperPlugin) AddFlags(cmd *cobra.Command) {
	cmd.Flags().BoolVar(&t.pullRequests, "no-pull-requests", false, "Ignore pull-requests")
	cmd.Flags().BoolVar(&t.issues, "no-issues", false, "Ignore issues")
}
func (t *TypeFilterWrapperPlugin) CheckFlags() error {
	if t.pullRequests && t.issues {
		return fmt.Errorf(
			"you can't ignore both pull-requests and issues")
	}
	return nil
}
func (o *KubernetesClientOptions) AddFlags(fs *flag.FlagSet) {
	fs.StringVar(&o.namespace, "namespace", v1.NamespaceDefault, "namespace to install on")
	fs.StringVar(&o.kubeConfig, "kubeconfig", "", "absolute path to the kubeConfig file")
	fs.BoolVar(&o.inMemory, "in_memory", false, "Use in memory client instead of CRD")
}
func (o *KubernetesClientOptions) Validate() error {
	if o.kubeConfig != "" {
		if _, err := os.Stat(o.kubeConfig); err != nil {
			return err
		}
	}
	return nil
}
func (o *KubernetesClientOptions) Client(t Type) (ClientInterface, error) {
	if o.inMemory {
		return newDummyClient(t), nil
	}
	return o.newCRDClient(t)
}
func (o *KubernetesClientOptions) newCRDClient(t Type) (*Client, error) {
	config, scheme, err := createRESTConfig(o.kubeConfig, t)
	if err != nil {
		return nil, err
	}

	if err = registerResource(config, t); err != nil {
		return nil, err
	}
	// creates the client
	var restClient *rest.RESTClient
	restClient, err = rest.RESTClientFor(config)
	if err != nil {
		return nil, err
	}
	rc := Client{cl: restClient, ns: o.namespace, t: t,
		codec: runtime.NewParameterCodec(scheme)}
	return &rc, nil
}
func createRESTConfig(kubeconfig string, t Type) (config *rest.Config, types *runtime.Scheme, err error) {
	if kubeconfig == "" {
		config, err = rest.InClusterConfig()
	} else {
		config, err = clientcmd.BuildConfigFromFlags("", kubeconfig)
	}

	if err != nil {
		return
	}

	version := schema.GroupVersion{
		Group:   group,
		Version: version,
	}

	config.GroupVersion = &version
	config.APIPath = "/apis"
	config.ContentType = runtime.ContentTypeJSON

	types = runtime.NewScheme()
	schemeBuilder := runtime.NewSchemeBuilder(
		func(scheme *runtime.Scheme) error {
			scheme.AddKnownTypes(version, t.Object, t.Collection)
			v1.AddToGroupVersion(scheme, version)
			return nil
		})
	err = schemeBuilder.AddToScheme(types)
	config.NegotiatedSerializer = serializer.DirectCodecFactory{CodecFactory: serializer.NewCodecFactory(types)}

	return
}
func registerResource(config *rest.Config, t Type) error {
	c, err := apiextensionsclient.NewForConfig(config)
	if err != nil {
		return err
	}

	crd := &apiextensionsv1beta1.CustomResourceDefinition{
		ObjectMeta: v1.ObjectMeta{
			Name: fmt.Sprintf("%s.%s", t.Plural, group),
		},
		Spec: apiextensionsv1beta1.CustomResourceDefinitionSpec{
			Group:   group,
			Version: version,
			Scope:   apiextensionsv1beta1.NamespaceScoped,
			Names: apiextensionsv1beta1.CustomResourceDefinitionNames{
				Singular: t.Singular,
				Plural:   t.Plural,
				Kind:     t.Kind,
				ListKind: t.ListKind,
			},
		},
	}
	if _, err := c.ApiextensionsV1beta1().CustomResourceDefinitions().Create(crd); err != nil && !apierrors.IsAlreadyExists(err) {
		return err
	}
	return nil
}
func newDummyClient(t Type) *dummyClient {
	c := &dummyClient{
		t:       t,
		objects: make(map[string]Object),
	}
	return c
}
func (c *dummyClient) Update(obj Object) (Object, error) {
	_, ok := c.objects[obj.GetName()]
	if !ok {
		return nil, fmt.Errorf("cannot find object %s", obj.GetName())
	}
	c.objects[obj.GetName()] = obj
	return obj, nil
}
func TrustedPullRequest(ghc githubClient, trigger plugins.Trigger, author, org, repo string, num int, l []github.Label) ([]github.Label, bool, error) {
	// First check if the author is a member of the org.
	if orgMember, err := TrustedUser(ghc, trigger, author, org, repo); err != nil {
		return l, false, fmt.Errorf("error checking %s for trust: %v", author, err)
	} else if orgMember {
		return l, true, nil
	}
	// Then check if PR has ok-to-test label
	if l == nil {
		var err error
		l, err = ghc.GetIssueLabels(org, repo, num)
		if err != nil {
			return l, false, err
		}
	}
	return l, github.HasLabel(labels.OkToTest, l), nil
}
func buildAll(c Client, pr *github.PullRequest, eventGUID string, elideSkippedContexts bool) error {
	org, repo, number, branch := pr.Base.Repo.Owner.Login, pr.Base.Repo.Name, pr.Number, pr.Base.Ref
	changes := config.NewGitHubDeferredChangedFilesProvider(c.GitHubClient, org, repo, number)
	toTest, toSkipSuperset, err := pjutil.FilterPresubmits(pjutil.TestAllFilter(), changes, branch, c.Config.Presubmits[pr.Base.Repo.FullName], c.Logger)
	if err != nil {
		return err
	}

	toSkip := determineSkippedPresubmits(toTest, toSkipSuperset, c.Logger)
	return runAndSkipJobs(c, pr, toTest, toSkip, eventGUID, elideSkippedContexts)
}
func (o Options) Run(ctx context.Context) (int, error) {
	spec, err := downwardapi.ResolveSpecFromEnv()
	if err != nil {
		return 0, fmt.Errorf("could not resolve job spec: %v", err)
	}

	ctx, cancel := context.WithCancel(ctx)

	// If we are being asked to terminate by the kubelet but we have
	// NOT seen the test process exit cleanly, we need a to start
	// uploading artifacts to GCS immediately. If we notice the process
	// exit while doing this best-effort upload, we can race with the
	// second upload but we can tolerate this as we'd rather get SOME
	// data into GCS than attempt to cancel these uploads and get none.
	interrupt := make(chan os.Signal)
	signal.Notify(interrupt, os.Interrupt, syscall.SIGTERM)
	go func() {
		select {
		case s := <-interrupt:
			logrus.Errorf("Received an interrupt: %s, cancelling...", s)
			cancel()
		case <-ctx.Done():
		}
	}()

	if o.DeprecatedWrapperOptions != nil {
		// This only fires if the prowjob controller and sidecar are at different commits
		logrus.Warnf("Using deprecated wrapper_options instead of entries. Please update prow/pod-utils/decorate before June 2019")
	}
	entries := o.entries()
	passed, aborted, failures := wait(ctx, entries)

	cancel()
	// If we are being asked to terminate by the kubelet but we have
	// seen the test process exit cleanly, we need a chance to upload
	// artifacts to GCS. The only valid way for this program to exit
	// after a SIGINT or SIGTERM in this situation is to finish
	// uploading, so we ignore the signals.
	signal.Ignore(os.Interrupt, syscall.SIGTERM)

	buildLog := logReader(entries)
	metadata := combineMetadata(entries)
	return failures, o.doUpload(spec, passed, aborted, metadata, buildLog)
}
func (s *Storage) AddConfig(conf common.ResourcesConfig) error {
	return s.configs.Add(conf)
}
func (s *Storage) DeleteConfig(name string) error {
	return s.configs.Delete(name)
}
func (s *Storage) UpdateConfig(conf common.ResourcesConfig) error {
	return s.configs.Update(conf)
}
func (s *Storage) GetConfig(name string) (common.ResourcesConfig, error) {
	i, err := s.configs.Get(name)
	if err != nil {
		return common.ResourcesConfig{}, err
	}
	var conf common.ResourcesConfig
	conf, err = common.ItemToResourcesConfig(i)
	if err != nil {
		return common.ResourcesConfig{}, err
	}
	return conf, nil
}
func (s *Storage) GetConfigs() ([]common.ResourcesConfig, error) {
	var configs []common.ResourcesConfig
	items, err := s.configs.List()
	if err != nil {
		return configs, err
	}
	for _, i := range items {
		var conf common.ResourcesConfig
		conf, err = common.ItemToResourcesConfig(i)
		if err != nil {
			return nil, err
		}
		configs = append(configs, conf)
	}
	return configs, nil
}
func (s *Storage) SyncConfigs(newConfigs []common.ResourcesConfig) error {
	s.configsLock.Lock()
	defer s.configsLock.Unlock()

	currentConfigs, err := s.GetConfigs()
	if err != nil {
		logrus.WithError(err).Error("cannot find configs")
		return err
	}

	currentSet := mapset.NewSet()
	newSet := mapset.NewSet()
	toUpdate := mapset.NewSet()

	configs := map[string]common.ResourcesConfig{}

	for _, c := range currentConfigs {
		currentSet.Add(c.Name)
		configs[c.Name] = c
	}

	for _, c := range newConfigs {
		newSet.Add(c.Name)
		if old, exists := configs[c.Name]; exists {
			if !reflect.DeepEqual(old, c) {
				toUpdate.Add(c.Name)
				configs[c.Name] = c
			}
		} else {
			configs[c.Name] = c
		}
	}

	var finalError error

	toDelete := currentSet.Difference(newSet)
	toAdd := newSet.Difference(currentSet)

	for _, n := range toDelete.ToSlice() {
		logrus.Infof("Deleting config %s", n.(string))
		if err := s.DeleteConfig(n.(string)); err != nil {
			logrus.WithError(err).Errorf("failed to delete config %s", n)
			finalError = multierror.Append(finalError, err)
		}
	}

	for _, n := range toAdd.ToSlice() {
		rc := configs[n.(string)]
		logrus.Infof("Adding config %s", n.(string))
		if err := s.AddConfig(rc); err != nil {
			logrus.WithError(err).Errorf("failed to create resources %s", n)
			finalError = multierror.Append(finalError, err)
		}
	}

	for _, n := range toUpdate.ToSlice() {
		rc := configs[n.(string)]
		logrus.Infof("Updating config %s", n.(string))
		if err := s.UpdateConfig(rc); err != nil {
			logrus.WithError(err).Errorf("failed to update resources %s", n)
			finalError = multierror.Append(finalError, err)
		}
	}

	return finalError
}
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&ProwJob{},
		&ProwJobList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
func NewController(continueOnError bool, addedPresubmitBlacklist sets.String, prowJobClient prowv1.ProwJobInterface, githubClient *github.Client, configAgent *config.Agent, pluginAgent *plugins.ConfigAgent) *Controller {
	return &Controller{
		continueOnError:         continueOnError,
		addedPresubmitBlacklist: addedPresubmitBlacklist,
		prowJobTriggerer: &kubeProwJobTriggerer{
			prowJobClient: prowJobClient,
			githubClient:  githubClient,
			configAgent:   configAgent,
		},
		githubClient: githubClient,
		statusMigrator: &gitHubMigrator{
			githubClient:    githubClient,
			continueOnError: continueOnError,
		},
		trustedChecker: &githubTrustedChecker{
			githubClient: githubClient,
			pluginAgent:  pluginAgent,
		},
	}
}
func (c *Controller) Run(stop <-chan os.Signal, changes <-chan config.Delta) {
	for {
		select {
		case change := <-changes:
			start := time.Now()
			if err := c.reconcile(change); err != nil {
				logrus.WithError(err).Error("Error reconciling statuses.")
			}
			logrus.WithField("duration", fmt.Sprintf("%v", time.Since(start))).Info("Statuses reconciled")
		case <-stop:
			logrus.Info("status-reconciler is shutting down...")
			return
		}
	}
}
func addedBlockingPresubmits(old, new map[string][]config.Presubmit) map[string][]config.Presubmit {
	added := map[string][]config.Presubmit{}

	for repo, oldPresubmits := range old {
		added[repo] = []config.Presubmit{}
		for _, newPresubmit := range new[repo] {
			if !newPresubmit.ContextRequired() || newPresubmit.NeedsExplicitTrigger() {
				continue
			}
			var found bool
			for _, oldPresubmit := range oldPresubmits {
				if oldPresubmit.Name == newPresubmit.Name {
					if oldPresubmit.SkipReport && !newPresubmit.SkipReport {
						added[repo] = append(added[repo], newPresubmit)
						logrus.WithFields(logrus.Fields{
							"repo": repo,
							"name": oldPresubmit.Name,
						}).Debug("Identified a newly-reporting blocking presubmit.")
					}
					if oldPresubmit.RunIfChanged != newPresubmit.RunIfChanged {
						added[repo] = append(added[repo], newPresubmit)
						logrus.WithFields(logrus.Fields{
							"repo": repo,
							"name": oldPresubmit.Name,
						}).Debug("Identified a blocking presubmit running over a different set of files.")
					}
					found = true
					break
				}
			}
			if !found {
				added[repo] = append(added[repo], newPresubmit)
				logrus.WithFields(logrus.Fields{
					"repo": repo,
					"name": newPresubmit.Name,
				}).Debug("Identified an added blocking presubmit.")
			}
		}
	}

	var numAdded int
	for _, presubmits := range added {
		numAdded += len(presubmits)
	}
	logrus.Infof("Identified %d added blocking presubmits.", numAdded)
	return added
}
func removedBlockingPresubmits(old, new map[string][]config.Presubmit) map[string][]config.Presubmit {
	removed := map[string][]config.Presubmit{}

	for repo, oldPresubmits := range old {
		removed[repo] = []config.Presubmit{}
		for _, oldPresubmit := range oldPresubmits {
			if !oldPresubmit.ContextRequired() {
				continue
			}
			var found bool
			for _, newPresubmit := range new[repo] {
				if oldPresubmit.Name == newPresubmit.Name {
					found = true
					break
				}
			}
			if !found {
				removed[repo] = append(removed[repo], oldPresubmit)
				logrus.WithFields(logrus.Fields{
					"repo": repo,
					"name": oldPresubmit.Name,
				}).Debug("Identified a removed blocking presubmit.")
			}
		}
	}

	var numRemoved int
	for _, presubmits := range removed {
		numRemoved += len(presubmits)
	}
	logrus.Infof("Identified %d removed blocking presubmits.", numRemoved)
	return removed
}
func migratedBlockingPresubmits(old, new map[string][]config.Presubmit) map[string][]presubmitMigration {
	migrated := map[string][]presubmitMigration{}

	for repo, oldPresubmits := range old {
		migrated[repo] = []presubmitMigration{}
		for _, newPresubmit := range new[repo] {
			if !newPresubmit.ContextRequired() {
				continue
			}
			for _, oldPresubmit := range oldPresubmits {
				if oldPresubmit.Context != newPresubmit.Context && oldPresubmit.Name == newPresubmit.Name {
					migrated[repo] = append(migrated[repo], presubmitMigration{from: oldPresubmit, to: newPresubmit})
					logrus.WithFields(logrus.Fields{
						"repo": repo,
						"name": oldPresubmit.Name,
						"from": oldPresubmit.Context,
						"to":   newPresubmit.Context,
					}).Debug("Identified a migrated blocking presubmit.")
				}
			}
		}
	}

	var numMigrated int
	for _, presubmits := range migrated {
		numMigrated += len(presubmits)
	}
	logrus.Infof("Identified %d migrated blocking presubmits.", numMigrated)
	return migrated
}
func Load(loader OptionLoader) error {
	if jsonConfig, provided := os.LookupEnv(loader.ConfigVar()); provided {
		if err := loader.LoadConfig(jsonConfig); err != nil {
			return fmt.Errorf("could not load config from JSON var %s: %v", loader.ConfigVar(), err)
		}
		return nil
	}

	fs := flag.NewFlagSet(os.Args[0], flag.ExitOnError)
	loader.AddFlags(fs)
	fs.Parse(os.Args[1:])
	loader.Complete(fs.Args())

	return nil
}
func (c *Controller) canExecuteConcurrently(pj *prowapi.ProwJob) bool {
	c.lock.Lock()
	defer c.lock.Unlock()

	if max := c.config().MaxConcurrency; max > 0 {
		var running int
		for _, num := range c.pendingJobs {
			running += num
		}
		if running >= max {
			c.log.WithFields(pjutil.ProwJobFields(pj)).Debugf("Not starting another job, already %d running.", running)
			return false
		}
	}

	if pj.Spec.MaxConcurrency == 0 {
		c.pendingJobs[pj.Spec.Job]++
		return true
	}

	numPending := c.pendingJobs[pj.Spec.Job]
	if numPending >= pj.Spec.MaxConcurrency {
		c.log.WithFields(pjutil.ProwJobFields(pj)).Debugf("Not starting another instance of %s, already %d running.", pj.Spec.Job, numPending)
		return false
	}
	c.pendingJobs[pj.Spec.Job]++
	return true
}
func getJenkinsJobs(pjs []prowapi.ProwJob) []BuildQueryParams {
	jenkinsJobs := []BuildQueryParams{}

	for _, pj := range pjs {
		if pj.Complete() {
			continue
		}

		jenkinsJobs = append(jenkinsJobs, BuildQueryParams{
			JobName:   getJobName(&pj.Spec),
			ProwJobID: pj.Name,
		})
	}

	return jenkinsJobs
}
func (c *Controller) terminateDupes(pjs []prowapi.ProwJob, jbs map[string]Build) error {
	// "job org/repo#number" -> newest job
	dupes := make(map[string]int)
	for i, pj := range pjs {
		if pj.Complete() || pj.Spec.Type != prowapi.PresubmitJob {
			continue
		}
		n := fmt.Sprintf("%s %s/%s#%d", pj.Spec.Job, pj.Spec.Refs.Org, pj.Spec.Refs.Repo, pj.Spec.Refs.Pulls[0].Number)
		prev, ok := dupes[n]
		if !ok {
			dupes[n] = i
			continue
		}
		cancelIndex := i
		if (&pjs[prev].Status.StartTime).Before(&pj.Status.StartTime) {
			cancelIndex = prev
			dupes[n] = i
		}
		toCancel := pjs[cancelIndex]
		// Allow aborting presubmit jobs for commits that have been superseded by
		// newer commits in GitHub pull requests.
		if c.config().AllowCancellations {
			build, buildExists := jbs[toCancel.ObjectMeta.Name]
			// Avoid cancelling enqueued builds.
			if buildExists && build.IsEnqueued() {
				continue
			}
			// Otherwise, abort it.
			if buildExists {
				if err := c.jc.Abort(getJobName(&toCancel.Spec), &build); err != nil {
					c.log.WithError(err).WithFields(pjutil.ProwJobFields(&toCancel)).Warn("Cannot cancel Jenkins build")
				}
			}
		}
		toCancel.SetComplete()
		prevState := toCancel.Status.State
		toCancel.Status.State = prowapi.AbortedState
		c.log.WithFields(pjutil.ProwJobFields(&toCancel)).
			WithField("from", prevState).
			WithField("to", toCancel.Status.State).Info("Transitioning states.")
		npj, err := c.prowJobClient.Update(&toCancel)
		if err != nil {
			return err
		}
		pjs[cancelIndex] = *npj
	}
	return nil
}
func (c *Client) Throttle(hourlyTokens, burst int) {
	c.log("Throttle", hourlyTokens, burst)
	c.throttle.lock.Lock()
	defer c.throttle.lock.Unlock()
	previouslyThrottled := c.throttle.ticker != nil
	if hourlyTokens <= 0 || burst <= 0 { // Disable throttle
		if previouslyThrottled { // Unwrap clients if necessary
			c.client = c.throttle.http
			c.gqlc = c.throttle.graph
			c.throttle.ticker.Stop()
			c.throttle.ticker = nil
		}
		return
	}
	rate := time.Hour / time.Duration(hourlyTokens)
	ticker := time.NewTicker(rate)
	throttle := make(chan time.Time, burst)
	for i := 0; i < burst; i++ { // Fill up the channel
		throttle <- time.Now()
	}
	go func() {
		// Refill the channel
		for t := range ticker.C {
			select {
			case throttle <- t:
			default:
			}
		}
	}()
	if !previouslyThrottled { // Wrap clients if we haven't already
		c.throttle.http = c.client
		c.throttle.graph = c.gqlc
		c.client = &c.throttle
		c.gqlc = &c.throttle
	}
	c.throttle.ticker = ticker
	c.throttle.throttle = throttle
}
func NewClientWithFields(fields logrus.Fields, getToken func() []byte, graphqlEndpoint string, bases ...string) *Client {
	return &Client{
		logger: logrus.WithFields(fields).WithField("client", "github"),
		time:   &standardTime{},
		gqlc: githubql.NewEnterpriseClient(
			graphqlEndpoint,
			&http.Client{
				Timeout:   maxRequestTime,
				Transport: &oauth2.Transport{Source: newReloadingTokenSource(getToken)},
			}),
		client:   &http.Client{Timeout: maxRequestTime},
		bases:    bases,
		getToken: getToken,
		dry:      false,
	}
}
func NewClient(getToken func() []byte, graphqlEndpoint string, bases ...string) *Client {
	return NewClientWithFields(logrus.Fields{}, getToken, graphqlEndpoint, bases...)
}
func NewDryRunClient(getToken func() []byte, graphqlEndpoint string, bases ...string) *Client {
	return NewDryRunClientWithFields(logrus.Fields{}, getToken, graphqlEndpoint, bases...)
}
func NewFakeClient() *Client {
	return &Client{
		logger: logrus.WithField("client", "github"),
		time:   &standardTime{},
		fake:   true,
		dry:    true,
	}
}
func (c *Client) request(r *request, ret interface{}) (int, error) {
	statusCode, b, err := c.requestRaw(r)
	if err != nil {
		return statusCode, err
	}
	if ret != nil {
		if err := json.Unmarshal(b, ret); err != nil {
			return statusCode, err
		}
	}
	return statusCode, nil
}
func (c *Client) requestRaw(r *request) (int, []byte, error) {
	if c.fake || (c.dry && r.method != http.MethodGet) {
		return r.exitCodes[0], nil, nil
	}
	resp, err := c.requestRetry(r.method, r.path, r.accept, r.requestBody)
	if err != nil {
		return 0, nil, err
	}
	defer resp.Body.Close()
	b, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return 0, nil, err
	}
	var okCode bool
	for _, code := range r.exitCodes {
		if code == resp.StatusCode {
			okCode = true
			break
		}
	}
	if !okCode {
		clientError := unmarshalClientError(b)
		err = requestError{
			ClientError: clientError,
			ErrorString: fmt.Sprintf("status code %d not one of %v, body: %s", resp.StatusCode, r.exitCodes, string(b)),
		}
	}
	return resp.StatusCode, b, err
}
func (c *Client) getUserData() error {
	c.log("User")
	var u User
	_, err := c.request(&request{
		method:    http.MethodGet,
		path:      "/user",
		exitCodes: []int{200},
	}, &u)
	if err != nil {
		return err
	}
	c.botName = u.Login
	// email needs to be publicly accessible via the profile
	// of the current account. Read below for more info
	// https://developer.github.com/v3/users/#get-a-single-user
	c.email = u.Email
	return nil
}
func (c *Client) readPaginatedResultsWithValues(path string, values url.Values, accept string, newObj func() interface{}, accumulate func(interface{})) error {
	pagedPath := path
	if len(values) > 0 {
		pagedPath += "?" + values.Encode()
	}
	for {
		resp, err := c.requestRetry(http.MethodGet, pagedPath, accept, nil)
		if err != nil {
			return err
		}
		defer resp.Body.Close()
		if resp.StatusCode < 200 || resp.StatusCode > 299 {
			return fmt.Errorf("return code not 2XX: %s", resp.Status)
		}

		b, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			return err
		}

		obj := newObj()
		if err := json.Unmarshal(b, obj); err != nil {
			return err
		}

		accumulate(obj)

		link := parseLinks(resp.Header.Get("Link"))["next"]
		if link == "" {
			break
		}
		u, err := url.Parse(link)
		if err != nil {
			return fmt.Errorf("failed to parse 'next' link: %v", err)
		}
		pagedPath = u.RequestURI()
	}
	return nil
}
func (c *Client) UpdatePullRequest(org, repo string, number int, title, body *string, open *bool, branch *string, canModify *bool) error {
	c.log("UpdatePullRequest", org, repo, title)
	data := struct {
		State *string `json:"state,omitempty"`
		Title *string `json:"title,omitempty"`
		Body  *string `json:"body,omitempty"`
		Base  *string `json:"base,omitempty"`
		// MaintainerCanModify allows maintainers of the repo to modify this
		// pull request, eg. push changes to it before merging.
		MaintainerCanModify *bool `json:"maintainer_can_modify,omitempty"`
	}{
		Title:               title,
		Body:                body,
		Base:                branch,
		MaintainerCanModify: canModify,
	}
	if open != nil && *open {
		op := "open"
		data.State = &op
	} else if open != nil {
		cl := "clossed"
		data.State = &cl
	}
	_, err := c.request(&request{
		// allow the description and draft fields
		// https://developer.github.com/changes/2018-02-22-label-description-search-preview/
		// https://developer.github.com/changes/2019-02-14-draft-pull-requests/
		accept:      "application/vnd.github.symmetra-preview+json, application/vnd.github.shadow-cat-preview",
		method:      http.MethodPatch,
		path:        fmt.Sprintf("/repos/%s/%s/pulls/%d", org, repo, number),
		requestBody: &data,
		exitCodes:   []int{200},
	}, nil)
	return err
}
func (c *Client) getLabels(path string) ([]Label, error) {
	var labels []Label
	if c.fake {
		return labels, nil
	}
	err := c.readPaginatedResults(
		path,
		"application/vnd.github.symmetra-preview+json", // allow the description field -- https://developer.github.com/changes/2018-02-22-label-description-search-preview/
		func() interface{} {
			return &[]Label{}
		},
		func(obj interface{}) {
			labels = append(labels, *(obj.(*[]Label))...)
		},
	)
	if err != nil {
		return nil, err
	}
	return labels, nil
}
func stateCannotBeChangedOrOriginalError(err error) error {
	requestErr, ok := err.(requestError)
	if ok {
		for _, errorMsg := range requestErr.ErrorMessages() {
			if strings.Contains(errorMsg, stateCannotBeChangedMessagePrefix) {
				return StateCannotBeChanged{
					Message: errorMsg,
				}
			}
		}
	}
	return err
}
func (c *Client) IsMergeable(org, repo string, number int, SHA string) (bool, error) {
	backoff := time.Second * 3
	maxTries := 3
	for try := 0; try < maxTries; try++ {
		pr, err := c.GetPullRequest(org, repo, number)
		if err != nil {
			return false, err
		}
		if pr.Head.SHA != SHA {
			return false, fmt.Errorf("pull request head changed while checking mergeability (%s -> %s)", SHA, pr.Head.SHA)
		}
		if pr.Merged {
			return false, errors.New("pull request was merged while checking mergeability")
		}
		if pr.Mergable != nil {
			return *pr.Mergable, nil
		}
		if try+1 < maxTries {
			c.time.Sleep(backoff)
			backoff *= 2
		}
	}
	return false, fmt.Errorf("reached maximum number of retries (%d) checking mergeability", maxTries)
}
func (s *reloadingTokenSource) Token() (*oauth2.Token, error) {
	return &oauth2.Token{
		AccessToken: string(s.getToken()),
	}, nil
}
func (s *Spyglass) ListArtifacts(src string) ([]string, error) {
	keyType, key, err := splitSrc(src)
	if err != nil {
		return []string{}, fmt.Errorf("error parsing src: %v", err)
	}
	gcsKey := ""
	switch keyType {
	case gcsKeyType:
		gcsKey = key
	case prowKeyType:
		if gcsKey, err = s.prowToGCS(key); err != nil {
			logrus.Warningf("Failed to get gcs source for prow job: %v", err)
		}
	default:
		return nil, fmt.Errorf("Unrecognized key type for src: %v", src)
	}

	artifactNames, err := s.GCSArtifactFetcher.artifacts(gcsKey)
	logFound := false
	for _, name := range artifactNames {
		if name == "build-log.txt" {
			logFound = true
			break
		}
	}
	if err != nil || !logFound {
		artifactNames = append(artifactNames, "build-log.txt")
	}
	return artifactNames, nil
}
func (*Spyglass) KeyToJob(src string) (jobName string, buildID string, err error) {
	src = strings.Trim(src, "/")
	parsed := strings.Split(src, "/")
	if len(parsed) < 2 {
		return "", "", fmt.Errorf("expected at least two path components in %q", src)
	}
	jobName = parsed[len(parsed)-2]
	buildID = parsed[len(parsed)-1]
	return jobName, buildID, nil
}
func (s *Spyglass) prowToGCS(prowKey string) (string, error) {
	jobName, buildID, err := s.KeyToJob(prowKey)
	if err != nil {
		return "", fmt.Errorf("could not get GCS src: %v", err)
	}

	job, err := s.jobAgent.GetProwJob(jobName, buildID)
	if err != nil {
		return "", fmt.Errorf("Failed to get prow job from src %q: %v", prowKey, err)
	}

	url := job.Status.URL
	prefix := s.config().Plank.GetJobURLPrefix(job.Spec.Refs)
	if !strings.HasPrefix(url, prefix) {
		return "", fmt.Errorf("unexpected job URL %q when finding GCS path: expected something starting with %q", url, prefix)
	}
	return url[len(prefix):], nil
}
func (s *Spyglass) FetchArtifacts(src string, podName string, sizeLimit int64, artifactNames []string) ([]lenses.Artifact, error) {
	artStart := time.Now()
	arts := []lenses.Artifact{}
	keyType, key, err := splitSrc(src)
	if err != nil {
		return arts, fmt.Errorf("error parsing src: %v", err)
	}
	jobName, buildID, err := s.KeyToJob(src)
	if err != nil {
		return arts, fmt.Errorf("could not derive job: %v", err)
	}
	gcsKey := ""
	switch keyType {
	case gcsKeyType:
		gcsKey = strings.TrimSuffix(key, "/")
	case prowKeyType:
		if gcsKey, err = s.prowToGCS(key); err != nil {
			logrus.Warningln(err)
		}
	default:
		return nil, fmt.Errorf("invalid src: %v", src)
	}

	podLogNeeded := false
	for _, name := range artifactNames {
		art, err := s.GCSArtifactFetcher.artifact(gcsKey, name, sizeLimit)
		if err == nil {
			// Actually try making a request, because calling GCSArtifactFetcher.artifact does no I/O.
			// (these files are being explicitly requested and so will presumably soon be accessed, so
			// the extra network I/O should not be too problematic).
			_, err = art.Size()
		}
		if err != nil {
			if name == "build-log.txt" {
				podLogNeeded = true
			}
			continue
		}
		arts = append(arts, art)
	}

	if podLogNeeded {
		art, err := s.PodLogArtifactFetcher.artifact(jobName, buildID, sizeLimit)
		if err != nil {
			logrus.Errorf("Failed to fetch pod log: %v", err)
		} else {
			arts = append(arts, art)
		}
	}

	logrus.WithField("duration", time.Since(artStart)).Infof("Retrieved artifacts for %v", src)
	return arts, nil
}
func (in *DecorationConfig) DeepCopy() *DecorationConfig {
	if in == nil {
		return nil
	}
	out := new(DecorationConfig)
	in.DeepCopyInto(out)
	return out
}
func (in *GCSConfiguration) DeepCopy() *GCSConfiguration {
	if in == nil {
		return nil
	}
	out := new(GCSConfiguration)
	in.DeepCopyInto(out)
	return out
}
func (in *JenkinsSpec) DeepCopy() *JenkinsSpec {
	if in == nil {
		return nil
	}
	out := new(JenkinsSpec)
	in.DeepCopyInto(out)
	return out
}
func (in *ProwJob) DeepCopy() *ProwJob {
	if in == nil {
		return nil
	}
	out := new(ProwJob)
	in.DeepCopyInto(out)
	return out
}
func (in *ProwJobList) DeepCopy() *ProwJobList {
	if in == nil {
		return nil
	}
	out := new(ProwJobList)
	in.DeepCopyInto(out)
	return out
}
func (in *ProwJobSpec) DeepCopy() *ProwJobSpec {
	if in == nil {
		return nil
	}
	out := new(ProwJobSpec)
	in.DeepCopyInto(out)
	return out
}
func (in *ProwJobStatus) DeepCopy() *ProwJobStatus {
	if in == nil {
		return nil
	}
	out := new(ProwJobStatus)
	in.DeepCopyInto(out)
	return out
}
func (in *Pull) DeepCopy() *Pull {
	if in == nil {
		return nil
	}
	out := new(Pull)
	in.DeepCopyInto(out)
	return out
}
func (in *Refs) DeepCopy() *Refs {
	if in == nil {
		return nil
	}
	out := new(Refs)
	in.DeepCopyInto(out)
	return out
}
func (in *UtilityImages) DeepCopy() *UtilityImages {
	if in == nil {
		return nil
	}
	out := new(UtilityImages)
	in.DeepCopyInto(out)
	return out
}
func upload(rsClient *resultstore.Client, inv resultstore.Invocation, target resultstore.Target, test resultstore.Test) (string, error) {

	targetID := test.Name
	const configID = resultstore.Default
	invName, err := rsClient.Invocations().Create(inv)
	if err != nil {
		return "", fmt.Errorf("create invocation: %v", err)
	}
	targetName, err := rsClient.Targets(invName).Create(targetID, target)
	if err != nil {
		return resultstore.URL(invName), fmt.Errorf("create target: %v", err)
	}
	url := resultstore.URL(targetName)
	_, err = rsClient.Configurations(invName).Create(configID)
	if err != nil {
		return url, fmt.Errorf("create configuration: %v", err)
	}
	ctName, err := rsClient.ConfiguredTargets(targetName, configID).Create(test.Action)
	if err != nil {
		return url, fmt.Errorf("create configured target: %v", err)
	}
	_, err = rsClient.Actions(ctName).Create("primary", test)
	if err != nil {
		return url, fmt.Errorf("create action: %v", err)
	}
	return url, nil
}
func (d *DecorationConfig) ApplyDefault(def *DecorationConfig) *DecorationConfig {
	if d == nil && def == nil {
		return nil
	}
	var merged DecorationConfig
	if d != nil {
		merged = *d
	} else {
		merged = *def
	}
	if d == nil || def == nil {
		return &merged
	}
	merged.UtilityImages = merged.UtilityImages.ApplyDefault(def.UtilityImages)
	merged.GCSConfiguration = merged.GCSConfiguration.ApplyDefault(def.GCSConfiguration)

	if merged.Timeout.Duration == 0 {
		merged.Timeout = def.Timeout
	}
	if merged.GracePeriod.Duration == 0 {
		merged.GracePeriod = def.GracePeriod
	}
	if merged.GCSCredentialsSecret == "" {
		merged.GCSCredentialsSecret = def.GCSCredentialsSecret
	}
	if len(merged.SSHKeySecrets) == 0 {
		merged.SSHKeySecrets = def.SSHKeySecrets
	}
	if len(merged.SSHHostFingerprints) == 0 {
		merged.SSHHostFingerprints = def.SSHHostFingerprints
	}
	if merged.SkipCloning == nil {
		merged.SkipCloning = def.SkipCloning
	}
	if merged.CookiefileSecret == "" {
		merged.CookiefileSecret = def.CookiefileSecret
	}

	return &merged
}
func (d *DecorationConfig) Validate() error {
	if d.UtilityImages == nil {
		return errors.New("utility image config is not specified")
	}
	var missing []string
	if d.UtilityImages.CloneRefs == "" {
		missing = append(missing, "clonerefs")
	}
	if d.UtilityImages.InitUpload == "" {
		missing = append(missing, "initupload")
	}
	if d.UtilityImages.Entrypoint == "" {
		missing = append(missing, "entrypoint")
	}
	if d.UtilityImages.Sidecar == "" {
		missing = append(missing, "sidecar")
	}
	if len(missing) > 0 {
		return fmt.Errorf("the following utility images are not specified: %q", missing)
	}

	if d.GCSConfiguration == nil {
		return errors.New("GCS upload configuration is not specified")
	}
	if d.GCSCredentialsSecret == "" {
		return errors.New("GCS upload credential secret is not specified")
	}
	if err := d.GCSConfiguration.Validate(); err != nil {
		return fmt.Errorf("GCS configuration is invalid: %v", err)
	}
	return nil
}
func (u *UtilityImages) ApplyDefault(def *UtilityImages) *UtilityImages {
	if u == nil {
		return def
	} else if def == nil {
		return u
	}

	merged := *u
	if merged.CloneRefs == "" {
		merged.CloneRefs = def.CloneRefs
	}
	if merged.InitUpload == "" {
		merged.InitUpload = def.InitUpload
	}
	if merged.Entrypoint == "" {
		merged.Entrypoint = def.Entrypoint
	}
	if merged.Sidecar == "" {
		merged.Sidecar = def.Sidecar
	}
	return &merged
}
func (g *GCSConfiguration) ApplyDefault(def *GCSConfiguration) *GCSConfiguration {
	if g == nil && def == nil {
		return nil
	}
	var merged GCSConfiguration
	if g != nil {
		merged = *g
	} else {
		merged = *def
	}
	if g == nil || def == nil {
		return &merged
	}

	if merged.Bucket == "" {
		merged.Bucket = def.Bucket
	}
	if merged.PathPrefix == "" {
		merged.PathPrefix = def.PathPrefix
	}
	if merged.PathStrategy == "" {
		merged.PathStrategy = def.PathStrategy
	}
	if merged.DefaultOrg == "" {
		merged.DefaultOrg = def.DefaultOrg
	}
	if merged.DefaultRepo == "" {
		merged.DefaultRepo = def.DefaultRepo
	}
	return &merged
}
func (g *GCSConfiguration) Validate() error {
	if g.PathStrategy != PathStrategyLegacy && g.PathStrategy != PathStrategyExplicit && g.PathStrategy != PathStrategySingle {
		return fmt.Errorf("gcs_path_strategy must be one of %q, %q, or %q", PathStrategyLegacy, PathStrategyExplicit, PathStrategySingle)
	}
	if g.PathStrategy != PathStrategyExplicit && (g.DefaultOrg == "" || g.DefaultRepo == "") {
		return fmt.Errorf("default org and repo must be provided for GCS strategy %q", g.PathStrategy)
	}
	return nil
}
func (j *ProwJob) ClusterAlias() string {
	if j.Spec.Cluster == "" {
		return DefaultClusterAlias
	}
	return j.Spec.Cluster
}
func NewResource(name, rtype, state, owner string, t time.Time) Resource {
	return Resource{
		Name:       name,
		Type:       rtype,
		State:      state,
		Owner:      owner,
		LastUpdate: t,
		UserData:   &UserData{},
	}
}
func NewResourcesFromConfig(e ResourceEntry) []Resource {
	var resources []Resource
	for _, name := range e.Names {
		resources = append(resources, NewResource(name, e.Type, e.State, "", time.Time{}))
	}
	return resources
}
func UserDataFromMap(m UserDataMap) *UserData {
	ud := &UserData{}
	for k, v := range m {
		ud.Store(k, v)
	}
	return ud
}
func (r *CommaSeparatedStrings) Set(value string) error {
	if len(*r) > 0 {
		return errors.New("resTypes flag already set")
	}
	for _, rtype := range strings.Split(value, ",") {
		*r = append(*r, rtype)
	}
	return nil
}
func (ud *UserData) UnmarshalJSON(data []byte) error {
	tmpMap := UserDataMap{}
	if err := json.Unmarshal(data, &tmpMap); err != nil {
		return err
	}
	ud.FromMap(tmpMap)
	return nil
}
func (ud *UserData) Extract(id string, out interface{}) error {
	content, ok := ud.Load(id)
	if !ok {
		return &UserDataNotFound{id}
	}
	return yaml.Unmarshal([]byte(content.(string)), out)
}
func (ud *UserData) Set(id string, in interface{}) error {
	b, err := yaml.Marshal(in)
	if err != nil {
		return err
	}
	ud.Store(id, string(b))
	return nil
}
func (ud *UserData) Update(new *UserData) {
	if new == nil {
		return
	}
	new.Range(func(key, value interface{}) bool {
		if value.(string) != "" {
			ud.Store(key, value)
		} else {
			ud.Delete(key)
		}
		return true
	})
}
func (ud *UserData) ToMap() UserDataMap {
	m := UserDataMap{}
	ud.Range(func(key, value interface{}) bool {
		m[key.(string)] = value.(string)
		return true
	})
	return m
}
func (ud *UserData) FromMap(m UserDataMap) {
	for key, value := range m {
		ud.Store(key, value)
	}
}
func ItemToResource(i Item) (Resource, error) {
	res, ok := i.(Resource)
	if !ok {
		return Resource{}, fmt.Errorf("cannot construct Resource from received object %v", i)
	}
	return res, nil
}
func (o Options) Run() error {
	var env []string
	if len(o.KeyFiles) > 0 {
		var err error
		env, err = addSSHKeys(o.KeyFiles)
		if err != nil {
			logrus.WithError(err).Error("Failed to add SSH keys.")
			// Continue on error. Clones will fail with an appropriate error message
			// that initupload can consume whereas quitting without writing the clone
			// record log is silent and results in an errored prow job instead of a
			// failed one.
		}
	}
	if len(o.HostFingerprints) > 0 {
		if err := addHostFingerprints(o.HostFingerprints); err != nil {
			logrus.WithError(err).Error("failed to add host fingerprints")
		}
	}

	var numWorkers int
	if o.MaxParallelWorkers != 0 {
		numWorkers = o.MaxParallelWorkers
	} else {
		numWorkers = len(o.GitRefs)
	}

	wg := &sync.WaitGroup{}
	wg.Add(numWorkers)

	input := make(chan prowapi.Refs)
	output := make(chan clone.Record, len(o.GitRefs))
	for i := 0; i < numWorkers; i++ {
		go func() {
			defer wg.Done()
			for ref := range input {
				output <- cloneFunc(ref, o.SrcRoot, o.GitUserName, o.GitUserEmail, o.CookiePath, env)
			}
		}()
	}

	for _, ref := range o.GitRefs {
		input <- ref
	}

	close(input)
	wg.Wait()
	close(output)

	var results []clone.Record
	for record := range output {
		results = append(results, record)
	}

	logData, err := json.Marshal(results)
	if err != nil {
		return fmt.Errorf("failed to marshal clone records: %v", err)
	}

	if err := ioutil.WriteFile(o.Log, logData, 0755); err != nil {
		return fmt.Errorf("failed to write clone records: %v", err)
	}

	return nil
}
func addSSHKeys(paths []string) ([]string, error) {
	vars, err := exec.Command("ssh-agent").CombinedOutput()
	if err != nil {
		return []string{}, fmt.Errorf("failed to start ssh-agent: %v", err)
	}
	logrus.Info("Started SSH agent")
	// ssh-agent will output three lines of text, in the form:
	// SSH_AUTH_SOCK=xxx; export SSH_AUTH_SOCK;
	// SSH_AGENT_PID=xxx; export SSH_AGENT_PID;
	// echo Agent pid xxx;
	// We need to parse out the environment variables from that.
	parts := strings.Split(string(vars), ";")
	env := []string{strings.TrimSpace(parts[0]), strings.TrimSpace(parts[2])}
	for _, keyPath := range paths {
		// we can be given literal paths to keys or paths to dirs
		// that are mounted from a secret, so we need to check which
		// we have
		if err := filepath.Walk(keyPath, func(path string, info os.FileInfo, err error) error {
			if strings.HasPrefix(info.Name(), "..") {
				// kubernetes volumes also include files we
				// should not look be looking into for keys
				if info.IsDir() {
					return filepath.SkipDir
				}
				return nil
			}
			if info.IsDir() {
				return nil
			}

			cmd := exec.Command("ssh-add", path)
			cmd.Env = append(cmd.Env, env...)
			if output, err := cmd.CombinedOutput(); err != nil {
				return fmt.Errorf("failed to add ssh key at %s: %v: %s", path, err, output)
			}
			logrus.Infof("Added SSH key at %s", path)
			return nil
		}); err != nil {
			return env, fmt.Errorf("error walking path %q: %v", keyPath, err)
		}
	}
	return env, nil
}
func (f *TriageFiler) Issues(c *creator.IssueCreator) ([]creator.Issue, error) {
	f.creator = c
	rawjson, err := ReadHTTP(clusterDataURL)
	if err != nil {
		return nil, err
	}
	clusters, err := f.loadClusters(rawjson)
	if err != nil {
		return nil, err
	}
	topclusters := topClusters(clusters, f.topClustersCount)
	issues := make([]creator.Issue, 0, len(topclusters))
	for _, clust := range topclusters {
		issues = append(issues, clust)
	}
	return issues, nil
}
func (f *TriageFiler) loadClusters(jsonIn []byte) ([]*Cluster, error) {
	var err error
	f.data, err = parseTriageData(jsonIn)
	if err != nil {
		return nil, err
	}
	if err = f.filterAndValidate(f.windowDays); err != nil {
		return nil, err
	}

	// Aggregate failing builds in each cluster by job (independent of tests).
	for _, clust := range f.data.Clustered {
		clust.filer = f
		clust.jobs = make(map[string][]int)

		for _, test := range clust.Tests {
			for _, job := range test.Jobs {
				for _, buildnum := range job.Builds {
					found := false
					for _, oldBuild := range clust.jobs[job.Name] {
						if oldBuild == buildnum {
							found = true
							break
						}
					}
					if !found {
						clust.jobs[job.Name] = append(clust.jobs[job.Name], buildnum)
					}
				}
			}
		}
		clust.totalJobs = len(clust.jobs)
		clust.totalTests = len(clust.Tests)
		clust.totalBuilds = 0
		for _, builds := range clust.jobs {
			clust.totalBuilds += len(builds)
		}
	}
	return f.data.Clustered, nil
}
func parseTriageData(jsonIn []byte) (*triageData, error) {
	var data triageData
	if err := json.Unmarshal(jsonIn, &data); err != nil {
		return nil, err
	}

	if data.Builds.Cols.Started == nil {
		return nil, fmt.Errorf("triage data json is missing the builds.cols.started key")
	}
	if data.Builds.JobsRaw == nil {
		return nil, fmt.Errorf("triage data is missing the builds.jobs key")
	}
	if data.Builds.JobPaths == nil {
		return nil, fmt.Errorf("triage data is missing the builds.job_paths key")
	}
	if data.Clustered == nil {
		return nil, fmt.Errorf("triage data is missing the clustered key")
	}
	// Populate 'Jobs' with the BuildIndexer for each job.
	data.Builds.Jobs = make(map[string]BuildIndexer)
	for jobID, mapper := range data.Builds.JobsRaw {
		switch mapper := mapper.(type) {
		case []interface{}:
			// In this case mapper is a 3 member array. 0:first buildnum, 1:number of builds, 2:start index.
			data.Builds.Jobs[jobID] = ContigIndexer{
				startBuild: int(mapper[0].(float64)),
				count:      int(mapper[1].(float64)),
				startRow:   int(mapper[2].(float64)),
			}
		case map[string]interface{}:
			// In this case mapper is a dictionary.
			data.Builds.Jobs[jobID] = DictIndexer(mapper)
		default:
			return nil, fmt.Errorf("the build number to row index mapping for job '%s' is not an accepted type. Type is: %v", jobID, reflect.TypeOf(mapper))
		}
	}
	return &data, nil
}
func topClusters(clusters []*Cluster, count int) []*Cluster {
	less := func(i, j int) bool { return clusters[i].totalBuilds > clusters[j].totalBuilds }
	sort.SliceStable(clusters, less)

	if len(clusters) < count {
		count = len(clusters)
	}
	return clusters[0:count]
}
func (c *Cluster) topJobsFailed(count int) []*Job {
	slice := make([]*Job, len(c.jobs))
	i := 0
	for jobName, builds := range c.jobs {
		slice[i] = &Job{Name: jobName, Builds: builds}
		i++
	}
	less := func(i, j int) bool { return len(slice[i].Builds) > len(slice[j].Builds) }
	sort.SliceStable(slice, less)

	if len(slice) < count {
		count = len(slice)
	}
	return slice[0:count]
}
func (c *Cluster) Title() string {
	return fmt.Sprintf("Failure cluster [%s...] failed %d builds, %d jobs, and %d tests over %d days",
		c.Identifier[0:6],
		c.totalBuilds,
		c.totalJobs,
		c.totalTests,
		c.filer.windowDays,
	)
}
func (c *Cluster) Labels() []string {
	labels := []string{"kind/flake"}

	topTests := make([]string, len(c.Tests))
	for i, test := range c.topTestsFailed(len(c.Tests)) {
		topTests[i] = test.Name
	}
	for sig := range c.filer.creator.TestsSIGs(topTests) {
		labels = append(labels, "sig/"+sig)
	}

	return labels
}
func New() *Cron {
	return &Cron{
		cronAgent: cron.New(),
		jobs:      map[string]*jobStatus{},
		logger:    logrus.WithField("client", "cron"),
	}
}
func (c *Cron) QueuedJobs() []string {
	c.lock.Lock()
	defer c.lock.Unlock()

	res := []string{}
	for k, v := range c.jobs {
		if v.triggered {
			res = append(res, k)
		}
		c.jobs[k].triggered = false
	}
	return res
}
func (c *Cron) HasJob(name string) bool {
	c.lock.Lock()
	defer c.lock.Unlock()

	_, ok := c.jobs[name]
	return ok
}
func (c *Cron) addJob(name, cron string) error {
	id, err := c.cronAgent.AddFunc("TZ=UTC "+cron, func() {
		c.lock.Lock()
		defer c.lock.Unlock()

		c.jobs[name].triggered = true
		c.logger.Infof("Triggering cron job %s.", name)
	})

	if err != nil {
		return fmt.Errorf("cronAgent fails to add job %s with cron %s: %v", name, cron, err)
	}

	c.jobs[name] = &jobStatus{
		entryID: id,
		cronStr: cron,
		// try to kick of a periodic trigger right away
		triggered: strings.HasPrefix(cron, "@every"),
	}

	c.logger.Infof("Added new cron job %s with trigger %s.", name, cron)
	return nil
}
func (c *Cron) removeJob(name string) error {
	job, ok := c.jobs[name]
	if !ok {
		return fmt.Errorf("job %s has not been added to cronAgent yet", name)
	}
	c.cronAgent.Remove(job.entryID)
	delete(c.jobs, name)
	c.logger.Infof("Removed previous cron job %s.", name)
	return nil
}
func UpdateComments(issueID int, pullRequest bool, db *gorm.DB, client ClientInterface) {
	latest := findLatestCommentUpdate(issueID, db, client.RepositoryName())

	updateIssueComments(issueID, latest, db, client)
	if pullRequest {
		updatePullComments(issueID, latest, db, client)
	}
}
func GatherProwJobMetrics(pjs []prowapi.ProwJob) {
	// map of job to job type to state to count
	metricMap := make(map[string]map[string]map[string]float64)

	for _, pj := range pjs {
		if metricMap[pj.Spec.Job] == nil {
			metricMap[pj.Spec.Job] = make(map[string]map[string]float64)
		}
		if metricMap[pj.Spec.Job][string(pj.Spec.Type)] == nil {
			metricMap[pj.Spec.Job][string(pj.Spec.Type)] = make(map[string]float64)
		}
		metricMap[pj.Spec.Job][string(pj.Spec.Type)][string(pj.Status.State)]++
	}

	// This may be racing with the prometheus server but we need to remove
	// stale metrics like triggered or pending jobs that are now complete.
	prowJobs.Reset()

	for job, jobMap := range metricMap {
		for jobType, typeMap := range jobMap {
			for state, count := range typeMap {
				prowJobs.WithLabelValues(job, jobType, state).Set(count)
			}
		}
	}
}
func optionOrDefault(option, defaultValue time.Duration) time.Duration {
	if option == 0 {
		return defaultValue
	}

	return option
}
func newGCSJobSource(src string) (*gcsJobSource, error) {
	gcsURL, err := url.Parse(fmt.Sprintf("gs://%s", src))
	if err != nil {
		return &gcsJobSource{}, ErrCannotParseSource
	}
	gcsPath := &gcs.Path{}
	err = gcsPath.SetURL(gcsURL)
	if err != nil {
		return &gcsJobSource{}, ErrCannotParseSource
	}

	tokens := strings.FieldsFunc(gcsPath.Object(), func(c rune) bool { return c == '/' })
	if len(tokens) < 2 {
		return &gcsJobSource{}, ErrCannotParseSource
	}
	buildID := tokens[len(tokens)-1]
	name := tokens[len(tokens)-2]
	return &gcsJobSource{
		source:     src,
		linkPrefix: "gs://",
		bucket:     gcsPath.Bucket(),
		jobPrefix:  path.Clean(gcsPath.Object()) + "/",
		jobName:    name,
		buildID:    buildID,
	}, nil
}
func (af *GCSArtifactFetcher) artifacts(key string) ([]string, error) {
	src, err := newGCSJobSource(key)
	if err != nil {
		return nil, fmt.Errorf("Failed to get GCS job source from %s: %v", key, err)
	}

	listStart := time.Now()
	bucketName, prefix := extractBucketPrefixPair(src.jobPath())
	artifacts := []string{}
	bkt := af.client.Bucket(bucketName)
	q := storage.Query{
		Prefix:   prefix,
		Versions: false,
	}
	objIter := bkt.Objects(context.Background(), &q)
	wait := []time.Duration{16, 32, 64, 128, 256, 256, 512, 512}
	for i := 0; ; {
		oAttrs, err := objIter.Next()
		if err == iterator.Done {
			break
		}
		if err != nil {
			logrus.WithFields(fieldsForJob(src)).WithError(err).Error("Error accessing GCS artifact.")
			if i >= len(wait) {
				return artifacts, fmt.Errorf("timed out: error accessing GCS artifact: %v", err)
			}
			time.Sleep((wait[i] + time.Duration(rand.Intn(10))) * time.Millisecond)
			i++
			continue
		}
		artifacts = append(artifacts, strings.TrimPrefix(oAttrs.Name, prefix))
		i = 0
	}
	listElapsed := time.Since(listStart)
	logrus.WithField("duration", listElapsed).Infof("Listed %d artifacts.", len(artifacts))
	return artifacts, nil
}
func (src *gcsJobSource) canonicalLink() string {
	return path.Join(src.linkPrefix, src.bucket, src.jobPrefix)
}
func (src *gcsJobSource) jobPath() string {
	return fmt.Sprintf("%s/%s", src.bucket, src.jobPrefix)
}
func targetURL(c config.Getter, pr *PullRequest, log *logrus.Entry) string {
	var link string
	if tideURL := c().Tide.TargetURL; tideURL != "" {
		link = tideURL
	} else if baseURL := c().Tide.PRStatusBaseURL; baseURL != "" {
		parseURL, err := url.Parse(baseURL)
		if err != nil {
			log.WithError(err).Error("Failed to parse PR status base URL")
		} else {
			prQuery := fmt.Sprintf("is:pr repo:%s author:%s head:%s", pr.Repository.NameWithOwner, pr.Author.Login, pr.HeadRefName)
			values := parseURL.Query()
			values.Set("query", prQuery)
			parseURL.RawQuery = values.Encode()
			link = parseURL.String()
		}
	}
	return link
}
func newBuildConfig(cfg rest.Config, stop chan struct{}) (*buildConfig, error) {
	bc, err := buildset.NewForConfig(&cfg)
	if err != nil {
		return nil, err
	}

	// Ensure the knative-build CRD is deployed
	// TODO(fejta): probably a better way to do this
	_, err = bc.BuildV1alpha1().Builds("").List(metav1.ListOptions{Limit: 1})
	if err != nil {
		return nil, err
	}
	// Assume watches receive updates, but resync every 30m in case something wonky happens
	bif := buildinfo.NewSharedInformerFactory(bc, 30*time.Minute)
	bif.Build().V1alpha1().Builds().Lister()
	go bif.Start(stop)
	return &buildConfig{
		client:   bc,
		informer: bif.Build().V1alpha1().Builds(),
	}, nil
}
func NewClient(token string, dryRun bool) *Client {
	httpClient := &http.Client{
		Transport: &oauth2.Transport{
			Base:   http.DefaultTransport,
			Source: oauth2.ReuseTokenSource(nil, oauth2.StaticTokenSource(&oauth2.Token{AccessToken: token})),
		},
	}
	client := github.NewClient(httpClient)
	return &Client{
		issueService:        client.Issues,
		prService:           client.PullRequests,
		repoService:         client.Repositories,
		userService:         client.Users,
		retries:             5,
		retryInitialBackoff: time.Second,
		tokenReserve:        50,
		dryRun:              dryRun,
	}
}
func (c *Client) retry(action string, call func() (*github.Response, error)) (*github.Response, error) {
	var err error
	var resp *github.Response

	for retryCount := 0; retryCount <= c.retries; retryCount++ {
		if resp, err = call(); err == nil {
			c.limitRate(&resp.Rate)
			return resp, nil
		}
		switch err := err.(type) {
		case *github.RateLimitError:
			c.limitRate(&err.Rate)
		case *github.TwoFactorAuthError:
			return resp, err
		case *retryAbort:
			return resp, err
		}

		if retryCount == c.retries {
			return resp, err
		}
		glog.Errorf("error %s: %v. Will retry.\n", action, err)
		c.sleepForAttempt(retryCount)
	}
	return resp, err
}
func (c *Client) depaginate(action string, opts *github.ListOptions, call func() ([]interface{}, *github.Response, error)) ([]interface{}, error) {
	var allItems []interface{}
	wrapper := func() (*github.Response, error) {
		items, resp, err := call()
		if err == nil {
			allItems = append(allItems, items...)
		}
		return resp, err
	}

	opts.Page = 1
	opts.PerPage = 100
	lastPage := 1
	for ; opts.Page <= lastPage; opts.Page++ {
		resp, err := c.retry(action, wrapper)
		if err != nil {
			return allItems, fmt.Errorf("error while depaginating page %d/%d: %v", opts.Page, lastPage, err)
		}
		if resp.LastPage > 0 {
			lastPage = resp.LastPage
		}
	}
	return allItems, nil
}
func NewHelpAgent(pa pluginAgent, ghc githubClient) *HelpAgent {
	l := logrus.WithField("client", "plugin-help")
	return &HelpAgent{
		log: l,
		pa:  pa,
		oa:  newOrgAgent(l, ghc, newRepoDetectionLimit),
	}
}
func (ha *HelpAgent) GeneratePluginHelp() *pluginhelp.Help {
	config := ha.pa.Config()
	orgToRepos := ha.oa.orgToReposMap(config)

	normalRevMap, externalRevMap := reversePluginMaps(config, orgToRepos)

	allPlugins, pluginHelp := ha.generateNormalPluginHelp(config, normalRevMap)

	allExternalPlugins, externalPluginHelp := ha.generateExternalPluginHelp(config, externalRevMap)

	// Load repo->plugins maps from config
	repoPlugins := map[string][]string{
		"": allPlugins,
	}
	for repo, plugins := range config.Plugins {
		repoPlugins[repo] = plugins
	}
	repoExternalPlugins := map[string][]string{
		"": allExternalPlugins,
	}
	for repo, exts := range config.ExternalPlugins {
		for _, ext := range exts {
			repoExternalPlugins[repo] = append(repoExternalPlugins[repo], ext.Name)
		}
	}

	return &pluginhelp.Help{
		AllRepos:            allRepos(config, orgToRepos),
		RepoPlugins:         repoPlugins,
		RepoExternalPlugins: repoExternalPlugins,
		PluginHelp:          pluginHelp,
		ExternalPluginHelp:  externalPluginHelp,
	}
}
func getPullCommitHash(pull string) (string, error) {
	match := pullCommitRe.FindStringSubmatch(pull)
	if len(match) != 2 {
		expected := "branch:hash,pullNumber:hash"
		return "", fmt.Errorf("unable to parse pull %q (expected %q)", pull, expected)
	}
	return match[1], nil
}
func listJobBuilds(bucket storageBucket, jobPrefixes []string) []jobBuilds {
	jobch := make(chan jobBuilds)
	defer close(jobch)
	for i, jobPrefix := range jobPrefixes {
		go func(i int, jobPrefix string) {
			buildPrefixes, err := bucket.listSubDirs(jobPrefix)
			if err != nil {
				logrus.WithError(err).Warningf("Error getting builds for job %s", jobPrefix)
			}
			jobch <- jobBuilds{
				name:          path.Base(jobPrefix),
				buildPrefixes: buildPrefixes,
			}
		}(i, jobPrefix)
	}
	jobs := []jobBuilds{}
	for range jobPrefixes {
		job := <-jobch
		jobs = append(jobs, job)
	}
	return jobs
}
func getPRBuildData(bucket storageBucket, jobs []jobBuilds) []buildData {
	buildch := make(chan buildData)
	defer close(buildch)
	expected := 0
	for _, job := range jobs {
		for j, buildPrefix := range job.buildPrefixes {
			go func(j int, jobName, buildPrefix string) {
				build, err := getBuildData(bucket, buildPrefix)
				if err != nil {
					logrus.WithError(err).Warningf("build %s information incomplete", buildPrefix)
				}
				split := strings.Split(strings.TrimSuffix(buildPrefix, "/"), "/")
				build.SpyglassLink = path.Join(spyglassPrefix, bucket.getName(), buildPrefix)
				build.ID = split[len(split)-1]
				build.jobName = jobName
				build.prefix = buildPrefix
				build.index = j
				buildch <- build
			}(j, job.name, buildPrefix)
			expected++
		}
	}
	builds := []buildData{}
	for k := 0; k < expected; k++ {
		build := <-buildch
		builds = append(builds, build)
	}
	return builds
}
func getGCSDirsForPR(config *config.Config, org, repo string, pr int) (map[string]sets.String, error) {
	toSearch := make(map[string]sets.String)
	fullRepo := org + "/" + repo
	presubmits, ok := config.Presubmits[fullRepo]
	if !ok {
		return toSearch, fmt.Errorf("couldn't find presubmits for %q in config", fullRepo)
	}

	for _, presubmit := range presubmits {
		var gcsConfig *v1.GCSConfiguration
		if presubmit.DecorationConfig != nil && presubmit.DecorationConfig.GCSConfiguration != nil {
			gcsConfig = presubmit.DecorationConfig.GCSConfiguration
		} else {
			// for undecorated jobs assume the default
			gcsConfig = config.Plank.DefaultDecorationConfig.GCSConfiguration
		}

		gcsPath, _, _ := gcsupload.PathsForJob(gcsConfig, &downwardapi.JobSpec{
			Type: v1.PresubmitJob,
			Job:  presubmit.Name,
			Refs: &v1.Refs{
				Repo: repo,
				Org:  org,
				Pulls: []v1.Pull{
					{Number: pr},
				},
			},
		}, "")
		gcsPath, _ = path.Split(path.Clean(gcsPath))
		if _, ok := toSearch[gcsConfig.Bucket]; !ok {
			toSearch[gcsConfig.Bucket] = sets.String{}
		}
		toSearch[gcsConfig.Bucket].Insert(gcsPath)
	}
	return toSearch, nil
}
func imageDeleteFromDisk(fingerprint string) {
	// Remove main image file.
	fname := shared.VarPath("images", fingerprint)
	if shared.PathExists(fname) {
		err := os.Remove(fname)
		if err != nil && !os.IsNotExist(err) {
			logger.Errorf("Error deleting image file %s: %s", fname, err)
		}
	}

	// Remove the rootfs file for the image.
	fname = shared.VarPath("images", fingerprint) + ".rootfs"
	if shared.PathExists(fname) {
		err := os.Remove(fname)
		if err != nil && !os.IsNotExist(err) {
			logger.Errorf("Error deleting image file %s: %s", fname, err)
		}
	}
}
func doNetworksCreate(d *Daemon, req api.NetworksPost, withDatabase bool) error {
	// Start the network
	n, err := networkLoadByName(d.State(), req.Name)
	if err != nil {
		return err
	}

	err = n.Start()
	if err != nil {
		n.Delete(withDatabase)
		return err
	}

	return nil
}
func networkLoadByName(s *state.State, name string) (*network, error) {
	id, dbInfo, err := s.Cluster.NetworkGet(name)
	if err != nil {
		return nil, err
	}

	n := network{state: s, id: id, name: name, description: dbInfo.Description, config: dbInfo.Config}

	return &n, nil
}
func (n *Node) Transaction(f func(*NodeTx) error) error {
	nodeTx := &NodeTx{}
	return query.Transaction(n.db, func(tx *sql.Tx) error {
		nodeTx.tx = tx
		return f(nodeTx)
	})
}
func ForLocalInspectionWithPreparedStmts(db *sql.DB) (*Cluster, error) {
	c := ForLocalInspection(db)

	stmts, err := cluster.PrepareStmts(c.db)
	if err != nil {
		return nil, errors.Wrap(err, "Prepare database statements")
	}

	c.stmts = stmts

	return c, nil
}
func (c *Cluster) SetDefaultTimeout(timeout time.Duration) {
	driver := c.db.Driver().(*dqlite.Driver)
	driver.SetContextTimeout(timeout)
}
func (c *Cluster) Transaction(f func(*ClusterTx) error) error {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.transaction(f)
}
func (c *Cluster) EnterExclusive() error {
	logger.Debug("Acquiring exclusive lock on cluster db")

	ch := make(chan struct{})
	go func() {
		c.mu.Lock()
		ch <- struct{}{}
	}()

	timeout := 20 * time.Second
	select {
	case <-ch:
		return nil
	case <-time.After(timeout):
		return fmt.Errorf("timeout (%s)", timeout)
	}
}
func (c *Cluster) ExitExclusive(f func(*ClusterTx) error) error {
	logger.Debug("Releasing exclusive lock on cluster db")
	defer c.mu.Unlock()
	return c.transaction(f)
}
func (c *Cluster) Close() error {
	for _, stmt := range c.stmts {
		stmt.Close()
	}
	return c.db.Close()
}
func TxCommit(tx *sql.Tx) error {
	err := tx.Commit()
	if err == nil || err == sql.ErrTxDone { // Ignore duplicate commits/rollbacks
		return nil
	}
	return err
}
func (c *Config) ParseRemote(raw string) (string, string, error) {
	result := strings.SplitN(raw, ":", 2)
	if len(result) == 1 {
		return c.DefaultRemote, raw, nil
	}

	_, ok := c.Remotes[result[0]]
	if !ok {
		// Attempt to play nice with snapshots containing ":"
		if shared.IsSnapshot(raw) && shared.IsSnapshot(result[0]) {
			return c.DefaultRemote, raw, nil
		}

		return "", "", fmt.Errorf("The remote \"%s\" doesn't exist", result[0])
	}

	return result[0], result[1], nil
}
func (c *Config) GetContainerServer(name string) (lxd.ContainerServer, error) {
	// Get the remote
	remote, ok := c.Remotes[name]
	if !ok {
		return nil, fmt.Errorf("The remote \"%s\" doesn't exist", name)
	}

	// Sanity checks
	if remote.Public || remote.Protocol == "simplestreams" {
		return nil, fmt.Errorf("The remote isn't a private LXD server")
	}

	// Get connection arguments
	args, err := c.getConnectionArgs(name)
	if err != nil {
		return nil, err
	}

	// Unix socket
	if strings.HasPrefix(remote.Addr, "unix:") {
		d, err := lxd.ConnectLXDUnix(strings.TrimPrefix(strings.TrimPrefix(remote.Addr, "unix:"), "//"), args)
		if err != nil {
			return nil, err
		}

		if remote.Project != "" && remote.Project != "default" {
			d = d.UseProject(remote.Project)
		}

		if c.ProjectOverride != "" {
			d = d.UseProject(c.ProjectOverride)
		}

		return d, nil
	}

	// HTTPs
	if remote.AuthType != "candid" && (args.TLSClientCert == "" || args.TLSClientKey == "") {
		return nil, fmt.Errorf("Missing TLS client certificate and key")
	}

	d, err := lxd.ConnectLXD(remote.Addr, args)
	if err != nil {
		return nil, err
	}

	if remote.Project != "" && remote.Project != "default" {
		d = d.UseProject(remote.Project)
	}

	if c.ProjectOverride != "" {
		d = d.UseProject(c.ProjectOverride)
	}

	return d, nil
}
func (c *Config) GetImageServer(name string) (lxd.ImageServer, error) {
	// Get the remote
	remote, ok := c.Remotes[name]
	if !ok {
		return nil, fmt.Errorf("The remote \"%s\" doesn't exist", name)
	}

	// Get connection arguments
	args, err := c.getConnectionArgs(name)
	if err != nil {
		return nil, err
	}

	// Unix socket
	if strings.HasPrefix(remote.Addr, "unix:") {
		d, err := lxd.ConnectLXDUnix(strings.TrimPrefix(strings.TrimPrefix(remote.Addr, "unix:"), "//"), args)
		if err != nil {
			return nil, err
		}

		if remote.Project != "" && remote.Project != "default" {
			d = d.UseProject(remote.Project)
		}

		if c.ProjectOverride != "" {
			d = d.UseProject(c.ProjectOverride)
		}

		return d, nil
	}

	// HTTPs (simplestreams)
	if remote.Protocol == "simplestreams" {
		d, err := lxd.ConnectSimpleStreams(remote.Addr, args)
		if err != nil {
			return nil, err
		}

		return d, nil
	}

	// HTTPs (public LXD)
	if remote.Public {
		d, err := lxd.ConnectPublicLXD(remote.Addr, args)
		if err != nil {
			return nil, err
		}

		return d, nil
	}

	// HTTPs (private LXD)
	d, err := lxd.ConnectLXD(remote.Addr, args)
	if err != nil {
		return nil, err
	}

	if remote.Project != "" && remote.Project != "default" {
		d = d.UseProject(remote.Project)
	}

	if c.ProjectOverride != "" {
		d = d.UseProject(c.ProjectOverride)
	}

	return d, nil
}
func (s *OS) initAppArmor() {
	/* Detect AppArmor availability */
	_, err := exec.LookPath("apparmor_parser")
	if os.Getenv("LXD_SECURITY_APPARMOR") == "false" {
		logger.Warnf("AppArmor support has been manually disabled")
	} else if !shared.IsDir("/sys/kernel/security/apparmor") {
		logger.Warnf("AppArmor support has been disabled because of lack of kernel support")
	} else if err != nil {
		logger.Warnf("AppArmor support has been disabled because 'apparmor_parser' couldn't be found")
	} else {
		s.AppArmorAvailable = true
	}

	/* Detect AppArmor stacking support */
	s.AppArmorStacking = appArmorCanStack()

	/* Detect existing AppArmor stack */
	if shared.PathExists("/sys/kernel/security/apparmor/.ns_stacked") {
		contentBytes, err := ioutil.ReadFile("/sys/kernel/security/apparmor/.ns_stacked")
		if err == nil && string(contentBytes) == "yes\n" {
			s.AppArmorStacked = true
		}
	}

	/* Detect AppArmor admin support */
	if !haveMacAdmin() {
		if s.AppArmorAvailable {
			logger.Warnf("Per-container AppArmor profiles are disabled because the mac_admin capability is missing")
		}
	} else if s.RunningInUserNS && !s.AppArmorStacked {
		if s.AppArmorAvailable {
			logger.Warnf("Per-container AppArmor profiles are disabled because LXD is running in an unprivileged container without stacking")
		}
	} else {
		s.AppArmorAdmin = true
	}

	/* Detect AppArmor confinment */
	profile := util.AppArmorProfile()
	if profile != "unconfined" && profile != "" {
		if s.AppArmorAvailable {
			logger.Warnf("Per-container AppArmor profiles are disabled because LXD is already protected by AppArmor")
		}
		s.AppArmorConfined = true
	}
}
func appArmorCanStack() bool {
	contentBytes, err := ioutil.ReadFile("/sys/kernel/security/apparmor/features/domain/stack")
	if err != nil {
		return false
	}

	if string(contentBytes) != "yes\n" {
		return false
	}

	contentBytes, err = ioutil.ReadFile("/sys/kernel/security/apparmor/features/domain/version")
	if err != nil {
		return false
	}

	content := string(contentBytes)

	parts := strings.Split(strings.TrimSpace(content), ".")

	if len(parts) == 0 {
		logger.Warn("Unknown apparmor domain version", log.Ctx{"version": content})
		return false
	}

	major, err := strconv.Atoi(parts[0])
	if err != nil {
		logger.Warn("Unknown apparmor domain version", log.Ctx{"version": content})
		return false
	}

	minor := 0
	if len(parts) == 2 {
		minor, err = strconv.Atoi(parts[1])
		if err != nil {
			logger.Warn("Unknown apparmor domain version", log.Ctx{"version": content})
			return false
		}
	}

	return major >= 1 && minor >= 2
}
func containerDeviceAdd(client lxd.ContainerServer, name string, devName string, dev map[string]string) error {
	// Get the container entry
	container, etag, err := client.GetContainer(name)
	if err != nil {
		return err
	}

	// Check if the device already exists
	_, ok := container.Devices[devName]
	if ok {
		return fmt.Errorf(i18n.G("Device already exists: %s"), devName)
	}

	container.Devices[devName] = dev

	op, err := client.UpdateContainer(name, container.Writable(), etag)
	if err != nil {
		return err
	}

	return op.Wait()
}
func profileDeviceAdd(client lxd.ContainerServer, name string, devName string, dev map[string]string) error {
	// Get the profile entry
	profile, profileEtag, err := client.GetProfile(name)
	if err != nil {
		return err
	}

	// Check if the device already exists
	_, ok := profile.Devices[devName]
	if ok {
		return fmt.Errorf(i18n.G("Device already exists: %s"), devName)
	}

	// Add the device to the container
	profile.Devices[devName] = dev

	err = client.UpdateProfile(name, profile.Writable(), profileEtag)
	if err != nil {
		return err
	}

	return nil
}
func ensureImageAliases(client lxd.ContainerServer, aliases []api.ImageAlias, fingerprint string) error {
	if len(aliases) == 0 {
		return nil
	}

	names := make([]string, len(aliases))
	for i, alias := range aliases {
		names[i] = alias.Name
	}
	sort.Strings(names)

	resp, err := client.GetImageAliases()
	if err != nil {
		return err
	}

	// Delete existing aliases that match provided ones
	for _, alias := range GetExistingAliases(names, resp) {
		err := client.DeleteImageAlias(alias.Name)
		if err != nil {
			fmt.Println(fmt.Sprintf(i18n.G("Failed to remove alias %s"), alias.Name))
		}
	}
	// Create new aliases
	for _, alias := range aliases {
		aliasPost := api.ImageAliasesPost{}
		aliasPost.Name = alias.Name
		aliasPost.Target = fingerprint
		err := client.CreateImageAlias(aliasPost)
		if err != nil {
			fmt.Println(fmt.Sprintf(i18n.G("Failed to create alias %s"), alias.Name))
		}
	}
	return nil
}
func GetExistingAliases(aliases []string, allAliases []api.ImageAliasesEntry) []api.ImageAliasesEntry {
	existing := []api.ImageAliasesEntry{}
	for _, alias := range allAliases {
		name := alias.Name
		pos := sort.SearchStrings(aliases, name)
		if pos < len(aliases) && aliases[pos] == name {
			existing = append(existing, alias)
		}
	}
	return existing
}
func (o StatusCode) String() string {
	return map[StatusCode]string{
		OperationCreated: "Operation created",
		Started:          "Started",
		Stopped:          "Stopped",
		Running:          "Running",
		Cancelling:       "Cancelling",
		Pending:          "Pending",
		Success:          "Success",
		Failure:          "Failure",
		Cancelled:        "Cancelled",
		Starting:         "Starting",
		Stopping:         "Stopping",
		Aborting:         "Aborting",
		Freezing:         "Freezing",
		Frozen:           "Frozen",
		Thawed:           "Thawed",
		Error:            "Error",
	}[o]
}
func (c *Cluster) ImagesGetExpired(expiry int64) ([]string, error) {
	q := `SELECT fingerprint, last_use_date, upload_date FROM images WHERE cached=1`

	var fpStr string
	var useStr string
	var uploadStr string

	inargs := []interface{}{}
	outfmt := []interface{}{fpStr, useStr, uploadStr}
	dbResults, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return []string{}, err
	}

	results := []string{}
	for _, r := range dbResults {
		// Figure out the expiry
		timestamp := r[2]
		if r[1] != "" {
			timestamp = r[1]
		}

		var imageExpiry time.Time
		err = imageExpiry.UnmarshalText([]byte(timestamp.(string)))
		if err != nil {
			return []string{}, err
		}
		imageExpiry = imageExpiry.Add(time.Duration(expiry*24) * time.Hour)

		// Check if expired
		if imageExpiry.After(time.Now()) {
			continue
		}

		results = append(results, r[0].(string))
	}

	return results, nil
}
func (c *Cluster) ImageSourceInsert(id int, server string, protocol string, certificate string, alias string) error {
	stmt := `INSERT INTO images_source (image_id, server, protocol, certificate, alias) values (?, ?, ?, ?, ?)`

	protocolInt := -1
	for protoInt, protoString := range ImageSourceProtocol {
		if protoString == protocol {
			protocolInt = protoInt
		}
	}

	if protocolInt == -1 {
		return fmt.Errorf("Invalid protocol: %s", protocol)
	}

	err := exec(c.db, stmt, id, server, protocolInt, certificate, alias)
	return err
}
func (c *Cluster) ImageSourceGet(imageID int) (int, api.ImageSource, error) {
	q := `SELECT id, server, protocol, certificate, alias FROM images_source WHERE image_id=?`

	id := 0
	protocolInt := -1
	result := api.ImageSource{}

	arg1 := []interface{}{imageID}
	arg2 := []interface{}{&id, &result.Server, &protocolInt, &result.Certificate, &result.Alias}
	err := dbQueryRowScan(c.db, q, arg1, arg2)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, api.ImageSource{}, ErrNoSuchObject
		}

		return -1, api.ImageSource{}, err
	}

	protocol, found := ImageSourceProtocol[protocolInt]
	if !found {
		return -1, api.ImageSource{}, fmt.Errorf("Invalid protocol: %d", protocolInt)
	}

	result.Protocol = protocol

	return id, result, nil

}
func (c *Cluster) ImageExists(project string, fingerprint string) (bool, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return false, err
	}

	var exists bool
	query := `
SELECT COUNT(*) > 0
  FROM images
  JOIN projects ON projects.id = images.project_id
 WHERE projects.name = ? AND fingerprint=?
`
	inargs := []interface{}{project, fingerprint}
	outargs := []interface{}{&exists}
	err = dbQueryRowScan(c.db, query, inargs, outargs)
	if err == sql.ErrNoRows {
		return exists, ErrNoSuchObject
	}

	return exists, err
}
func (c *Cluster) ImageGet(project, fingerprint string, public bool, strictMatching bool) (int, *api.Image, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return -1, nil, err
	}

	var create, expire, used, upload *time.Time // These hold the db-returned times

	// The object we'll actually return
	image := api.Image{}
	id := -1
	arch := -1

	// These two humongous things will be filled by the call to DbQueryRowScan
	outfmt := []interface{}{&id, &image.Fingerprint, &image.Filename,
		&image.Size, &image.Cached, &image.Public, &image.AutoUpdate, &arch,
		&create, &expire, &used, &upload}

	inargs := []interface{}{project}
	query := `
        SELECT
            images.id, fingerprint, filename, size, cached, public, auto_update, architecture,
            creation_date, expiry_date, last_use_date, upload_date
        FROM images
        JOIN projects ON projects.id = images.project_id
       WHERE projects.name = ?`
	if strictMatching {
		inargs = append(inargs, fingerprint)
		query += " AND fingerprint = ?"
	} else {
		inargs = append(inargs, fingerprint+"%")
		query += " AND fingerprint LIKE ?"
	}

	if public {
		query += " AND public=1"
	}

	err = dbQueryRowScan(c.db, query, inargs, outfmt)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, nil, ErrNoSuchObject
		}

		return -1, nil, err // Likely: there are no rows for this fingerprint
	}

	// Validate we only have a single match
	if !strictMatching {
		query = `
SELECT COUNT(images.id)
  FROM images
  JOIN projects ON projects.id = images.project_id
 WHERE projects.name = ?
   AND fingerprint LIKE ?
`
		count := 0
		outfmt := []interface{}{&count}

		err = dbQueryRowScan(c.db, query, inargs, outfmt)
		if err != nil {
			return -1, nil, err
		}

		if count > 1 {
			return -1, nil, fmt.Errorf("Partial fingerprint matches more than one image")
		}
	}

	err = c.imageFill(id, &image, create, expire, used, upload, arch)
	if err != nil {
		return -1, nil, errors.Wrapf(err, "Fill image details")
	}

	return id, &image, nil
}
func (c *Cluster) ImageGetFromAnyProject(fingerprint string) (int, *api.Image, error) {
	var create, expire, used, upload *time.Time // These hold the db-returned times

	// The object we'll actually return
	image := api.Image{}
	id := -1
	arch := -1

	// These two humongous things will be filled by the call to DbQueryRowScan
	outfmt := []interface{}{&id, &image.Fingerprint, &image.Filename,
		&image.Size, &image.Cached, &image.Public, &image.AutoUpdate, &arch,
		&create, &expire, &used, &upload}

	inargs := []interface{}{fingerprint}
	query := `
        SELECT
            images.id, fingerprint, filename, size, cached, public, auto_update, architecture,
            creation_date, expiry_date, last_use_date, upload_date
        FROM images
        WHERE fingerprint = ?
        LIMIT 1`

	err := dbQueryRowScan(c.db, query, inargs, outfmt)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, nil, ErrNoSuchObject
		}

		return -1, nil, err // Likely: there are no rows for this fingerprint
	}

	err = c.imageFill(id, &image, create, expire, used, upload, arch)
	if err != nil {
		return -1, nil, errors.Wrapf(err, "Fill image details")
	}

	return id, &image, nil
}
func (c *Cluster) imageFill(id int, image *api.Image, create, expire, used, upload *time.Time, arch int) error {
	// Some of the dates can be nil in the DB, let's process them.
	if create != nil {
		image.CreatedAt = *create
	} else {
		image.CreatedAt = time.Time{}
	}

	if expire != nil {
		image.ExpiresAt = *expire
	} else {
		image.ExpiresAt = time.Time{}
	}

	if used != nil {
		image.LastUsedAt = *used
	} else {
		image.LastUsedAt = time.Time{}
	}

	image.Architecture, _ = osarch.ArchitectureName(arch)

	// The upload date is enforced by NOT NULL in the schema, so it can never be nil.
	image.UploadedAt = *upload

	// Get the properties
	q := "SELECT key, value FROM images_properties where image_id=?"
	var key, value, name, desc string
	inargs := []interface{}{id}
	outfmt := []interface{}{key, value}
	results, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return err
	}

	properties := map[string]string{}
	for _, r := range results {
		key = r[0].(string)
		value = r[1].(string)
		properties[key] = value
	}

	image.Properties = properties

	// Get the aliases
	q = "SELECT name, description FROM images_aliases WHERE image_id=?"
	inargs = []interface{}{id}
	outfmt = []interface{}{name, desc}
	results, err = queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return err
	}

	aliases := []api.ImageAlias{}
	for _, r := range results {
		name = r[0].(string)
		desc = r[1].(string)
		a := api.ImageAlias{Name: name, Description: desc}
		aliases = append(aliases, a)
	}

	image.Aliases = aliases

	_, source, err := c.ImageSourceGet(id)
	if err == nil {
		image.UpdateSource = &source
	}

	return nil
}
func (c *Cluster) ImageLocate(fingerprint string) (string, error) {
	stmt := `
SELECT nodes.address FROM nodes
  LEFT JOIN images_nodes ON images_nodes.node_id = nodes.id
  LEFT JOIN images ON images_nodes.image_id = images.id
WHERE images.fingerprint = ?
`
	var localAddress string // Address of this node
	var addresses []string  // Addresses of online nodes with the image

	err := c.Transaction(func(tx *ClusterTx) error {
		offlineThreshold, err := tx.NodeOfflineThreshold()
		if err != nil {
			return err
		}

		localAddress, err = tx.NodeAddress()
		if err != nil {
			return err
		}
		allAddresses, err := query.SelectStrings(tx.tx, stmt, fingerprint)
		if err != nil {
			return err
		}
		for _, address := range allAddresses {
			node, err := tx.NodeByAddress(address)
			if err != nil {
				return err
			}
			if address != localAddress && node.IsOffline(offlineThreshold) {
				continue
			}
			addresses = append(addresses, address)
		}
		return err
	})
	if err != nil {
		return "", err
	}
	if len(addresses) == 0 {
		return "", fmt.Errorf("image not available on any online node")
	}

	for _, address := range addresses {
		if address == localAddress {
			return "", nil
		}
	}

	return addresses[0], nil
}
func (c *Cluster) ImageAssociateNode(project, fingerprint string) error {
	imageID, _, err := c.ImageGet(project, fingerprint, false, true)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		_, err := tx.tx.Exec("INSERT INTO images_nodes(image_id, node_id) VALUES(?, ?)", imageID, c.nodeID)
		return err
	})
	return err
}
func (c *Cluster) ImageDelete(id int) error {
	err := exec(c.db, "DELETE FROM images WHERE id=?", id)
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) ImageAliasesGet(project string) ([]string, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	q := `
SELECT images_aliases.name
  FROM images_aliases
  JOIN projects ON projects.id=images_aliases.project_id
 WHERE projects.name=?
`
	var name string
	inargs := []interface{}{project}
	outfmt := []interface{}{name}
	results, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return nil, err
	}
	names := []string{}
	for _, res := range results {
		names = append(names, res[0].(string))
	}
	return names, nil
}
func (c *Cluster) ImageAliasGet(project, name string, isTrustedClient bool) (int, api.ImageAliasesEntry, error) {
	id := -1
	entry := api.ImageAliasesEntry{}

	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return id, entry, err
	}

	q := `SELECT images_aliases.id, images.fingerprint, images_aliases.description
			 FROM images_aliases
			 INNER JOIN images
			 ON images_aliases.image_id=images.id
                         INNER JOIN projects
                         ON images_aliases.project_id=projects.id
			 WHERE projects.name=? AND images_aliases.name=?`
	if !isTrustedClient {
		q = q + ` AND images.public=1`
	}

	var fingerprint, description string

	arg1 := []interface{}{project, name}
	arg2 := []interface{}{&id, &fingerprint, &description}
	err = dbQueryRowScan(c.db, q, arg1, arg2)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, entry, ErrNoSuchObject
		}

		return -1, entry, err
	}

	entry.Name = name
	entry.Target = fingerprint
	entry.Description = description

	return id, entry, nil
}
func (c *Cluster) ImageAliasRename(id int, name string) error {
	err := exec(c.db, "UPDATE images_aliases SET name=? WHERE id=?", name, id)
	return err
}
func (c *Cluster) ImageAliasDelete(project, name string) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return err
	}

	err = exec(c.db, `
DELETE
  FROM images_aliases
 WHERE project_id = (SELECT id FROM projects WHERE name = ?) AND name = ?
`, project, name)
	return err
}
func (c *Cluster) ImageAliasesMove(source int, destination int) error {
	err := exec(c.db, "UPDATE images_aliases SET image_id=? WHERE image_id=?", destination, source)
	return err
}
func (c *Cluster) ImageAliasAdd(project, name string, imageID int, desc string) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return err
	}

	stmt := `
INSERT INTO images_aliases (name, image_id, description, project_id)
     VALUES (?, ?, ?, (SELECT id FROM projects WHERE name = ?))
`
	err = exec(c.db, stmt, name, imageID, desc, project)
	return err
}
func (c *Cluster) ImageAliasUpdate(id int, imageID int, desc string) error {
	stmt := `UPDATE images_aliases SET image_id=?, description=? WHERE id=?`
	err := exec(c.db, stmt, imageID, desc, id)
	return err
}
func (c *Cluster) ImageLastAccessUpdate(fingerprint string, date time.Time) error {
	stmt := `UPDATE images SET last_use_date=? WHERE fingerprint=?`
	err := exec(c.db, stmt, date, fingerprint)
	return err
}
func (c *Cluster) ImageLastAccessInit(fingerprint string) error {
	stmt := `UPDATE images SET cached=1, last_use_date=strftime("%s") WHERE fingerprint=?`
	err := exec(c.db, stmt, fingerprint)
	return err
}
func (c *Cluster) ImageUpdate(id int, fname string, sz int64, public bool, autoUpdate bool, architecture string, createdAt time.Time, expiresAt time.Time, properties map[string]string) error {
	arch, err := osarch.ArchitectureId(architecture)
	if err != nil {
		arch = 0
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		publicInt := 0
		if public {
			publicInt = 1
		}

		autoUpdateInt := 0
		if autoUpdate {
			autoUpdateInt = 1
		}

		stmt, err := tx.tx.Prepare(`UPDATE images SET filename=?, size=?, public=?, auto_update=?, architecture=?, creation_date=?, expiry_date=? WHERE id=?`)
		if err != nil {
			return err
		}
		defer stmt.Close()

		_, err = stmt.Exec(fname, sz, publicInt, autoUpdateInt, arch, createdAt, expiresAt, id)
		if err != nil {
			return err
		}

		_, err = tx.tx.Exec(`DELETE FROM images_properties WHERE image_id=?`, id)
		if err != nil {
			return err
		}

		stmt2, err := tx.tx.Prepare(`INSERT INTO images_properties (image_id, type, key, value) VALUES (?, ?, ?, ?)`)
		if err != nil {
			return err
		}
		defer stmt2.Close()

		for key, value := range properties {
			_, err = stmt2.Exec(id, 0, key, value)
			if err != nil {
				return err
			}
		}

		return nil
	})
	return err
}
func (c *Cluster) ImageInsert(project, fp string, fname string, sz int64, public bool, autoUpdate bool, architecture string, createdAt time.Time, expiresAt time.Time, properties map[string]string) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasImages(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has images")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return err
	}

	arch, err := osarch.ArchitectureId(architecture)
	if err != nil {
		arch = 0
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		publicInt := 0
		if public {
			publicInt = 1
		}

		autoUpdateInt := 0
		if autoUpdate {
			autoUpdateInt = 1
		}

		stmt, err := tx.tx.Prepare(`INSERT INTO images (project_id, fingerprint, filename, size, public, auto_update, architecture, creation_date, expiry_date, upload_date) VALUES ((SELECT id FROM projects WHERE name = ?), ?, ?, ?, ?, ?, ?, ?, ?, ?)`)
		if err != nil {
			return err
		}
		defer stmt.Close()

		result, err := stmt.Exec(project, fp, fname, sz, publicInt, autoUpdateInt, arch, createdAt, expiresAt, time.Now().UTC())
		if err != nil {
			return err
		}

		id64, err := result.LastInsertId()
		if err != nil {
			return err
		}
		id := int(id64)

		if len(properties) > 0 {
			pstmt, err := tx.tx.Prepare(`INSERT INTO images_properties (image_id, type, key, value) VALUES (?, 0, ?, ?)`)
			if err != nil {
				return err
			}
			defer pstmt.Close()

			for k, v := range properties {
				// we can assume, that there is just one
				// value per key
				_, err = pstmt.Exec(id, k, v)
				if err != nil {
					return err
				}
			}

		}

		_, err = tx.tx.Exec("INSERT INTO images_nodes(image_id, node_id) VALUES(?, ?)", id, c.nodeID)
		if err != nil {
			return err
		}

		return nil
	})
	return err
}
func (c *Cluster) ImageGetPools(imageFingerprint string) ([]int64, error) {
	poolID := int64(-1)
	query := "SELECT storage_pool_id FROM storage_volumes WHERE node_id=? AND name=? AND type=?"
	inargs := []interface{}{c.nodeID, imageFingerprint, StoragePoolVolumeTypeImage}
	outargs := []interface{}{poolID}

	result, err := queryScan(c.db, query, inargs, outargs)
	if err != nil {
		return []int64{}, err
	}

	poolIDs := []int64{}
	for _, r := range result {
		poolIDs = append(poolIDs, r[0].(int64))
	}

	return poolIDs, nil
}
func (c *Cluster) ImageGetPoolNamesFromIDs(poolIDs []int64) ([]string, error) {
	var poolName string
	query := "SELECT name FROM storage_pools WHERE id=?"

	poolNames := []string{}
	for _, poolID := range poolIDs {
		inargs := []interface{}{poolID}
		outargs := []interface{}{poolName}

		result, err := queryScan(c.db, query, inargs, outargs)
		if err != nil {
			return []string{}, err
		}

		for _, r := range result {
			poolNames = append(poolNames, r[0].(string))
		}
	}

	return poolNames, nil
}
func (c *Cluster) ImageUploadedAt(id int, uploadedAt time.Time) error {
	err := exec(c.db, "UPDATE images SET upload_date=? WHERE id=?", uploadedAt, id)
	return err
}
func (c *Cluster) ImagesGetOnCurrentNode() (map[string][]string, error) {
	return c.ImagesGetByNodeID(c.nodeID)
}
func (c *Cluster) ImagesGetByNodeID(id int64) (map[string][]string, error) {
	images := make(map[string][]string) // key is fingerprint, value is list of projects
	err := c.Transaction(func(tx *ClusterTx) error {
		stmt := `
    SELECT images.fingerprint, projects.name FROM images
      LEFT JOIN images_nodes ON images.id = images_nodes.image_id
			LEFT JOIN nodes ON images_nodes.node_id = nodes.id
			LEFT JOIN projects ON images.project_id = projects.id
    WHERE nodes.id = ?
		`
		rows, err := tx.tx.Query(stmt, id)
		if err != nil {
			return err
		}

		var fingerprint string
		var projectName string
		for rows.Next() {
			err := rows.Scan(&fingerprint, &projectName)
			if err != nil {
				return err
			}

			images[fingerprint] = append(images[fingerprint], projectName)
		}

		return rows.Err()
	})
	return images, err
}
func (c *Cluster) ImageGetNodesWithImage(fingerprint string) ([]string, error) {
	q := `
SELECT DISTINCT nodes.address FROM nodes
  LEFT JOIN images_nodes ON images_nodes.node_id = nodes.id
  LEFT JOIN images ON images_nodes.image_id = images.id
WHERE images.fingerprint = ?
	`
	return c.getNodesByImageFingerprint(q, fingerprint)
}
func (c *Cluster) ImageGetNodesWithoutImage(fingerprint string) ([]string, error) {
	q := `
SELECT DISTINCT nodes.address FROM nodes WHERE nodes.address NOT IN (
  SELECT DISTINCT nodes.address FROM nodes
    LEFT JOIN images_nodes ON images_nodes.node_id = nodes.id
    LEFT JOIN images ON images_nodes.image_id = images.id
  WHERE images.fingerprint = ?)
`
	return c.getNodesByImageFingerprint(q, fingerprint)
}
func (g *Group) Add(f Func, schedule Schedule) *Task {
	i := len(g.tasks)
	g.tasks = append(g.tasks, Task{
		f:        f,
		schedule: schedule,
		reset:    make(chan struct{}, 16), // Buffered to not block senders
	})
	return &g.tasks[i]
}
func (g *Group) Start() {
	ctx := context.Background()
	ctx, g.cancel = context.WithCancel(ctx)
	g.wg.Add(len(g.tasks))

	g.mu.Lock()
	if g.running == nil {
		g.running = make(map[int]bool)
	}
	g.mu.Unlock()

	for i := range g.tasks {
		g.mu.Lock()
		if g.running[i] {
			g.mu.Unlock()
			continue
		}

		g.running[i] = true
		task := g.tasks[i] // Local variable for the closure below.
		g.mu.Unlock()

		go func(i int) {
			task.loop(ctx)
			g.wg.Done()

			g.mu.Lock()
			g.running[i] = false
			g.mu.Unlock()
		}(i)
	}
}
func zfsIsEnabled() bool {
	out, err := exec.LookPath("zfs")
	if err != nil || len(out) == 0 {
		return false
	}

	return true
}
func zfsToolVersionGet() (string, error) {
	// This function is only really ever relevant on Ubuntu as the only
	// distro that ships out of sync tools and kernel modules
	out, err := shared.RunCommand("dpkg-query", "--showformat=${Version}", "--show", "zfsutils-linux")
	if err != nil {
		return "", err
	}

	return strings.TrimSpace(string(out)), nil
}
func zfsModuleVersionGet() (string, error) {
	var zfsVersion string

	if shared.PathExists("/sys/module/zfs/version") {
		out, err := ioutil.ReadFile("/sys/module/zfs/version")
		if err != nil {
			return "", fmt.Errorf("Could not determine ZFS module version")
		}

		zfsVersion = string(out)
	} else {
		out, err := shared.RunCommand("modinfo", "-F", "version", "zfs")
		if err != nil {
			return "", fmt.Errorf("Could not determine ZFS module version")
		}

		zfsVersion = out
	}

	return strings.TrimSpace(zfsVersion), nil
}
func zfsPoolVolumeCreate(dataset string, properties ...string) (string, error) {
	cmd := []string{"zfs", "create"}

	for _, prop := range properties {
		cmd = append(cmd, []string{"-o", prop}...)
	}

	cmd = append(cmd, []string{"-p", dataset}...)

	return shared.RunCommand(cmd[0], cmd[1:]...)
}
func zfsPoolVolumeExists(dataset string) (bool, error) {
	output, err := shared.RunCommand(
		"zfs", "list", "-Ho", "name")

	if err != nil {
		return false, err
	}

	for _, name := range strings.Split(output, "\n") {
		if name == dataset {
			return true, nil
		}
	}
	return false, nil
}
func (c *ClusterTx) NetworkIDsNotPending() (map[string]int64, error) {
	networks := []struct {
		id   int64
		name string
	}{}
	dest := func(i int) []interface{} {
		networks = append(networks, struct {
			id   int64
			name string
		}{})
		return []interface{}{&networks[i].id, &networks[i].name}

	}
	stmt, err := c.tx.Prepare("SELECT id, name FROM networks WHERE NOT state=?")
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, networkPending)
	if err != nil {
		return nil, err
	}
	ids := map[string]int64{}
	for _, network := range networks {
		ids[network.name] = network.id
	}
	return ids, nil
}
func (c *ClusterTx) NetworkConfigAdd(networkID, nodeID int64, config map[string]string) error {
	return networkConfigAdd(c.tx, networkID, nodeID, config)
}
func (c *ClusterTx) NetworkNodeJoin(networkID, nodeID int64) error {
	columns := []string{"network_id", "node_id"}
	values := []interface{}{networkID, nodeID}
	_, err := query.UpsertObject(c.tx, "networks_nodes", columns, values)
	return err
}
func (c *ClusterTx) NetworkCreatePending(node, name string, conf map[string]string) error {
	// First check if a network with the given name exists, and, if
	// so, that it's in the pending state.
	network := struct {
		id    int64
		state int
	}{}

	var errConsistency error
	dest := func(i int) []interface{} {
		// Sanity check that there is at most one pool with the given name.
		if i != 0 {
			errConsistency = fmt.Errorf("more than one network exists with the given name")
		}
		return []interface{}{&network.id, &network.state}
	}
	stmt, err := c.tx.Prepare("SELECT id, state FROM networks WHERE name=?")
	if err != nil {
		return err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, name)
	if err != nil {
		return err
	}
	if errConsistency != nil {
		return errConsistency
	}

	var networkID = network.id
	if networkID == 0 {
		// No existing network with the given name was found, let's create
		// one.
		columns := []string{"name"}
		values := []interface{}{name}
		networkID, err = query.UpsertObject(c.tx, "networks", columns, values)
		if err != nil {
			return err
		}
	} else {
		// Check that the existing network  is in the pending state.
		if network.state != networkPending {
			return fmt.Errorf("network is not in pending state")
		}
	}

	// Get the ID of the node with the given name.
	nodeInfo, err := c.NodeByName(node)
	if err != nil {
		return err
	}

	// Check that no network entry for this node and network exists yet.
	count, err := query.Count(
		c.tx, "networks_nodes", "network_id=? AND node_id=?", networkID, nodeInfo.ID)
	if err != nil {
		return err
	}
	if count != 0 {
		return ErrAlreadyDefined
	}

	// Insert the node-specific configuration.
	columns := []string{"network_id", "node_id"}
	values := []interface{}{networkID, nodeInfo.ID}
	_, err = query.UpsertObject(c.tx, "networks_nodes", columns, values)
	if err != nil {
		return err
	}
	err = c.NetworkConfigAdd(networkID, nodeInfo.ID, conf)
	if err != nil {
		return err
	}

	return nil
}
func (c *ClusterTx) NetworkCreated(name string) error {
	return c.networkState(name, networkCreated)
}
func (c *ClusterTx) NetworkErrored(name string) error {
	return c.networkState(name, networkErrored)
}
func (c *Cluster) NetworkGet(name string) (int64, *api.Network, error) {
	description := sql.NullString{}
	id := int64(-1)
	state := 0

	q := "SELECT id, description, state FROM networks WHERE name=?"
	arg1 := []interface{}{name}
	arg2 := []interface{}{&id, &description, &state}
	err := dbQueryRowScan(c.db, q, arg1, arg2)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, nil, ErrNoSuchObject
		}

		return -1, nil, err
	}

	config, err := c.NetworkConfigGet(id)
	if err != nil {
		return -1, nil, err
	}

	network := api.Network{
		Name:    name,
		Managed: true,
		Type:    "bridge",
	}
	network.Description = description.String
	network.Config = config

	switch state {
	case networkPending:
		network.Status = "Pending"
	case networkCreated:
		network.Status = "Created"
	default:
		network.Status = "Unknown"
	}

	nodes, err := c.networkNodes(id)
	if err != nil {
		return -1, nil, err
	}
	network.Locations = nodes

	return id, &network, nil
}
func (c *Cluster) networkNodes(networkID int64) ([]string, error) {
	stmt := `
SELECT nodes.name FROM nodes
  JOIN networks_nodes ON networks_nodes.node_id = nodes.id
  WHERE networks_nodes.network_id = ?
`
	var nodes []string
	err := c.Transaction(func(tx *ClusterTx) error {
		var err error
		nodes, err = query.SelectStrings(tx.tx, stmt, networkID)
		return err
	})
	if err != nil {
		return nil, err
	}
	return nodes, nil
}
func (c *Cluster) NetworkGetInterface(devName string) (int64, *api.Network, error) {
	id := int64(-1)
	name := ""
	value := ""

	q := "SELECT networks.id, networks.name, networks_config.value FROM networks LEFT JOIN networks_config ON networks.id=networks_config.network_id WHERE networks_config.key=\"bridge.external_interfaces\" AND networks_config.node_id=?"
	arg1 := []interface{}{c.nodeID}
	arg2 := []interface{}{id, name, value}
	result, err := queryScan(c.db, q, arg1, arg2)
	if err != nil {
		return -1, nil, err
	}

	for _, r := range result {
		for _, entry := range strings.Split(r[2].(string), ",") {
			entry = strings.TrimSpace(entry)

			if entry == devName {
				id = r[0].(int64)
				name = r[1].(string)
			}
		}
	}

	if id == -1 {
		return -1, nil, fmt.Errorf("No network found for interface: %s", devName)
	}

	config, err := c.NetworkConfigGet(id)
	if err != nil {
		return -1, nil, err
	}

	network := api.Network{
		Name:    name,
		Managed: true,
		Type:    "bridge",
	}
	network.Config = config

	return id, &network, nil
}
func (c *Cluster) NetworkConfigGet(id int64) (map[string]string, error) {
	var key, value string
	query := `
        SELECT
            key, value
        FROM networks_config
		WHERE network_id=?
                AND (node_id=? OR node_id IS NULL)`
	inargs := []interface{}{id, c.nodeID}
	outfmt := []interface{}{key, value}
	results, err := queryScan(c.db, query, inargs, outfmt)
	if err != nil {
		return nil, fmt.Errorf("Failed to get network '%d'", id)
	}

	if len(results) == 0 {
		/*
		 * If we didn't get any rows here, let's check to make sure the
		 * network really exists; if it doesn't, let's send back a 404.
		 */
		query := "SELECT id FROM networks WHERE id=?"
		var r int
		results, err := queryScan(c.db, query, []interface{}{id}, []interface{}{r})
		if err != nil {
			return nil, err
		}

		if len(results) == 0 {
			return nil, ErrNoSuchObject
		}
	}

	config := map[string]string{}

	for _, r := range results {
		key = r[0].(string)
		value = r[1].(string)

		config[key] = value
	}

	return config, nil
}
func (c *Cluster) NetworkCreate(name, description string, config map[string]string) (int64, error) {
	var id int64
	err := c.Transaction(func(tx *ClusterTx) error {
		result, err := tx.tx.Exec("INSERT INTO networks (name, description, state) VALUES (?, ?, ?)", name, description, networkCreated)
		if err != nil {
			return err
		}

		id, err := result.LastInsertId()
		if err != nil {
			return err
		}

		// Insert a node-specific entry pointing to ourselves.
		columns := []string{"network_id", "node_id"}
		values := []interface{}{id, c.nodeID}
		_, err = query.UpsertObject(tx.tx, "networks_nodes", columns, values)
		if err != nil {
			return err
		}

		err = networkConfigAdd(tx.tx, id, c.nodeID, config)
		if err != nil {
			return err
		}

		return nil
	})
	if err != nil {
		id = -1
	}
	return id, err
}
func (c *Cluster) NetworkUpdate(name, description string, config map[string]string) error {
	id, _, err := c.NetworkGet(name)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		err = NetworkUpdateDescription(tx.tx, id, description)
		if err != nil {
			return err
		}

		err = NetworkConfigClear(tx.tx, id, c.nodeID)
		if err != nil {
			return err
		}

		err = networkConfigAdd(tx.tx, id, c.nodeID, config)
		if err != nil {
			return err
		}
		return nil
	})

	return err
}
func NetworkUpdateDescription(tx *sql.Tx, id int64, description string) error {
	_, err := tx.Exec("UPDATE networks SET description=? WHERE id=?", description, id)
	return err
}
func NetworkConfigClear(tx *sql.Tx, networkID, nodeID int64) error {
	_, err := tx.Exec(
		"DELETE FROM networks_config WHERE network_id=? AND (node_id=? OR node_id IS NULL)",
		networkID, nodeID)
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) NetworkDelete(name string) error {
	id, _, err := c.NetworkGet(name)
	if err != nil {
		return err
	}

	err = exec(c.db, "DELETE FROM networks WHERE id=?", id)
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) NetworkRename(oldName string, newName string) error {
	id, _, err := c.NetworkGet(oldName)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		_, err = tx.tx.Exec("UPDATE networks SET name=? WHERE id=?", newName, id)
		return err
	})

	return err
}
func (r *ProtocolLXD) GetContainers() ([]api.Container, error) {
	containers := []api.Container{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/containers?recursion=1", nil, "", &containers)
	if err != nil {
		return nil, err
	}

	return containers, nil
}
func (r *ProtocolLXD) GetContainersFull() ([]api.ContainerFull, error) {
	containers := []api.ContainerFull{}

	if !r.HasExtension("container_full") {
		return nil, fmt.Errorf("The server is missing the required \"container_full\" API extension")
	}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/containers?recursion=2", nil, "", &containers)
	if err != nil {
		return nil, err
	}

	return containers, nil
}
func (r *ProtocolLXD) GetContainer(name string) (*api.Container, string, error) {
	container := api.Container{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s", url.QueryEscape(name)), nil, "", &container)
	if err != nil {
		return nil, "", err
	}

	return &container, etag, nil
}
func (r *ProtocolLXD) CreateContainerFromBackup(args ContainerBackupArgs) (Operation, error) {
	if !r.HasExtension("container_backup") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	if args.PoolName == "" {
		// Send the request
		op, _, err := r.queryOperation("POST", "/containers", args.BackupFile, "")
		if err != nil {
			return nil, err
		}

		return op, nil
	}

	if !r.HasExtension("container_backup_override_pool") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup_override_pool\" API extension")
	}

	// Prepare the HTTP request
	reqURL, err := r.setQueryAttributes(fmt.Sprintf("%s/1.0/containers", r.httpHost))
	if err != nil {
		return nil, err
	}

	req, err := http.NewRequest("POST", reqURL, args.BackupFile)
	if err != nil {
		return nil, err
	}

	req.Header.Set("Content-Type", "application/octet-stream")
	req.Header.Set("X-LXD-pool", args.PoolName)

	// Set the user agent
	if r.httpUserAgent != "" {
		req.Header.Set("User-Agent", r.httpUserAgent)
	}

	// Send the request
	resp, err := r.do(req)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	// Handle errors
	response, _, err := lxdParseResponse(resp)
	if err != nil {
		return nil, err
	}

	// Get to the operation
	respOperation, err := response.MetadataAsOperation()
	if err != nil {
		return nil, err
	}

	// Setup an Operation wrapper
	op := operation{
		Operation: *respOperation,
		r:         r,
		chActive:  make(chan bool),
	}

	return &op, nil
}
func (r *ProtocolLXD) CreateContainer(container api.ContainersPost) (Operation, error) {
	if container.Source.ContainerOnly {
		if !r.HasExtension("container_only_migration") {
			return nil, fmt.Errorf("The server is missing the required \"container_only_migration\" API extension")
		}
	}

	// Send the request
	op, _, err := r.queryOperation("POST", "/containers", container, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) CreateContainerFromImage(source ImageServer, image api.Image, req api.ContainersPost) (RemoteOperation, error) {
	// Set the minimal source fields
	req.Source.Type = "image"

	// Optimization for the local image case
	if r == source {
		// Always use fingerprints for local case
		req.Source.Fingerprint = image.Fingerprint
		req.Source.Alias = ""

		op, err := r.CreateContainer(req)
		if err != nil {
			return nil, err
		}

		rop := remoteOperation{
			targetOp: op,
			chDone:   make(chan bool),
		}

		// Forward targetOp to remote op
		go func() {
			rop.err = rop.targetOp.Wait()
			close(rop.chDone)
		}()

		return &rop, nil
	}

	// Minimal source fields for remote image
	req.Source.Mode = "pull"

	// If we have an alias and the image is public, use that
	if req.Source.Alias != "" && image.Public {
		req.Source.Fingerprint = ""
	} else {
		req.Source.Fingerprint = image.Fingerprint
		req.Source.Alias = ""
	}

	// Get source server connection information
	info, err := source.GetConnectionInfo()
	if err != nil {
		return nil, err
	}

	req.Source.Protocol = info.Protocol
	req.Source.Certificate = info.Certificate

	// Generate secret token if needed
	if !image.Public {
		secret, err := source.GetImageSecret(image.Fingerprint)
		if err != nil {
			return nil, err
		}

		req.Source.Secret = secret
	}

	return r.tryCreateContainer(req, info.Addresses)
}
func (r *ProtocolLXD) UpdateContainer(name string, container api.ContainerPut, ETag string) (Operation, error) {
	// Send the request
	op, _, err := r.queryOperation("PUT", fmt.Sprintf("/containers/%s", url.QueryEscape(name)), container, ETag)
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) RenameContainer(name string, container api.ContainerPost) (Operation, error) {
	// Sanity check
	if container.Migration {
		return nil, fmt.Errorf("Can't ask for a migration through RenameContainer")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s", url.QueryEscape(name)), container, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) ExecContainer(containerName string, exec api.ContainerExecPost, args *ContainerExecArgs) (Operation, error) {
	if exec.RecordOutput {
		if !r.HasExtension("container_exec_recording") {
			return nil, fmt.Errorf("The server is missing the required \"container_exec_recording\" API extension")
		}
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s/exec", url.QueryEscape(containerName)), exec, "")
	if err != nil {
		return nil, err
	}
	opAPI := op.Get()

	// Process additional arguments
	if args != nil {
		// Parse the fds
		fds := map[string]string{}

		value, ok := opAPI.Metadata["fds"]
		if ok {
			values := value.(map[string]interface{})
			for k, v := range values {
				fds[k] = v.(string)
			}
		}

		// Call the control handler with a connection to the control socket
		if args.Control != nil && fds["control"] != "" {
			conn, err := r.GetOperationWebsocket(opAPI.ID, fds["control"])
			if err != nil {
				return nil, err
			}

			go args.Control(conn)
		}

		if exec.Interactive {
			// Handle interactive sections
			if args.Stdin != nil && args.Stdout != nil {
				// Connect to the websocket
				conn, err := r.GetOperationWebsocket(opAPI.ID, fds["0"])
				if err != nil {
					return nil, err
				}

				// And attach stdin and stdout to it
				go func() {
					shared.WebsocketSendStream(conn, args.Stdin, -1)
					<-shared.WebsocketRecvStream(args.Stdout, conn)
					conn.Close()

					if args.DataDone != nil {
						close(args.DataDone)
					}
				}()
			} else {
				if args.DataDone != nil {
					close(args.DataDone)
				}
			}
		} else {
			// Handle non-interactive sessions
			dones := map[int]chan bool{}
			conns := []*websocket.Conn{}

			// Handle stdin
			if fds["0"] != "" {
				conn, err := r.GetOperationWebsocket(opAPI.ID, fds["0"])
				if err != nil {
					return nil, err
				}

				conns = append(conns, conn)
				dones[0] = shared.WebsocketSendStream(conn, args.Stdin, -1)
			}

			// Handle stdout
			if fds["1"] != "" {
				conn, err := r.GetOperationWebsocket(opAPI.ID, fds["1"])
				if err != nil {
					return nil, err
				}

				conns = append(conns, conn)
				dones[1] = shared.WebsocketRecvStream(args.Stdout, conn)
			}

			// Handle stderr
			if fds["2"] != "" {
				conn, err := r.GetOperationWebsocket(opAPI.ID, fds["2"])
				if err != nil {
					return nil, err
				}

				conns = append(conns, conn)
				dones[2] = shared.WebsocketRecvStream(args.Stderr, conn)
			}

			// Wait for everything to be done
			go func() {
				for i, chDone := range dones {
					// Skip stdin, dealing with it separately below
					if i == 0 {
						continue
					}

					<-chDone
				}

				if fds["0"] != "" {
					if args.Stdin != nil {
						args.Stdin.Close()
					}

					// Empty the stdin channel but don't block on it as
					// stdin may be stuck in Read()
					go func() {
						<-dones[0]
					}()
				}

				for _, conn := range conns {
					conn.Close()
				}

				if args.DataDone != nil {
					close(args.DataDone)
				}
			}()
		}
	}

	return op, nil
}
func (r *ProtocolLXD) GetContainerFile(containerName string, path string) (io.ReadCloser, *ContainerFileResponse, error) {
	// Prepare the HTTP request
	requestURL, err := shared.URLEncode(
		fmt.Sprintf("%s/1.0/containers/%s/files", r.httpHost, url.QueryEscape(containerName)),
		map[string]string{"path": path})
	if err != nil {
		return nil, nil, err
	}

	requestURL, err = r.setQueryAttributes(requestURL)
	if err != nil {
		return nil, nil, err
	}

	req, err := http.NewRequest("GET", requestURL, nil)
	if err != nil {
		return nil, nil, err
	}

	// Set the user agent
	if r.httpUserAgent != "" {
		req.Header.Set("User-Agent", r.httpUserAgent)
	}

	// Send the request
	resp, err := r.do(req)
	if err != nil {
		return nil, nil, err
	}

	// Check the return value for a cleaner error
	if resp.StatusCode != http.StatusOK {
		_, _, err := lxdParseResponse(resp)
		if err != nil {
			return nil, nil, err
		}
	}

	// Parse the headers
	uid, gid, mode, fileType, _ := shared.ParseLXDFileHeaders(resp.Header)
	fileResp := ContainerFileResponse{
		UID:  uid,
		GID:  gid,
		Mode: mode,
		Type: fileType,
	}

	if fileResp.Type == "directory" {
		// Decode the response
		response := api.Response{}
		decoder := json.NewDecoder(resp.Body)

		err = decoder.Decode(&response)
		if err != nil {
			return nil, nil, err
		}

		// Get the file list
		entries := []string{}
		err = response.MetadataAsStruct(&entries)
		if err != nil {
			return nil, nil, err
		}

		fileResp.Entries = entries

		return nil, &fileResp, err
	}

	return resp.Body, &fileResp, err
}
func (r *ProtocolLXD) CreateContainerFile(containerName string, path string, args ContainerFileArgs) error {
	if args.Type == "directory" {
		if !r.HasExtension("directory_manipulation") {
			return fmt.Errorf("The server is missing the required \"directory_manipulation\" API extension")
		}
	}

	if args.Type == "symlink" {
		if !r.HasExtension("file_symlinks") {
			return fmt.Errorf("The server is missing the required \"file_symlinks\" API extension")
		}
	}

	if args.WriteMode == "append" {
		if !r.HasExtension("file_append") {
			return fmt.Errorf("The server is missing the required \"file_append\" API extension")
		}
	}

	// Prepare the HTTP request
	requestURL := fmt.Sprintf("%s/1.0/containers/%s/files?path=%s", r.httpHost, url.QueryEscape(containerName), url.QueryEscape(path))

	requestURL, err := r.setQueryAttributes(requestURL)
	if err != nil {
		return err
	}

	req, err := http.NewRequest("POST", requestURL, args.Content)
	if err != nil {
		return err
	}

	// Set the user agent
	if r.httpUserAgent != "" {
		req.Header.Set("User-Agent", r.httpUserAgent)
	}

	// Set the various headers
	if args.UID > -1 {
		req.Header.Set("X-LXD-uid", fmt.Sprintf("%d", args.UID))
	}

	if args.GID > -1 {
		req.Header.Set("X-LXD-gid", fmt.Sprintf("%d", args.GID))
	}

	if args.Mode > -1 {
		req.Header.Set("X-LXD-mode", fmt.Sprintf("%04o", args.Mode))
	}

	if args.Type != "" {
		req.Header.Set("X-LXD-type", args.Type)
	}

	if args.WriteMode != "" {
		req.Header.Set("X-LXD-write", args.WriteMode)
	}

	// Send the request
	resp, err := r.do(req)
	if err != nil {
		return err
	}

	// Check the return value for a cleaner error
	_, _, err = lxdParseResponse(resp)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) DeleteContainerFile(containerName string, path string) error {
	if !r.HasExtension("file_delete") {
		return fmt.Errorf("The server is missing the required \"file_delete\" API extension")
	}

	// Send the request
	_, _, err := r.query("DELETE", fmt.Sprintf("/containers/%s/files?path=%s", url.QueryEscape(containerName), url.QueryEscape(path)), nil, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) GetContainerSnapshotNames(containerName string) ([]string, error) {
	urls := []string{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/snapshots", url.QueryEscape(containerName)), nil, "", &urls)
	if err != nil {
		return nil, err
	}

	// Parse it
	names := []string{}
	for _, uri := range urls {
		fields := strings.Split(uri, fmt.Sprintf("/containers/%s/snapshots/", url.QueryEscape(containerName)))
		names = append(names, fields[len(fields)-1])
	}

	return names, nil
}
func (r *ProtocolLXD) GetContainerSnapshots(containerName string) ([]api.ContainerSnapshot, error) {
	snapshots := []api.ContainerSnapshot{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/snapshots?recursion=1", url.QueryEscape(containerName)), nil, "", &snapshots)
	if err != nil {
		return nil, err
	}

	return snapshots, nil
}
func (r *ProtocolLXD) GetContainerSnapshot(containerName string, name string) (*api.ContainerSnapshot, string, error) {
	snapshot := api.ContainerSnapshot{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/snapshots/%s", url.QueryEscape(containerName), url.QueryEscape(name)), nil, "", &snapshot)
	if err != nil {
		return nil, "", err
	}

	return &snapshot, etag, nil
}
func (r *ProtocolLXD) CreateContainerSnapshot(containerName string, snapshot api.ContainerSnapshotsPost) (Operation, error) {
	// Validate the request
	if snapshot.ExpiresAt != nil && !r.HasExtension("snapshot_expiry_creation") {
		return nil, fmt.Errorf("The server is missing the required \"snapshot_expiry_creation\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s/snapshots", url.QueryEscape(containerName)), snapshot, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) MigrateContainerSnapshot(containerName string, name string, container api.ContainerSnapshotPost) (Operation, error) {
	// Sanity check
	if !container.Migration {
		return nil, fmt.Errorf("Can't ask for a rename through MigrateContainerSnapshot")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s/snapshots/%s", url.QueryEscape(containerName), url.QueryEscape(name)), container, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) UpdateContainerSnapshot(containerName string, name string, container api.ContainerSnapshotPut, ETag string) (Operation, error) {
	if !r.HasExtension("snapshot_expiry") {
		return nil, fmt.Errorf("The server is missing the required \"snapshot_expiry\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("PUT", fmt.Sprintf("/containers/%s/snapshots/%s",
		url.QueryEscape(containerName), url.QueryEscape(name)), container, ETag)
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) GetContainerState(name string) (*api.ContainerState, string, error) {
	state := api.ContainerState{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/state", url.QueryEscape(name)), nil, "", &state)
	if err != nil {
		return nil, "", err
	}

	return &state, etag, nil
}
func (r *ProtocolLXD) UpdateContainerState(name string, state api.ContainerStatePut, ETag string) (Operation, error) {
	// Send the request
	op, _, err := r.queryOperation("PUT", fmt.Sprintf("/containers/%s/state", url.QueryEscape(name)), state, ETag)
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) GetContainerLogfiles(name string) ([]string, error) {
	urls := []string{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/logs", url.QueryEscape(name)), nil, "", &urls)
	if err != nil {
		return nil, err
	}

	// Parse it
	logfiles := []string{}
	for _, uri := range logfiles {
		fields := strings.Split(uri, fmt.Sprintf("/containers/%s/logs/", url.QueryEscape(name)))
		logfiles = append(logfiles, fields[len(fields)-1])
	}

	return logfiles, nil
}
func (r *ProtocolLXD) GetContainerLogfile(name string, filename string) (io.ReadCloser, error) {
	// Prepare the HTTP request
	url := fmt.Sprintf("%s/1.0/containers/%s/logs/%s", r.httpHost, url.QueryEscape(name), url.QueryEscape(filename))

	url, err := r.setQueryAttributes(url)
	if err != nil {
		return nil, err
	}

	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return nil, err
	}

	// Set the user agent
	if r.httpUserAgent != "" {
		req.Header.Set("User-Agent", r.httpUserAgent)
	}

	// Send the request
	resp, err := r.do(req)
	if err != nil {
		return nil, err
	}

	// Check the return value for a cleaner error
	if resp.StatusCode != http.StatusOK {
		_, _, err := lxdParseResponse(resp)
		if err != nil {
			return nil, err
		}
	}

	return resp.Body, err
}
func (r *ProtocolLXD) GetContainerMetadata(name string) (*api.ImageMetadata, string, error) {
	if !r.HasExtension("container_edit_metadata") {
		return nil, "", fmt.Errorf("The server is missing the required \"container_edit_metadata\" API extension")
	}

	metadata := api.ImageMetadata{}

	url := fmt.Sprintf("/containers/%s/metadata", url.QueryEscape(name))
	etag, err := r.queryStruct("GET", url, nil, "", &metadata)
	if err != nil {
		return nil, "", err
	}

	return &metadata, etag, err
}
func (r *ProtocolLXD) SetContainerMetadata(name string, metadata api.ImageMetadata, ETag string) error {
	if !r.HasExtension("container_edit_metadata") {
		return fmt.Errorf("The server is missing the required \"container_edit_metadata\" API extension")
	}

	url := fmt.Sprintf("/containers/%s/metadata", url.QueryEscape(name))
	_, _, err := r.query("PUT", url, metadata, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) GetContainerTemplateFiles(containerName string) ([]string, error) {
	if !r.HasExtension("container_edit_metadata") {
		return nil, fmt.Errorf("The server is missing the required \"container_edit_metadata\" API extension")
	}

	templates := []string{}

	url := fmt.Sprintf("/containers/%s/metadata/templates", url.QueryEscape(containerName))
	_, err := r.queryStruct("GET", url, nil, "", &templates)
	if err != nil {
		return nil, err
	}

	return templates, nil
}
func (r *ProtocolLXD) CreateContainerTemplateFile(containerName string, templateName string, content io.ReadSeeker) error {
	return r.setContainerTemplateFile(containerName, templateName, content, "POST")
}
func (r *ProtocolLXD) DeleteContainerTemplateFile(name string, templateName string) error {
	if !r.HasExtension("container_edit_metadata") {
		return fmt.Errorf("The server is missing the required \"container_edit_metadata\" API extension")
	}
	_, _, err := r.query("DELETE", fmt.Sprintf("/containers/%s/metadata/templates?path=%s", url.QueryEscape(name), url.QueryEscape(templateName)), nil, "")
	return err
}
func (r *ProtocolLXD) ConsoleContainer(containerName string, console api.ContainerConsolePost, args *ContainerConsoleArgs) (Operation, error) {
	if !r.HasExtension("console") {
		return nil, fmt.Errorf("The server is missing the required \"console\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s/console", url.QueryEscape(containerName)), console, "")
	if err != nil {
		return nil, err
	}
	opAPI := op.Get()

	if args == nil || args.Terminal == nil {
		return nil, fmt.Errorf("A terminal must be set")
	}

	if args.Control == nil {
		return nil, fmt.Errorf("A control channel must be set")
	}

	// Parse the fds
	fds := map[string]string{}

	value, ok := opAPI.Metadata["fds"]
	if ok {
		values := value.(map[string]interface{})
		for k, v := range values {
			fds[k] = v.(string)
		}
	}

	var controlConn *websocket.Conn
	// Call the control handler with a connection to the control socket
	if fds["control"] == "" {
		return nil, fmt.Errorf("Did not receive a file descriptor for the control channel")
	}

	controlConn, err = r.GetOperationWebsocket(opAPI.ID, fds["control"])
	if err != nil {
		return nil, err
	}

	go args.Control(controlConn)

	// Connect to the websocket
	conn, err := r.GetOperationWebsocket(opAPI.ID, fds["0"])
	if err != nil {
		return nil, err
	}

	// Detach from console.
	go func(consoleDisconnect <-chan bool) {
		<-consoleDisconnect
		msg := websocket.FormatCloseMessage(websocket.CloseNormalClosure, "Detaching from console")
		// We don't care if this fails. This is just for convenience.
		controlConn.WriteMessage(websocket.CloseMessage, msg)
		controlConn.Close()
	}(args.ConsoleDisconnect)

	// And attach stdin and stdout to it
	go func() {
		shared.WebsocketSendStream(conn, args.Terminal, -1)
		<-shared.WebsocketRecvStream(args.Terminal, conn)
		conn.Close()
	}()

	return op, nil
}
func (r *ProtocolLXD) GetContainerConsoleLog(containerName string, args *ContainerConsoleLogArgs) (io.ReadCloser, error) {
	if !r.HasExtension("console") {
		return nil, fmt.Errorf("The server is missing the required \"console\" API extension")
	}

	// Prepare the HTTP request
	url := fmt.Sprintf("%s/1.0/containers/%s/console", r.httpHost, url.QueryEscape(containerName))

	url, err := r.setQueryAttributes(url)
	if err != nil {
		return nil, err
	}

	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return nil, err
	}

	// Set the user agent
	if r.httpUserAgent != "" {
		req.Header.Set("User-Agent", r.httpUserAgent)
	}

	// Send the request
	resp, err := r.do(req)
	if err != nil {
		return nil, err
	}

	// Check the return value for a cleaner error
	if resp.StatusCode != http.StatusOK {
		_, _, err := lxdParseResponse(resp)
		if err != nil {
			return nil, err
		}
	}

	return resp.Body, err
}
func (r *ProtocolLXD) DeleteContainerConsoleLog(containerName string, args *ContainerConsoleLogArgs) error {
	if !r.HasExtension("console") {
		return fmt.Errorf("The server is missing the required \"console\" API extension")
	}

	// Send the request
	_, _, err := r.query("DELETE", fmt.Sprintf("/containers/%s/console", url.QueryEscape(containerName)), nil, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) GetContainerBackups(containerName string) ([]api.ContainerBackup, error) {
	if !r.HasExtension("container_backup") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	// Fetch the raw value
	backups := []api.ContainerBackup{}

	_, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/backups?recursion=1", url.QueryEscape(containerName)), nil, "", &backups)
	if err != nil {
		return nil, err
	}

	return backups, nil
}
func (r *ProtocolLXD) GetContainerBackup(containerName string, name string) (*api.ContainerBackup, string, error) {
	if !r.HasExtension("container_backup") {
		return nil, "", fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	// Fetch the raw value
	backup := api.ContainerBackup{}
	etag, err := r.queryStruct("GET", fmt.Sprintf("/containers/%s/backups/%s", url.QueryEscape(containerName), url.QueryEscape(name)), nil, "", &backup)
	if err != nil {
		return nil, "", err
	}

	return &backup, etag, nil
}
func (r *ProtocolLXD) CreateContainerBackup(containerName string, backup api.ContainerBackupsPost) (Operation, error) {
	if !r.HasExtension("container_backup") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s/backups",
		url.QueryEscape(containerName)), backup, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) RenameContainerBackup(containerName string, name string, backup api.ContainerBackupPost) (Operation, error) {
	if !r.HasExtension("container_backup") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/containers/%s/backups/%s",
		url.QueryEscape(containerName), url.QueryEscape(name)), backup, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) DeleteContainerBackup(containerName string, name string) (Operation, error) {
	if !r.HasExtension("container_backup") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("DELETE", fmt.Sprintf("/containers/%s/backups/%s",
		url.QueryEscape(containerName), url.QueryEscape(name)), nil, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) GetContainerBackupFile(containerName string, name string, req *BackupFileRequest) (*BackupFileResponse, error) {
	if !r.HasExtension("container_backup") {
		return nil, fmt.Errorf("The server is missing the required \"container_backup\" API extension")
	}

	// Build the URL
	uri := fmt.Sprintf("%s/1.0/containers/%s/backups/%s/export", r.httpHost,
		url.QueryEscape(containerName), url.QueryEscape(name))
	if r.project != "" {
		uri += fmt.Sprintf("?project=%s", url.QueryEscape(r.project))
	}

	// Prepare the download request
	request, err := http.NewRequest("GET", uri, nil)
	if err != nil {
		return nil, err
	}

	if r.httpUserAgent != "" {
		request.Header.Set("User-Agent", r.httpUserAgent)
	}

	// Start the request
	response, doneCh, err := cancel.CancelableDownload(req.Canceler, r.http, request)
	if err != nil {
		return nil, err
	}
	defer response.Body.Close()
	defer close(doneCh)

	if response.StatusCode != http.StatusOK {
		_, _, err := lxdParseResponse(response)
		if err != nil {
			return nil, err
		}
	}

	// Handle the data
	body := response.Body
	if req.ProgressHandler != nil {
		body = &ioprogress.ProgressReader{
			ReadCloser: response.Body,
			Tracker: &ioprogress.ProgressTracker{
				Length: response.ContentLength,
				Handler: func(percent int64, speed int64) {
					req.ProgressHandler(ioprogress.ProgressData{Text: fmt.Sprintf("%d%% (%s/s)", percent, shared.GetByteSizeString(speed, 2))})
				},
			},
		}
	}

	size, err := io.Copy(req.BackupFile, body)
	if err != nil {
		return nil, err
	}

	resp := BackupFileResponse{}
	resp.Size = size

	return &resp, nil
}
func RsyncSend(name string, path string, conn *websocket.Conn, readWrapper func(io.ReadCloser) io.ReadCloser, features []string, bwlimit string, execPath string) error {
	cmd, dataSocket, stderr, err := rsyncSendSetup(name, path, bwlimit, execPath, features)
	if err != nil {
		return err
	}

	if dataSocket != nil {
		defer dataSocket.Close()
	}

	readPipe := io.ReadCloser(dataSocket)
	if readWrapper != nil {
		readPipe = readWrapper(dataSocket)
	}

	readDone, writeDone := shared.WebsocketMirror(conn, dataSocket, readPipe, nil, nil)

	chError := make(chan error, 1)
	go func() {
		err = cmd.Wait()
		if err != nil {
			dataSocket.Close()
			readPipe.Close()
		}
		chError <- err
	}()

	output, err := ioutil.ReadAll(stderr)
	if err != nil {
		cmd.Process.Kill()
	}

	err = <-chError
	if err != nil {
		logger.Errorf("Rsync send failed: %s: %s: %s", path, err, string(output))
	}

	<-readDone
	<-writeDone

	return err
}
func patchesGetNames() []string {
	names := make([]string, len(patches))
	for i, patch := range patches {
		names[i] = patch.name
	}
	return names
}
func patchRenameCustomVolumeLVs(name string, d *Daemon) error {
	// Ignore the error since it will also fail if there are no pools.
	pools, _ := d.cluster.StoragePools()

	for _, poolName := range pools {
		poolID, pool, err := d.cluster.StoragePoolGet(poolName)
		if err != nil {
			return err
		}

		sType, err := storageStringToType(pool.Driver)
		if err != nil {
			return err
		}

		if sType != storageTypeLvm {
			continue
		}

		volumes, err := d.cluster.StoragePoolNodeVolumesGetType(storagePoolVolumeTypeCustom, poolID)
		if err != nil {
			return err
		}

		vgName := poolName
		if pool.Config["lvm.vg_name"] != "" {
			vgName = pool.Config["lvm.vg_name"]
		}

		for _, volume := range volumes {
			oldName := fmt.Sprintf("%s/custom_%s", vgName, volume)
			newName := fmt.Sprintf("%s/custom_%s", vgName, containerNameToLVName(volume))

			exists, err := storageLVExists(newName)
			if err != nil {
				return err
			}

			if exists || oldName == newName {
				continue
			}

			err = lvmLVRename(vgName, oldName, newName)
			if err != nil {
				return err
			}

			logger.Info("Successfully renamed LV", log.Ctx{"old_name": oldName, "new_name": newName})
		}
	}

	return nil
}
func patchLvmNodeSpecificConfigKeys(name string, d *Daemon) error {
	tx, err := d.cluster.Begin()
	if err != nil {
		return errors.Wrap(err, "failed to begin transaction")
	}

	// Fetch the IDs of all existing nodes.
	nodeIDs, err := query.SelectIntegers(tx, "SELECT id FROM nodes")
	if err != nil {
		return errors.Wrap(err, "failed to get IDs of current nodes")
	}

	// Fetch the IDs of all existing lvm pools.
	poolIDs, err := query.SelectIntegers(tx, "SELECT id FROM storage_pools WHERE driver='lvm'")
	if err != nil {
		return errors.Wrap(err, "failed to get IDs of current lvm pools")
	}

	for _, poolID := range poolIDs {
		// Fetch the config for this lvm pool and check if it has the
		// lvn.thinpool_name or lvm.vg_name keys.
		config, err := query.SelectConfig(
			tx, "storage_pools_config", "storage_pool_id=? AND node_id IS NULL", poolID)
		if err != nil {
			return errors.Wrap(err, "failed to fetch of lvm pool config")
		}

		for _, key := range []string{"lvm.thinpool_name", "lvm.vg_name"} {
			value, ok := config[key]
			if !ok {
				continue
			}

			// Delete the current key
			_, err = tx.Exec(`
DELETE FROM storage_pools_config WHERE key=? AND storage_pool_id=? AND node_id IS NULL
`, key, poolID)
			if err != nil {
				return errors.Wrapf(err, "failed to delete %s config", key)
			}

			// Add the config entry for each node
			for _, nodeID := range nodeIDs {
				_, err := tx.Exec(`
INSERT INTO storage_pools_config(storage_pool_id, node_id, key, value)
  VALUES(?, ?, ?, ?)
`, poolID, nodeID, key, value)
				if err != nil {
					return errors.Wrapf(err, "failed to create %s node config", key)
				}
			}
		}
	}

	err = tx.Commit()
	if err != nil {
		return errors.Wrap(err, "failed to commit transaction")
	}

	return err
}
func (r *ProtocolLXD) GetHTTPClient() (*http.Client, error) {
	if r.http == nil {
		return nil, fmt.Errorf("HTTP client isn't set, bad connection")
	}

	return r.http, nil
}
func (r *ProtocolLXD) do(req *http.Request) (*http.Response, error) {
	if r.bakeryClient != nil {
		r.addMacaroonHeaders(req)
		return r.bakeryClient.Do(req)
	}

	return r.http.Do(req)
}
func (r *ProtocolLXD) RawQuery(method string, path string, data interface{}, ETag string) (*api.Response, string, error) {
	// Generate the URL
	url := fmt.Sprintf("%s%s", r.httpHost, path)

	return r.rawQuery(method, url, data, ETag)
}
func (r *ProtocolLXD) RawWebsocket(path string) (*websocket.Conn, error) {
	return r.websocket(path)
}
func (r *ProtocolLXD) RawOperation(method string, path string, data interface{}, ETag string) (Operation, string, error) {
	return r.queryOperation(method, path, data, ETag)
}
func ProfileToAPI(profile *Profile) *api.Profile {
	p := &api.Profile{
		Name:   profile.Name,
		UsedBy: profile.UsedBy,
	}
	p.Description = profile.Description
	p.Config = profile.Config
	p.Devices = profile.Devices

	return p
}
func (c *Cluster) Profiles(project string) ([]string, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has profiles")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	q := fmt.Sprintf(`
SELECT profiles.name
 FROM profiles
 JOIN projects ON projects.id = profiles.project_id
WHERE projects.name = ?
`)
	inargs := []interface{}{project}
	var name string
	outfmt := []interface{}{name}
	result, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return []string{}, err
	}

	response := []string{}
	for _, r := range result {
		response = append(response, r[0].(string))
	}

	return response, nil
}
func (c *Cluster) ProfileGet(project, name string) (int64, *api.Profile, error) {
	var result *api.Profile
	var id int64

	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has profiles")
		}
		if !enabled {
			project = "default"
		}

		profile, err := tx.ProfileGet(project, name)
		if err != nil {
			return err
		}

		result = ProfileToAPI(profile)
		id = int64(profile.ID)

		return nil
	})
	if err != nil {
		return -1, nil, err
	}

	return id, result, nil
}
func (c *Cluster) ProfilesGet(project string, names []string) ([]api.Profile, error) {
	profiles := make([]api.Profile, len(names))

	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has profiles")
		}
		if !enabled {
			project = "default"
		}

		for i, name := range names {
			profile, err := tx.ProfileGet(project, name)
			if err != nil {
				return errors.Wrapf(err, "Load profile %q", name)
			}
			profiles[i] = *ProfileToAPI(profile)
		}

		return nil
	})
	if err != nil {
		return nil, err
	}

	return profiles, nil
}
func (c *Cluster) ProfileConfig(project, name string) (map[string]string, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has profiles")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	var key, value string
	query := `
        SELECT
            key, value
        FROM profiles_config
        JOIN profiles ON profiles_config.profile_id=profiles.id
        JOIN projects ON projects.id = profiles.project_id
        WHERE projects.name=? AND profiles.name=?`
	inargs := []interface{}{project, name}
	outfmt := []interface{}{key, value}
	results, err := queryScan(c.db, query, inargs, outfmt)
	if err != nil {
		return nil, errors.Wrapf(err, "Failed to get profile '%s'", name)
	}

	if len(results) == 0 {
		/*
		 * If we didn't get any rows here, let's check to make sure the
		 * profile really exists; if it doesn't, let's send back a 404.
		 */
		query := "SELECT id FROM profiles WHERE name=?"
		var id int
		results, err := queryScan(c.db, query, []interface{}{name}, []interface{}{id})
		if err != nil {
			return nil, err
		}

		if len(results) == 0 {
			return nil, ErrNoSuchObject
		}
	}

	config := map[string]string{}

	for _, r := range results {
		key = r[0].(string)
		value = r[1].(string)

		config[key] = value
	}

	return config, nil
}
func ProfileConfigClear(tx *sql.Tx, id int64) error {
	_, err := tx.Exec("DELETE FROM profiles_config WHERE profile_id=?", id)
	if err != nil {
		return err
	}

	_, err = tx.Exec(`DELETE FROM profiles_devices_config WHERE id IN
		(SELECT profiles_devices_config.id
		 FROM profiles_devices_config JOIN profiles_devices
		 ON profiles_devices_config.profile_device_id=profiles_devices.id
		 WHERE profiles_devices.profile_id=?)`, id)
	if err != nil {
		return err
	}
	_, err = tx.Exec("DELETE FROM profiles_devices WHERE profile_id=?", id)
	if err != nil {
		return err
	}
	return nil
}
func ProfileConfigAdd(tx *sql.Tx, id int64, config map[string]string) error {
	str := fmt.Sprintf("INSERT INTO profiles_config (profile_id, key, value) VALUES(?, ?, ?)")
	stmt, err := tx.Prepare(str)
	defer stmt.Close()
	if err != nil {
		return err
	}

	for k, v := range config {
		if v == "" {
			continue
		}

		_, err = stmt.Exec(id, k, v)
		if err != nil {
			return err
		}
	}

	return nil
}
func (c *Cluster) ProfileContainersGet(project, profile string) (map[string][]string, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check if project has profiles")
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	q := `SELECT containers.name, projects.name FROM containers
		JOIN containers_profiles ON containers.id == containers_profiles.container_id
		JOIN projects ON projects.id == containers.project_id
		WHERE containers_profiles.profile_id ==
		  (SELECT profiles.id FROM profiles
		   JOIN projects ON projects.id == profiles.project_id
		   WHERE profiles.name=? AND projects.name=?)
		AND containers.type == 0`

	results := map[string][]string{}
	inargs := []interface{}{profile, project}
	var name string
	outfmt := []interface{}{name, name}

	output, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return nil, err
	}

	for _, r := range output {
		if results[r[1].(string)] == nil {
			results[r[1].(string)] = []string{}
		}

		results[r[1].(string)] = append(results[r[1].(string)], r[0].(string))
	}

	return results, nil
}
func (c *Cluster) ProfileCleanupLeftover() error {
	stmt := `
DELETE FROM profiles_config WHERE profile_id NOT IN (SELECT id FROM profiles);
DELETE FROM profiles_devices WHERE profile_id NOT IN (SELECT id FROM profiles);
DELETE FROM profiles_devices_config WHERE profile_device_id NOT IN (SELECT id FROM profiles_devices);
`
	err := exec(c.db, stmt)
	if err != nil {
		return err
	}

	return nil
}
func ProfilesExpandConfig(config map[string]string, profiles []api.Profile) map[string]string {
	expandedConfig := map[string]string{}

	// Apply all the profiles
	profileConfigs := make([]map[string]string, len(profiles))
	for i, profile := range profiles {
		profileConfigs[i] = profile.Config
	}

	for i := range profileConfigs {
		for k, v := range profileConfigs[i] {
			expandedConfig[k] = v
		}
	}

	// Stick the given config on top
	for k, v := range config {
		expandedConfig[k] = v
	}

	return expandedConfig
}
func ProfilesExpandDevices(devices types.Devices, profiles []api.Profile) types.Devices {
	expandedDevices := types.Devices{}

	// Apply all the profiles
	profileDevices := make([]types.Devices, len(profiles))
	for i, profile := range profiles {
		profileDevices[i] = profile.Devices
	}
	for i := range profileDevices {
		for k, v := range profileDevices[i] {
			expandedDevices[k] = v
		}
	}

	// Stick the given devices on top
	for k, v := range devices {
		expandedDevices[k] = v
	}

	return expandedDevices
}
func (r *ProtocolLXD) GetServer() (*api.Server, string, error) {
	server := api.Server{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", "", nil, "", &server)
	if err != nil {
		return nil, "", err
	}

	// Fill in certificate fingerprint if not provided
	if server.Environment.CertificateFingerprint == "" && server.Environment.Certificate != "" {
		var err error
		server.Environment.CertificateFingerprint, err = shared.CertFingerprintStr(server.Environment.Certificate)
		if err != nil {
			return nil, "", err
		}
	}

	if !server.Public && len(server.AuthMethods) == 0 {
		// TLS is always available for LXD servers
		server.AuthMethods = []string{"tls"}
	}

	// Add the value to the cache
	r.server = &server

	return &server, etag, nil
}
func (r *ProtocolLXD) UpdateServer(server api.ServerPut, ETag string) error {
	// Send the request
	_, _, err := r.query("PUT", "", server, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) HasExtension(extension string) bool {
	// If no cached API information, just assume we're good
	// This is needed for those rare cases where we must avoid a GetServer call
	if r.server == nil {
		return true
	}

	for _, entry := range r.server.APIExtensions {
		if entry == extension {
			return true
		}
	}

	return false
}
func (r *ProtocolLXD) GetServerResources() (*api.Resources, error) {
	if !r.HasExtension("resources") {
		return nil, fmt.Errorf("The server is missing the required \"resources\" API extension")
	}

	resources := api.Resources{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/resources", nil, "", &resources)
	if err != nil {
		return nil, err
	}

	return &resources, nil
}
func (r *ProtocolLXD) UseProject(name string) ContainerServer {
	return &ProtocolLXD{
		server:               r.server,
		http:                 r.http,
		httpCertificate:      r.httpCertificate,
		httpHost:             r.httpHost,
		httpProtocol:         r.httpProtocol,
		httpUserAgent:        r.httpUserAgent,
		bakeryClient:         r.bakeryClient,
		bakeryInteractor:     r.bakeryInteractor,
		requireAuthenticated: r.requireAuthenticated,
		clusterTarget:        r.clusterTarget,
		project:              name,
	}
}
func sqliteOpen(path string) (*sql.DB, error) {
	timeout := 5 // TODO - make this command-line configurable?

	// These are used to tune the transaction BEGIN behavior instead of using the
	// similar "locking_mode" pragma (locking for the whole database connection).
	openPath := fmt.Sprintf("%s?_busy_timeout=%d&_txlock=exclusive", path, timeout*1000)

	// Open the database. If the file doesn't exist it is created.
	return sql.Open("sqlite3_with_fk", openPath)
}
func Rebalance(state *state.State, gateway *Gateway) (string, []db.RaftNode, error) {
	// First get the current raft members, since this method should be
	// called after a node has left.
	currentRaftNodes, err := gateway.currentRaftNodes()
	if err != nil {
		return "", nil, errors.Wrap(err, "failed to get current raft nodes")
	}
	if len(currentRaftNodes) >= membershipMaxRaftNodes {
		// We're already at full capacity.
		return "", nil, nil
	}

	currentRaftAddresses := make([]string, len(currentRaftNodes))
	for i, node := range currentRaftNodes {
		currentRaftAddresses[i] = node.Address
	}

	// Check if we have a spare node that we can turn into a database one.
	address := ""
	err = state.Cluster.Transaction(func(tx *db.ClusterTx) error {
		config, err := ConfigLoad(tx)
		if err != nil {
			return errors.Wrap(err, "failed load cluster configuration")
		}
		nodes, err := tx.Nodes()
		if err != nil {
			return errors.Wrap(err, "failed to get cluster nodes")
		}
		// Find a node that is not part of the raft cluster yet.
		for _, node := range nodes {
			if shared.StringInSlice(node.Address, currentRaftAddresses) {
				continue // This is already a database node
			}
			if node.IsOffline(config.OfflineThreshold()) {
				continue // This node is offline
			}
			logger.Debugf(
				"Found spare node %s (%s) to be promoted as database node", node.Name, node.Address)
			address = node.Address
			break
		}

		return nil
	})
	if err != nil {
		return "", nil, err
	}

	if address == "" {
		// No node to promote
		return "", nil, nil
	}

	// Update the local raft_table adding the new member and building a new
	// list.
	updatedRaftNodes := currentRaftNodes
	err = gateway.db.Transaction(func(tx *db.NodeTx) error {
		id, err := tx.RaftNodeAdd(address)
		if err != nil {
			return errors.Wrap(err, "failed to add new raft node")
		}
		updatedRaftNodes = append(updatedRaftNodes, db.RaftNode{ID: id, Address: address})
		err = tx.RaftNodesReplace(updatedRaftNodes)
		if err != nil {
			return errors.Wrap(err, "failed to update raft nodes")
		}
		return nil
	})
	if err != nil {
		return "", nil, err
	}
	return address, updatedRaftNodes, nil
}
func Promote(state *state.State, gateway *Gateway, nodes []db.RaftNode) error {
	logger.Info("Promote node to database node")

	// Sanity check that this is not already a database node
	if gateway.IsDatabaseNode() {
		return fmt.Errorf("this node is already a database node")
	}

	// Figure out our own address.
	address := ""
	err := state.Cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		address, err = tx.NodeAddress()
		if err != nil {
			return errors.Wrap(err, "failed to fetch the address of this node")
		}
		return nil
	})
	if err != nil {
		return err
	}

	// Sanity check that we actually have an address.
	if address == "" {
		return fmt.Errorf("node is not exposed on the network")
	}

	// Figure out our raft node ID, and an existing target raft node that
	// we'll contact to add ourselves as member.
	id := ""
	target := ""
	for _, node := range nodes {
		if node.Address == address {
			id = strconv.Itoa(int(node.ID))
		} else {
			target = node.Address
		}
	}

	// Sanity check that our address was actually included in the given
	// list of raft nodes.
	if id == "" {
		return fmt.Errorf("this node is not included in the given list of database nodes")
	}

	// Replace our local list of raft nodes with the given one (which
	// includes ourselves). This will make the gateway start a raft node
	// when restarted.
	err = state.Node.Transaction(func(tx *db.NodeTx) error {
		err = tx.RaftNodesReplace(nodes)
		if err != nil {
			return errors.Wrap(err, "failed to set raft nodes")
		}

		return nil
	})
	if err != nil {
		return err
	}

	// Lock regular access to the cluster database since we don't want any
	// other database code to run while we're reconfiguring raft.
	err = state.Cluster.EnterExclusive()
	if err != nil {
		return errors.Wrap(err, "failed to acquire cluster database lock")
	}

	// Wipe all existing raft data, for good measure (perhaps they were
	// somehow leftover).
	err = os.RemoveAll(state.OS.GlobalDatabaseDir())
	if err != nil {
		return errors.Wrap(err, "failed to remove existing raft data")
	}

	// Re-initialize the gateway. This will create a new raft factory an
	// dqlite driver instance, which will be exposed over gRPC by the
	// gateway handlers.
	err = gateway.init()
	if err != nil {
		return errors.Wrap(err, "failed to re-initialize gRPC SQL gateway")
	}

	logger.Info(
		"Joining dqlite raft cluster",
		log15.Ctx{"id": id, "address": address, "target": target})
	changer := gateway.raft.MembershipChanger()
	err = changer.Join(raft.ServerID(id), raft.ServerAddress(target), 5*time.Second)
	if err != nil {
		return err
	}

	// Unlock regular access to our cluster database, and make sure our
	// gateway still works correctly.
	err = state.Cluster.ExitExclusive(func(tx *db.ClusterTx) error {
		_, err := tx.Nodes()
		return err
	})
	if err != nil {
		return errors.Wrap(err, "cluster database initialization failed")
	}
	return nil
}
func Purge(cluster *db.Cluster, name string) error {
	logger.Debugf("Remove node %s from the database", name)

	return cluster.Transaction(func(tx *db.ClusterTx) error {
		// Get the node (if it doesn't exists an error is returned).
		node, err := tx.NodeByName(name)
		if err != nil {
			return errors.Wrapf(err, "failed to get node %s", name)
		}

		err = tx.NodeClear(node.ID)
		if err != nil {
			return errors.Wrapf(err, "failed to clear node %s", name)
		}

		err = tx.NodeRemove(node.ID)
		if err != nil {
			return errors.Wrapf(err, "failed to remove node %s", name)
		}
		return nil
	})
}
func List(state *state.State) ([]api.ClusterMember, error) {
	addresses := []string{} // Addresses of database nodes
	err := state.Node.Transaction(func(tx *db.NodeTx) error {
		nodes, err := tx.RaftNodes()
		if err != nil {
			return errors.Wrap(err, "failed to fetch current raft nodes")
		}
		for _, node := range nodes {
			addresses = append(addresses, node.Address)
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	var nodes []db.NodeInfo
	var offlineThreshold time.Duration

	err = state.Cluster.Transaction(func(tx *db.ClusterTx) error {
		nodes, err = tx.Nodes()
		if err != nil {
			return err
		}
		offlineThreshold, err = tx.NodeOfflineThreshold()
		if err != nil {
			return err
		}

		return nil
	})
	if err != nil {
		return nil, err
	}

	result := make([]api.ClusterMember, len(nodes))
	now := time.Now()
	version := nodes[0].Version()
	for i, node := range nodes {
		result[i].ServerName = node.Name
		result[i].URL = fmt.Sprintf("https://%s", node.Address)
		result[i].Database = shared.StringInSlice(node.Address, addresses)
		if node.IsOffline(offlineThreshold) {
			result[i].Status = "Offline"
			result[i].Message = fmt.Sprintf(
				"no heartbeat since %s", now.Sub(node.Heartbeat))
		} else {
			result[i].Status = "Online"
			result[i].Message = "fully operational"
		}

		n, err := util.CompareVersions(version, node.Version())
		if err != nil {
			result[i].Status = "Broken"
			result[i].Message = "inconsistent version"
			continue
		}

		if n == 1 {
			// This node's version is lower, which means the
			// version that the previous node in the loop has been
			// upgraded.
			version = node.Version()
		}
	}

	// Update the state of online nodes that have been upgraded and whose
	// schema is more recent than the rest of the nodes.
	for i, node := range nodes {
		if result[i].Status != "Online" {
			continue
		}
		n, err := util.CompareVersions(version, node.Version())
		if err != nil {
			continue
		}
		if n == 2 {
			result[i].Status = "Blocked"
			result[i].Message = "waiting for other nodes to be upgraded"
		}
	}

	return result, nil
}
func Count(state *state.State) (int, error) {
	var count int
	err := state.Cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		count, err = tx.NodesCount()
		return err
	})
	return count, err
}
func Enabled(node *db.Node) (bool, error) {
	enabled := false
	err := node.Transaction(func(tx *db.NodeTx) error {
		addresses, err := tx.RaftNodeAddresses()
		if err != nil {
			return err
		}
		enabled = len(addresses) > 0
		return nil
	})
	return enabled, err
}
func membershipCheckNodeStateForBootstrapOrJoin(tx *db.NodeTx, address string) error {
	nodes, err := tx.RaftNodes()
	if err != nil {
		return errors.Wrap(err, "failed to fetch current raft nodes")
	}

	hasClusterAddress := address != ""
	hasRaftNodes := len(nodes) > 0

	// Sanity check that we're not in an inconsistent situation, where no
	// cluster address is set, but still there are entries in the
	// raft_nodes table.
	if !hasClusterAddress && hasRaftNodes {
		return fmt.Errorf("inconsistent state: found leftover entries in raft_nodes")
	}

	if !hasClusterAddress {
		return fmt.Errorf("no cluster.https_address config is set on this node")
	}
	if hasRaftNodes {
		return fmt.Errorf("the node is already part of a cluster")
	}

	return nil
}
func membershipCheckClusterStateForBootstrapOrJoin(tx *db.ClusterTx) error {
	nodes, err := tx.Nodes()
	if err != nil {
		return errors.Wrap(err, "failed to fetch current cluster nodes")
	}
	if len(nodes) != 1 {
		return fmt.Errorf("inconsistent state: found leftover entries in nodes")
	}
	return nil
}
func membershipCheckClusterStateForAccept(tx *db.ClusterTx, name string, address string, schema int, api int) error {
	nodes, err := tx.Nodes()
	if err != nil {
		return errors.Wrap(err, "failed to fetch current cluster nodes")
	}
	if len(nodes) == 1 && nodes[0].Address == "0.0.0.0" {
		return fmt.Errorf("clustering not enabled")
	}

	for _, node := range nodes {
		if node.Name == name {
			return fmt.Errorf("cluster already has node with name %s", name)
		}
		if node.Address == address {
			return fmt.Errorf("cluster already has node with address %s", address)
		}
		if node.Schema != schema {
			return fmt.Errorf("schema version mismatch: cluster has %d", node.Schema)
		}
		if node.APIExtensions != api {
			return fmt.Errorf("API version mismatch: cluster has %d", node.APIExtensions)
		}
	}

	return nil
}
func membershipCheckClusterStateForLeave(tx *db.ClusterTx, nodeID int64) error {
	// Check that it has no containers or images.
	message, err := tx.NodeIsEmpty(nodeID)
	if err != nil {
		return err
	}
	if message != "" {
		return fmt.Errorf(message)
	}

	// Check that it's not the last node.
	nodes, err := tx.Nodes()
	if err != nil {
		return err
	}
	if len(nodes) == 1 {
		return fmt.Errorf("node is the only node in the cluster")
	}
	return nil
}
func membershipCheckNoLeftoverClusterCert(dir string) error {
	// Sanity check that there's no leftover cluster certificate
	for _, basename := range []string{"cluster.crt", "cluster.key", "cluster.ca"} {
		if shared.PathExists(filepath.Join(dir, basename)) {
			return fmt.Errorf("inconsistent state: found leftover cluster certificate")
		}
	}
	return nil
}
func ConfigLoad(tx *db.NodeTx) (*Config, error) {
	// Load current raw values from the database, any error is fatal.
	values, err := tx.Config()
	if err != nil {
		return nil, fmt.Errorf("cannot fetch node config from database: %v", err)
	}

	m, err := config.SafeLoad(ConfigSchema, values)
	if err != nil {
		return nil, fmt.Errorf("failed to load node config: %v", err)
	}

	return &Config{tx: tx, m: m}, nil
}
func (c *Config) Replace(values map[string]interface{}) (map[string]string, error) {
	return c.update(values)
}
func (c *Config) Patch(patch map[string]interface{}) (map[string]string, error) {
	values := c.Dump() // Use current values as defaults
	for name, value := range patch {
		values[name] = value
	}

	return c.update(values)
}
func HTTPSAddress(node *db.Node) (string, error) {
	var config *Config
	err := node.Transaction(func(tx *db.NodeTx) error {
		var err error
		config, err = ConfigLoad(tx)
		return err
	})
	if err != nil {
		return "", err
	}

	return config.HTTPSAddress(), nil
}
func (c *Cluster) CertificatesGet() (certs []*CertInfo, err error) {
	err = c.Transaction(func(tx *ClusterTx) error {
		rows, err := tx.tx.Query(
			"SELECT id, fingerprint, type, name, certificate FROM certificates",
		)
		if err != nil {
			return err
		}

		defer rows.Close()

		for rows.Next() {
			cert := new(CertInfo)
			rows.Scan(
				&cert.ID,
				&cert.Fingerprint,
				&cert.Type,
				&cert.Name,
				&cert.Certificate,
			)
			certs = append(certs, cert)
		}

		return rows.Err()
	})
	if err != nil {
		return certs, err
	}

	return certs, nil
}
func (c *Cluster) CertificateGet(fingerprint string) (cert *CertInfo, err error) {
	cert = new(CertInfo)

	inargs := []interface{}{fingerprint + "%"}
	outfmt := []interface{}{
		&cert.ID,
		&cert.Fingerprint,
		&cert.Type,
		&cert.Name,
		&cert.Certificate,
	}

	query := `
		SELECT
			id, fingerprint, type, name, certificate
		FROM
			certificates
		WHERE fingerprint LIKE ?`

	if err = dbQueryRowScan(c.db, query, inargs, outfmt); err != nil {
		if err == sql.ErrNoRows {
			return nil, ErrNoSuchObject
		}

		return nil, err
	}

	return cert, err
}
func (c *Cluster) CertSave(cert *CertInfo) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		stmt, err := tx.tx.Prepare(`
			INSERT INTO certificates (
				fingerprint,
				type,
				name,
				certificate
			) VALUES (?, ?, ?, ?)`,
		)
		if err != nil {
			return err
		}
		defer stmt.Close()
		_, err = stmt.Exec(
			cert.Fingerprint,
			cert.Type,
			cert.Name,
			cert.Certificate,
		)
		if err != nil {
			return err
		}
		return nil
	})
	return err
}
func (c *Cluster) CertDelete(fingerprint string) error {
	err := exec(c.db, "DELETE FROM certificates WHERE fingerprint=?", fingerprint)
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) CertUpdate(fingerprint string, certName string, certType int) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		_, err := tx.tx.Exec("UPDATE certificates SET name=?, type=? WHERE fingerprint=?", certName, certType, fingerprint)
		return err
	})
	return err
}
func createDevLxdlListener(dir string) (net.Listener, error) {
	path := filepath.Join(dir, "devlxd", "sock")

	// If this socket exists, that means a previous LXD instance died and
	// didn't clean up. We assume that such LXD instance is actually dead
	// if we get this far, since localCreateListener() tries to connect to
	// the actual lxd socket to make sure that it is actually dead. So, it
	// is safe to remove it here without any checks.
	//
	// Also, it would be nice to SO_REUSEADDR here so we don't have to
	// delete the socket, but we can't:
	//   http://stackoverflow.com/questions/15716302/so-reuseaddr-and-af-unix
	//
	// Note that this will force clients to reconnect when LXD is restarted.
	err := socketUnixRemoveStale(path)
	if err != nil {
		return nil, err
	}

	listener, err := socketUnixListen(path)
	if err != nil {
		return nil, err
	}

	err = socketUnixSetPermissions(path, 0666)
	if err != nil {
		listener.Close()
		return nil, err
	}

	return listener, nil
}
func (i *raftInstance) Servers() ([]raft.Server, error) {
	if i.raft.State() != raft.Leader {
		return nil, raft.ErrNotLeader
	}
	future := i.raft.GetConfiguration()
	err := future.Error()
	if err != nil {
		return nil, err
	}
	configuration := future.Configuration()
	return configuration.Servers, nil
}
func (i *raftInstance) Shutdown() error {
	logger.Debug("Stop raft instance")

	// Invoke raft APIs asynchronously to allow for a timeout.
	timeout := 10 * time.Second

	errCh := make(chan error)
	timer := time.After(timeout)
	go func() {
		errCh <- i.raft.Shutdown().Error()
	}()
	select {
	case err := <-errCh:
		if err != nil {
			return errors.Wrap(err, "failed to shutdown raft")
		}
	case <-timer:
		logger.Debug("Timeout waiting for raft to shutdown")
		return fmt.Errorf("raft did not shutdown within %s", timeout)

	}
	err := i.logs.Close()
	if err != nil {
		return errors.Wrap(err, "failed to close boltdb logs store")
	}
	return nil
}
func raftNetworkTransport(
	db *db.Node,
	address string,
	logger *log.Logger,
	timeout time.Duration,
	dial rafthttp.Dial) (raft.Transport, *rafthttp.Handler, *rafthttp.Layer, error) {
	handler := rafthttp.NewHandlerWithLogger(logger)
	addr, err := net.ResolveTCPAddr("tcp", address)
	if err != nil {
		return nil, nil, nil, errors.Wrap(err, "invalid node address")
	}

	layer := rafthttp.NewLayer(raftEndpoint, addr, handler, dial)
	config := &raft.NetworkTransportConfig{
		Logger:                logger,
		Stream:                layer,
		MaxPool:               2,
		Timeout:               timeout,
		ServerAddressProvider: &raftAddressProvider{db: db},
	}
	transport := raft.NewNetworkTransportWithConfig(config)

	return transport, handler, layer, nil
}
func raftConfig(latency float64) *raft.Config {
	config := raft.DefaultConfig()
	scale := func(duration *time.Duration) {
		*duration = time.Duration((math.Ceil(float64(*duration) * latency)))
	}
	durations := []*time.Duration{
		&config.HeartbeatTimeout,
		&config.ElectionTimeout,
		&config.CommitTimeout,
		&config.LeaderLeaseTimeout,
	}
	for _, duration := range durations {
		scale(duration)
	}

	config.SnapshotThreshold = 1024
	config.TrailingLogs = 512

	return config
}
func raftMaybeBootstrap(
	conf *raft.Config,
	logs *raftboltdb.BoltStore,
	snaps raft.SnapshotStore,
	trans raft.Transport) error {
	// First check if we were already bootstrapped.
	hasExistingState, err := raft.HasExistingState(logs, logs, snaps)
	if err != nil {
		return errors.Wrap(err, "failed to check if raft has existing state")
	}
	if hasExistingState {
		return nil
	}
	server := raft.Server{
		ID:      conf.LocalID,
		Address: trans.LocalAddr(),
	}
	configuration := raft.Configuration{
		Servers: []raft.Server{server},
	}
	return raft.BootstrapCluster(conf, logs, logs, snaps, trans, configuration)
}
func CPUResource() (*api.ResourcesCPU, error) {
	c := api.ResourcesCPU{}

	threads, err := getThreads()
	if err != nil {
		return nil, err
	}

	var cur *api.ResourcesCPUSocket
	c.Total = uint64(len(threads))

	for _, v := range threads {
		if uint64(len(c.Sockets)) <= v.socketID {
			c.Sockets = append(c.Sockets, api.ResourcesCPUSocket{})
			cur = &c.Sockets[v.socketID]

			// Count the number of cores on the socket
			// Note that we can't assume sequential core IDs
			socketCores := map[uint64]bool{}
			for _, thread := range threads {
				if thread.socketID != v.socketID {
					continue
				}

				socketCores[thread.coreID] = true
			}
			cur.Cores = uint64(len(socketCores))
		} else {
			cur = &c.Sockets[v.socketID]
		}

		cur.Socket = v.socketID
		cur.NUMANode = v.numaNode
		cur.Threads++
		cur.Name = v.name
		cur.Vendor = v.vendor
		cur.Frequency = v.frequency
		cur.FrequencyTurbo = v.frequencyTurbo
	}

	return &c, nil
}
func MemoryResource() (*api.ResourcesMemory, error) {
	var buffers uint64
	var cached uint64
	var free uint64
	var total uint64

	f, err := os.Open("/proc/meminfo")
	if err != nil {
		return nil, err
	}
	defer f.Close()

	cleanLine := func(l string) (string, error) {
		l = strings.TrimSpace(l)
		idx := strings.LastIndex(l, "kB")
		if idx < 0 {
			return "", fmt.Errorf(`Failed to detect "kB" suffix`)
		}

		return strings.TrimSpace(l[:idx]), nil
	}

	mem := api.ResourcesMemory{}
	scanner := bufio.NewScanner(f)
	found := 0
	for scanner.Scan() {
		var err error
		line := scanner.Text()

		if strings.HasPrefix(line, "MemTotal:") {
			line, err = cleanLine(line[len("MemTotal:"):])
			if err != nil {
				return nil, err
			}

			total, err = strconv.ParseUint(line, 10, 64)
			if err != nil {
				return nil, err
			}

			found++
		} else if strings.HasPrefix(line, "MemFree:") {
			line, err = cleanLine(line[len("MemFree:"):])
			if err != nil {
				return nil, err
			}

			free, err = strconv.ParseUint(line, 10, 64)
			if err != nil {
				return nil, err
			}

			found++
		} else if strings.HasPrefix(line, "Cached:") {
			line, err = cleanLine(line[len("Cached:"):])
			if err != nil {
				return nil, err
			}

			cached, err = strconv.ParseUint(line, 10, 64)
			if err != nil {
				return nil, err
			}

			found++
		} else if strings.HasPrefix(line, "Buffers:") {
			line, err = cleanLine(line[len("Buffers:"):])
			if err != nil {
				return nil, err
			}

			buffers, err = strconv.ParseUint(line, 10, 64)
			if err != nil {
				return nil, err
			}

			found++
		}

		if found == 4 {
			break
		}
	}

	mem.Total = total * 1024
	mem.Used = (total - free - cached - buffers) * 1024

	return &mem, err
}
func (r *ProtocolLXD) GetOperationUUIDs() ([]string, error) {
	urls := []string{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/operations", nil, "", &urls)
	if err != nil {
		return nil, err
	}

	// Parse it
	uuids := []string{}
	for _, url := range urls {
		fields := strings.Split(url, "/operations/")
		uuids = append(uuids, fields[len(fields)-1])
	}

	return uuids, nil
}
func (r *ProtocolLXD) GetOperations() ([]api.Operation, error) {
	apiOperations := map[string][]api.Operation{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/operations?recursion=1", nil, "", &apiOperations)
	if err != nil {
		return nil, err
	}

	// Turn it into just a list of operations
	operations := []api.Operation{}
	for _, v := range apiOperations {
		for _, operation := range v {
			operations = append(operations, operation)
		}
	}

	return operations, nil
}
func (r *ProtocolLXD) GetOperation(uuid string) (*api.Operation, string, error) {
	op := api.Operation{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/operations/%s", url.QueryEscape(uuid)), nil, "", &op)
	if err != nil {
		return nil, "", err
	}

	return &op, etag, nil
}
func (r *ProtocolLXD) GetOperationWebsocket(uuid string, secret string) (*websocket.Conn, error) {
	path := fmt.Sprintf("/operations/%s/websocket", url.QueryEscape(uuid))
	if secret != "" {
		path = fmt.Sprintf("%s?secret=%s", path, url.QueryEscape(secret))
	}

	return r.websocket(path)
}
func tryMount(src string, dst string, fs string, flags uintptr, options string) error {
	var err error

	for i := 0; i < 20; i++ {
		err = syscall.Mount(src, dst, fs, flags, options)
		if err == nil {
			break
		}

		time.Sleep(500 * time.Millisecond)
	}

	if err != nil {
		return err
	}

	return nil
}
func lxdUsesPool(dbObj *db.Cluster, onDiskPoolName string, driver string, onDiskProperty string) (bool, string, error) {
	pools, err := dbObj.StoragePools()
	if err != nil && err != db.ErrNoSuchObject {
		return false, "", err
	}

	for _, pool := range pools {
		_, pl, err := dbObj.StoragePoolGet(pool)
		if err != nil {
			continue
		}

		if pl.Driver != driver {
			continue
		}

		if pl.Config[onDiskProperty] == onDiskPoolName {
			return true, pl.Name, nil
		}
	}

	return false, "", nil
}
func (c *ClusterTx) ProjectURIs(filter ProjectFilter) ([]string, error) {
	// Check which filter criteria are active.
	criteria := map[string]interface{}{}
	if filter.Name != "" {
		criteria["Name"] = filter.Name
	}

	// Pick the prepared statement and arguments to use based on active criteria.
	var stmt *sql.Stmt
	var args []interface{}

	if criteria["Name"] != nil {
		stmt = c.stmt(projectNamesByName)
		args = []interface{}{
			filter.Name,
		}
	} else {
		stmt = c.stmt(projectNames)
		args = []interface{}{}
	}

	code := cluster.EntityTypes["project"]
	formatter := cluster.EntityFormatURIs[code]

	return query.SelectURIs(stmt, formatter, args...)
}
func (c *ClusterTx) ProjectList(filter ProjectFilter) ([]api.Project, error) {
	// Result slice.
	objects := make([]api.Project, 0)

	// Check which filter criteria are active.
	criteria := map[string]interface{}{}
	if filter.Name != "" {
		criteria["Name"] = filter.Name
	}

	// Pick the prepared statement and arguments to use based on active criteria.
	var stmt *sql.Stmt
	var args []interface{}

	if criteria["Name"] != nil {
		stmt = c.stmt(projectObjectsByName)
		args = []interface{}{
			filter.Name,
		}
	} else {
		stmt = c.stmt(projectObjects)
		args = []interface{}{}
	}

	// Dest function for scanning a row.
	dest := func(i int) []interface{} {
		objects = append(objects, api.Project{})
		return []interface{}{
			&objects[i].Description,
			&objects[i].Name,
		}
	}

	// Select.
	err := query.SelectObjects(stmt, dest, args...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch projects")
	}

	// Fill field Config.
	configObjects, err := c.ProjectConfigRef(filter)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch field Config")
	}

	for i := range objects {
		value := configObjects[objects[i].Name]
		if value == nil {
			value = map[string]string{}
		}
		objects[i].Config = value
	}

	// Fill field UsedBy.
	usedByObjects, err := c.ProjectUsedByRef(filter)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch field UsedBy")
	}

	for i := range objects {
		value := usedByObjects[objects[i].Name]
		if value == nil {
			value = []string{}
		}
		objects[i].UsedBy = value
	}

	return objects, nil
}
func (c *ClusterTx) ProjectGet(name string) (*api.Project, error) {
	filter := ProjectFilter{}
	filter.Name = name

	objects, err := c.ProjectList(filter)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch Project")
	}

	switch len(objects) {
	case 0:
		return nil, ErrNoSuchObject
	case 1:
		return &objects[0], nil
	default:
		return nil, fmt.Errorf("More than one project matches")
	}
}
func (c *ClusterTx) ProjectExists(name string) (bool, error) {
	_, err := c.ProjectID(name)
	if err != nil {
		if err == ErrNoSuchObject {
			return false, nil
		}
		return false, err
	}

	return true, nil
}
func (c *ClusterTx) ProjectCreate(object api.ProjectsPost) (int64, error) {
	// Check if a project with the same key exists.
	exists, err := c.ProjectExists(object.Name)
	if err != nil {
		return -1, errors.Wrap(err, "Failed to check for duplicates")
	}
	if exists {
		return -1, fmt.Errorf("This project already exists")
	}

	args := make([]interface{}, 2)

	// Populate the statement arguments.
	args[0] = object.Description
	args[1] = object.Name

	// Prepared statement to use.
	stmt := c.stmt(projectCreate)

	// Execute the statement.
	result, err := stmt.Exec(args...)
	if err != nil {
		return -1, errors.Wrap(err, "Failed to create project")
	}

	id, err := result.LastInsertId()
	if err != nil {
		return -1, errors.Wrap(err, "Failed to fetch project ID")
	}

	// Insert config reference.
	stmt = c.stmt(projectCreateConfigRef)
	for key, value := range object.Config {
		_, err := stmt.Exec(id, key, value)
		if err != nil {
			return -1, errors.Wrap(err, "Insert config for project")
		}
	}

	return id, nil
}
func (c *ClusterTx) ProjectUsedByRef(filter ProjectFilter) (map[string][]string, error) {
	// Result slice.
	objects := make([]struct {
		Name  string
		Value string
	}, 0)

	// Check which filter criteria are active.
	criteria := map[string]interface{}{}
	if filter.Name != "" {
		criteria["Name"] = filter.Name
	}

	// Pick the prepared statement and arguments to use based on active criteria.
	var stmt *sql.Stmt
	var args []interface{}

	if criteria["Name"] != nil {
		stmt = c.stmt(projectUsedByRefByName)
		args = []interface{}{
			filter.Name,
		}
	} else {
		stmt = c.stmt(projectUsedByRef)
		args = []interface{}{}
	}

	// Dest function for scanning a row.
	dest := func(i int) []interface{} {
		objects = append(objects, struct {
			Name  string
			Value string
		}{})
		return []interface{}{
			&objects[i].Name,
			&objects[i].Value,
		}
	}

	// Select.
	err := query.SelectObjects(stmt, dest, args...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch string ref for projects")
	}

	// Build index by primary name.
	index := map[string][]string{}

	for _, object := range objects {
		item, ok := index[object.Name]
		if !ok {
			item = []string{}
		}

		index[object.Name] = append(item, object.Value)
	}

	return index, nil
}
func (c *ClusterTx) ProjectRename(name string, to string) error {
	stmt := c.stmt(projectRename)
	result, err := stmt.Exec(to, name)
	if err != nil {
		return errors.Wrap(err, "Rename project")
	}

	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "Fetch affected rows")
	}
	if n != 1 {
		return fmt.Errorf("Query affected %d rows instead of 1", n)
	}
	return nil
}
func (c *ClusterTx) ProjectDelete(name string) error {
	stmt := c.stmt(projectDelete)
	result, err := stmt.Exec(name)
	if err != nil {
		return errors.Wrap(err, "Delete project")
	}

	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "Fetch affected rows")
	}
	if n != 1 {
		return fmt.Errorf("Query deleted %d rows instead of 1", n)
	}

	return nil
}
func PasswordCheck(secret string, password string) error {
	// No password set
	if secret == "" {
		return fmt.Errorf("No password is set")
	}

	// Compare the password
	buff, err := hex.DecodeString(secret)
	if err != nil {
		return err
	}

	salt := buff[0:32]
	hash, err := scrypt.Key([]byte(password), salt, 1<<14, 8, 1, 64)
	if err != nil {
		return err
	}

	if !bytes.Equal(hash, buff[32:]) {
		return fmt.Errorf("Bad password provided")
	}

	return nil
}
func LoadCert(dir string) (*shared.CertInfo, error) {
	prefix := "server"
	if shared.PathExists(filepath.Join(dir, "cluster.crt")) {
		prefix = "cluster"
	}
	cert, err := shared.KeyPairAndCA(dir, prefix, shared.CertServer)
	if err != nil {
		return nil, errors.Wrap(err, "failed to load TLS certificate")
	}
	return cert, nil
}
func WriteCert(dir, prefix string, cert, key, ca []byte) error {
	err := ioutil.WriteFile(filepath.Join(dir, prefix+".crt"), cert, 0644)
	if err != nil {
		return err
	}

	err = ioutil.WriteFile(filepath.Join(dir, prefix+".key"), key, 0600)
	if err != nil {
		return err
	}

	if ca != nil {
		err = ioutil.WriteFile(filepath.Join(dir, prefix+".ca"), ca, 0644)
		if err != nil {
			return err
		}
	}

	return nil
}
func NewDaemon(config *DaemonConfig, os *sys.OS) *Daemon {
	return &Daemon{
		config:       config,
		os:           os,
		setupChan:    make(chan struct{}),
		readyChan:    make(chan struct{}),
		shutdownChan: make(chan struct{}),
	}
}
func DefaultDaemon() *Daemon {
	config := DefaultDaemonConfig()
	os := sys.DefaultOS()
	return NewDaemon(config, os)
}
func AllowProjectPermission(feature string, permission string) func(d *Daemon, r *http.Request) Response {
	return func(d *Daemon, r *http.Request) Response {
		// Shortcut for speed
		if d.userIsAdmin(r) {
			return EmptySyncResponse
		}

		// Get the project
		project := projectParam(r)

		// Validate whether the user has the needed permission
		if !d.userHasPermission(r, project, permission) {
			return Forbidden(nil)
		}

		return EmptySyncResponse
	}
}
func (d *Daemon) checkTrustedClient(r *http.Request) error {
	trusted, _, _, err := d.Authenticate(r)
	if !trusted || err != nil {
		if err != nil {
			return err
		}

		return fmt.Errorf("Not authorized")
	}

	return nil
}
func (d *Daemon) Authenticate(r *http.Request) (bool, string, string, error) {
	// Allow internal cluster traffic
	if r.TLS != nil {
		cert, _ := x509.ParseCertificate(d.endpoints.NetworkCert().KeyPair().Certificate[0])
		clusterCerts := map[string]x509.Certificate{"0": *cert}
		for i := range r.TLS.PeerCertificates {
			trusted, _ := util.CheckTrustState(*r.TLS.PeerCertificates[i], clusterCerts)
			if trusted {
				return true, "", "cluster", nil
			}
		}
	}

	// Local unix socket queries
	if r.RemoteAddr == "@" {
		return true, "", "unix", nil
	}

	// Devlxd unix socket credentials on main API
	if r.RemoteAddr == "@devlxd" {
		return false, "", "", fmt.Errorf("Main API query can't come from /dev/lxd socket")
	}

	// Cluster notification with wrong certificate
	if isClusterNotification(r) {
		return false, "", "", fmt.Errorf("Cluster notification isn't using cluster certificate")
	}

	// Bad query, no TLS found
	if r.TLS == nil {
		return false, "", "", fmt.Errorf("Bad/missing TLS on network query")
	}

	if d.externalAuth != nil && r.Header.Get(httpbakery.BakeryProtocolHeader) != "" {
		// Validate external authentication
		ctx := httpbakery.ContextWithRequest(context.TODO(), r)
		authChecker := d.externalAuth.bakery.Checker.Auth(httpbakery.RequestMacaroons(r)...)

		ops := []bakery.Op{{
			Entity: r.URL.Path,
			Action: r.Method,
		}}

		info, err := authChecker.Allow(ctx, ops...)
		if err != nil {
			// Bad macaroon
			return false, "", "", err
		}

		if info != nil && info.Identity != nil {
			// Valid identity macaroon found
			return true, info.Identity.Id(), "candid", nil
		}

		// Valid macaroon with no identity information
		return true, "", "candid", nil
	}

	// Validate normal TLS access
	for i := range r.TLS.PeerCertificates {
		trusted, username := util.CheckTrustState(*r.TLS.PeerCertificates[i], d.clientCerts)
		if trusted {
			return true, username, "tls", nil
		}
	}

	// Reject unauthorized
	return false, "", "", nil
}
func (d *Daemon) State() *state.State {
	return state.NewState(d.db, d.cluster, d.maas, d.os, d.endpoints)
}
func (d *Daemon) UnixSocket() string {
	path := os.Getenv("LXD_SOCKET")
	if path != "" {
		return path
	}

	return filepath.Join(d.os.VarDir, "unix.socket")
}
func (d *Daemon) Stop() error {
	logger.Info("Starting shutdown sequence")
	errs := []error{}
	trackError := func(err error) {
		if err != nil {
			errs = append(errs, err)
		}
	}

	if d.endpoints != nil {
		trackError(d.endpoints.Down())
	}

	trackError(d.tasks.Stop(3 * time.Second))        // Give tasks a bit of time to cleanup.
	trackError(d.clusterTasks.Stop(3 * time.Second)) // Give tasks a bit of time to cleanup.

	shouldUnmount := false
	if d.cluster != nil {
		// It might be that database nodes are all down, in that case
		// we don't want to wait too much.
		//
		// FIXME: it should be possible to provide a context or a
		//        timeout for database queries.
		ch := make(chan bool)
		go func() {
			n, err := d.numRunningContainers()
			ch <- err != nil || n == 0
		}()
		select {
		case shouldUnmount = <-ch:
		case <-time.After(2 * time.Second):
			shouldUnmount = true
		}

		logger.Infof("Closing the database")
		err := d.cluster.Close()
		// If we got io.EOF the network connection was interrupted and
		// it's likely that the other node shutdown. Let's just log the
		// event and return cleanly.
		if errors.Cause(err) == driver.ErrBadConn {
			logger.Debugf("Could not close remote database cleanly: %v", err)
		} else {
			trackError(err)
		}
	}
	if d.db != nil {
		trackError(d.db.Close())
	}

	if d.gateway != nil {
		trackError(d.gateway.Shutdown())
	}
	if d.endpoints != nil {
		trackError(d.endpoints.Down())
	}

	if d.endpoints != nil {
		trackError(d.endpoints.Down())
	}

	if shouldUnmount {
		logger.Infof("Unmounting temporary filesystems")

		syscall.Unmount(shared.VarPath("devlxd"), syscall.MNT_DETACH)
		syscall.Unmount(shared.VarPath("shmounts"), syscall.MNT_DETACH)

		logger.Infof("Done unmounting temporary filesystems")
	} else {
		logger.Debugf(
			"Not unmounting temporary filesystems (containers are still running)")
	}

	var err error
	if n := len(errs); n > 0 {
		format := "%v"
		if n > 1 {
			format += fmt.Sprintf(" (and %d more errors)", n)
		}
		err = fmt.Errorf(format, errs[0])
	}
	if err != nil {
		logger.Errorf("Failed to cleanly shutdown daemon: %v", err)
	}
	return err
}
func (d *Daemon) setupExternalAuthentication(authEndpoint string, authPubkey string, expiry int64, domains string) error {
	// Parse the list of domains
	authDomains := []string{}
	for _, domain := range strings.Split(domains, ",") {
		if domain == "" {
			continue
		}

		authDomains = append(authDomains, strings.TrimSpace(domain))
	}

	// Allow disable external authentication
	if authEndpoint == "" {
		d.externalAuth = nil
		return nil
	}

	// Setup the candid client
	idmClient, err := candidclient.New(candidclient.NewParams{
		BaseURL: authEndpoint,
	})
	if err != nil {
		return err
	}

	idmClientWrapper := &IdentityClientWrapper{
		client:       idmClient,
		ValidDomains: authDomains,
	}

	// Generate an internal private key
	key, err := bakery.GenerateKey()
	if err != nil {
		return err
	}

	pkCache := bakery.NewThirdPartyStore()
	pkLocator := httpbakery.NewThirdPartyLocator(nil, pkCache)
	if authPubkey != "" {
		// Parse the public key
		pkKey := bakery.Key{}
		err := pkKey.UnmarshalText([]byte(authPubkey))
		if err != nil {
			return err
		}

		// Add the key information
		pkCache.AddInfo(authEndpoint, bakery.ThirdPartyInfo{
			PublicKey: bakery.PublicKey{Key: pkKey},
			Version:   3,
		})

		// Allow http URLs if we have a public key set
		if strings.HasPrefix(authEndpoint, "http://") {
			pkLocator.AllowInsecure()
		}
	}

	// Setup the bakery
	bakery := identchecker.NewBakery(identchecker.BakeryParams{
		Key:            key,
		Location:       authEndpoint,
		Locator:        pkLocator,
		Checker:        httpbakery.NewChecker(),
		IdentityClient: idmClientWrapper,
		Authorizer: identchecker.ACLAuthorizer{
			GetACL: func(ctx context.Context, op bakery.Op) ([]string, bool, error) {
				return []string{identchecker.Everyone}, false, nil
			},
		},
	})

	// Store our settings
	d.externalAuth = &externalAuth{
		endpoint: authEndpoint,
		expiry:   expiry,
		bakery:   bakery,
	}

	return nil
}
func initializeDbObject(d *Daemon) (*db.Dump, error) {
	logger.Info("Initializing local database")
	// Rename the old database name if needed.
	if shared.PathExists(d.os.LegacyLocalDatabasePath()) {
		if shared.PathExists(d.os.LocalDatabasePath()) {
			return nil, fmt.Errorf("Both legacy and new local database files exists")
		}
		logger.Info("Renaming local database file from lxd.db to database/local.db")
		err := os.Rename(d.os.LegacyLocalDatabasePath(), d.os.LocalDatabasePath())
		if err != nil {
			return nil, errors.Wrap(err, "Failed to rename legacy local database file")
		}
	}

	// NOTE: we use the legacyPatches parameter to run a few
	// legacy non-db updates that were in place before the
	// patches mechanism was introduced in lxd/patches.go. The
	// rest of non-db patches will be applied separately via
	// patchesApplyAll. See PR #3322 for more details.
	legacy := map[int]*db.LegacyPatch{}
	for i, patch := range legacyPatches {
		legacy[i] = &db.LegacyPatch{
			Hook: func(node *sql.DB) error {
				// FIXME: Use the low-level *node* SQL db as backend for both the
				//        db.Node and db.Cluster objects, since at this point we
				//        haven't migrated the data to the cluster database yet.
				cluster := d.cluster
				defer func() {
					d.cluster = cluster
				}()
				d.db = db.ForLegacyPatches(node)
				d.cluster = db.ForLocalInspection(node)
				return patch(d)
			},
		}
	}
	for _, i := range legacyPatchesNeedingDB {
		legacy[i].NeedsDB = true
	}

	// Hook to run when the local database is created from scratch. It will
	// create the default profile and mark all patches as applied.
	freshHook := func(db *db.Node) error {
		for _, patchName := range patchesGetNames() {
			err := db.PatchesMarkApplied(patchName)
			if err != nil {
				return err
			}
		}
		return nil
	}
	var err error
	var dump *db.Dump
	d.db, dump, err = db.OpenNode(filepath.Join(d.os.VarDir, "database"), freshHook, legacy)
	if err != nil {
		return nil, fmt.Errorf("Error creating database: %s", err)
	}

	return dump, nil
}
func WriteJSON(w http.ResponseWriter, body interface{}, debug bool) error {
	var output io.Writer
	var captured *bytes.Buffer

	output = w
	if debug {
		captured = &bytes.Buffer{}
		output = io.MultiWriter(w, captured)
	}

	err := json.NewEncoder(output).Encode(body)

	if captured != nil {
		shared.DebugJson(captured)
	}

	return err
}
func EtagHash(data interface{}) (string, error) {
	etag := sha256.New()
	err := json.NewEncoder(etag).Encode(data)
	if err != nil {
		return "", err
	}

	return fmt.Sprintf("%x", etag.Sum(nil)), nil
}
func EtagCheck(r *http.Request, data interface{}) error {
	match := r.Header.Get("If-Match")
	if match == "" {
		return nil
	}

	hash, err := EtagHash(data)
	if err != nil {
		return err
	}

	if hash != match {
		return fmt.Errorf("ETag doesn't match: %s vs %s", hash, match)
	}

	return nil
}
func HTTPClient(certificate string, proxy proxyFunc) (*http.Client, error) {
	var err error
	var cert *x509.Certificate

	if certificate != "" {
		certBlock, _ := pem.Decode([]byte(certificate))
		if certBlock == nil {
			return nil, fmt.Errorf("Invalid certificate")
		}

		cert, err = x509.ParseCertificate(certBlock.Bytes)
		if err != nil {
			return nil, err
		}
	}

	tlsConfig, err := shared.GetTLSConfig("", "", "", cert)
	if err != nil {
		return nil, err
	}

	tr := &http.Transport{
		TLSClientConfig:   tlsConfig,
		Dial:              shared.RFC3493Dialer,
		Proxy:             proxy,
		DisableKeepAlives: true,
	}

	myhttp := http.Client{
		Transport: tr,
	}

	// Setup redirect policy
	myhttp.CheckRedirect = func(req *http.Request, via []*http.Request) error {
		// Replicate the headers
		req.Header = via[len(via)-1].Header

		return nil
	}

	return &myhttp, nil
}
func IsRecursionRequest(r *http.Request) bool {
	recursionStr := r.FormValue("recursion")

	recursion, err := strconv.Atoi(recursionStr)
	if err != nil {
		return false
	}

	return recursion != 0
}
func GetListeners(start int) []net.Listener {
	defer func() {
		os.Unsetenv("LISTEN_PID")
		os.Unsetenv("LISTEN_FDS")
	}()

	pid, err := strconv.Atoi(os.Getenv("LISTEN_PID"))
	if err != nil {
		return nil
	}

	if pid != os.Getpid() {
		return nil
	}

	fds, err := strconv.Atoi(os.Getenv("LISTEN_FDS"))
	if err != nil {
		return nil
	}

	listeners := []net.Listener{}

	for i := start; i < start+fds; i++ {
		syscall.CloseOnExec(i)

		file := os.NewFile(uintptr(i), fmt.Sprintf("inherited-fd%d", i))
		listener, err := net.FileListener(file)
		if err != nil {
			continue
		}

		listeners = append(listeners, listener)
	}

	return listeners
}
func internalSQLGet(d *Daemon, r *http.Request) Response {
	database := r.FormValue("database")

	if !shared.StringInSlice(database, []string{"local", "global"}) {
		return BadRequest(fmt.Errorf("Invalid database"))
	}

	schemaFormValue := r.FormValue("schema")
	schemaOnly, err := strconv.Atoi(schemaFormValue)
	if err != nil {
		schemaOnly = 0
	}

	var schema string
	var db *sql.DB
	if database == "global" {
		db = d.cluster.DB()
		schema = cluster.FreshSchema()
	} else {
		db = d.db.DB()
		schema = node.FreshSchema()
	}

	tx, err := db.Begin()
	if err != nil {
		return SmartError(errors.Wrap(err, "failed to start transaction"))
	}
	defer tx.Rollback()
	dump, err := query.Dump(tx, schema, schemaOnly == 1)
	if err != nil {
		return SmartError(errors.Wrapf(err, "failed dump database %s", database))
	}
	return SyncResponse(true, internalSQLDump{Text: dump})
}
func internalSQLPost(d *Daemon, r *http.Request) Response {
	req := &internalSQLQuery{}
	// Parse the request.
	err := json.NewDecoder(r.Body).Decode(&req)
	if err != nil {
		return BadRequest(err)
	}

	if !shared.StringInSlice(req.Database, []string{"local", "global"}) {
		return BadRequest(fmt.Errorf("Invalid database"))
	}

	if req.Query == "" {
		return BadRequest(fmt.Errorf("No query provided"))
	}

	var db *sql.DB
	if req.Database == "global" {
		db = d.cluster.DB()
	} else {
		db = d.db.DB()
	}

	batch := internalSQLBatch{}

	if req.Query == ".sync" {
		d.gateway.Sync()
		return SyncResponse(true, batch)
	}

	for _, query := range strings.Split(req.Query, ";") {
		query = strings.TrimLeft(query, " ")

		if query == "" {
			continue
		}

		result := internalSQLResult{}

		tx, err := db.Begin()
		if err != nil {
			return SmartError(err)
		}

		if strings.HasPrefix(strings.ToUpper(query), "SELECT") {
			err = internalSQLSelect(tx, query, &result)
			tx.Rollback()
		} else {
			err = internalSQLExec(tx, query, &result)
			if err != nil {
				tx.Rollback()
			} else {
				err = tx.Commit()
			}
		}
		if err != nil {
			return SmartError(err)
		}

		batch.Results = append(batch.Results, result)
	}

	return SyncResponse(true, batch)
}
func (c *CertInfo) PublicKey() []byte {
	data := c.KeyPair().Certificate[0]
	return pem.EncodeToMemory(&pem.Block{Type: "CERTIFICATE", Bytes: data})
}
func (c *CertInfo) PrivateKey() []byte {
	ecKey, ok := c.KeyPair().PrivateKey.(*ecdsa.PrivateKey)
	if ok {
		data, err := x509.MarshalECPrivateKey(ecKey)
		if err != nil {
			return nil
		}

		return pem.EncodeToMemory(&pem.Block{Type: "EC PRIVATE KEY", Bytes: data})
	}

	rsaKey, ok := c.KeyPair().PrivateKey.(*rsa.PrivateKey)
	if ok {
		data := x509.MarshalPKCS1PrivateKey(rsaKey)
		return pem.EncodeToMemory(&pem.Block{Type: "RSA PRIVATE KEY", Bytes: data})
	}

	return nil
}
func (c *CertInfo) Fingerprint() string {
	fingerprint, err := CertFingerprintStr(string(c.PublicKey()))
	// Parsing should never fail, since we generated the cert ourselves,
	// but let's check the error for good measure.
	if err != nil {
		panic("invalid public key material")
	}
	return fingerprint
}
func GenCert(certf string, keyf string, certtype bool) error {
	/* Create the basenames if needed */
	dir := path.Dir(certf)
	err := os.MkdirAll(dir, 0750)
	if err != nil {
		return err
	}
	dir = path.Dir(keyf)
	err = os.MkdirAll(dir, 0750)
	if err != nil {
		return err
	}

	certBytes, keyBytes, err := GenerateMemCert(certtype)
	if err != nil {
		return err
	}

	certOut, err := os.Create(certf)
	if err != nil {
		return fmt.Errorf("Failed to open %s for writing: %v", certf, err)
	}
	certOut.Write(certBytes)
	certOut.Close()

	keyOut, err := os.OpenFile(keyf, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)
	if err != nil {
		return fmt.Errorf("Failed to open %s for writing: %v", keyf, err)
	}
	keyOut.Write(keyBytes)
	keyOut.Close()
	return nil
}
func PrintServerInfo(c lxd.ContainerServer) error {
	server, _, err := c.GetServer()
	if err != nil {
		return err
	}
	env := server.Environment
	fmt.Printf("Test environment:\n")
	fmt.Printf("  Server backend: %s\n", env.Server)
	fmt.Printf("  Server version: %s\n", env.ServerVersion)
	fmt.Printf("  Kernel: %s\n", env.Kernel)
	fmt.Printf("  Kernel architecture: %s\n", env.KernelArchitecture)
	fmt.Printf("  Kernel version: %s\n", env.KernelVersion)
	fmt.Printf("  Storage backend: %s\n", env.Storage)
	fmt.Printf("  Storage version: %s\n", env.StorageVersion)
	fmt.Printf("  Container backend: %s\n", env.Driver)
	fmt.Printf("  Container version: %s\n", env.DriverVersion)
	fmt.Printf("\n")
	return nil
}
func LaunchContainers(c lxd.ContainerServer, count int, parallel int, image string, privileged bool, start bool, freeze bool) (time.Duration, error) {
	var duration time.Duration

	batchSize, err := getBatchSize(parallel)
	if err != nil {
		return duration, err
	}

	printTestConfig(count, batchSize, image, privileged, freeze)

	fingerprint, err := ensureImage(c, image)
	if err != nil {
		return duration, err
	}

	batchStart := func(index int, wg *sync.WaitGroup) {
		defer wg.Done()

		name := getContainerName(count, index)

		err := createContainer(c, fingerprint, name, privileged)
		if err != nil {
			logf("Failed to launch container '%s': %s", name, err)
			return
		}

		if start {
			err := startContainer(c, name)
			if err != nil {
				logf("Failed to start container '%s': %s", name, err)
				return
			}

			if freeze {
				err := freezeContainer(c, name)
				if err != nil {
					logf("Failed to freeze container '%s': %s", name, err)
					return
				}
			}
		}
	}

	duration = processBatch(count, batchSize, batchStart)
	return duration, nil
}
func CreateContainers(c lxd.ContainerServer, count int, parallel int, fingerprint string, privileged bool) (time.Duration, error) {
	var duration time.Duration

	batchSize, err := getBatchSize(parallel)
	if err != nil {
		return duration, err
	}

	batchCreate := func(index int, wg *sync.WaitGroup) {
		defer wg.Done()

		name := getContainerName(count, index)

		err := createContainer(c, fingerprint, name, privileged)
		if err != nil {
			logf("Failed to launch container '%s': %s", name, err)
			return
		}
	}

	duration = processBatch(count, batchSize, batchCreate)

	return duration, nil
}
func GetContainers(c lxd.ContainerServer) ([]api.Container, error) {
	containers := []api.Container{}

	allContainers, err := c.GetContainers()
	if err != nil {
		return containers, err
	}

	for _, container := range allContainers {
		if container.Config[userConfigKey] == "true" {
			containers = append(containers, container)
		}
	}

	return containers, nil
}
func StartContainers(c lxd.ContainerServer, containers []api.Container, parallel int) (time.Duration, error) {
	var duration time.Duration

	batchSize, err := getBatchSize(parallel)
	if err != nil {
		return duration, err
	}

	count := len(containers)
	logf("Starting %d containers", count)

	batchStart := func(index int, wg *sync.WaitGroup) {
		defer wg.Done()

		container := containers[index]
		if !container.IsActive() {
			err := startContainer(c, container.Name)
			if err != nil {
				logf("Failed to start container '%s': %s", container.Name, err)
				return
			}
		}
	}

	duration = processBatch(count, batchSize, batchStart)
	return duration, nil
}
func setQueryParam(uri, param, value string) (string, error) {
	fields, err := url.Parse(uri)
	if err != nil {
		return "", err
	}

	values := fields.Query()
	values.Set(param, url.QueryEscape(value))

	fields.RawQuery = values.Encode()

	return fields.String(), nil
}
func (r *ProtocolLXD) GetImages() ([]api.Image, error) {
	images := []api.Image{}

	_, err := r.queryStruct("GET", "/images?recursion=1", nil, "", &images)
	if err != nil {
		return nil, err
	}

	return images, nil
}
func (r *ProtocolLXD) GetImageFile(fingerprint string, req ImageFileRequest) (*ImageFileResponse, error) {
	return r.GetPrivateImageFile(fingerprint, "", req)
}
func (r *ProtocolLXD) GetImageSecret(fingerprint string) (string, error) {
	op, err := r.CreateImageSecret(fingerprint)
	if err != nil {
		return "", err
	}
	opAPI := op.Get()

	return opAPI.Metadata["secret"].(string), nil
}
func (r *ProtocolLXD) GetPrivateImage(fingerprint string, secret string) (*api.Image, string, error) {
	image := api.Image{}

	// Build the API path
	path := fmt.Sprintf("/images/%s", url.QueryEscape(fingerprint))
	var err error
	path, err = r.setQueryAttributes(path)
	if err != nil {
		return nil, "", err
	}

	if secret != "" {
		path, err = setQueryParam(path, "secret", secret)
		if err != nil {
			return nil, "", err
		}
	}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", path, nil, "", &image)
	if err != nil {
		return nil, "", err
	}

	return &image, etag, nil
}
func (r *ProtocolLXD) GetPrivateImageFile(fingerprint string, secret string, req ImageFileRequest) (*ImageFileResponse, error) {
	// Sanity checks
	if req.MetaFile == nil && req.RootfsFile == nil {
		return nil, fmt.Errorf("No file requested")
	}

	uri := fmt.Sprintf("/1.0/images/%s/export", url.QueryEscape(fingerprint))

	var err error
	uri, err = r.setQueryAttributes(uri)
	if err != nil {
		return nil, err
	}

	// Attempt to download from host
	if secret == "" && shared.PathExists("/dev/lxd/sock") && os.Geteuid() == 0 {
		unixURI := fmt.Sprintf("http://unix.socket%s", uri)

		// Setup the HTTP client
		devlxdHTTP, err := unixHTTPClient(nil, "/dev/lxd/sock")
		if err == nil {
			resp, err := lxdDownloadImage(fingerprint, unixURI, r.httpUserAgent, devlxdHTTP, req)
			if err == nil {
				return resp, nil
			}
		}
	}

	// Build the URL
	uri = fmt.Sprintf("%s%s", r.httpHost, uri)
	if secret != "" {
		uri, err = setQueryParam(uri, "secret", secret)
		if err != nil {
			return nil, err
		}
	}

	return lxdDownloadImage(fingerprint, uri, r.httpUserAgent, r.http, req)
}
func (r *ProtocolLXD) GetImageAliases() ([]api.ImageAliasesEntry, error) {
	aliases := []api.ImageAliasesEntry{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/images/aliases?recursion=1", nil, "", &aliases)
	if err != nil {
		return nil, err
	}

	return aliases, nil
}
func (r *ProtocolLXD) tryCopyImage(req api.ImagesPost, urls []string) (RemoteOperation, error) {
	if len(urls) == 0 {
		return nil, fmt.Errorf("The source server isn't listening on the network")
	}

	rop := remoteOperation{
		chDone: make(chan bool),
	}

	// For older servers, apply the aliases after copy
	if !r.HasExtension("image_create_aliases") && req.Aliases != nil {
		rop.chPost = make(chan bool)

		go func() {
			defer close(rop.chPost)

			// Wait for the main operation to finish
			<-rop.chDone
			if rop.err != nil {
				return
			}

			// Get the operation data
			op, err := rop.GetTarget()
			if err != nil {
				return
			}

			// Extract the fingerprint
			fingerprint := op.Metadata["fingerprint"].(string)

			// Add the aliases
			for _, entry := range req.Aliases {
				alias := api.ImageAliasesPost{}
				alias.Name = entry.Name
				alias.Target = fingerprint

				r.CreateImageAlias(alias)
			}
		}()
	}

	// Forward targetOp to remote op
	go func() {
		success := false
		errors := map[string]error{}
		for _, serverURL := range urls {
			req.Source.Server = serverURL

			op, err := r.CreateImage(req, nil)
			if err != nil {
				errors[serverURL] = err
				continue
			}

			rop.targetOp = op

			for _, handler := range rop.handlers {
				rop.targetOp.AddHandler(handler)
			}

			err = rop.targetOp.Wait()
			if err != nil {
				errors[serverURL] = err
				continue
			}

			success = true
			break
		}

		if !success {
			rop.err = remoteOperationError("Failed remote image download", errors)
		}

		close(rop.chDone)
	}()

	return &rop, nil
}
func (r *ProtocolLXD) CopyImage(source ImageServer, image api.Image, args *ImageCopyArgs) (RemoteOperation, error) {
	// Sanity checks
	if r == source {
		return nil, fmt.Errorf("The source and target servers must be different")
	}

	// Get source server connection information
	info, err := source.GetConnectionInfo()
	if err != nil {
		return nil, err
	}

	// Prepare the copy request
	req := api.ImagesPost{
		Source: &api.ImagesPostSource{
			ImageSource: api.ImageSource{
				Certificate: info.Certificate,
				Protocol:    info.Protocol,
			},
			Fingerprint: image.Fingerprint,
			Mode:        "pull",
			Type:        "image",
		},
	}

	// Generate secret token if needed
	if !image.Public {
		secret, err := source.GetImageSecret(image.Fingerprint)
		if err != nil {
			return nil, err
		}

		req.Source.Secret = secret
	}

	// Process the arguments
	if args != nil {
		req.Aliases = args.Aliases
		req.AutoUpdate = args.AutoUpdate
		req.Public = args.Public

		if args.CopyAliases {
			req.Aliases = image.Aliases
			if args.Aliases != nil {
				req.Aliases = append(req.Aliases, args.Aliases...)
			}
		}
	}

	return r.tryCopyImage(req, info.Addresses)
}
func (r *ProtocolLXD) UpdateImage(fingerprint string, image api.ImagePut, ETag string) error {
	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/images/%s", url.QueryEscape(fingerprint)), image, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) DeleteImage(fingerprint string) (Operation, error) {
	// Send the request
	op, _, err := r.queryOperation("DELETE", fmt.Sprintf("/images/%s", url.QueryEscape(fingerprint)), nil, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) RefreshImage(fingerprint string) (Operation, error) {
	if !r.HasExtension("image_force_refresh") {
		return nil, fmt.Errorf("The server is missing the required \"image_force_refresh\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/images/%s/refresh", url.QueryEscape(fingerprint)), nil, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) CreateImageAlias(alias api.ImageAliasesPost) error {
	// Send the request
	_, _, err := r.query("POST", "/images/aliases", alias, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) UpdateImageAlias(name string, alias api.ImageAliasesEntryPut, ETag string) error {
	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/images/aliases/%s", url.QueryEscape(name)), alias, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) RenameImageAlias(name string, alias api.ImageAliasesEntryPost) error {
	// Send the request
	_, _, err := r.query("POST", fmt.Sprintf("/images/aliases/%s", url.QueryEscape(name)), alias, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) DeleteImageAlias(name string) error {
	// Send the request
	_, _, err := r.query("DELETE", fmt.Sprintf("/images/aliases/%s", url.QueryEscape(name)), nil, "")
	if err != nil {
		return err
	}

	return nil
}
func Open(dir string) (*sql.DB, error) {
	path := filepath.Join(dir, "local.db")
	db, err := sqliteOpen(path)
	if err != nil {
		return nil, fmt.Errorf("cannot open node database: %v", err)
	}

	return db, nil
}
func EnsureSchema(db *sql.DB, dir string, hook schema.Hook) (int, error) {
	backupDone := false

	schema := Schema()
	schema.File(filepath.Join(dir, "patch.local.sql")) // Optional custom queries
	schema.Hook(func(version int, tx *sql.Tx) error {
		if !backupDone {
			logger.Infof("Updating the LXD database schema. Backup made as \"local.db.bak\"")
			path := filepath.Join(dir, "local.db")
			err := shared.FileCopy(path, path+".bak")
			if err != nil {
				return err
			}

			backupDone = true
		}

		if version == -1 {
			logger.Debugf("Running pre-update queries from file for local DB schema")
		} else {
			logger.Debugf("Updating DB schema from %d to %d", version, version+1)
		}

		// Run the given hook only against actual update versions, not
		// when a custom query file is passed (signaled by version == -1).
		if hook != nil && version != -1 {
			err := hook(version, tx)
			if err != nil {
			}
		}

		return nil
	})
	return schema.Ensure(db)
}
func FilesystemDetect(path string) (string, error) {
	fs := syscall.Statfs_t{}

	err := syscall.Statfs(path, &fs)
	if err != nil {
		return "", err
	}

	switch fs.Type {
	case FilesystemSuperMagicBtrfs:
		return "btrfs", nil
	case FilesystemSuperMagicZfs:
		return "zfs", nil
	case FilesystemSuperMagicTmpfs:
		return "tmpfs", nil
	case FilesystemSuperMagicExt4:
		return "ext4", nil
	case FilesystemSuperMagicXfs:
		return "xfs", nil
	case FilesystemSuperMagicNfs:
		return "nfs", nil
	default:
		logger.Debugf("Unknown backing filesystem type: 0x%x", fs.Type)
		return string(fs.Type), nil
	}
}
func Schema() *schema.Schema {
	schema := schema.NewFromMap(updates)
	schema.Fresh(freshSchema)
	return schema
}
func updateFromV37(tx *sql.Tx) error {
	count, err := query.Count(tx, "raft_nodes", "")
	if err != nil {
		return errors.Wrap(err, "Fetch count of Raft nodes")
	}

	if count == 0 {
		// This node is not clustered, nothing to do.
		return nil
	}

	// Copy the core.https_address config.
	_, err = tx.Exec(`
INSERT INTO config (key, value)
  SELECT 'cluster.https_address', value FROM config WHERE key = 'core.https_address'
`)
	if err != nil {
		return errors.Wrap(err, "Insert cluster.https_address config")
	}

	return nil
}
func ArchitectureGetLocal() (string, error) {
	uname, err := shared.Uname()
	if err != nil {
		return ArchitectureDefault, err
	}

	return uname.Machine, nil
}
func NewController(url string, key string, machine string) (*Controller, error) {
	baseURL := fmt.Sprintf("%s/api/2.0/", url)

	// Connect to MAAS
	srv, err := gomaasapi.NewController(gomaasapi.ControllerArgs{
		BaseURL: baseURL,
		APIKey:  key,
	})
	if err != nil {
		// Juju errors aren't user-friendly, try to extract what actually happened
		if !strings.Contains(err.Error(), "unsupported version") {
			return nil, err
		}

		return nil, fmt.Errorf("Unable to connect MAAS at '%s': %v", baseURL,
			strings.Split(strings.Split(err.Error(), "unsupported version: ")[1], " (")[0])
	}

	srvRaw, err := gomaasapi.NewAuthenticatedClient(baseURL, key)
	if err != nil {
		return nil, err
	}

	// Find the right machine
	machines, err := srv.Machines(gomaasapi.MachinesArgs{Hostnames: []string{machine}})
	if err != nil {
		return nil, err
	}

	if len(machines) != 1 {
		return nil, fmt.Errorf("Couldn't find the specified machine: %s", machine)
	}

	// Setup the struct
	c := Controller{}
	c.srv = srv
	c.srvRaw = *srvRaw
	c.machine = machines[0]
	c.url = baseURL

	return &c, err
}
func (c *Controller) CreateContainer(name string, interfaces []ContainerInterface) error {
	// Parse the provided interfaces
	macInterfaces, err := parseInterfaces(interfaces)
	if err != nil {
		return err
	}

	// Get all the subnets
	subnets, err := c.getSubnets()
	if err != nil {
		return err
	}

	// Create the device and first interface
	device, err := c.machine.CreateDevice(gomaasapi.CreateMachineDeviceArgs{
		Hostname:      name,
		InterfaceName: interfaces[0].Name,
		MACAddress:    interfaces[0].MACAddress,
		VLAN:          subnets[interfaces[0].Subnets[0].Name].VLAN(),
	})
	if err != nil {
		return err
	}

	// Wipe the container entry if anything fails
	success := false
	defer func() {
		if success == true {
			return
		}

		c.DeleteContainer(name)
	}()

	// Create the rest of the interfaces
	for _, iface := range interfaces[1:] {
		_, err := device.CreateInterface(gomaasapi.CreateInterfaceArgs{
			Name:       iface.Name,
			MACAddress: iface.MACAddress,
			VLAN:       subnets[iface.Subnets[0].Name].VLAN(),
		})
		if err != nil {
			return err
		}
	}

	// Get a fresh copy of the device
	device, err = c.getDevice(name)
	if err != nil {
		return err
	}

	// Setup the interfaces
	for _, entry := range device.InterfaceSet() {
		// Get our record
		iface, ok := macInterfaces[entry.MACAddress()]
		if !ok {
			return fmt.Errorf("MAAS created an interface with a bad MAC: %s", entry.MACAddress())
		}

		// Add the subnets
		for _, subnet := range iface.Subnets {
			err := entry.LinkSubnet(gomaasapi.LinkSubnetArgs{
				Mode:      gomaasapi.LinkModeStatic,
				Subnet:    subnets[subnet.Name],
				IPAddress: subnet.Address,
			})
			if err != nil {
				return err
			}
		}
	}

	success = true
	return nil
}
func (c *Controller) DefinedContainer(name string) (bool, error) {
	devs, err := c.machine.Devices(gomaasapi.DevicesArgs{Hostname: []string{name}})
	if err != nil {
		return false, err
	}

	if len(devs) == 1 {
		return true, nil
	}

	return false, nil
}
func (c *Controller) UpdateContainer(name string, interfaces []ContainerInterface) error {
	// Parse the provided interfaces
	macInterfaces, err := parseInterfaces(interfaces)
	if err != nil {
		return err
	}

	// Get all the subnets
	subnets, err := c.getSubnets()
	if err != nil {
		return err
	}

	device, err := c.getDevice(name)
	if err != nil {
		return err
	}

	// Iterate over existing interfaces, drop all removed ones and update existing ones
	existingInterfaces := map[string]gomaasapi.Interface{}
	for _, entry := range device.InterfaceSet() {
		// Check if the interface has been removed from the container
		iface, ok := macInterfaces[entry.MACAddress()]
		if !ok {
			// Delete the interface in MAAS
			err = entry.Delete()
			if err != nil {
				return err
			}

			continue
		}

		// Update the subnets
		existingSubnets := map[string]gomaasapi.Subnet{}
		for _, link := range entry.Links() {
			// Check if the MAAS subnet matches any of the container's
			found := false
			for _, subnet := range iface.Subnets {
				if subnet.Name == link.Subnet().Name() {
					if subnet.Address == "" || subnet.Address == link.IPAddress() {
						found = true
					}
					break
				}
			}

			// If no exact match could be found, remove it from MAAS
			if !found {
				err = entry.UnlinkSubnet(link.Subnet())
				if err != nil {
					return err
				}

				continue
			}

			// Record the existing up to date subnet
			existingSubnets[link.Subnet().Name()] = link.Subnet()
		}

		// Add any missing (or updated) subnet to MAAS
		for _, subnet := range iface.Subnets {
			// Check that it's not configured yet
			_, ok := existingSubnets[subnet.Name]
			if ok {
				continue
			}

			// Add the link
			err := entry.LinkSubnet(gomaasapi.LinkSubnetArgs{
				Mode:      gomaasapi.LinkModeStatic,
				Subnet:    subnets[subnet.Name],
				IPAddress: subnet.Address,
			})
			if err != nil {
				return err
			}
		}

		// Record the interface has being configured
		existingInterfaces[entry.MACAddress()] = entry
	}

	// Iterate over expected interfaces, add any missing one
	for _, iface := range macInterfaces {
		_, ok := existingInterfaces[iface.MACAddress]
		if ok {
			// We already have it so just move on
			continue
		}

		// Create the new interface
		entry, err := device.CreateInterface(gomaasapi.CreateInterfaceArgs{
			Name:       iface.Name,
			MACAddress: iface.MACAddress,
			VLAN:       subnets[iface.Subnets[0].Name].VLAN(),
		})
		if err != nil {
			return err
		}

		// Add the subnets
		for _, subnet := range iface.Subnets {
			err := entry.LinkSubnet(gomaasapi.LinkSubnetArgs{
				Mode:      gomaasapi.LinkModeStatic,
				Subnet:    subnets[subnet.Name],
				IPAddress: subnet.Address,
			})
			if err != nil {
				return err
			}
		}
	}

	return nil
}
func (c *Controller) RenameContainer(name string, newName string) error {
	device, err := c.getDevice(name)
	if err != nil {
		return err
	}

	// FIXME: We should convince the Juju folks to implement an Update() method on Device
	uri, err := url.Parse(fmt.Sprintf("%s/devices/%s/", c.url, device.SystemID()))
	if err != nil {
		return err
	}

	values := url.Values{}
	values.Set("hostname", newName)

	_, err = c.srvRaw.Put(uri, values)
	if err != nil {
		return err
	}

	return nil
}
func (c *Controller) DeleteContainer(name string) error {
	device, err := c.getDevice(name)
	if err != nil {
		return err
	}

	err = device.Delete()
	if err != nil {
		return err
	}

	return nil
}
func (s *Schema) Add(update Update) {
	s.updates = append(s.updates, update)
}
func ensureSchemaTableExists(tx *sql.Tx) error {
	exists, err := DoesSchemaTableExist(tx)
	if err != nil {
		return fmt.Errorf("failed to check if schema table is there: %v", err)
	}
	if !exists {
		err := createSchemaTable(tx)
		if err != nil {
			return fmt.Errorf("failed to create schema table: %v", err)
		}
	}
	return nil
}
func queryCurrentVersion(tx *sql.Tx) (int, error) {
	versions, err := selectSchemaVersions(tx)
	if err != nil {
		return -1, fmt.Errorf("failed to fetch update versions: %v", err)
	}

	// Fix bad upgrade code between 30 and 32
	hasVersion := func(v int) bool { return shared.IntInSlice(v, versions) }
	if hasVersion(30) && hasVersion(32) && !hasVersion(31) {
		err = insertSchemaVersion(tx, 31)
		if err != nil {
			return -1, fmt.Errorf("failed to insert missing schema version 31")
		}

		versions, err = selectSchemaVersions(tx)
		if err != nil {
			return -1, fmt.Errorf("failed to fetch update versions: %v", err)
		}
	}

	// Fix broken schema version between 37 and 38
	if hasVersion(37) && !hasVersion(38) {
		count, err := query.Count(tx, "config", "key = 'cluster.https_address'")
		if err != nil {
			return -1, fmt.Errorf("Failed to check if cluster.https_address is set: %v", err)
		}
		if count == 1 {
			// Insert the missing version.
			err := insertSchemaVersion(tx, 38)
			if err != nil {
				return -1, fmt.Errorf("Failed to insert missing schema version 38")
			}
			versions = append(versions, 38)
		}
	}

	current := 0
	if len(versions) > 0 {
		err = checkSchemaVersionsHaveNoHoles(versions)
		if err != nil {
			return -1, err
		}
		current = versions[len(versions)-1] // Highest recorded version
	}

	return current, nil
}
func ensureUpdatesAreApplied(tx *sql.Tx, current int, updates []Update, hook Hook) error {
	if current > len(updates) {
		return fmt.Errorf(
			"schema version '%d' is more recent than expected '%d'",
			current, len(updates))
	}

	// If there are no updates, there's nothing to do.
	if len(updates) == 0 {
		return nil
	}

	// Apply missing updates.
	for _, update := range updates[current:] {
		if hook != nil {
			err := hook(current, tx)
			if err != nil {
				return fmt.Errorf(
					"failed to execute hook (version %d): %v", current, err)
			}
		}
		err := update(tx)
		if err != nil {
			return fmt.Errorf("failed to apply update %d: %v", current, err)
		}
		current++

		err = insertSchemaVersion(tx, current)
		if err != nil {
			return fmt.Errorf("failed to insert version %d", current)
		}
	}

	return nil
}
func checkSchemaVersionsHaveNoHoles(versions []int) error {
	// Sanity check that there are no "holes" in the recorded
	// versions.
	for i := range versions[:len(versions)-1] {
		if versions[i+1] != versions[i]+1 {
			return fmt.Errorf("Missing updates: %d to %d", versions[i], versions[i+1])
		}
	}
	return nil
}
func checkAllUpdatesAreApplied(tx *sql.Tx, updates []Update) error {
	versions, err := selectSchemaVersions(tx)
	if err != nil {
		return fmt.Errorf("failed to fetch update versions: %v", err)
	}

	if len(versions) == 0 {
		return fmt.Errorf("expected schema table to contain at least one row")
	}

	err = checkSchemaVersionsHaveNoHoles(versions)
	if err != nil {
		return err
	}

	current := versions[len(versions)-1]
	if current != len(updates) {
		return fmt.Errorf("update level is %d, expected %d", current, len(updates))
	}
	return nil
}
func formatSQL(statement string) string {
	lines := strings.Split(statement, "\n")
	for i, line := range lines {
		if strings.Contains(line, "UNIQUE") {
			// Let UNIQUE(x, y) constraints alone.
			continue
		}
		lines[i] = strings.Replace(line, ", ", ",\n    ", -1)
	}
	return strings.Join(lines, "\n")
}
func GetAllXattr(path string) (xattrs map[string]string, err error) {
	e1 := fmt.Errorf("Extended attributes changed during retrieval")

	// Call llistxattr() twice: First, to determine the size of the buffer
	// we need to allocate to store the extended attributes, second, to
	// actually store the extended attributes in the buffer. Also, check if
	// the size/number of extended attributes hasn't changed between the two
	// calls.
	pre, err := llistxattr(path, nil)
	if err != nil || pre < 0 {
		return nil, err
	}
	if pre == 0 {
		return nil, nil
	}

	dest := make([]byte, pre)

	post, err := llistxattr(path, dest)
	if err != nil || post < 0 {
		return nil, err
	}
	if post != pre {
		return nil, e1
	}

	split := strings.Split(string(dest), "\x00")
	if split == nil {
		return nil, fmt.Errorf("No valid extended attribute key found")
	}
	// *listxattr functions return a list of  names  as  an unordered array
	// of null-terminated character strings (attribute names are separated
	// by null bytes ('\0')), like this: user.name1\0system.name1\0user.name2\0
	// Since we split at the '\0'-byte the last element of the slice will be
	// the empty string. We remove it:
	if split[len(split)-1] == "" {
		split = split[:len(split)-1]
	}

	xattrs = make(map[string]string, len(split))

	for _, x := range split {
		xattr := string(x)
		// Call Getxattr() twice: First, to determine the size of the
		// buffer we need to allocate to store the extended attributes,
		// second, to actually store the extended attributes in the
		// buffer. Also, check if the size of the extended attribute
		// hasn't changed between the two calls.
		pre, err = syscall.Getxattr(path, xattr, nil)
		if err != nil || pre < 0 {
			return nil, err
		}

		dest = make([]byte, pre)
		post := 0
		if pre > 0 {
			post, err = syscall.Getxattr(path, xattr, dest)
			if err != nil || post < 0 {
				return nil, err
			}
		}

		if post != pre {
			return nil, e1
		}

		xattrs[xattr] = string(dest)
	}

	return xattrs, nil
}
func GetErrno(err error) (errno error, iserrno bool) {
	sysErr, ok := err.(*os.SyscallError)
	if ok {
		return sysErr.Err, true
	}

	pathErr, ok := err.(*os.PathError)
	if ok {
		return pathErr.Err, true
	}

	tmpErrno, ok := err.(syscall.Errno)
	if ok {
		return tmpErrno, true
	}

	return nil, false
}
func Uname() (*Utsname, error) {
	/*
	 * Based on: https://groups.google.com/forum/#!topic/golang-nuts/Jel8Bb-YwX8
	 * there is really no better way to do this, which is
	 * unfortunate. Also, we ditch the more accepted CharsToString
	 * version in that thread, since it doesn't seem as portable,
	 * viz. github issue #206.
	 */

	uname := syscall.Utsname{}
	err := syscall.Uname(&uname)
	if err != nil {
		return nil, err
	}

	return &Utsname{
		Sysname:    intArrayToString(uname.Sysname),
		Nodename:   intArrayToString(uname.Nodename),
		Release:    intArrayToString(uname.Release),
		Version:    intArrayToString(uname.Version),
		Machine:    intArrayToString(uname.Machine),
		Domainname: intArrayToString(uname.Domainname),
	}, nil
}
func RegisterStmt(sql string) int {
	code := len(stmts)
	stmts[code] = sql
	return code
}
func PrepareStmts(db *sql.DB) (map[int]*sql.Stmt, error) {
	index := map[int]*sql.Stmt{}

	for code, sql := range stmts {
		stmt, err := db.Prepare(sql)
		if err != nil {
			return nil, errors.Wrapf(err, "%q", sql)
		}
		index[code] = stmt
	}

	return index, nil
}
func NewGateway(db *db.Node, cert *shared.CertInfo, options ...Option) (*Gateway, error) {
	ctx, cancel := context.WithCancel(context.Background())

	o := newOptions()
	for _, option := range options {
		option(o)

	}

	gateway := &Gateway{
		db:        db,
		cert:      cert,
		options:   o,
		ctx:       ctx,
		cancel:    cancel,
		upgradeCh: make(chan struct{}, 16),
		acceptCh:  make(chan net.Conn),
		store:     &dqliteServerStore{},
	}

	err := gateway.init()
	if err != nil {
		return nil, err
	}

	return gateway, nil
}
func (g *Gateway) DialFunc() dqlite.DialFunc {
	return func(ctx context.Context, address string) (net.Conn, error) {
		// Memory connection.
		if g.memoryDial != nil {
			return g.memoryDial(ctx, address)
		}

		return dqliteNetworkDial(ctx, address, g.cert)
	}
}
func (g *Gateway) Shutdown() error {
	logger.Debugf("Stop database gateway")

	if g.raft != nil {
		err := g.raft.Shutdown()
		if err != nil {
			return errors.Wrap(err, "Failed to shutdown raft")
		}
	}

	if g.server != nil {
		g.Sync()
		g.server.Close()

		// Unset the memory dial, since Shutdown() is also called for
		// switching between in-memory and network mode.
		g.memoryDial = nil
	}

	return nil
}
func (g *Gateway) Sync() {
	if g.server == nil {
		return
	}

	dir := filepath.Join(g.db.Dir(), "global")
	err := g.server.Dump("db.bin", dir)
	if err != nil {
		// Just log a warning, since this is not fatal.
		logger.Warnf("Failed to dump database to disk: %v", err)
	}
}
func (g *Gateway) Reset(cert *shared.CertInfo) error {
	err := g.Shutdown()
	if err != nil {
		return err
	}
	err = os.RemoveAll(filepath.Join(g.db.Dir(), "global"))
	if err != nil {
		return err
	}
	err = g.db.Transaction(func(tx *db.NodeTx) error {
		return tx.RaftNodesReplace(nil)
	})
	if err != nil {
		return err
	}
	g.cert = cert
	return g.init()
}
func (g *Gateway) LeaderAddress() (string, error) {
	// If we aren't clustered, return an error.
	if g.memoryDial != nil {
		return "", fmt.Errorf("Node is not clustered")
	}

	ctx, cancel := context.WithTimeout(g.ctx, 5*time.Second)
	defer cancel()

	// If this is a raft node, return the address of the current leader, or
	// wait a bit until one is elected.
	if g.raft != nil {
		for ctx.Err() == nil {
			address := string(g.raft.Raft().Leader())
			if address != "" {
				return address, nil
			}
			time.Sleep(time.Second)
		}
		return "", ctx.Err()

	}

	// If this isn't a raft node, contact a raft node and ask for the
	// address of the current leader.
	config, err := tlsClientConfig(g.cert)
	if err != nil {
		return "", err
	}
	addresses := []string{}
	err = g.db.Transaction(func(tx *db.NodeTx) error {
		nodes, err := tx.RaftNodes()
		if err != nil {
			return err
		}
		for _, node := range nodes {
			addresses = append(addresses, node.Address)
		}
		return nil
	})
	if err != nil {
		return "", errors.Wrap(err, "Failed to fetch raft nodes addresses")
	}

	if len(addresses) == 0 {
		// This should never happen because the raft_nodes table should
		// be never empty for a clustered node, but check it for good
		// measure.
		return "", fmt.Errorf("No raft node known")
	}

	for _, address := range addresses {
		url := fmt.Sprintf("https://%s%s", address, databaseEndpoint)
		request, err := http.NewRequest("GET", url, nil)
		if err != nil {
			return "", err
		}
		request = request.WithContext(ctx)
		client := &http.Client{Transport: &http.Transport{TLSClientConfig: config}}
		response, err := client.Do(request)
		if err != nil {
			logger.Debugf("Failed to fetch leader address from %s", address)
			continue
		}
		if response.StatusCode != http.StatusOK {
			logger.Debugf("Request for leader address from %s failed", address)
			continue
		}
		info := map[string]string{}
		err = shared.ReadToJSON(response.Body, &info)
		if err != nil {
			logger.Debugf("Failed to parse leader address from %s", address)
			continue
		}
		leader := info["leader"]
		if leader == "" {
			logger.Debugf("Raft node %s returned no leader address", address)
			continue
		}
		return leader, nil
	}

	return "", fmt.Errorf("RAFT cluster is unavailable")
}
func (g *Gateway) waitLeadership() error {
	n := 80
	sleep := 250 * time.Millisecond
	for i := 0; i < n; i++ {
		if g.raft.raft.State() == raft.Leader {
			return nil
		}
		time.Sleep(sleep)
	}
	return fmt.Errorf("RAFT node did not self-elect within %s", time.Duration(n)*sleep)
}
func (g *Gateway) currentRaftNodes() ([]db.RaftNode, error) {
	if g.raft == nil {
		return nil, raft.ErrNotLeader
	}
	servers, err := g.raft.Servers()
	if err != nil {
		return nil, err
	}
	provider := raftAddressProvider{db: g.db}
	nodes := make([]db.RaftNode, len(servers))
	for i, server := range servers {
		address, err := provider.ServerAddr(server.ID)
		if err != nil {
			if err != db.ErrNoSuchObject {
				return nil, errors.Wrap(err, "Failed to fetch raft server address")
			}
			// Use the initial address as fallback. This is an edge
			// case that happens when a new leader is elected and
			// its raft_nodes table is not fully up-to-date yet.
			address = server.Address
		}
		id, err := strconv.Atoi(string(server.ID))
		if err != nil {
			return nil, errors.Wrap(err, "Non-numeric server ID")
		}
		nodes[i].ID = int64(id)
		nodes[i].Address = string(address)
	}
	return nodes, nil
}
func (g *Gateway) cachedRaftNodes() ([]string, error) {
	var addresses []string
	err := g.db.Transaction(func(tx *db.NodeTx) error {
		var err error
		addresses, err = tx.RaftNodeAddresses()
		return err
	})
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch raft nodes")
	}
	return addresses, nil
}
func dqliteMemoryDial(listener net.Listener) dqlite.DialFunc {
	return func(ctx context.Context, address string) (net.Conn, error) {
		return net.Dial("unix", listener.Addr().String())
	}
}
func DqliteLog(l dqlite.LogLevel, format string, a ...interface{}) {
	format = fmt.Sprintf("Dqlite: %s", format)
	switch l {
	case dqlite.LogDebug:
		logger.Debugf(format, a...)
	case dqlite.LogInfo:
		logger.Debugf(format, a...)
	case dqlite.LogWarn:
		logger.Warnf(format, a...)
	case dqlite.LogError:
		logger.Errorf(format, a...)
	}
}
func (r *Response) MetadataAsMap() (map[string]interface{}, error) {
	ret := map[string]interface{}{}
	err := r.MetadataAsStruct(&ret)
	if err != nil {
		return nil, err
	}

	return ret, nil
}
func (r *Response) MetadataAsOperation() (*Operation, error) {
	op := Operation{}
	err := r.MetadataAsStruct(&op)
	if err != nil {
		return nil, err
	}

	return &op, nil
}
func (r *Response) MetadataAsStringSlice() ([]string, error) {
	sl := []string{}
	err := r.MetadataAsStruct(&sl)
	if err != nil {
		return nil, err
	}

	return sl, nil
}
func (r *Response) MetadataAsStruct(target interface{}) error {
	return json.Unmarshal(r.Metadata, &target)
}
func (r *CSVReport) Load() error {
	file, err := os.Open(r.Filename)
	if err != nil {
		return err
	}
	defer file.Close()

	reader := csv.NewReader(file)
	for line := 1; err != io.EOF; line++ {
		record, err := reader.Read()
		if err == io.EOF {
			break
		} else if err != nil {
			return err
		}

		err = r.addRecord(record)
		if err != nil {
			return err
		}
	}
	logf("Loaded report file %s", r.Filename)
	return nil
}
func (r *CSVReport) Write() error {
	file, err := os.OpenFile(r.Filename, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0640)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := csv.NewWriter(file)
	err = writer.WriteAll(r.records)
	if err != nil {
		return err
	}

	logf("Written report file %s", r.Filename)
	return nil
}
func (r *CSVReport) AddRecord(label string, elapsed time.Duration) error {
	if len(r.records) == 0 {
		r.addRecord(csvFields)
	}

	record := []string{
		fmt.Sprintf("%d", time.Now().UnixNano()/int64(time.Millisecond)), // timestamp
		fmt.Sprintf("%d", elapsed/time.Millisecond),
		label,
		"",     // responseCode is not used
		"true", // success"
	}
	return r.addRecord(record)
}
func LoadConfig(path string) (*Config, error) {
	// Open the config file
	content, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("Unable to read the configuration file: %v", err)
	}

	// Decode the yaml document
	c := NewConfig(filepath.Dir(path), false)
	err = yaml.Unmarshal(content, &c)
	if err != nil {
		return nil, fmt.Errorf("Unable to decode the configuration: %v", err)
	}

	for k, r := range c.Remotes {
		if !r.Public && r.AuthType == "" {
			r.AuthType = "tls"
			c.Remotes[k] = r
		}
	}

	// Set default values
	if c.Remotes == nil {
		c.Remotes = make(map[string]Remote)
	}

	// Apply the static remotes
	for k, v := range StaticRemotes {
		if c.Remotes[k].Project != "" {
			v.Project = c.Remotes[k].Project
		}

		c.Remotes[k] = v
	}

	// NOTE: Remove this once we only see a small fraction of non-simplestreams users
	// Upgrade users to the "simplestreams" protocol
	images, ok := c.Remotes["images"]
	if ok && images.Protocol != ImagesRemote.Protocol && images.Addr == ImagesRemote.Addr {
		c.Remotes["images"] = ImagesRemote
		c.SaveConfig(path)
	}

	return c, nil
}
func (c *Config) SaveConfig(path string) error {
	// Create a new copy for the config file
	conf := Config{}
	err := shared.DeepCopy(c, &conf)
	if err != nil {
		return fmt.Errorf("Unable to copy the configuration: %v", err)
	}

	// Remove the static remotes
	for k := range StaticRemotes {
		if k == "local" {
			continue
		}

		delete(conf.Remotes, k)
	}

	// Create the config file (or truncate an existing one)
	f, err := os.Create(path)
	if err != nil {
		return fmt.Errorf("Unable to create the configuration file: %v", err)
	}
	defer f.Close()

	// Write the new config
	data, err := yaml.Marshal(conf)
	if err != nil {
		return fmt.Errorf("Unable to marshal the configuration: %v", err)
	}

	_, err = f.Write(data)
	if err != nil {
		return fmt.Errorf("Unable to write the configuration: %v", err)
	}

	return nil
}
func (l ChrootLoader) Get(path string) (io.Reader, error) {
	// Get the full path
	path, err := filepath.EvalSymlinks(path)
	if err != nil {
		return nil, err
	}

	basePath, err := filepath.EvalSymlinks(l.Path)
	if err != nil {
		return nil, err
	}

	// Validate that we're under the expected prefix
	if !strings.HasPrefix(path, basePath) {
		return nil, fmt.Errorf("Attempting to access a file outside the container")
	}

	// Open and read the file
	buf, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}

	return bytes.NewReader(buf), nil
}
func (c *Config) ConfigPath(paths ...string) string {
	path := []string{c.ConfigDir}
	path = append(path, paths...)

	return filepath.Join(path...)
}
func (c *Config) ServerCertPath(remote string) string {
	return c.ConfigPath("servercerts", fmt.Sprintf("%s.crt", remote))
}
func NewConfig(configDir string, defaults bool) *Config {
	config := &Config{ConfigDir: configDir}
	if defaults {
		config.Remotes = DefaultRemotes
		config.DefaultRemote = "local"
	}

	return config
}
func (s *migrationSourceWs) checkForPreDumpSupport() (bool, int) {
	// Ask CRIU if this architecture/kernel/criu combination
	// supports pre-copy (dirty memory tracking)
	criuMigrationArgs := CriuMigrationArgs{
		cmd:          lxc.MIGRATE_FEATURE_CHECK,
		stateDir:     "",
		function:     "feature-check",
		stop:         false,
		actionScript: false,
		dumpDir:      "",
		preDumpDir:   "",
		features:     lxc.FEATURE_MEM_TRACK,
	}
	err := s.container.Migrate(&criuMigrationArgs)

	if err != nil {
		// CRIU says it does not know about dirty memory tracking.
		// This means the rest of this function is irrelevant.
		return false, 0
	}

	// CRIU says it can actually do pre-dump. Let's set it to true
	// unless the user wants something else.
	use_pre_dumps := true

	// What does the configuration say about pre-copy
	tmp := s.container.ExpandedConfig()["migration.incremental.memory"]

	if tmp != "" {
		use_pre_dumps = shared.IsTrue(tmp)
	}

	var max_iterations int

	// migration.incremental.memory.iterations is the value after which the
	// container will be definitely migrated, even if the remaining number
	// of memory pages is below the defined threshold.
	tmp = s.container.ExpandedConfig()["migration.incremental.memory.iterations"]
	if tmp != "" {
		max_iterations, _ = strconv.Atoi(tmp)
	} else {
		// default to 10
		max_iterations = 10
	}
	if max_iterations > 999 {
		// the pre-dump directory is hardcoded to a string
		// with maximal 3 digits. 999 pre-dumps makes no
		// sense at all, but let's make sure the number
		// is not higher than this.
		max_iterations = 999
	}
	logger.Debugf("Using maximal %d iterations for pre-dumping", max_iterations)

	return use_pre_dumps, max_iterations
}
func (s *migrationSourceWs) preDumpLoop(args *preDumpLoopArgs) (bool, error) {
	// Do a CRIU pre-dump
	criuMigrationArgs := CriuMigrationArgs{
		cmd:          lxc.MIGRATE_PRE_DUMP,
		stop:         false,
		actionScript: false,
		preDumpDir:   args.preDumpDir,
		dumpDir:      args.dumpDir,
		stateDir:     args.checkpointDir,
		function:     "migration",
	}

	logger.Debugf("Doing another pre-dump in %s", args.preDumpDir)

	final := args.final

	err := s.container.Migrate(&criuMigrationArgs)
	if err != nil {
		return final, err
	}

	// Send the pre-dump.
	ctName, _, _ := containerGetParentAndSnapshotName(s.container.Name())
	state := s.container.DaemonState()
	err = RsyncSend(ctName, shared.AddSlash(args.checkpointDir), s.criuConn, nil, args.rsyncFeatures, args.bwlimit, state.OS.ExecPath)
	if err != nil {
		return final, err
	}

	// Read the CRIU's 'stats-dump' file
	dumpPath := shared.AddSlash(args.checkpointDir)
	dumpPath += shared.AddSlash(args.dumpDir)
	written, skipped_parent, err := readCriuStatsDump(dumpPath)
	if err != nil {
		return final, err
	}

	logger.Debugf("CRIU pages written %d", written)
	logger.Debugf("CRIU pages skipped %d", skipped_parent)

	total_pages := written + skipped_parent

	percentage_skipped := int(100 - ((100 * written) / total_pages))

	logger.Debugf("CRIU pages skipped percentage %d%%", percentage_skipped)

	// threshold is the percentage of memory pages that needs
	// to be pre-copied for the pre-copy migration to stop.
	var threshold int
	tmp := s.container.ExpandedConfig()["migration.incremental.memory.goal"]
	if tmp != "" {
		threshold, _ = strconv.Atoi(tmp)
	} else {
		// defaults to 70%
		threshold = 70
	}

	if percentage_skipped > threshold {
		logger.Debugf("Memory pages skipped (%d%%) due to pre-copy is larger than threshold (%d%%)", percentage_skipped, threshold)
		logger.Debugf("This was the last pre-dump; next dump is the final dump")
		final = true
	}

	// If in pre-dump mode, the receiving side
	// expects a message to know if this was the
	// last pre-dump
	logger.Debugf("Sending another header")
	sync := migration.MigrationSync{
		FinalPreDump: proto.Bool(final),
	}

	data, err := proto.Marshal(&sync)

	if err != nil {
		return final, err
	}

	err = s.criuConn.WriteMessage(websocket.BinaryMessage, data)
	if err != nil {
		s.sendControl(err)
		return final, err
	}
	logger.Debugf("Sending another header done")

	return final, nil
}
func newRoot() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "lxd-generate",
		Short: "Code generation tool for LXD development",
		Long: `This is the entry point for all "go:generate" directives
used in LXD's source code.`,
		RunE: func(cmd *cobra.Command, args []string) error {
			return fmt.Errorf("Not implemented")
		},
	}
	cmd.AddCommand(newDb())

	return cmd
}
func APIExtensionsCount() int {
	count := len(APIExtensions)

	// This environment variable is an internal one to force the code
	// to believe that we an API extensions version greater than we
	// actually have. It's used by integration tests to exercise the
	// cluster upgrade process.
	artificialBump := os.Getenv("LXD_ARTIFICIALLY_BUMP_API_EXTENSIONS")
	if artificialBump != "" {
		n, err := strconv.Atoi(artificialBump)
		if err == nil {
			count += n
		}
	}

	return count
}
func SelectURIs(stmt *sql.Stmt, f func(a ...interface{}) string, args ...interface{}) ([]string, error) {
	rows, err := stmt.Query(args...)
	if err != nil {
		return nil, errors.Wrapf(err, "Failed to query URIs")
	}
	defer rows.Close()

	columns, err := rows.Columns()
	if err != nil {
		return nil, errors.Wrap(err, "Rows columns")
	}

	params := make([]interface{}, len(columns))

	dest := make([]interface{}, len(params))
	for i := range params {
		params[i] = ""
		dest[i] = &params[i]
	}

	uris := []string{}

	for rows.Next() {
		err := rows.Scan(dest...)
		if err != nil {
			return nil, errors.Wrapf(err, "Failed to scan URI params")
		}

		uri := f(params...)
		uris = append(uris, uri)
	}

	err = rows.Err()
	if err != nil {
		return nil, errors.Wrapf(err, "Failed to close URI result set")
	}

	return uris, nil
}
func SelectStrings(tx *sql.Tx, query string, args ...interface{}) ([]string, error) {
	values := []string{}
	scan := func(rows *sql.Rows) error {
		var value string
		err := rows.Scan(&value)
		if err != nil {
			return err
		}
		values = append(values, value)
		return nil
	}

	err := scanSingleColumn(tx, query, args, "TEXT", scan)
	if err != nil {
		return nil, err
	}

	return values, nil
}
func scanSingleColumn(tx *sql.Tx, query string, args []interface{}, typeName string, scan scanFunc) error {
	rows, err := tx.Query(query, args...)
	if err != nil {
		return err
	}
	defer rows.Close()

	for rows.Next() {
		err := scan(rows)
		if err != nil {
			return err
		}
	}

	err = rows.Err()
	if err != nil {
		return err
	}
	return nil
}
func LazyHandler(h Handler) Handler {
	return FuncHandler(func(r *Record) error {
		// go through the values (odd indices) and reassign
		// the values of any lazy fn to the result of its execution
		hadErr := false
		for i := 1; i < len(r.Ctx); i += 2 {
			lz, ok := r.Ctx[i].(Lazy)
			if ok {
				v, err := evaluateLazy(lz)
				if err != nil {
					hadErr = true
					r.Ctx[i] = err
				} else {
					if cs, ok := v.(stack.Trace); ok {
						v = cs.TrimBelow(stack.Call(r.CallPC[0])).
							TrimRuntime()
					}
					r.Ctx[i] = v
				}
			}
		}

		if hadErr {
			r.Ctx = append(r.Ctx, errorKey, "bad lazy")
		}

		return h.Log(r)
	})
}
func Callers() Trace {
	pcs := poolBuf()
	pcs = pcs[:cap(pcs)]
	n := runtime.Callers(2, pcs)
	cs := make([]Call, n)
	for i, pc := range pcs[:n] {
		cs[i] = Call(pc)
	}
	putPoolBuf(pcs)
	return cs
}
func (pc Call) name() string {
	pcFix := uintptr(pc) - 1 // work around for go issue #7690
	fn := runtime.FuncForPC(pcFix)
	if fn == nil {
		return "???"
	}
	return fn.Name()
}
func (pcs Trace) TrimBelow(pc Call) Trace {
	for len(pcs) > 0 && pcs[0] != pc {
		pcs = pcs[1:]
	}
	return pcs
}
func (pcs Trace) TrimAbove(pc Call) Trace {
	for len(pcs) > 0 && pcs[len(pcs)-1] != pc {
		pcs = pcs[:len(pcs)-1]
	}
	return pcs
}
func (pcs Trace) TrimBelowName(name string) Trace {
	for len(pcs) > 0 && pcs[0].name() != name {
		pcs = pcs[1:]
	}
	return pcs
}
func (pcs Trace) TrimAboveName(name string) Trace {
	for len(pcs) > 0 && pcs[len(pcs)-1].name() != name {
		pcs = pcs[:len(pcs)-1]
	}
	return pcs
}
func (pcs Trace) TrimRuntime() Trace {
	for len(pcs) > 0 && inGoroot(pcs[len(pcs)-1].file()) {
		pcs = pcs[:len(pcs)-1]
	}
	return pcs
}
func GetCaps(path string) ([]byte, error) {
	xattrs, err := shared.GetAllXattr(path)
	if err != nil {
		return nil, err
	}

	valueStr, ok := xattrs["security.capability"]
	if !ok {
		return nil, nil
	}

	return []byte(valueStr), nil
}
func SetCaps(path string, caps []byte, uid int64) error {
	cpath := C.CString(path)
	defer C.free(unsafe.Pointer(cpath))

	ccaps := C.CString(string(caps))
	defer C.free(unsafe.Pointer(ccaps))

	r := C.set_vfs_ns_caps(cpath, ccaps, C.ssize_t(len(caps)), C.uint32_t(uid))
	if r != 0 {
		return fmt.Errorf("Failed to apply capabilities to: %s", path)
	}

	return nil
}
func (pt *ProgressReader) Read(p []byte) (int, error) {
	// Do normal reader tasks
	n, err := pt.ReadCloser.Read(p)

	// Do the actual progress tracking
	if pt.Tracker != nil {
		pt.Tracker.total += int64(n)
		pt.Tracker.update(n)
	}

	return n, err
}
func Supported(path string) (bool, error) {
	// Get the backing device
	devPath, err := devForPath(path)
	if err != nil {
		return false, err
	}

	// Call quotactl through CGo
	cDevPath := C.CString(devPath)
	defer C.free(unsafe.Pointer(cDevPath))

	return C.quota_supported(cDevPath) == 0, nil
}
func GetProject(path string) (uint32, error) {
	// Call ioctl through CGo
	cPath := C.CString(path)
	defer C.free(unsafe.Pointer(cPath))

	id := C.quota_get_path(cPath)
	if id < 0 {
		return 0, fmt.Errorf("Failed to get project from '%s'", path)
	}

	return uint32(id), nil
}
func SetProject(path string, id uint32) error {
	// Call ioctl through CGo
	cPath := C.CString(path)
	defer C.free(unsafe.Pointer(cPath))

	if C.quota_set_path(cPath, C.uint32_t(id)) != 0 {
		return fmt.Errorf("Failed to set project id '%d' on '%s'", id, path)
	}

	return nil
}
func DeleteProject(path string, id uint32) error {
	// Unset the project from the path
	err := SetProject(path, 0)
	if err != nil {
		return err
	}

	// Unset the quota on the project
	err = SetProjectQuota(path, id, 0)
	if err != nil {
		return err
	}

	return nil
}
func GetProjectUsage(path string, id uint32) (int64, error) {
	// Get the backing device
	devPath, err := devForPath(path)
	if err != nil {
		return -1, err
	}

	// Call quotactl through CGo
	cDevPath := C.CString(devPath)
	defer C.free(unsafe.Pointer(cDevPath))

	size := C.quota_get_usage(cDevPath, C.uint32_t(id))
	if size < 0 {
		return -1, fmt.Errorf("Failed to get project consumption for id '%d' on '%s'", id, path)
	}

	return int64(size), nil
}
func SetProjectQuota(path string, id uint32, bytes int64) error {
	// Get the backing device
	devPath, err := devForPath(path)
	if err != nil {
		return err
	}

	// Call quotactl through CGo
	cDevPath := C.CString(devPath)
	defer C.free(unsafe.Pointer(cDevPath))

	if C.quota_set(cDevPath, C.uint32_t(id), C.int(bytes/1024)) != 0 {
		return fmt.Errorf("Failed to set project quota for id '%d' on '%s'", id, path)
	}

	return nil
}
func backupLoadByName(s *state.State, project, name string) (*backup, error) {
	// Get the backup database record
	args, err := s.Cluster.ContainerGetBackup(project, name)
	if err != nil {
		return nil, errors.Wrap(err, "Load backup from database")
	}

	// Load the container it belongs to
	c, err := containerLoadById(s, args.ContainerID)
	if err != nil {
		return nil, errors.Wrap(err, "Load container from database")
	}

	// Return the backup struct
	return &backup{
		state:            s,
		container:        c,
		id:               args.ID,
		name:             name,
		creationDate:     args.CreationDate,
		expiryDate:       args.ExpiryDate,
		containerOnly:    args.ContainerOnly,
		optimizedStorage: args.OptimizedStorage,
	}, nil
}
func backupCreate(s *state.State, args db.ContainerBackupArgs, sourceContainer container) error {
	// Create the database entry
	err := s.Cluster.ContainerBackupCreate(args)
	if err != nil {
		if err == db.ErrAlreadyDefined {
			return fmt.Errorf("backup '%s' already exists", args.Name)
		}

		return errors.Wrap(err, "Insert backup info into database")
	}

	// Get the backup struct
	b, err := backupLoadByName(s, sourceContainer.Project(), args.Name)
	if err != nil {
		return errors.Wrap(err, "Load backup object")
	}

	// Now create the empty snapshot
	err = sourceContainer.Storage().ContainerBackupCreate(*b, sourceContainer)
	if err != nil {
		s.Cluster.ContainerBackupRemove(args.Name)
		return errors.Wrap(err, "Backup storage")
	}

	return nil
}
func (b *backup) Rename(newName string) error {
	oldBackupPath := shared.VarPath("backups", b.name)
	newBackupPath := shared.VarPath("backups", newName)

	// Create the new backup path
	backupsPath := shared.VarPath("backups", b.container.Name())
	if !shared.PathExists(backupsPath) {
		err := os.MkdirAll(backupsPath, 0700)
		if err != nil {
			return err
		}
	}

	// Rename the backup directory
	err := os.Rename(oldBackupPath, newBackupPath)
	if err != nil {
		return err
	}

	// Check if we can remove the container directory
	empty, _ := shared.PathIsEmpty(backupsPath)
	if empty {
		err := os.Remove(backupsPath)
		if err != nil {
			return err
		}
	}

	// Rename the database record
	err = b.state.Cluster.ContainerBackupRename(b.name, newName)
	if err != nil {
		return err
	}

	return nil
}
func (b *backup) Delete() error {
	return doBackupDelete(b.state, b.name, b.container.Name())
}
func backupFixStoragePool(c *db.Cluster, b backupInfo, useDefaultPool bool) error {
	var poolName string

	if useDefaultPool {
		// Get the default profile
		_, profile, err := c.ProfileGet("default", "default")
		if err != nil {
			return err
		}

		_, v, err := shared.GetRootDiskDevice(profile.Devices)
		if err != nil {
			return err
		}

		poolName = v["pool"]
	} else {
		poolName = b.Pool
	}

	// Get the default's profile pool
	_, pool, err := c.StoragePoolGet(poolName)
	if err != nil {
		return err
	}

	f := func(path string) error {
		// Read in the backup.yaml file.
		backup, err := slurpBackupFile(path)
		if err != nil {
			return err
		}

		rootDiskDeviceFound := false

		// Change the pool in the backup.yaml
		backup.Pool = pool
		if backup.Container.Devices != nil {
			devName, _, err := shared.GetRootDiskDevice(backup.Container.Devices)
			if err == nil {
				backup.Container.Devices[devName]["pool"] = poolName
				rootDiskDeviceFound = true
			}
		}

		if backup.Container.ExpandedDevices != nil {
			devName, _, err := shared.GetRootDiskDevice(backup.Container.ExpandedDevices)
			if err == nil {
				backup.Container.ExpandedDevices[devName]["pool"] = poolName
				rootDiskDeviceFound = true
			}
		}

		if !rootDiskDeviceFound {
			return fmt.Errorf("No root device could be found")
		}

		file, err := os.Create(path)
		if err != nil {
			return err
		}
		defer file.Close()

		data, err := yaml.Marshal(&backup)
		if err != nil {
			return err
		}

		_, err = file.Write(data)
		if err != nil {
			return err
		}

		return nil
	}

	err = f(shared.VarPath("storage-pools", pool.Name, "containers", b.Name, "backup.yaml"))
	if err != nil {
		return err
	}

	for _, snap := range b.Snapshots {
		err = f(shared.VarPath("storage-pools", pool.Name, "containers-snapshots", b.Name, snap,
			"backup.yaml"))
		if err != nil {
			return err
		}
	}
	return nil
}
func Count(tx *sql.Tx, table string, where string, args ...interface{}) (int, error) {
	stmt := fmt.Sprintf("SELECT COUNT(*) FROM %s", table)
	if where != "" {
		stmt += fmt.Sprintf(" WHERE %s", where)
	}
	rows, err := tx.Query(stmt, args...)
	if err != nil {
		return -1, err
	}
	defer rows.Close()

	// For sanity, make sure we read one and only one row.
	if !rows.Next() {
		return -1, fmt.Errorf("no rows returned")
	}
	var count int
	err = rows.Scan(&count)
	if err != nil {
		return -1, fmt.Errorf("failed to scan count column")
	}
	if rows.Next() {
		return -1, fmt.Errorf("more than one row returned")
	}
	err = rows.Err()
	if err != nil {
		return -1, err
	}

	return count, nil
}
func CountAll(tx *sql.Tx) (map[string]int, error) {
	tables, err := SelectStrings(tx, "SELECT name FROM sqlite_master WHERE type = 'table'")
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch table names")
	}

	counts := map[string]int{}
	for _, table := range tables {
		count, err := Count(tx, table, "")
		if err != nil {
			return nil, errors.Wrapf(err, "Failed to count rows of %s", table)
		}
		counts[table] = count
	}

	return counts, nil
}
func InitTLSConfig() *tls.Config {
	return &tls.Config{
		MinVersion: tls.VersionTLS12,
		CipherSuites: []uint16{
			tls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,
			tls.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,
			tls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
			tls.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,
			tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,
			tls.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,
			tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
			tls.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,
		},
		PreferServerCipherSuites: true,
	}
}
func (s *storageLvm) copyContainerThinpool(target container, source container, readonly bool) error {
	err := s.createSnapshotContainer(target, source, readonly)
	if err != nil {
		logger.Errorf("Error creating snapshot LV for copy: %s", err)
		return err
	}

	// Generate a new xfs's UUID
	LVFilesystem := s.getLvmFilesystem()
	poolName := s.getOnDiskPoolName()
	containerName := target.Name()
	containerLvmName := containerNameToLVName(containerName)
	containerLvDevPath := getLvmDevPath(target.Project(), poolName,
		storagePoolVolumeAPIEndpointContainers, containerLvmName)

	// If btrfstune sees two btrfs filesystems with the same UUID it
	// gets confused and wants both of them unmounted. So unmount
	// the source as well.
	if LVFilesystem == "btrfs" {
		ourUmount, err := s.ContainerUmount(source, source.Path())
		if err != nil {
			return err
		}

		if ourUmount {
			defer s.ContainerMount(source)
		}
	}

	msg, err := fsGenerateNewUUID(LVFilesystem, containerLvDevPath)
	if err != nil {
		logger.Errorf("Failed to create new \"%s\" UUID for container \"%s\" on storage pool \"%s\": %s", LVFilesystem, containerName, s.pool.Name, msg)
		return err
	}

	return nil
}
func (s *storageLvm) copyContainerLv(target container, source container, readonly bool, refresh bool) error {
	exists, err := storageLVExists(getLvmDevPath(target.Project(), s.getOnDiskPoolName(),
		storagePoolVolumeAPIEndpointContainers, containerNameToLVName(target.Name())))
	if err != nil {
		return err
	}

	// Only create container/snapshot if it doesn't already exist
	if !exists {
		err := s.ContainerCreate(target)
		if err != nil {
			return err
		}
	}

	targetName := target.Name()
	targetStart, err := target.StorageStart()
	if err != nil {
		return err
	}
	if targetStart {
		defer target.StorageStop()
	}

	sourceName := source.Name()
	sourceStart, err := source.StorageStart()
	if err != nil {
		return err
	}
	if sourceStart {
		defer source.StorageStop()
	}

	sourcePool, err := source.StoragePool()
	if err != nil {
		return err
	}
	sourceContainerMntPoint := getContainerMountPoint(source.Project(), sourcePool, sourceName)
	if source.IsSnapshot() {
		sourceContainerMntPoint = getSnapshotMountPoint(source.Project(), sourcePool, sourceName)
	}

	targetContainerMntPoint := getContainerMountPoint(target.Project(), s.pool.Name, targetName)
	if target.IsSnapshot() {
		targetContainerMntPoint = getSnapshotMountPoint(source.Project(), s.pool.Name, targetName)
	}

	if source.IsRunning() {
		err = source.Freeze()
		if err != nil {
			return err
		}
		defer source.Unfreeze()
	}

	bwlimit := s.pool.Config["rsync.bwlimit"]
	output, err := rsyncLocalCopy(sourceContainerMntPoint, targetContainerMntPoint, bwlimit)
	if err != nil {
		return fmt.Errorf("failed to rsync container: %s: %s", string(output), err)
	}

	if readonly {
		targetLvmName := containerNameToLVName(targetName)
		poolName := s.getOnDiskPoolName()
		output, err := shared.TryRunCommand("lvchange", "-pr", fmt.Sprintf("%s/%s_%s", poolName, storagePoolVolumeAPIEndpointContainers, targetLvmName))
		if err != nil {
			logger.Errorf("Failed to make LVM snapshot \"%s\" read-write: %s", targetName, output)
			return err
		}
	}

	return nil
}
func (s *storageLvm) copyContainer(target container, source container, refresh bool) error {
	targetPool, err := target.StoragePool()
	if err != nil {
		return err
	}

	targetContainerMntPoint := getContainerMountPoint(target.Project(), targetPool, target.Name())
	err = createContainerMountpoint(targetContainerMntPoint, target.Path(), target.IsPrivileged())
	if err != nil {
		return err
	}

	sourcePool, err := source.StoragePool()
	if err != nil {
		return err
	}

	if s.useThinpool && targetPool == sourcePool && !refresh {
		// If the storage pool uses a thinpool we can have snapshots of
		// snapshots.
		err = s.copyContainerThinpool(target, source, false)
	} else {
		// If the storage pools does not use a thinpool we need to
		// perform full copies.
		err = s.copyContainerLv(target, source, false, refresh)
	}
	if err != nil {
		return err
	}

	err = target.TemplateApply("copy")
	if err != nil {
		return err
	}

	return nil
}
func (s *storageLvm) copyVolume(sourcePool string, source string) error {
	targetMntPoint := getStoragePoolVolumeMountPoint(s.pool.Name, s.volume.Name)

	err := os.MkdirAll(targetMntPoint, 0711)
	if err != nil {
		return err
	}

	if s.useThinpool && sourcePool == s.pool.Name {
		err = s.copyVolumeThinpool(source, s.volume.Name, false)
	} else {
		err = s.copyVolumeLv(sourcePool, source, s.volume.Name, false)
	}
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolSimpleStreams) GetPrivateImage(fingerprint string, secret string) (*api.Image, string, error) {
	return nil, "", fmt.Errorf("Private images aren't supported by the simplestreams protocol")
}
func (r *ProtocolSimpleStreams) GetPrivateImageFile(fingerprint string, secret string, req ImageFileRequest) (*ImageFileResponse, error) {
	return nil, fmt.Errorf("Private images aren't supported by the simplestreams protocol")
}
func (r *ProtocolSimpleStreams) GetImageAliasNames() ([]string, error) {
	// Get all the images from simplestreams
	aliases, err := r.ssClient.ListAliases()
	if err != nil {
		return nil, err
	}

	// And now extract just the names
	names := []string{}
	for _, alias := range aliases {
		names = append(names, alias.Name)
	}

	return names, nil
}
func ProtoRecv(ws *websocket.Conn, msg proto.Message) error {
	mt, r, err := ws.NextReader()
	if err != nil {
		return err
	}

	if mt != websocket.BinaryMessage {
		return fmt.Errorf("Only binary messages allowed")
	}

	buf, err := ioutil.ReadAll(r)
	if err != nil {
		return err
	}

	err = proto.Unmarshal(buf, msg)
	if err != nil {
		return err
	}

	return nil
}
func ProtoSend(ws *websocket.Conn, msg proto.Message) error {
	w, err := ws.NextWriter(websocket.BinaryMessage)
	if err != nil {
		return err
	}
	defer w.Close()

	data, err := proto.Marshal(msg)
	if err != nil {
		return err
	}

	err = shared.WriteAll(w, data)
	if err != nil {
		return err
	}

	return nil
}
func ProtoSendControl(ws *websocket.Conn, err error) {
	message := ""
	if err != nil {
		message = err.Error()
	}

	msg := MigrationControl{
		Success: proto.Bool(err == nil),
		Message: proto.String(message),
	}

	ProtoSend(ws, &msg)
}
func (er stdinMirror) Read(p []byte) (int, error) {
	n, err := er.r.Read(p)

	v := rune(p[0])
	if v == '\u0001' && !*er.foundEscape {
		*er.foundEscape = true
		return 0, err
	}

	if v == 'q' && *er.foundEscape {
		select {
		case er.consoleDisconnect <- true:
			return 0, err
		default:
			return 0, err
		}
	}

	*er.foundEscape = false
	return n, err
}
func doContainersGetFromNode(project, node string, cert *shared.CertInfo) ([]api.Container, error) {
	f := func() ([]api.Container, error) {
		client, err := cluster.Connect(node, cert, true)
		if err != nil {
			return nil, errors.Wrapf(err, "Failed to connect to node %s", node)
		}

		client = client.UseProject(project)

		containers, err := client.GetContainers()
		if err != nil {
			return nil, errors.Wrapf(err, "Failed to get containers from node %s", node)
		}

		return containers, nil
	}

	timeout := time.After(30 * time.Second)
	done := make(chan struct{})

	var containers []api.Container
	var err error

	go func() {
		containers, err = f()
		done <- struct{}{}
	}()

	select {
	case <-timeout:
		err = fmt.Errorf("Timeout getting containers from node %s", node)
	case <-done:
	}

	return containers, err
}
func Retry(f func() error) error {
	// TODO: the retry loop should be configurable.
	var err error
	for i := 0; i < 5; i++ {
		err = f()
		if err != nil {
			logger.Debugf("Database error: %#v", err)

			if IsRetriableError(err) {
				logger.Debugf("Retry failed db interaction (%v)", err)
				time.Sleep(250 * time.Millisecond)
				continue
			}
		}
		break
	}
	return err
}
func IsRetriableError(err error) bool {
	err = errors.Cause(err)

	if err == nil {
		return false
	}
	if err == sqlite3.ErrLocked || err == sqlite3.ErrBusy {
		return true
	}

	if strings.Contains(err.Error(), "database is locked") {
		return true
	}
	if strings.Contains(err.Error(), "bad connection") {
		return true
	}

	// Despite the description this is usually a lost leadership error.
	if strings.Contains(err.Error(), "disk I/O error") {
		return true
	}

	return false
}
func AppArmorProfile() string {
	contents, err := ioutil.ReadFile("/proc/self/attr/current")
	if err == nil {
		return strings.TrimSpace(string(contents))
	}

	return ""
}
func (s *storageBtrfs) StoragePoolVolumeCreate() error {
	logger.Infof("Creating BTRFS storage volume \"%s\" on storage pool \"%s\"", s.volume.Name, s.pool.Name)

	_, err := s.StoragePoolMount()
	if err != nil {
		return err
	}

	isSnapshot := shared.IsSnapshot(s.volume.Name)

	// Create subvolume path on the storage pool.
	var customSubvolumePath string

	if isSnapshot {
		customSubvolumePath = s.getCustomSnapshotSubvolumePath(s.pool.Name)
	} else {
		customSubvolumePath = s.getCustomSubvolumePath(s.pool.Name)
	}

	if !shared.PathExists(customSubvolumePath) {
		err := os.MkdirAll(customSubvolumePath, 0700)
		if err != nil {
			return err
		}
	}

	// Create subvolume.
	var customSubvolumeName string

	if isSnapshot {
		customSubvolumeName = getStoragePoolVolumeSnapshotMountPoint(s.pool.Name, s.volume.Name)
	} else {
		customSubvolumeName = getStoragePoolVolumeMountPoint(s.pool.Name, s.volume.Name)
	}

	err = btrfsSubVolumeCreate(customSubvolumeName)
	if err != nil {
		return err
	}

	// apply quota
	if s.volume.Config["size"] != "" {
		size, err := shared.ParseByteSizeString(s.volume.Config["size"])
		if err != nil {
			return err
		}

		err = s.StorageEntitySetQuota(storagePoolVolumeTypeCustom, size, nil)
		if err != nil {
			return err
		}
	}

	logger.Infof("Created BTRFS storage volume \"%s\" on storage pool \"%s\"", s.volume.Name, s.pool.Name)
	return nil
}
func (s *storageBtrfs) ContainerStorageReady(container container) bool {
	containerMntPoint := getContainerMountPoint(container.Project(), s.pool.Name, container.Name())
	return isBtrfsSubVolume(containerMntPoint)
}
func (s *storageBtrfs) ContainerCreateFromImage(container container, fingerprint string, tracker *ioprogress.ProgressTracker) error {
	logger.Debugf("Creating BTRFS storage volume for container \"%s\" on storage pool \"%s\"", s.volume.Name, s.pool.Name)

	source := s.pool.Config["source"]
	if source == "" {
		return fmt.Errorf("no \"source\" property found for the storage pool")
	}

	_, err := s.StoragePoolMount()
	if err != nil {
		return errors.Wrap(err, "Failed to mount storage pool")
	}

	// We can only create the btrfs subvolume under the mounted storage
	// pool. The on-disk layout for containers on a btrfs storage pool will
	// thus be
	// ${LXD_DIR}/storage-pools/<pool>/containers/. The btrfs tool will
	// complain if the intermediate path does not exist, so create it if it
	// doesn't already.
	containerSubvolumePath := s.getContainerSubvolumePath(s.pool.Name)
	if !shared.PathExists(containerSubvolumePath) {
		err := os.MkdirAll(containerSubvolumePath, containersDirMode)
		if err != nil {
			return errors.Wrap(err, "Failed to create volume directory")
		}
	}

	// Mountpoint of the image:
	// ${LXD_DIR}/images/<fingerprint>
	imageMntPoint := getImageMountPoint(s.pool.Name, fingerprint)
	imageStoragePoolLockID := getImageCreateLockID(s.pool.Name, fingerprint)
	lxdStorageMapLock.Lock()
	if waitChannel, ok := lxdStorageOngoingOperationMap[imageStoragePoolLockID]; ok {
		lxdStorageMapLock.Unlock()
		if _, ok := <-waitChannel; ok {
			logger.Warnf("Received value over semaphore, this should not have happened")
		}
	} else {
		lxdStorageOngoingOperationMap[imageStoragePoolLockID] = make(chan bool)
		lxdStorageMapLock.Unlock()

		var imgerr error
		if !shared.PathExists(imageMntPoint) || !isBtrfsSubVolume(imageMntPoint) {
			imgerr = s.ImageCreate(fingerprint, tracker)
		}

		lxdStorageMapLock.Lock()
		if waitChannel, ok := lxdStorageOngoingOperationMap[imageStoragePoolLockID]; ok {
			close(waitChannel)
			delete(lxdStorageOngoingOperationMap, imageStoragePoolLockID)
		}
		lxdStorageMapLock.Unlock()

		if imgerr != nil {
			return errors.Wrap(imgerr, "Failed to create image volume")
		}
	}

	// Create a rw snapshot at
	// ${LXD_DIR}/storage-pools/<pool>/containers/<name>
	// from the mounted ro image snapshot mounted at
	// ${LXD_DIR}/storage-pools/<pool>/images/<fingerprint>
	containerSubvolumeName := getContainerMountPoint(container.Project(), s.pool.Name, container.Name())
	err = s.btrfsPoolVolumesSnapshot(imageMntPoint, containerSubvolumeName, false, false)
	if err != nil {
		return errors.Wrap(err, "Failed to storage pool volume snapshot")
	}

	// Create the mountpoint for the container at:
	// ${LXD_DIR}/containers/<name>
	err = createContainerMountpoint(containerSubvolumeName, container.Path(), container.IsPrivileged())
	if err != nil {
		return errors.Wrap(err, "Failed to create container mountpoint")
	}

	logger.Debugf("Created BTRFS storage volume for container \"%s\" on storage pool \"%s\"", s.volume.Name, s.pool.Name)
	err = container.TemplateApply("create")
	if err != nil {
		return errors.Wrap(err, "Failed to apply container template")
	}
	return nil
}
func (s *storageBtrfs) ContainerSnapshotRename(snapshotContainer container, newName string) error {
	logger.Debugf("Renaming BTRFS storage volume for snapshot \"%s\" from %s to %s", s.volume.Name, s.volume.Name, newName)

	// The storage pool must be mounted.
	_, err := s.StoragePoolMount()
	if err != nil {
		return err
	}

	// Unmount the snapshot if it is mounted otherwise we'll get EBUSY.
	// Rename the subvolume on the storage pool.
	oldSnapshotSubvolumeName := getSnapshotMountPoint(snapshotContainer.Project(), s.pool.Name, snapshotContainer.Name())
	newSnapshotSubvolumeName := getSnapshotMountPoint(snapshotContainer.Project(), s.pool.Name, newName)
	err = os.Rename(oldSnapshotSubvolumeName, newSnapshotSubvolumeName)
	if err != nil {
		return err
	}

	logger.Debugf("Renamed BTRFS storage volume for snapshot \"%s\" from %s to %s", s.volume.Name, s.volume.Name, newName)
	return nil
}
func (s *storageBtrfs) ContainerSnapshotCreateEmpty(snapshotContainer container) error {
	logger.Debugf("Creating empty BTRFS storage volume for snapshot \"%s\" on storage pool \"%s\"", s.volume.Name, s.pool.Name)

	// Mount the storage pool.
	_, err := s.StoragePoolMount()
	if err != nil {
		return err
	}

	// Create the snapshot subvole path on the storage pool.
	sourceName, _, _ := containerGetParentAndSnapshotName(snapshotContainer.Name())
	snapshotSubvolumePath := getSnapshotSubvolumePath(snapshotContainer.Project(), s.pool.Name, sourceName)
	snapshotSubvolumeName := getSnapshotMountPoint(snapshotContainer.Project(), s.pool.Name, snapshotContainer.Name())
	if !shared.PathExists(snapshotSubvolumePath) {
		err := os.MkdirAll(snapshotSubvolumePath, containersDirMode)
		if err != nil {
			return err
		}
	}

	err = btrfsSubVolumeCreate(snapshotSubvolumeName)
	if err != nil {
		return err
	}

	snapshotMntPointSymlinkTarget := shared.VarPath("storage-pools", s.pool.Name, "containers-snapshots", projectPrefix(snapshotContainer.Project(), sourceName))
	snapshotMntPointSymlink := shared.VarPath("snapshots", projectPrefix(snapshotContainer.Project(), sourceName))
	if !shared.PathExists(snapshotMntPointSymlink) {
		err := createContainerMountpoint(snapshotMntPointSymlinkTarget, snapshotMntPointSymlink, snapshotContainer.IsPrivileged())
		if err != nil {
			return err
		}
	}

	logger.Debugf("Created empty BTRFS storage volume for snapshot \"%s\" on storage pool \"%s\"", s.volume.Name, s.pool.Name)
	return nil
}
func btrfsSubVolumesDelete(subvol string) error {
	// Delete subsubvols.
	subsubvols, err := btrfsSubVolumesGet(subvol)
	if err != nil {
		return err
	}
	sort.Sort(sort.Reverse(sort.StringSlice(subsubvols)))

	for _, subsubvol := range subsubvols {
		err := btrfsSubVolumeDelete(path.Join(subvol, subsubvol))
		if err != nil {
			return err
		}
	}

	// Delete the subvol itself
	err = btrfsSubVolumeDelete(subvol)
	if err != nil {
		return err
	}

	return nil
}
func isBtrfsSubVolume(subvolPath string) bool {
	fs := syscall.Stat_t{}
	err := syscall.Lstat(subvolPath, &fs)
	if err != nil {
		return false
	}

	// Check if BTRFS_FIRST_FREE_OBJECTID
	if fs.Ino != 256 {
		return false
	}

	return true
}
func SelectConfig(tx *sql.Tx, table string, where string, args ...interface{}) (map[string]string, error) {
	query := fmt.Sprintf("SELECT key, value FROM %s", table)
	if where != "" {
		query += fmt.Sprintf(" WHERE %s", where)
	}

	rows, err := tx.Query(query, args...)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	values := map[string]string{}
	for rows.Next() {
		var key string
		var value string

		err := rows.Scan(&key, &value)
		if err != nil {
			return nil, err
		}

		values[key] = value
	}

	err = rows.Err()
	if err != nil {
		return nil, err
	}

	return values, nil
}
func UpdateConfig(tx *sql.Tx, table string, values map[string]string) error {
	changes := map[string]string{}
	deletes := []string{}

	for key, value := range values {
		if value == "" {
			deletes = append(deletes, key)
			continue
		}
		changes[key] = value
	}

	err := upsertConfig(tx, table, changes)
	if err != nil {
		return errors.Wrap(err, "updating values failed")
	}

	err = deleteConfig(tx, table, deletes)
	if err != nil {
		return errors.Wrap(err, "deleting values failed")
	}

	return nil
}
func deleteConfig(tx *sql.Tx, table string, keys []string) error {
	n := len(keys)

	if n == 0 {
		return nil // Nothing to delete.
	}

	query := fmt.Sprintf("DELETE FROM %s WHERE key IN %s", table, Params(n))
	values := make([]interface{}, n)
	for i, key := range keys {
		values[i] = key
	}
	_, err := tx.Exec(query, values...)
	return err
}
func FormatSection(header string, content string) string {
	out := ""

	// Add section header
	if header != "" {
		out += header + ":\n"
	}

	// Indent the content
	for _, line := range strings.Split(content, "\n") {
		if line != "" {
			out += "  "
		}

		out += line + "\n"
	}

	if header != "" {
		// Section separator (when rendering a full section
		out += "\n"
	} else {
		// Remove last newline when rendering partial section
		out = strings.TrimSuffix(out, "\n")
	}

	return out
}
func (r *ProtocolLXD) GetProjects() ([]api.Project, error) {
	if !r.HasExtension("projects") {
		return nil, fmt.Errorf("The server is missing the required \"projects\" API extension")
	}

	projects := []api.Project{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/projects?recursion=1", nil, "", &projects)
	if err != nil {
		return nil, err
	}

	return projects, nil
}
func (r *ProtocolLXD) GetProject(name string) (*api.Project, string, error) {
	if !r.HasExtension("projects") {
		return nil, "", fmt.Errorf("The server is missing the required \"projects\" API extension")
	}

	project := api.Project{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/projects/%s", url.QueryEscape(name)), nil, "", &project)
	if err != nil {
		return nil, "", err
	}

	return &project, etag, nil
}
func (r *ProtocolLXD) CreateProject(project api.ProjectsPost) error {
	if !r.HasExtension("projects") {
		return fmt.Errorf("The server is missing the required \"projects\" API extension")
	}

	// Send the request
	_, _, err := r.query("POST", "/projects", project, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) UpdateProject(name string, project api.ProjectPut, ETag string) error {
	if !r.HasExtension("projects") {
		return fmt.Errorf("The server is missing the required \"projects\" API extension")
	}

	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/projects/%s", url.QueryEscape(name)), project, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) RenameProject(name string, project api.ProjectPost) (Operation, error) {
	if !r.HasExtension("projects") {
		return nil, fmt.Errorf("The server is missing the required \"projects\" API extension")
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/projects/%s", url.QueryEscape(name)), project, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (er Reader) Read(p []byte) (int, error) {
again:
	n, err := er.Reader.Read(p)
	if err == nil {
		return n, nil
	}

	// keep retrying on EAGAIN
	errno, ok := shared.GetErrno(err)
	if ok && (errno == syscall.EAGAIN || errno == syscall.EINTR) {
		goto again
	}

	return n, err
}
func (ew Writer) Write(p []byte) (int, error) {
again:
	n, err := ew.Writer.Write(p)
	if err == nil {
		return n, nil
	}

	// keep retrying on EAGAIN
	errno, ok := shared.GetErrno(err)
	if ok && (errno == syscall.EAGAIN || errno == syscall.EINTR) {
		goto again
	}

	return n, err
}
func NewCanceler() *Canceler {
	c := Canceler{}

	c.lock.Lock()
	c.reqChCancel = make(map[*http.Request]chan struct{})
	c.lock.Unlock()

	return &c
}
func (c *Canceler) Cancelable() bool {
	c.lock.Lock()
	length := len(c.reqChCancel)
	c.lock.Unlock()

	return length > 0
}
func (c *Canceler) Cancel() error {
	if !c.Cancelable() {
		return fmt.Errorf("This operation can't be canceled at this time")
	}

	c.lock.Lock()
	for req, ch := range c.reqChCancel {
		close(ch)
		delete(c.reqChCancel, req)
	}
	c.lock.Unlock()

	return nil
}
func CancelableDownload(c *Canceler, client *http.Client, req *http.Request) (*http.Response, chan bool, error) {
	chDone := make(chan bool)
	chCancel := make(chan struct{})
	if c != nil {
		c.lock.Lock()
		c.reqChCancel[req] = chCancel
		c.lock.Unlock()
	}
	req.Cancel = chCancel

	go func() {
		<-chDone
		if c != nil {
			c.lock.Lock()
			delete(c.reqChCancel, req)
			c.lock.Unlock()
		}
	}()

	resp, err := client.Do(req)
	return resp, chDone, err
}
func clusterGet(d *Daemon, r *http.Request) Response {
	name := ""
	err := d.cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		name, err = tx.NodeName()
		return err
	})
	if err != nil {
		return SmartError(err)
	}

	// If the name is set to the hard-coded default node name, then
	// clustering is not enabled.
	if name == "none" {
		name = ""
	}

	memberConfig, err := clusterGetMemberConfig(d.cluster)
	if err != nil {
		return SmartError(err)
	}

	cluster := api.Cluster{
		ServerName:   name,
		Enabled:      name != "",
		MemberConfig: memberConfig,
	}

	return SyncResponseETag(true, cluster, cluster)
}
func clusterGetMemberConfig(cluster *db.Cluster) ([]api.ClusterMemberConfigKey, error) {
	var pools map[string]map[string]string
	var networks map[string]map[string]string

	keys := []api.ClusterMemberConfigKey{}

	err := cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error

		pools, err = tx.StoragePoolsNodeConfig()
		if err != nil {
			return errors.Wrapf(err, "Failed to fetch storage pools configuration")
		}

		networks, err = tx.NetworksNodeConfig()
		if err != nil {
			return errors.Wrapf(err, "Failed to fetch networks configuration")
		}

		return nil
	})
	if err != nil {
		return nil, err
	}

	for pool, config := range pools {
		for key := range config {
			if strings.HasPrefix(key, "volatile.") {
				continue
			}

			key := api.ClusterMemberConfigKey{
				Entity:      "storage-pool",
				Name:        pool,
				Key:         key,
				Description: fmt.Sprintf("\"%s\" property for storage pool \"%s\"", key, pool),
			}
			keys = append(keys, key)
		}
	}

	for network, config := range networks {
		for key := range config {
			if strings.HasPrefix(key, "volatile.") {
				continue
			}

			key := api.ClusterMemberConfigKey{
				Entity:      "network",
				Name:        network,
				Key:         key,
				Description: fmt.Sprintf("\"%s\" property for network \"%s\"", key, network),
			}
			keys = append(keys, key)
		}
	}

	return keys, nil
}
func clusterPutDisable(d *Daemon) Response {
	// Close the cluster database
	err := d.cluster.Close()
	if err != nil {
		return SmartError(err)
	}

	// Update our TLS configuration using our original certificate.
	for _, suffix := range []string{"crt", "key", "ca"} {
		path := filepath.Join(d.os.VarDir, "cluster."+suffix)
		if !shared.PathExists(path) {
			continue
		}
		err := os.Remove(path)
		if err != nil {
			return InternalError(err)
		}
	}
	cert, err := util.LoadCert(d.os.VarDir)
	if err != nil {
		return InternalError(errors.Wrap(err, "failed to parse node certificate"))
	}

	// Reset the cluster database and make it local to this node.
	d.endpoints.NetworkUpdateCert(cert)
	err = d.gateway.Reset(cert)
	if err != nil {
		return SmartError(err)
	}

	// Re-open the cluster database
	address, err := node.HTTPSAddress(d.db)
	if err != nil {
		return SmartError(err)
	}
	store := d.gateway.ServerStore()
	d.cluster, err = db.OpenCluster(
		"db.bin", store, address, "/unused/db/dir",
		d.config.DqliteSetupTimeout,
		dqlite.WithDialFunc(d.gateway.DialFunc()),
		dqlite.WithContext(d.gateway.Context()),
	)
	if err != nil {
		return SmartError(err)
	}

	// Stop the clustering tasks
	d.stopClusterTasks()

	// Remove the cluster flag from the agent
	version.UserAgentFeatures(nil)

	return EmptySyncResponse
}
func tryClusterRebalance(d *Daemon) error {
	leader, err := d.gateway.LeaderAddress()
	if err != nil {
		// This is not a fatal error, so let's just log it.
		return errors.Wrap(err, "failed to get current leader node")
	}
	cert := d.endpoints.NetworkCert()
	client, err := cluster.Connect(leader, cert, true)
	if err != nil {
		return errors.Wrap(err, "failed to connect to leader node")
	}
	_, _, err = client.RawQuery("POST", "/internal/cluster/rebalance", nil, "")
	if err != nil {
		return errors.Wrap(err, "request to rebalance cluster failed")
	}
	return nil
}
func internalClusterPostRebalance(d *Daemon, r *http.Request) Response {
	// Redirect all requests to the leader, which is the one with with
	// up-to-date knowledge of what nodes are part of the raft cluster.
	localAddress, err := node.ClusterAddress(d.db)
	if err != nil {
		return SmartError(err)
	}
	leader, err := d.gateway.LeaderAddress()
	if err != nil {
		return InternalError(err)
	}
	if localAddress != leader {
		logger.Debugf("Redirect cluster rebalance request to %s", leader)
		url := &url.URL{
			Scheme: "https",
			Path:   "/internal/cluster/rebalance",
			Host:   leader,
		}
		return SyncResponseRedirect(url.String())
	}

	logger.Debugf("Rebalance cluster")

	// Check if we have a spare node to promote.
	address, nodes, err := cluster.Rebalance(d.State(), d.gateway)
	if err != nil {
		return SmartError(err)
	}
	if address == "" {
		return SyncResponse(true, nil) // Nothing to change
	}

	// Tell the node to promote itself.
	post := &internalClusterPostPromoteRequest{}
	for _, node := range nodes {
		post.RaftNodes = append(post.RaftNodes, internalRaftNode{
			ID:      node.ID,
			Address: node.Address,
		})
	}

	cert := d.endpoints.NetworkCert()
	client, err := cluster.Connect(address, cert, false)
	if err != nil {
		return SmartError(err)
	}
	_, _, err = client.RawQuery("POST", "/internal/cluster/promote", post, "")
	if err != nil {
		return SmartError(err)
	}

	return SyncResponse(true, nil)
}
func internalClusterPostPromote(d *Daemon, r *http.Request) Response {
	req := internalClusterPostPromoteRequest{}

	// Parse the request
	err := json.NewDecoder(r.Body).Decode(&req)
	if err != nil {
		return BadRequest(err)
	}

	// Sanity checks
	if len(req.RaftNodes) == 0 {
		return BadRequest(fmt.Errorf("No raft nodes provided"))
	}

	nodes := make([]db.RaftNode, len(req.RaftNodes))
	for i, node := range req.RaftNodes {
		nodes[i].ID = node.ID
		nodes[i].Address = node.Address
	}
	err = cluster.Promote(d.State(), d.gateway, nodes)
	if err != nil {
		return SmartError(err)
	}

	return SyncResponse(true, nil)
}
func Filters(pkg *ast.Package, entity string) [][]string {
	objects := pkg.Scope.Objects
	filters := [][]string{}

	prefix := fmt.Sprintf("%sObjectsBy", entity)

	for name := range objects {
		if !strings.HasPrefix(name, prefix) {
			continue
		}
		rest := name[len(prefix):]
		filters = append(filters, strings.Split(rest, "And"))
	}

	sort.SliceStable(filters, func(i, j int) bool {
		return len(filters[i]) > len(filters[j])
	})

	return filters
}
func Parse(pkg *ast.Package, name string) (*Mapping, error) {
	str := findStruct(pkg.Scope, name)
	if str == nil {
		return nil, fmt.Errorf("No declaration found for %q", name)
	}

	fields, err := parseStruct(str)
	if err != nil {
		return nil, errors.Wrapf(err, "Failed to parse %q", name)
	}

	m := &Mapping{
		Package: pkg.Name,
		Name:    name,
		Fields:  fields,
	}

	return m, nil
}
func findStruct(scope *ast.Scope, name string) *ast.StructType {
	obj := scope.Lookup(name)
	if obj == nil {
		return nil
	}

	typ, ok := obj.Decl.(*ast.TypeSpec)
	if !ok {
		return nil
	}

	str, ok := typ.Type.(*ast.StructType)
	if !ok {
		return nil
	}

	return str
}
func parseStruct(str *ast.StructType) ([]*Field, error) {
	fields := make([]*Field, 0)

	for _, f := range str.Fields.List {
		if len(f.Names) == 0 {
			// Check if this is a parent struct.
			ident, ok := f.Type.(*ast.Ident)
			if !ok {
				continue
			}

			typ, ok := ident.Obj.Decl.(*ast.TypeSpec)
			if !ok {
				continue
			}

			parentStr, ok := typ.Type.(*ast.StructType)
			if !ok {
				continue
			}

			parentFields, err := parseStruct(parentStr)
			if err != nil {
				return nil, errors.Wrapf(err, "Failed to parse parent struct")
			}
			fields = append(fields, parentFields...)

			continue
		}

		if len(f.Names) != 1 {
			return nil, fmt.Errorf("Expected a single field name, got %q", f.Names)
		}

		field, err := parseField(f)
		if err != nil {
			return nil, err
		}

		fields = append(fields, field)
	}

	return fields, nil
}
func (r *ProtocolLXD) GetProfileNames() ([]string, error) {
	urls := []string{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/profiles", nil, "", &urls)
	if err != nil {
		return nil, err
	}

	// Parse it
	names := []string{}
	for _, url := range urls {
		fields := strings.Split(url, "/profiles/")
		names = append(names, strings.Split(fields[len(fields)-1], "?")[0])
	}

	return names, nil
}
func (r *ProtocolLXD) GetProfiles() ([]api.Profile, error) {
	profiles := []api.Profile{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/profiles?recursion=1", nil, "", &profiles)
	if err != nil {
		return nil, err
	}

	return profiles, nil
}
func (r *ProtocolLXD) GetProfile(name string) (*api.Profile, string, error) {
	profile := api.Profile{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/profiles/%s", url.QueryEscape(name)), nil, "", &profile)
	if err != nil {
		return nil, "", err
	}

	return &profile, etag, nil
}
func (r *ProtocolLXD) CreateProfile(profile api.ProfilesPost) error {
	// Send the request
	_, _, err := r.query("POST", "/profiles", profile, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) UpdateProfile(name string, profile api.ProfilePut, ETag string) error {
	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/profiles/%s", url.QueryEscape(name)), profile, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) RenameProfile(name string, profile api.ProfilePost) error {
	// Send the request
	_, _, err := r.query("POST", fmt.Sprintf("/profiles/%s", url.QueryEscape(name)), profile, "")
	if err != nil {
		return err
	}

	return nil
}
func Load(schema Schema, values map[string]string) (Map, error) {
	m := Map{
		schema: schema,
	}

	// Populate the initial values.
	_, err := m.update(values)
	return m, err
}
func (m *Map) Dump() map[string]interface{} {
	values := map[string]interface{}{}

	for name, key := range m.schema {
		value := m.GetRaw(name)
		if value != key.Default {
			if key.Hidden {
				values[name] = true
			} else {
				values[name] = value
			}
		}
	}

	return values
}
func (m *Map) GetRaw(name string) string {
	key := m.schema.mustGetKey(name)
	value, ok := m.values[name]
	if !ok {
		value = key.Default
	}
	return value
}
func (m *Map) GetString(name string) string {
	m.schema.assertKeyType(name, String)
	return m.GetRaw(name)
}
func (m *Map) GetBool(name string) bool {
	m.schema.assertKeyType(name, Bool)
	return shared.IsTrue(m.GetRaw(name))
}
func (m *Map) GetInt64(name string) int64 {
	m.schema.assertKeyType(name, Int64)
	n, err := strconv.ParseInt(m.GetRaw(name), 10, 64)
	if err != nil {
		panic(fmt.Sprintf("cannot convert to int64: %v", err))
	}
	return n
}
func (m *Map) update(values map[string]string) ([]string, error) {
	// Detect if this is the first time we're setting values. This happens
	// when Load is called.
	initial := m.values == nil

	if initial {
		m.values = make(map[string]string, len(values))
	}

	// Update our keys with the values from the given map, and keep track
	// of which keys actually changed their value.
	errors := ErrorList{}
	names := []string{}
	for name, value := range values {
		changed, err := m.set(name, value, initial)
		if err != nil {
			errors.add(name, value, err.Error())
			continue
		}
		if changed {
			names = append(names, name)
		}
	}
	sort.Strings(names)

	var err error
	if errors.Len() > 0 {
		errors.sort()
		err = errors
	}

	return names, err
}
func (m *Map) set(name string, value string, initial bool) (bool, error) {
	key, ok := m.schema[name]
	if !ok {
		return false, fmt.Errorf("unknown key")
	}

	err := key.validate(value)
	if err != nil {
		return false, err
	}

	// Normalize boolan values, so the comparison below works fine.
	current := m.GetRaw(name)
	if key.Type == Bool {
		value = normalizeBool(value)
		current = normalizeBool(current)
	}

	// Compare the new value with the current one, and return now if they
	// are equal.
	if value == current {
		return false, nil
	}

	// Trigger the Setter if this is not an initial load and the key's
	// schema has declared it.
	if !initial && key.Setter != nil {
		value, err = key.Setter(value)
		if err != nil {
			return false, err
		}
	}

	if value == "" {
		delete(m.values, name)
	} else {
		m.values[name] = value
	}

	return true, nil
}
func DoesSchemaTableExist(tx *sql.Tx) (bool, error) {
	statement := `
SELECT COUNT(name) FROM sqlite_master WHERE type = 'table' AND name = 'schema'
`
	rows, err := tx.Query(statement)
	if err != nil {
		return false, err
	}
	defer rows.Close()

	if !rows.Next() {
		return false, fmt.Errorf("schema table query returned no rows")
	}

	var count int

	err = rows.Scan(&count)
	if err != nil {
		return false, err
	}

	return count == 1, nil
}
func selectSchemaVersions(tx *sql.Tx) ([]int, error) {
	statement := `
SELECT version FROM schema ORDER BY version
`
	return query.SelectIntegers(tx, statement)
}
func selectTablesSQL(tx *sql.Tx) ([]string, error) {
	statement := `
SELECT sql FROM sqlite_master WHERE
  type IN ('table', 'index', 'view') AND
  name != 'schema' AND
  name NOT LIKE 'sqlite_%'
ORDER BY name
`
	return query.SelectStrings(tx, statement)
}
func createSchemaTable(tx *sql.Tx) error {
	statement := `
CREATE TABLE schema (
    id         INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
    version    INTEGER NOT NULL,
    updated_at DATETIME NOT NULL,
    UNIQUE (version)
)
`
	_, err := tx.Exec(statement)
	return err
}
func insertSchemaVersion(tx *sql.Tx, new int) error {
	statement := `
INSERT INTO schema (version, updated_at) VALUES (?, strftime("%s"))
`
	_, err := tx.Exec(statement, new)
	return err
}
func NewState(node *db.Node, cluster *db.Cluster, maas *maas.Controller, os *sys.OS, endpoints *endpoints.Endpoints) *State {
	return &State{
		Node:      node,
		Cluster:   cluster,
		MAAS:      maas,
		OS:        os,
		Endpoints: endpoints,
	}
}
func containerLXCUnload(c *containerLXC) {
	runtime.SetFinalizer(c, nil)
	if c.c != nil {
		c.c.Release()
		c.c = nil
	}
}
func containerLXCInstantiate(s *state.State, args db.ContainerArgs) *containerLXC {
	return &containerLXC{
		state:        s,
		id:           args.ID,
		project:      args.Project,
		name:         args.Name,
		description:  args.Description,
		ephemeral:    args.Ephemeral,
		architecture: args.Architecture,
		cType:        args.Ctype,
		creationDate: args.CreationDate,
		lastUsedDate: args.LastUsedDate,
		profiles:     args.Profiles,
		localConfig:  args.Config,
		localDevices: args.Devices,
		stateful:     args.Stateful,
		node:         args.Node,
		expiryDate:   args.ExpiryDate,
	}
}
func (c *containerLXC) initStorage() error {
	if c.storage != nil {
		return nil
	}

	s, err := storagePoolVolumeContainerLoadInit(c.state, c.Project(), c.Name())
	if err != nil {
		return err
	}

	c.storage = s

	return nil
}
func (c *containerLXC) OnNetworkUp(deviceName string, hostName string) error {
	device := c.expandedDevices[deviceName]
	device["host_name"] = hostName
	return c.setupHostVethDevice(device)
}
func (c *containerLXC) setupHostVethDevice(device types.Device) error {
	// If not already, populate network device with host name.
	if device["host_name"] == "" {
		device["host_name"] = c.getHostInterface(device["name"])
	}

	// Check whether host device resolution succeeded.
	if device["host_name"] == "" {
		return fmt.Errorf("LXC doesn't know about this device and the host_name property isn't set, can't find host side veth name")
	}

	// Refresh tc limits
	err := c.setNetworkLimits(device)
	if err != nil {
		return err
	}

	// Setup static routes to container
	err = c.setNetworkRoutes(device)
	if err != nil {
		return err
	}

	return nil
}
func (c *containerLXC) getLxcState() (lxc.State, error) {
	if c.IsSnapshot() {
		return lxc.StateMap["STOPPED"], nil
	}

	// Load the go-lxc struct
	err := c.initLXC(false)
	if err != nil {
		return lxc.StateMap["STOPPED"], err
	}

	monitor := make(chan lxc.State, 1)

	go func(c *lxc.Container) {
		monitor <- c.State()
	}(c.c)

	select {
	case state := <-monitor:
		return state, nil
	case <-time.After(5 * time.Second):
		return lxc.StateMap["FROZEN"], LxcMonitorStateError
	}
}
func (c *containerLXC) StorageStartSensitive() (bool, error) {
	// Initialize storage interface for the container.
	err := c.initStorage()
	if err != nil {
		return false, err
	}

	var isOurOperation bool
	if c.IsSnapshot() {
		isOurOperation, err = c.storage.ContainerSnapshotStart(c)
	} else {
		isOurOperation, err = c.storage.ContainerMount(c)
	}

	return isOurOperation, err
}
func (c *containerLXC) deviceExistsInDevicesFolder(prefix string, path string) bool {
	relativeDestPath := strings.TrimPrefix(path, "/")
	devName := fmt.Sprintf("%s.%s", strings.Replace(prefix, "/", "-", -1), strings.Replace(relativeDestPath, "/", "-", -1))
	devPath := filepath.Join(c.DevicesPath(), devName)

	return shared.PathExists(devPath)
}
func (c *containerLXC) createDiskDevice(name string, m types.Device) (string, error) {
	// source paths
	relativeDestPath := strings.TrimPrefix(m["path"], "/")
	devName := fmt.Sprintf("disk.%s.%s", strings.Replace(name, "/", "-", -1), strings.Replace(relativeDestPath, "/", "-", -1))
	devPath := filepath.Join(c.DevicesPath(), devName)
	srcPath := shared.HostPath(m["source"])

	// Check if read-only
	isOptional := shared.IsTrue(m["optional"])
	isReadOnly := shared.IsTrue(m["readonly"])
	isRecursive := shared.IsTrue(m["recursive"])

	isFile := false
	if m["pool"] == "" {
		isFile = !shared.IsDir(srcPath) && !deviceIsBlockdev(srcPath)
	} else {
		// Deal with mounting storage volumes created via the storage
		// api. Extract the name of the storage volume that we are
		// supposed to attach. We assume that the only syntactically
		// valid ways of specifying a storage volume are:
		// - <volume_name>
		// - <type>/<volume_name>
		// Currently, <type> must either be empty or "custom". We do not
		// yet support container mounts.

		if filepath.IsAbs(m["source"]) {
			return "", fmt.Errorf("When the \"pool\" property is set \"source\" must specify the name of a volume, not a path")
		}

		volumeTypeName := ""
		volumeName := filepath.Clean(m["source"])
		slash := strings.Index(volumeName, "/")
		if (slash > 0) && (len(volumeName) > slash) {
			// Extract volume name.
			volumeName = m["source"][(slash + 1):]
			// Extract volume type.
			volumeTypeName = m["source"][:slash]
		}

		switch volumeTypeName {
		case storagePoolVolumeTypeNameContainer:
			return "", fmt.Errorf("Using container storage volumes is not supported")
		case "":
			// We simply received the name of a storage volume.
			volumeTypeName = storagePoolVolumeTypeNameCustom
			fallthrough
		case storagePoolVolumeTypeNameCustom:
			srcPath = shared.VarPath("storage-pools", m["pool"], volumeTypeName, volumeName)
		case storagePoolVolumeTypeNameImage:
			return "", fmt.Errorf("Using image storage volumes is not supported")
		default:
			return "", fmt.Errorf("Unknown storage type prefix \"%s\" found", volumeTypeName)
		}

		// Initialize a new storage interface and check if the
		// pool/volume is mounted. If it is not, mount it.
		volumeType, _ := storagePoolVolumeTypeNameToType(volumeTypeName)
		s, err := storagePoolVolumeAttachInit(c.state, m["pool"], volumeName, volumeType, c)
		if err != nil && !isOptional {
			return "", fmt.Errorf("Failed to initialize storage volume \"%s\" of type \"%s\" on storage pool \"%s\": %s",
				volumeName,
				volumeTypeName,
				m["pool"], err)
		} else if err == nil {
			_, err = s.StoragePoolVolumeMount()
			if err != nil {
				msg := fmt.Sprintf("Could not mount storage volume \"%s\" of type \"%s\" on storage pool \"%s\": %s.",
					volumeName,
					volumeTypeName,
					m["pool"], err)
				if !isOptional {
					logger.Errorf(msg)
					return "", err
				}
				logger.Warnf(msg)
			}
		}
	}

	// Check if the source exists
	if !shared.PathExists(srcPath) {
		if isOptional {
			return "", nil
		}
		return "", fmt.Errorf("Source path %s doesn't exist for device %s", srcPath, name)
	}

	// Create the devices directory if missing
	if !shared.PathExists(c.DevicesPath()) {
		err := os.Mkdir(c.DevicesPath(), 0711)
		if err != nil {
			return "", err
		}
	}

	// Clean any existing entry
	if shared.PathExists(devPath) {
		err := os.Remove(devPath)
		if err != nil {
			return "", err
		}
	}

	// Create the mount point
	if isFile {
		f, err := os.Create(devPath)
		if err != nil {
			return "", err
		}

		f.Close()
	} else {
		err := os.Mkdir(devPath, 0700)
		if err != nil {
			return "", err
		}
	}

	// Mount the fs
	err := deviceMountDisk(srcPath, devPath, isReadOnly, isRecursive, m["propagation"])
	if err != nil {
		return "", err
	}

	return devPath, nil
}
func (c *containerLXC) setNetworkRoutes(m types.Device) error {
	if !shared.PathExists(fmt.Sprintf("/sys/class/net/%s", m["host_name"])) {
		return fmt.Errorf("Unknown or missing host side veth: %s", m["host_name"])
	}

	// Flush all IPv4 routes
	_, err := shared.RunCommand("ip", "-4", "route", "flush", "dev", m["host_name"], "proto", "static")
	if err != nil {
		return err
	}

	// Flush all IPv6 routes
	_, err = shared.RunCommand("ip", "-6", "route", "flush", "dev", m["host_name"], "proto", "static")
	if err != nil {
		return err
	}

	// Add additional IPv4 routes
	if m["ipv4.routes"] != "" {
		for _, route := range strings.Split(m["ipv4.routes"], ",") {
			route = strings.TrimSpace(route)
			_, err := shared.RunCommand("ip", "-4", "route", "add", "dev", m["host_name"], route, "proto", "static")
			if err != nil {
				return err
			}
		}
	}

	// Add additional IPv6 routes
	if m["ipv6.routes"] != "" {
		for _, route := range strings.Split(m["ipv6.routes"], ",") {
			route = strings.TrimSpace(route)
			_, err := shared.RunCommand("ip", "-6", "route", "add", "dev", m["host_name"], route, "proto", "static")
			if err != nil {
				return err
			}
		}
	}

	return nil
}
func (c *containerLXC) Path() string {
	name := projectPrefix(c.Project(), c.Name())
	return containerPath(name, c.IsSnapshot())
}
func (c *containerLXC) maasInterfaces() ([]maas.ContainerInterface, error) {
	interfaces := []maas.ContainerInterface{}
	for k, m := range c.expandedDevices {
		if m["type"] != "nic" {
			continue
		}

		if m["maas.subnet.ipv4"] == "" && m["maas.subnet.ipv6"] == "" {
			continue
		}

		m, err := c.fillNetworkDevice(k, m)
		if err != nil {
			return nil, err
		}

		subnets := []maas.ContainerInterfaceSubnet{}

		// IPv4
		if m["maas.subnet.ipv4"] != "" {
			subnet := maas.ContainerInterfaceSubnet{
				Name:    m["maas.subnet.ipv4"],
				Address: m["ipv4.address"],
			}

			subnets = append(subnets, subnet)
		}

		// IPv6
		if m["maas.subnet.ipv6"] != "" {
			subnet := maas.ContainerInterfaceSubnet{
				Name:    m["maas.subnet.ipv6"],
				Address: m["ipv6.address"],
			}

			subnets = append(subnets, subnet)
		}

		iface := maas.ContainerInterface{
			Name:       m["name"],
			MACAddress: m["hwaddr"],
			Subnets:    subnets,
		}

		interfaces = append(interfaces, iface)
	}

	return interfaces, nil
}
func getSystemHandler(syslog string, debug bool, format log.Format) log.Handler {
	// SyslogHandler
	if syslog != "" {
		if !debug {
			return log.LvlFilterHandler(
				log.LvlInfo,
				log.Must.SyslogHandler(syslog, format),
			)
		}

		return log.Must.SyslogHandler(syslog, format)
	}

	return nil
}
func findNvidiaMinor(pci string) (string, error) {
	nvidiaPath := fmt.Sprintf("/proc/driver/nvidia/gpus/%s/information", pci)
	buf, err := ioutil.ReadFile(nvidiaPath)
	if err != nil {
		return "", err
	}

	strBuf := strings.TrimSpace(string(buf))
	idx := strings.Index(strBuf, "Device Minor:")
	if idx != -1 {
		idx += len("Device Minor:")
		strBuf = strBuf[idx:]
		strBuf = strings.TrimSpace(strBuf)
		parts := strings.SplitN(strBuf, "\n", 2)
		_, err = strconv.Atoi(parts[0])
		if err == nil {
			return parts[0], nil
		}
	}

	minor, err := findNvidiaMinorOld()
	if err == nil {
		return minor, nil
	}

	return "", err
}
func GetLogger(syslog string, logfile string, verbose bool, debug bool, customHandler log.Handler) (logger.Logger, error) {
	Log := log.New()

	var handlers []log.Handler
	var syshandler log.Handler

	// System specific handler
	syshandler = getSystemHandler(syslog, debug, LogfmtFormat())
	if syshandler != nil {
		handlers = append(handlers, syshandler)
	}

	// FileHandler
	if logfile != "" {
		if !pathExists(filepath.Dir(logfile)) {
			return nil, fmt.Errorf("Log file path doesn't exist: %s", filepath.Dir(logfile))
		}

		if !debug {
			handlers = append(
				handlers,
				log.LvlFilterHandler(
					log.LvlInfo,
					log.Must.FileHandler(logfile, LogfmtFormat()),
				),
			)
		} else {
			handlers = append(handlers, log.Must.FileHandler(logfile, LogfmtFormat()))
		}
	}

	// StderrHandler
	format := LogfmtFormat()
	if term.IsTty(os.Stderr.Fd()) {
		format = TerminalFormat()
	}

	if verbose || debug {
		if !debug {
			handlers = append(
				handlers,
				log.LvlFilterHandler(
					log.LvlInfo,
					log.StreamHandler(os.Stderr, format),
				),
			)
		} else {
			handlers = append(handlers, log.StreamHandler(os.Stderr, format))
		}
	} else {
		handlers = append(
			handlers,
			log.LvlFilterHandler(
				log.LvlWarn,
				log.StreamHandler(os.Stderr, format),
			),
		)
	}

	if customHandler != nil {
		handlers = append(handlers, customHandler)
	}

	Log.SetHandler(log.MultiHandler(handlers...))

	return Log, nil
}
func SetLogger(newLogger logger.Logger) func() {
	origLog := logger.Log
	logger.Log = newLogger
	return func() {
		logger.Log = origLog
	}
}
func WaitRecord(ch chan *log.Record, timeout time.Duration) *log.Record {
	select {
	case record := <-ch:
		return record
	case <-time.After(timeout):
		return nil
	}
}
func AddContext(logger logger.Logger, ctx log.Ctx) logger.Logger {
	log15logger, ok := logger.(log.Logger)
	if !ok {
		logger.Error("couldn't downcast logger to add context", log.Ctx{"logger": log15logger, "ctx": ctx})
		return logger
	}

	return log15logger.New(ctx)
}
func NewDottedVersion(versionString string) (*DottedVersion, error) {
	formatError := fmt.Errorf("Invalid version format: %s", versionString)
	split := strings.Split(versionString, ".")
	if len(split) < 2 {
		return nil, formatError
	}

	maj, err := strconv.Atoi(split[0])
	if err != nil {
		return nil, formatError
	}

	min, err := strconv.Atoi(split[1])
	if err != nil {
		return nil, formatError
	}

	patch := -1
	if len(split) == 3 {
		patch, err = strconv.Atoi(split[2])
		if err != nil {
			return nil, formatError
		}
	}

	return &DottedVersion{
		Major: maj,
		Minor: min,
		Patch: patch,
	}, nil
}
func Parse(s string) (*DottedVersion, error) {
	r, _ := regexp.Compile(`^([0-9]+.[0-9]+(.[0-9]+))?.*`)
	matches := r.FindAllStringSubmatch(s, -1)
	if len(matches[0]) < 2 {
		return nil, fmt.Errorf("Can't parse a version")
	}
	return NewDottedVersion(matches[0][1])
}
func (v *DottedVersion) String() string {
	version := fmt.Sprintf("%d.%d", v.Major, v.Minor)
	if v.Patch != -1 {
		version += fmt.Sprintf(".%d", v.Patch)
	}
	return version
}
func (v *DottedVersion) Compare(other *DottedVersion) int {
	result := compareInts(v.Major, other.Major)
	if result != 0 {
		return result
	}
	result = compareInts(v.Minor, other.Minor)
	if result != 0 {
		return result
	}
	return compareInts(v.Patch, other.Patch)
}
func projectCreateDefaultProfile(tx *db.ClusterTx, project string) error {
	// Create a default profile
	profile := db.Profile{}
	profile.Project = project
	profile.Name = "default"
	profile.Description = fmt.Sprintf("Default LXD profile for project %s", project)
	profile.Config = map[string]string{}
	profile.Devices = types.Devices{}

	_, err := tx.ProfileCreate(profile)
	if err != nil {
		return errors.Wrap(err, "Add default profile to database")
	}
	return nil
}
func projectChange(d *Daemon, project *api.Project, req api.ProjectPut) Response {
	// Flag indicating if any feature has changed.
	featuresChanged := req.Config["features.images"] != project.Config["features.images"] || req.Config["features.profiles"] != project.Config["features.profiles"]

	// Sanity checks
	if project.Name == "default" && featuresChanged {
		return BadRequest(fmt.Errorf("You can't change the features of the default project"))
	}

	if !projectIsEmpty(project) && featuresChanged {
		return BadRequest(fmt.Errorf("Features can only be changed on empty projects"))
	}

	// Validate the configuration
	err := projectValidateConfig(req.Config)
	if err != nil {
		return BadRequest(err)
	}

	// Update the database entry
	err = d.cluster.Transaction(func(tx *db.ClusterTx) error {
		err := tx.ProjectUpdate(project.Name, req)
		if err != nil {
			return errors.Wrap(err, "Persist profile changes")
		}

		if req.Config["features.profiles"] != project.Config["features.profiles"] {
			if req.Config["features.profiles"] == "true" {
				err = projectCreateDefaultProfile(tx, project.Name)
				if err != nil {
					return err
				}
			} else {
				// Delete the project-specific default profile.
				err = tx.ProfileDelete(project.Name, "default")
				if err != nil {
					return errors.Wrap(err, "Delete project default profile")
				}
			}
		}

		return nil

	})

	if err != nil {
		return SmartError(err)
	}

	return EmptySyncResponse
}
func projectIsEmpty(project *api.Project) bool {
	if len(project.UsedBy) > 0 {
		// Check if the only entity is the default profile.
		if len(project.UsedBy) == 1 && strings.Contains(project.UsedBy[0], "/profiles/default") {
			return true
		}
		return false
	}
	return true
}
func (r *ProtocolLXD) GetCertificateFingerprints() ([]string, error) {
	certificates := []string{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/certificates", nil, "", &certificates)
	if err != nil {
		return nil, err
	}

	// Parse it
	fingerprints := []string{}
	for _, fingerprint := range certificates {
		fields := strings.Split(fingerprint, "/certificates/")
		fingerprints = append(fingerprints, fields[len(fields)-1])
	}

	return fingerprints, nil
}
func (r *ProtocolLXD) GetCertificates() ([]api.Certificate, error) {
	certificates := []api.Certificate{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/certificates?recursion=1", nil, "", &certificates)
	if err != nil {
		return nil, err
	}

	return certificates, nil
}
func (r *ProtocolLXD) GetCertificate(fingerprint string) (*api.Certificate, string, error) {
	certificate := api.Certificate{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/certificates/%s", url.QueryEscape(fingerprint)), nil, "", &certificate)
	if err != nil {
		return nil, "", err
	}

	return &certificate, etag, nil
}
func (r *ProtocolLXD) CreateCertificate(certificate api.CertificatesPost) error {
	// Send the request
	_, _, err := r.query("POST", "/certificates", certificate, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) UpdateCertificate(fingerprint string, certificate api.CertificatePut, ETag string) error {
	if !r.HasExtension("certificate_update") {
		return fmt.Errorf("The server is missing the required \"certificate_update\" API extension")
	}

	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/certificates/%s", url.QueryEscape(fingerprint)), certificate, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) DeleteCertificate(fingerprint string) error {
	// Send the request
	_, _, err := r.query("DELETE", fmt.Sprintf("/certificates/%s", url.QueryEscape(fingerprint)), nil, "")
	if err != nil {
		return err
	}

	return nil
}
func containerMetadataTemplatesGet(d *Daemon, r *http.Request) Response {
	project := projectParam(r)
	name := mux.Vars(r)["name"]

	// Handle requests targeted to a container on a different node
	response, err := ForwardedResponseIfContainerIsRemote(d, r, project, name)
	if err != nil {
		return SmartError(err)
	}
	if response != nil {
		return response
	}

	// Load the container
	c, err := containerLoadByProjectAndName(d.State(), project, name)
	if err != nil {
		return SmartError(err)
	}

	// Start the storage if needed
	ourStart, err := c.StorageStart()
	if err != nil {
		return SmartError(err)
	}
	if ourStart {
		defer c.StorageStop()
	}

	// Look at the request
	templateName := r.FormValue("path")
	if templateName == "" {
		// List templates
		templatesPath := filepath.Join(c.Path(), "templates")
		filesInfo, err := ioutil.ReadDir(templatesPath)
		if err != nil {
			return InternalError(err)
		}

		templates := []string{}
		for _, info := range filesInfo {
			if !info.IsDir() {
				templates = append(templates, info.Name())
			}
		}

		return SyncResponse(true, templates)
	}

	// Check if the template exists
	templatePath, err := getContainerTemplatePath(c, templateName)
	if err != nil {
		return SmartError(err)
	}

	if !shared.PathExists(templatePath) {
		return NotFound(fmt.Errorf("Path '%s' not found", templatePath))
	}

	// Create a temporary file with the template content (since the container
	// storage might not be available when the file is read from FileResponse)
	template, err := os.Open(templatePath)
	if err != nil {
		return SmartError(err)
	}
	defer template.Close()

	tempfile, err := ioutil.TempFile("", "lxd_template")
	if err != nil {
		return SmartError(err)
	}
	defer tempfile.Close()

	_, err = io.Copy(tempfile, template)
	if err != nil {
		return InternalError(err)
	}

	files := make([]fileResponseEntry, 1)
	files[0].identifier = templateName
	files[0].path = tempfile.Name()
	files[0].filename = templateName
	return FileResponse(r, files, nil, true)
}
func containerMetadataTemplatesPostPut(d *Daemon, r *http.Request) Response {
	project := projectParam(r)
	name := mux.Vars(r)["name"]

	// Handle requests targeted to a container on a different node
	response, err := ForwardedResponseIfContainerIsRemote(d, r, project, name)
	if err != nil {
		return SmartError(err)
	}
	if response != nil {
		return response
	}

	// Load the container
	c, err := containerLoadByProjectAndName(d.State(), project, name)
	if err != nil {
		return SmartError(err)
	}

	// Start the storage if needed
	ourStart, err := c.StorageStart()
	if err != nil {
		return SmartError(err)
	}
	if ourStart {
		defer c.StorageStop()
	}

	// Look at the request
	templateName := r.FormValue("path")
	if templateName == "" {
		return BadRequest(fmt.Errorf("missing path argument"))
	}

	// Check if the template already exists
	templatePath, err := getContainerTemplatePath(c, templateName)
	if err != nil {
		return SmartError(err)
	}

	if r.Method == "POST" && shared.PathExists(templatePath) {
		return BadRequest(fmt.Errorf("Template already exists"))
	}

	// Write the new template
	template, err := os.OpenFile(templatePath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0644)
	if err != nil {
		return SmartError(err)
	}
	defer template.Close()

	_, err = io.Copy(template, r.Body)
	if err != nil {
		return InternalError(err)
	}

	return EmptySyncResponse
}
func containerMetadataTemplatesDelete(d *Daemon, r *http.Request) Response {
	project := projectParam(r)

	name := mux.Vars(r)["name"]

	// Handle requests targeted to a container on a different node
	response, err := ForwardedResponseIfContainerIsRemote(d, r, project, name)
	if err != nil {
		return SmartError(err)
	}
	if response != nil {
		return response
	}

	// Load the container
	c, err := containerLoadByProjectAndName(d.State(), project, name)
	if err != nil {
		return SmartError(err)
	}

	// Start the storage if needed
	ourStart, err := c.StorageStart()
	if err != nil {
		return SmartError(err)
	}
	if ourStart {
		defer c.StorageStop()
	}

	// Look at the request
	templateName := r.FormValue("path")
	if templateName == "" {
		return BadRequest(fmt.Errorf("missing path argument"))
	}

	templatePath, err := getContainerTemplatePath(c, templateName)
	if err != nil {
		return SmartError(err)
	}

	if !shared.PathExists(templatePath) {
		return NotFound(fmt.Errorf("Path '%s' not found", templatePath))
	}

	// Delete the template
	err = os.Remove(templatePath)
	if err != nil {
		return InternalError(err)
	}

	return EmptySyncResponse
}
func getContainerTemplatePath(c container, filename string) (string, error) {
	if strings.Contains(filename, "/") {
		return "", fmt.Errorf("Invalid template filename")
	}

	return filepath.Join(c.Path(), "templates", filename), nil
}
func (e Error) Error() string {
	message := fmt.Sprintf("cannot set '%s'", e.Name)
	if e.Value != nil {
		message += fmt.Sprintf(" to '%v'", e.Value)
	}
	return message + fmt.Sprintf(": %s", e.Reason)
}
func (l ErrorList) Error() string {
	switch len(l) {
	case 0:
		return "no errors"
	case 1:
		return l[0].Error()
	}
	return fmt.Sprintf("%s (and %d more errors)", l[0], len(l)-1)
}
func (l *ErrorList) add(name string, value interface{}, reason string) {
	*l = append(*l, &Error{name, value, reason})
}
func UpdateSchema() error {
	err := cluster.SchemaDotGo()
	if err != nil {
		return errors.Wrap(err, "Update cluster database schema")
	}

	err = node.SchemaDotGo()
	if err != nil {
		return errors.Wrap(err, "Update node database schema")
	}

	return nil
}
func doProfileUpdateCluster(d *Daemon, project, name string, old api.ProfilePut) error {
	nodeName := ""
	err := d.cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		nodeName, err = tx.NodeName()
		return err
	})
	if err != nil {
		return errors.Wrap(err, "failed to query local node name")
	}

	containers, err := getProfileContainersInfo(d.cluster, project, name)
	if err != nil {
		return errors.Wrapf(err, "failed to query containers associated with profile '%s'", name)
	}

	failures := map[string]error{}
	for _, args := range containers {
		err := doProfileUpdateContainer(d, name, old, nodeName, args)
		if err != nil {
			failures[args.Name] = err
		}
	}

	if len(failures) != 0 {
		msg := "The following containers failed to update (profile change still saved):\n"
		for cname, err := range failures {
			msg += fmt.Sprintf(" - %s: %s\n", cname, err)
		}
		return fmt.Errorf("%s", msg)
	}

	return nil
}
func doProfileUpdateContainer(d *Daemon, name string, old api.ProfilePut, nodeName string, args db.ContainerArgs) error {
	if args.Node != "" && args.Node != nodeName {
		// No-op, this container does not belong to this node.
		return nil
	}

	profiles, err := d.cluster.ProfilesGet(args.Project, args.Profiles)
	if err != nil {
		return err
	}
	for i, profileName := range args.Profiles {
		if profileName == name {
			// Use the old config and devices.
			profiles[i].Config = old.Config
			profiles[i].Devices = old.Devices
			break
		}
	}

	c := containerLXCInstantiate(d.State(), args)

	c.expandConfig(profiles)
	c.expandDevices(profiles)

	return c.Update(db.ContainerArgs{
		Architecture: c.Architecture(),
		Config:       c.LocalConfig(),
		Description:  c.Description(),
		Devices:      c.LocalDevices(),
		Ephemeral:    c.IsEphemeral(),
		Profiles:     c.Profiles(),
		Project:      c.Project(),
	}, true)
}
func getProfileContainersInfo(cluster *db.Cluster, project, profile string) ([]db.ContainerArgs, error) {
	// Query the db for information about containers associated with the
	// given profile.
	names, err := cluster.ProfileContainersGet(project, profile)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to query containers with profile '%s'", profile)
	}

	containers := []db.ContainerArgs{}
	err = cluster.Transaction(func(tx *db.ClusterTx) error {
		for ctProject, ctNames := range names {
			for _, ctName := range ctNames {
				container, err := tx.ContainerGet(ctProject, ctName)
				if err != nil {
					return err
				}

				containers = append(containers, db.ContainerToArgs(container))
			}
		}

		return nil
	})
	if err != nil {
		return nil, errors.Wrapf(err, "Failed to fetch containers")
	}

	return containers, nil
}
func (r *ProtocolLXD) GetNetworkNames() ([]string, error) {
	if !r.HasExtension("network") {
		return nil, fmt.Errorf("The server is missing the required \"network\" API extension")
	}

	urls := []string{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/networks", nil, "", &urls)
	if err != nil {
		return nil, err
	}

	// Parse it
	names := []string{}
	for _, url := range urls {
		fields := strings.Split(url, "/networks/")
		names = append(names, fields[len(fields)-1])
	}

	return names, nil
}
func (r *ProtocolLXD) GetNetworks() ([]api.Network, error) {
	if !r.HasExtension("network") {
		return nil, fmt.Errorf("The server is missing the required \"network\" API extension")
	}

	networks := []api.Network{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/networks?recursion=1", nil, "", &networks)
	if err != nil {
		return nil, err
	}

	return networks, nil
}
func (r *ProtocolLXD) GetNetwork(name string) (*api.Network, string, error) {
	if !r.HasExtension("network") {
		return nil, "", fmt.Errorf("The server is missing the required \"network\" API extension")
	}

	network := api.Network{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/networks/%s", url.QueryEscape(name)), nil, "", &network)
	if err != nil {
		return nil, "", err
	}

	return &network, etag, nil
}
func (r *ProtocolLXD) GetNetworkLeases(name string) ([]api.NetworkLease, error) {
	if !r.HasExtension("network_leases") {
		return nil, fmt.Errorf("The server is missing the required \"network_leases\" API extension")
	}

	leases := []api.NetworkLease{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/networks/%s/leases", url.QueryEscape(name)), nil, "", &leases)
	if err != nil {
		return nil, err
	}

	return leases, nil
}
func (r *ProtocolLXD) GetNetworkState(name string) (*api.NetworkState, error) {
	if !r.HasExtension("network_state") {
		return nil, fmt.Errorf("The server is missing the required \"network_state\" API extension")
	}

	state := api.NetworkState{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/networks/%s/state", url.QueryEscape(name)), nil, "", &state)
	if err != nil {
		return nil, err
	}

	return &state, nil
}
func (r *ProtocolLXD) CreateNetwork(network api.NetworksPost) error {
	if !r.HasExtension("network") {
		return fmt.Errorf("The server is missing the required \"network\" API extension")
	}

	// Send the request
	_, _, err := r.query("POST", "/networks", network, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) UpdateNetwork(name string, network api.NetworkPut, ETag string) error {
	if !r.HasExtension("network") {
		return fmt.Errorf("The server is missing the required \"network\" API extension")
	}

	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/networks/%s", url.QueryEscape(name)), network, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) RenameNetwork(name string, network api.NetworkPost) error {
	if !r.HasExtension("network") {
		return fmt.Errorf("The server is missing the required \"network\" API extension")
	}

	// Send the request
	_, _, err := r.query("POST", fmt.Sprintf("/networks/%s", url.QueryEscape(name)), network, "")
	if err != nil {
		return err
	}

	return nil
}
func Open(name string, store dqlite.ServerStore, options ...dqlite.DriverOption) (*sql.DB, error) {
	driver, err := dqlite.NewDriver(store, options...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to create dqlite driver")
	}

	driverName := dqliteDriverName()
	sql.Register(driverName, driver)

	// Create the cluster db. This won't immediately establish any network
	// connection, that will happen only when a db transaction is started
	// (see the database/sql connection pooling code for more details).
	if name == "" {
		name = "db.bin"
	}
	db, err := sql.Open(driverName, name)
	if err != nil {
		return nil, fmt.Errorf("cannot open cluster database: %v", err)
	}

	return db, nil
}
func URLEncode(path string, query map[string]string) (string, error) {
	u, err := url.Parse(path)
	if err != nil {
		return "", err
	}

	params := url.Values{}
	for key, value := range query {
		params.Add(key, value)
	}
	u.RawQuery = params.Encode()
	return u.String(), nil
}
func IsUnixSocket(path string) bool {
	stat, err := os.Stat(path)
	if err != nil {
		return false
	}
	return (stat.Mode() & os.ModeSocket) == os.ModeSocket
}
func HostPath(path string) string {
	// Ignore empty paths
	if len(path) == 0 {
		return path
	}

	// Don't prefix stdin/stdout
	if path == "-" {
		return path
	}

	// Check if we're running in a snap package
	snap := os.Getenv("SNAP")
	snapName := os.Getenv("SNAP_NAME")
	if snap == "" || snapName != "lxd" {
		return path
	}

	// Handle relative paths
	if path[0] != os.PathSeparator {
		// Use the cwd of the parent as snap-confine alters our own cwd on launch
		ppid := os.Getppid()
		if ppid < 1 {
			return path
		}

		pwd, err := os.Readlink(fmt.Sprintf("/proc/%d/cwd", ppid))
		if err != nil {
			return path
		}

		path = filepath.Clean(strings.Join([]string{pwd, path}, string(os.PathSeparator)))
	}

	// Check if the path is already snap-aware
	for _, prefix := range []string{"/dev", "/snap", "/var/snap", "/var/lib/snapd"} {
		if path == prefix || strings.HasPrefix(path, fmt.Sprintf("%s/", prefix)) {
			return path
		}
	}

	return fmt.Sprintf("/var/lib/snapd/hostfs%s", path)
}
func FileMove(oldPath string, newPath string) error {
	err := os.Rename(oldPath, newPath)
	if err == nil {
		return nil
	}

	err = FileCopy(oldPath, newPath)
	if err != nil {
		return err
	}

	os.Remove(oldPath)

	return nil
}
func DirCopy(source string, dest string) error {
	// Get info about source.
	info, err := os.Stat(source)
	if err != nil {
		return errors.Wrapf(err, "failed to get source directory info")
	}

	if !info.IsDir() {
		return fmt.Errorf("source is not a directory")
	}

	// Remove dest if it already exists.
	if PathExists(dest) {
		err := os.RemoveAll(dest)
		if err != nil {
			return errors.Wrapf(err, "failed to remove destination directory %s", dest)
		}
	}

	// Create dest.
	err = os.MkdirAll(dest, info.Mode())
	if err != nil {
		return errors.Wrapf(err, "failed to create destination directory %s", dest)
	}

	// Copy all files.
	entries, err := ioutil.ReadDir(source)
	if err != nil {
		return errors.Wrapf(err, "failed to read source directory %s", source)
	}

	for _, entry := range entries {

		sourcePath := filepath.Join(source, entry.Name())
		destPath := filepath.Join(dest, entry.Name())

		if entry.IsDir() {
			err := DirCopy(sourcePath, destPath)
			if err != nil {
				return errors.Wrapf(err, "failed to copy sub-directory from %s to %s", sourcePath, destPath)
			}
		} else {
			err := FileCopy(sourcePath, destPath)
			if err != nil {
				return errors.Wrapf(err, "failed to copy file from %s to %s", sourcePath, destPath)
			}
		}

	}

	return nil
}
func StringMapHasStringKey(m map[string]string, keys ...string) bool {
	for _, k := range keys {
		if _, ok := m[k]; ok {
			return true
		}
	}

	return false
}
func TextEditor(inPath string, inContent []byte) ([]byte, error) {
	var f *os.File
	var err error
	var path string

	// Detect the text editor to use
	editor := os.Getenv("VISUAL")
	if editor == "" {
		editor = os.Getenv("EDITOR")
		if editor == "" {
			for _, p := range []string{"editor", "vi", "emacs", "nano"} {
				_, err := exec.LookPath(p)
				if err == nil {
					editor = p
					break
				}
			}
			if editor == "" {
				return []byte{}, fmt.Errorf("No text editor found, please set the EDITOR environment variable")
			}
		}
	}

	if inPath == "" {
		// If provided input, create a new file
		f, err = ioutil.TempFile("", "lxd_editor_")
		if err != nil {
			return []byte{}, err
		}

		err = os.Chmod(f.Name(), 0600)
		if err != nil {
			f.Close()
			os.Remove(f.Name())
			return []byte{}, err
		}

		f.Write(inContent)
		f.Close()

		path = fmt.Sprintf("%s.yaml", f.Name())
		os.Rename(f.Name(), path)
		defer os.Remove(path)
	} else {
		path = inPath
	}

	cmdParts := strings.Fields(editor)
	cmd := exec.Command(cmdParts[0], append(cmdParts[1:], path)...)
	cmd.Stdin = os.Stdin
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	err = cmd.Run()
	if err != nil {
		return []byte{}, err
	}

	content, err := ioutil.ReadFile(path)
	if err != nil {
		return []byte{}, err
	}

	return content, nil
}
func WriteTempFile(dir string, prefix string, content string) (string, error) {
	f, err := ioutil.TempFile(dir, prefix)
	if err != nil {
		return "", err
	}
	defer f.Close()

	_, err = f.WriteString(content)
	return f.Name(), err
}
func RenderTemplate(template string, ctx pongo2.Context) (string, error) {
	// Load template from string
	tpl, err := pongo2.FromString("{% autoescape off %}" + template + "{% endautoescape %}")
	if err != nil {
		return "", err
	}

	// Get rendered template
	ret, err := tpl.Execute(ctx)
	if err != nil {
		return ret, err
	}

	// Looks like we're nesting templates so run pongo again
	if strings.Contains(ret, "{{") || strings.Contains(ret, "{%") {
		return RenderTemplate(ret, ctx)
	}

	return ret, err
}
func Every(interval time.Duration, options ...EveryOption) Schedule {
	every := &every{}
	for _, option := range options {
		option(every)
	}
	first := true
	return func() (time.Duration, error) {
		var err error
		if first && every.skipFirst {
			err = ErrSkip
		}
		first = false
		return interval, err
	}
}
func (s *storageLvm) StoragePoolMount() (bool, error) {
	source := s.pool.Config["source"]
	if source == "" {
		return false, fmt.Errorf("no \"source\" property found for the storage pool")
	}

	if !filepath.IsAbs(source) {
		return true, nil
	}

	poolMountLockID := getPoolMountLockID(s.pool.Name)
	lxdStorageMapLock.Lock()
	if waitChannel, ok := lxdStorageOngoingOperationMap[poolMountLockID]; ok {
		lxdStorageMapLock.Unlock()
		if _, ok := <-waitChannel; ok {
			logger.Warnf("Received value over semaphore, this should not have happened")
		}
		// Give the benefit of the doubt and assume that the other
		// thread actually succeeded in mounting the storage pool.
		return false, nil
	}

	lxdStorageOngoingOperationMap[poolMountLockID] = make(chan bool)
	lxdStorageMapLock.Unlock()

	removeLockFromMap := func() {
		lxdStorageMapLock.Lock()
		if waitChannel, ok := lxdStorageOngoingOperationMap[poolMountLockID]; ok {
			close(waitChannel)
			delete(lxdStorageOngoingOperationMap, poolMountLockID)
		}
		lxdStorageMapLock.Unlock()
	}

	defer removeLockFromMap()

	if filepath.IsAbs(source) && !shared.IsBlockdevPath(source) {
		// Try to prepare new loop device.
		loopF, loopErr := prepareLoopDev(source, 0)
		if loopErr != nil {
			return false, loopErr
		}
		// Make sure that LO_FLAGS_AUTOCLEAR is unset.
		loopErr = unsetAutoclearOnLoopDev(int(loopF.Fd()))
		if loopErr != nil {
			return false, loopErr
		}
		s.loopInfo = loopF
	}

	return true, nil
}
func Dump(tx *sql.Tx, schema string, schemaOnly bool) (string, error) {
	schemas := dumpParseSchema(schema)

	// Begin
	dump := `PRAGMA foreign_keys=OFF;
BEGIN TRANSACTION;
`
	// Schema table
	tableDump, err := dumpTable(tx, "schema", dumpSchemaTable)
	if err != nil {
		return "", errors.Wrapf(err, "failed to dump table schema")
	}
	dump += tableDump

	// All other tables
	tables := make([]string, 0)
	for table := range schemas {
		tables = append(tables, table)
	}
	sort.Strings(tables)
	for _, table := range tables {
		if schemaOnly {
			// Dump only the schema.
			dump += schemas[table] + "\n"
			continue
		}
		tableDump, err := dumpTable(tx, table, schemas[table])
		if err != nil {
			return "", errors.Wrapf(err, "failed to dump table %s", table)
		}
		dump += tableDump
	}

	// Sequences (unless the schemaOnly flag is true)
	if !schemaOnly {
		tableDump, err = dumpTable(tx, "sqlite_sequence", "DELETE FROM sqlite_sequence;")
		if err != nil {
			return "", errors.Wrapf(err, "failed to dump table sqlite_sequence")
		}
		dump += tableDump
	}

	// Commit
	dump += "COMMIT;\n"

	return dump, nil
}
func dumpTable(tx *sql.Tx, table, schema string) (string, error) {
	statements := []string{schema}

	// Query all rows.
	rows, err := tx.Query(fmt.Sprintf("SELECT * FROM %s ORDER BY rowid", table))
	if err != nil {
		return "", errors.Wrap(err, "failed to fetch rows")
	}
	defer rows.Close()

	// Figure column names
	columns, err := rows.Columns()
	if err != nil {
		return "", errors.Wrap(err, "failed to get columns")
	}

	// Generate an INSERT statement for each row.
	for i := 0; rows.Next(); i++ {
		raw := make([]interface{}, len(columns)) // Raw column values
		row := make([]interface{}, len(columns))
		for i := range raw {
			row[i] = &raw[i]
		}
		err := rows.Scan(row...)
		if err != nil {
			return "", errors.Wrapf(err, "failed to scan row %d", i)
		}
		values := make([]string, len(columns))
		for j, v := range raw {
			switch v := v.(type) {
			case int64:
				values[j] = strconv.FormatInt(v, 10)
			case string:
				values[j] = fmt.Sprintf("'%s'", v)
			case []byte:
				values[j] = fmt.Sprintf("'%s'", string(v))
			case time.Time:
				values[j] = strconv.FormatInt(v.Unix(), 10)
			default:
				if v != nil {
					return "", fmt.Errorf("bad type in column %s of row %d", columns[j], i)
				}
				values[j] = "NULL"
			}
		}
		statement := fmt.Sprintf("INSERT INTO %s VALUES(%s);", table, strings.Join(values, ","))
		statements = append(statements, statement)
	}
	return strings.Join(statements, "\n") + "\n", nil
}
func (c *ClusterTx) ProjectHasProfiles(name string) (bool, error) {
	return projectHasProfiles(c.tx, name)
}
func (c *ClusterTx) ProjectNames() ([]string, error) {
	stmt := "SELECT name FROM projects"

	names, err := query.SelectStrings(c.tx, stmt)
	if err != nil {
		return nil, errors.Wrap(err, "Fetch project names")
	}

	return names, nil
}
func (c *ClusterTx) ProjectMap() (map[int64]string, error) {
	stmt := "SELECT id, name FROM projects"

	rows, err := c.tx.Query(stmt)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	result := map[int64]string{}
	for i := 0; rows.Next(); i++ {
		var id int64
		var name string

		err := rows.Scan(&id, &name)
		if err != nil {
			return nil, err
		}

		result[id] = name
	}

	err = rows.Err()
	if err != nil {
		return nil, err
	}

	return result, nil
}
func (c *ClusterTx) ProjectHasImages(name string) (bool, error) {
	project, err := c.ProjectGet(name)
	if err != nil {
		return false, errors.Wrap(err, "fetch project")
	}

	enabled := project.Config["features.images"] == "true"

	return enabled, nil
}
func (c *ClusterTx) ProjectUpdate(name string, object api.ProjectPut) error {
	stmt := c.stmt(projectUpdate)
	result, err := stmt.Exec(object.Description, name)
	if err != nil {
		return errors.Wrap(err, "Update project")
	}

	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "Fetch affected rows")
	}
	if n != 1 {
		return fmt.Errorf("Query updated %d rows instead of 1", n)
	}

	id, err := c.ProjectID(name)
	if err != nil {
		return errors.Wrap(err, "Fetch project ID")
	}

	// Clear config.
	_, err = c.tx.Exec(`
DELETE FROM projects_config WHERE projects_config.project_id = ?
`, id)
	if err != nil {
		return errors.Wrap(err, "Delete project config")
	}

	// Insert new config.
	stmt = c.stmt(projectCreateConfigRef)
	for key, value := range object.Config {
		_, err := stmt.Exec(id, key, value)
		if err != nil {
			return errors.Wrap(err, "Insert config for project")
		}
	}

	return nil
}
func (r *ProtocolLXD) GetCluster() (*api.Cluster, string, error) {
	if !r.HasExtension("clustering") {
		return nil, "", fmt.Errorf("The server is missing the required \"clustering\" API extension")
	}

	cluster := &api.Cluster{}
	etag, err := r.queryStruct("GET", "/cluster", nil, "", &cluster)
	if err != nil {
		return nil, "", err
	}

	return cluster, etag, nil
}
func (r *ProtocolLXD) UpdateCluster(cluster api.ClusterPut, ETag string) (Operation, error) {
	if !r.HasExtension("clustering") {
		return nil, fmt.Errorf("The server is missing the required \"clustering\" API extension")
	}

	if cluster.ServerAddress != "" || cluster.ClusterPassword != "" || len(cluster.MemberConfig) > 0 {
		if !r.HasExtension("clustering_join") {
			return nil, fmt.Errorf("The server is missing the required \"clustering_join\" API extension")
		}
	}

	op, _, err := r.queryOperation("PUT", "/cluster", cluster, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) GetClusterMemberNames() ([]string, error) {
	if !r.HasExtension("clustering") {
		return nil, fmt.Errorf("The server is missing the required \"clustering\" API extension")
	}

	urls := []string{}
	_, err := r.queryStruct("GET", "/cluster/members", nil, "", &urls)
	if err != nil {
		return nil, err
	}

	return urls, nil
}
func (r *ProtocolLXD) GetClusterMembers() ([]api.ClusterMember, error) {
	if !r.HasExtension("clustering") {
		return nil, fmt.Errorf("The server is missing the required \"clustering\" API extension")
	}

	members := []api.ClusterMember{}
	_, err := r.queryStruct("GET", "/cluster/members?recursion=1", nil, "", &members)
	if err != nil {
		return nil, err
	}

	return members, nil
}
func (r *ProtocolLXD) GetClusterMember(name string) (*api.ClusterMember, string, error) {
	if !r.HasExtension("clustering") {
		return nil, "", fmt.Errorf("The server is missing the required \"clustering\" API extension")
	}

	member := api.ClusterMember{}
	etag, err := r.queryStruct("GET", fmt.Sprintf("/cluster/members/%s", name), nil, "", &member)
	if err != nil {
		return nil, "", err
	}

	return &member, etag, nil
}
func (r *ProtocolLXD) RenameClusterMember(name string, member api.ClusterMemberPost) error {
	if !r.HasExtension("clustering") {
		return fmt.Errorf("The server is missing the required \"clustering\" API extension")
	}

	_, _, err := r.query("POST", fmt.Sprintf("/cluster/members/%s", name), member, "")
	if err != nil {
		return err
	}

	return nil
}
func (e *EventListener) Disconnect() {
	if e.disconnected {
		return
	}

	// Handle locking
	e.r.eventListenersLock.Lock()
	defer e.r.eventListenersLock.Unlock()

	// Locate and remove it from the global list
	for i, listener := range e.r.eventListeners {
		if listener == e {
			copy(e.r.eventListeners[i:], e.r.eventListeners[i+1:])
			e.r.eventListeners[len(e.r.eventListeners)-1] = nil
			e.r.eventListeners = e.r.eventListeners[:len(e.r.eventListeners)-1]
			break
		}
	}

	// Turn off the handler
	e.err = nil
	e.disconnected = true
	close(e.chActive)
}
func CompareVersions(version1, version2 [2]int) (int, error) {
	schema1, extensions1 := version1[0], version1[1]
	schema2, extensions2 := version2[0], version2[1]

	if schema1 == schema2 && extensions1 == extensions2 {
		return 0, nil
	}
	if schema1 >= schema2 && extensions1 >= extensions2 {
		return 1, nil
	}
	if schema1 <= schema2 && extensions1 <= extensions2 {
		return 2, nil
	}

	return -1, fmt.Errorf("nodes have inconsistent versions")
}
func (c *Config) HasClientCertificate() bool {
	certf := c.ConfigPath("client.crt")
	keyf := c.ConfigPath("client.key")
	if !shared.PathExists(certf) || !shared.PathExists(keyf) {
		return false
	}

	return true
}
func (c *Config) GenerateClientCertificate() error {
	if c.HasClientCertificate() {
		return nil
	}

	certf := c.ConfigPath("client.crt")
	keyf := c.ConfigPath("client.key")

	return shared.FindOrGenCert(certf, keyf, true)
}
func LoadModule(module string) error {
	if shared.PathExists(fmt.Sprintf("/sys/module/%s", module)) {
		return nil
	}

	_, err := shared.RunCommand("modprobe", module)
	return err
}
func Parse(name string) (*ast.Package, error) {
	base := os.Getenv("GOPATH")
	if base == "" {
		base = "~/go"
	}
	dir := filepath.Join(base, "src", name)

	fset := token.NewFileSet()

	paths, err := filepath.Glob(filepath.Join(dir, "*.go"))
	if err != nil {
		return nil, errors.Wrap(err, "Search source file")
	}

	files := map[string]*ast.File{}
	for _, path := range paths {
		// Skip test files.
		if strings.Contains(path, "_test.go") {
			continue
		}

		file, err := parser.ParseFile(fset, path, nil, parser.ParseComments)
		if err != nil {
			return nil, fmt.Errorf("Parse Go source file %q", path)
		}

		files[path] = file
	}

	// Ignore errors because they are typically about unresolved symbols.
	pkg, _ := ast.NewPackage(fset, files, nil, nil)

	return pkg, nil
}
func (e *Endpoints) PprofAddress() string {
	e.mu.RLock()
	defer e.mu.RUnlock()

	listener := e.listeners[pprof]
	if listener == nil {
		return ""
	}

	return listener.Addr().String()
}
func (e *Endpoints) PprofUpdateAddress(address string) error {
	if address != "" {
		address = util.CanonicalNetworkAddress(address)
	}

	oldAddress := e.NetworkAddress()
	if address == oldAddress {
		return nil
	}

	logger.Infof("Update pprof address")

	e.mu.Lock()
	defer e.mu.Unlock()

	// Close the previous socket
	e.closeListener(pprof)

	// If turning off listening, we're done
	if address == "" {
		return nil
	}

	// Attempt to setup the new listening socket
	getListener := func(address string) (*net.Listener, error) {
		var err error
		var listener net.Listener

		for i := 0; i < 10; i++ { // Ten retries over a second seems reasonable.
			listener, err = net.Listen("tcp", address)
			if err == nil {
				break
			}

			time.Sleep(100 * time.Millisecond)
		}

		if err != nil {
			return nil, fmt.Errorf("Cannot listen on http socket: %v", err)
		}

		return &listener, nil
	}

	// If setting a new address, setup the listener
	if address != "" {
		listener, err := getListener(address)
		if err != nil {
			// Attempt to revert to the previous address
			listener, err1 := getListener(oldAddress)
			if err1 == nil {
				e.listeners[pprof] = *listener
				e.serveHTTP(pprof)
			}

			return err
		}

		e.listeners[pprof] = *listener
		e.serveHTTP(pprof)
	}

	return nil
}
func NewMethod(database, pkg, entity, kind string, config map[string]string) (*Method, error) {
	packages, err := Packages()
	if err != nil {
		return nil, err
	}

	method := &Method{
		db:       database,
		pkg:      pkg,
		entity:   entity,
		kind:     kind,
		config:   config,
		packages: packages,
	}

	return method, nil
}
func (m *Method) Generate(buf *file.Buffer) error {
	if strings.HasSuffix(m.kind, "Ref") {
		return m.ref(buf)
	}
	switch m.kind {
	case "URIs":
		return m.uris(buf)
	case "List":
		return m.list(buf)
	case "Get":
		return m.get(buf)
	case "ID":
		return m.id(buf)
	case "Exists":
		return m.exists(buf)
	case "Create":
		return m.create(buf)
	case "Rename":
		return m.rename(buf)
	case "Update":
		return m.update(buf)
	case "Delete":
		return m.delete(buf)
	default:
		return fmt.Errorf("Unknown method kind '%s'", m.kind)
	}
}
func (m *Method) fillSliceReferenceField(buf *file.Buffer, nk []*Field, field *Field) error {
	objectsVar := fmt.Sprintf("%sObjects", lex.Minuscule(field.Name))
	methodName := fmt.Sprintf("%s%sRef", lex.Capital(m.entity), field.Name)

	buf.L("// Fill field %s.", field.Name)
	buf.L("%s, err := c.%s(filter)", objectsVar, methodName)
	buf.L("if err != nil {")
	buf.L("        return nil, errors.Wrap(err, \"Failed to fetch field %s\")", field.Name)
	buf.L("}")
	buf.N()
	buf.L("for i := range objects {")
	needle := ""
	for i, key := range nk[:len(nk)-1] {
		needle += fmt.Sprintf("[objects[i].%s]", key.Name)
		subIndexTyp := indexType(nk[i+1:], field.Type.Name)
		buf.L("        _, ok := %s%s", objectsVar, needle)
		buf.L("        if !ok {")
		buf.L("                subIndex := %s{}", subIndexTyp)
		buf.L("                %s%s = subIndex", objectsVar, needle)
		buf.L("        }")
		buf.N()
	}

	needle += fmt.Sprintf("[objects[i].%s]", nk[len(nk)-1].Name)
	buf.L("        value := %s%s", objectsVar, needle)
	buf.L("        if value == nil {")
	buf.L("                value = %s{}", field.Type.Name)
	buf.L("        }")
	buf.L("        objects[i].%s = value", field.Name)
	buf.L("}")
	buf.N()

	return nil
}
func (c *ClusterTx) StoragePoolID(name string) (int64, error) {
	stmt := "SELECT id FROM storage_pools WHERE name=?"
	ids, err := query.SelectIntegers(c.tx, stmt, name)
	if err != nil {
		return -1, err
	}
	switch len(ids) {
	case 0:
		return -1, ErrNoSuchObject
	case 1:
		return int64(ids[0]), nil
	default:
		return -1, fmt.Errorf("more than one pool has the given name")
	}
}
func (c *ClusterTx) StoragePoolIDsNotPending() (map[string]int64, error) {
	pools := []struct {
		id   int64
		name string
	}{}
	dest := func(i int) []interface{} {
		pools = append(pools, struct {
			id   int64
			name string
		}{})
		return []interface{}{&pools[i].id, &pools[i].name}

	}
	stmt, err := c.tx.Prepare("SELECT id, name FROM storage_pools WHERE NOT state=?")
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, storagePoolPending)
	if err != nil {
		return nil, err
	}
	ids := map[string]int64{}
	for _, pool := range pools {
		ids[pool.name] = pool.id
	}
	return ids, nil
}
func (c *ClusterTx) StoragePoolNodeJoin(poolID, nodeID int64) error {
	columns := []string{"storage_pool_id", "node_id"}
	values := []interface{}{poolID, nodeID}
	_, err := query.UpsertObject(c.tx, "storage_pools_nodes", columns, values)
	if err != nil {
		return errors.Wrap(err, "failed to add storage pools node entry")
	}

	return nil
}
func (c *ClusterTx) StoragePoolNodeJoinCeph(poolID, nodeID int64) error {
	// Get the IDs of the other nodes (they should be all linked to
	// the pool).
	stmt := "SELECT node_id FROM storage_pools_nodes WHERE storage_pool_id=?"
	nodeIDs, err := query.SelectIntegers(c.tx, stmt, poolID)
	if err != nil {
		return errors.Wrap(err, "failed to fetch IDs of nodes with ceph pool")
	}
	if len(nodeIDs) == 0 {
		return fmt.Errorf("ceph pool is not linked to any node")
	}
	otherNodeID := nodeIDs[0]

	// Create entries of all the ceph volumes for the new node.
	_, err = c.tx.Exec(`
INSERT INTO storage_volumes(name, storage_pool_id, node_id, type, description, project_id)
  SELECT name, storage_pool_id, ?, type, description, 1
    FROM storage_volumes WHERE storage_pool_id=? AND node_id=?
`, nodeID, poolID, otherNodeID)
	if err != nil {
		return errors.Wrap(err, "failed to create node ceph volumes")
	}

	// Create entries of all the ceph volumes configs for the new node.
	stmt = `
SELECT id FROM storage_volumes WHERE storage_pool_id=? AND node_id=?
  ORDER BY name, type
`
	volumeIDs, err := query.SelectIntegers(c.tx, stmt, poolID, nodeID)
	if err != nil {
		return errors.Wrap(err, "failed to get joining node's ceph volume IDs")
	}
	otherVolumeIDs, err := query.SelectIntegers(c.tx, stmt, poolID, otherNodeID)
	if err != nil {
		return errors.Wrap(err, "failed to get other node's ceph volume IDs")
	}
	if len(volumeIDs) != len(otherVolumeIDs) { // Sanity check
		return fmt.Errorf("not all ceph volumes were copied")
	}
	for i, otherVolumeID := range otherVolumeIDs {
		config, err := query.SelectConfig(
			c.tx, "storage_volumes_config", "storage_volume_id=?", otherVolumeID)
		if err != nil {
			return errors.Wrap(err, "failed to get storage volume config")
		}
		for key, value := range config {
			_, err := c.tx.Exec(`
INSERT INTO storage_volumes_config(storage_volume_id, key, value) VALUES(?, ?, ?)
`, volumeIDs[i], key, value)
			if err != nil {
				return errors.Wrap(err, "failed to copy volume config")
			}
		}
	}

	return nil
}
func (c *ClusterTx) StoragePoolConfigAdd(poolID, nodeID int64, config map[string]string) error {
	return storagePoolConfigAdd(c.tx, poolID, nodeID, config)
}
func (c *ClusterTx) StoragePoolCreatePending(node, name, driver string, conf map[string]string) error {
	// First check if a storage pool with the given name exists, and, if
	// so, that it has a matching driver and it's in the pending state.
	pool := struct {
		id     int64
		driver string
		state  int
	}{}

	var errConsistency error
	dest := func(i int) []interface{} {
		// Sanity check that there is at most one pool with the given name.
		if i != 0 {
			errConsistency = fmt.Errorf("more than one pool exists with the given name")
		}
		return []interface{}{&pool.id, &pool.driver, &pool.state}
	}
	stmt, err := c.tx.Prepare("SELECT id, driver, state FROM storage_pools WHERE name=?")
	if err != nil {
		return err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, name)
	if err != nil {
		return err
	}
	if errConsistency != nil {
		return errConsistency
	}

	var poolID = pool.id
	if poolID == 0 {
		// No existing pool with the given name was found, let's create
		// one.
		columns := []string{"name", "driver"}
		values := []interface{}{name, driver}
		poolID, err = query.UpsertObject(c.tx, "storage_pools", columns, values)
		if err != nil {
			return err
		}
	} else {
		// Check that the existing pools matches the given driver and
		// is in the pending state.
		if pool.driver != driver {
			return fmt.Errorf("pool already exists with a different driver")
		}
		if pool.state != storagePoolPending {
			return fmt.Errorf("pool is not in pending state")
		}
	}

	// Get the ID of the node with the given name.
	nodeInfo, err := c.NodeByName(node)
	if err != nil {
		return err
	}

	// Check that no storage_pool entry of this node and pool exists yet.
	count, err := query.Count(
		c.tx, "storage_pools_nodes", "storage_pool_id=? AND node_id=?", poolID, nodeInfo.ID)
	if err != nil {
		return err
	}
	if count != 0 {
		return ErrAlreadyDefined
	}

	// Insert the node-specific configuration.
	columns := []string{"storage_pool_id", "node_id"}
	values := []interface{}{poolID, nodeInfo.ID}
	_, err = query.UpsertObject(c.tx, "storage_pools_nodes", columns, values)
	if err != nil {
		return err
	}
	err = c.StoragePoolConfigAdd(poolID, nodeInfo.ID, conf)
	if err != nil {
		return err
	}

	return nil
}
func (c *ClusterTx) StoragePoolCreated(name string) error {
	return c.storagePoolState(name, storagePoolCreated)
}
func (c *ClusterTx) StoragePoolErrored(name string) error {
	return c.storagePoolState(name, storagePoolErrored)
}
func (c *ClusterTx) StoragePoolNodeConfigs(poolID int64) (map[string]map[string]string, error) {
	// Fetch all nodes.
	nodes, err := c.Nodes()
	if err != nil {
		return nil, err
	}

	// Fetch the names of the nodes where the storage pool is defined.
	stmt := `
SELECT nodes.name FROM nodes
  LEFT JOIN storage_pools_nodes ON storage_pools_nodes.node_id = nodes.id
  LEFT JOIN storage_pools ON storage_pools_nodes.storage_pool_id = storage_pools.id
WHERE storage_pools.id = ? AND storage_pools.state = ?
`
	defined, err := query.SelectStrings(c.tx, stmt, poolID, storagePoolPending)
	if err != nil {
		return nil, err
	}

	// Figure which nodes are missing
	missing := []string{}
	for _, node := range nodes {
		if !shared.StringInSlice(node.Name, defined) {
			missing = append(missing, node.Name)
		}
	}

	if len(missing) > 0 {
		return nil, fmt.Errorf("Pool not defined on nodes: %s", strings.Join(missing, ", "))
	}

	configs := map[string]map[string]string{}
	for _, node := range nodes {
		config, err := query.SelectConfig(
			c.tx, "storage_pools_config", "storage_pool_id=? AND node_id=?", poolID, node.ID)
		if err != nil {
			return nil, err
		}
		configs[node.Name] = config
	}

	return configs, nil
}
func (c *Cluster) StoragePoolsGetDrivers() ([]string, error) {
	var poolDriver string
	query := "SELECT DISTINCT driver FROM storage_pools"
	inargs := []interface{}{}
	outargs := []interface{}{poolDriver}

	result, err := queryScan(c.db, query, inargs, outargs)
	if err != nil {
		return []string{}, err
	}

	if len(result) == 0 {
		return []string{}, ErrNoSuchObject
	}

	drivers := []string{}
	for _, driver := range result {
		drivers = append(drivers, driver[0].(string))
	}

	return drivers, nil
}
func (c *Cluster) StoragePoolGetID(poolName string) (int64, error) {
	poolID := int64(-1)
	query := "SELECT id FROM storage_pools WHERE name=?"
	inargs := []interface{}{poolName}
	outargs := []interface{}{&poolID}

	err := dbQueryRowScan(c.db, query, inargs, outargs)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, ErrNoSuchObject
		}
	}

	return poolID, nil
}
func (c *Cluster) StoragePoolGet(poolName string) (int64, *api.StoragePool, error) {
	var poolDriver string
	poolID := int64(-1)
	description := sql.NullString{}
	var state int

	query := "SELECT id, driver, description, state FROM storage_pools WHERE name=?"
	inargs := []interface{}{poolName}
	outargs := []interface{}{&poolID, &poolDriver, &description, &state}

	err := dbQueryRowScan(c.db, query, inargs, outargs)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, nil, ErrNoSuchObject
		}
		return -1, nil, err
	}

	config, err := c.StoragePoolConfigGet(poolID)
	if err != nil {
		return -1, nil, err
	}

	storagePool := api.StoragePool{
		Name:   poolName,
		Driver: poolDriver,
	}
	storagePool.Description = description.String
	storagePool.Config = config

	switch state {
	case storagePoolPending:
		storagePool.Status = "Pending"
	case storagePoolCreated:
		storagePool.Status = "Created"
	default:
		storagePool.Status = "Unknown"
	}

	nodes, err := c.storagePoolNodes(poolID)
	if err != nil {
		return -1, nil, err
	}
	storagePool.Locations = nodes

	return poolID, &storagePool, nil
}
func (c *Cluster) storagePoolNodes(poolID int64) ([]string, error) {
	stmt := `
SELECT nodes.name FROM nodes
  JOIN storage_pools_nodes ON storage_pools_nodes.node_id = nodes.id
  WHERE storage_pools_nodes.storage_pool_id = ?
`
	var nodes []string
	err := c.Transaction(func(tx *ClusterTx) error {
		var err error
		nodes, err = query.SelectStrings(tx.tx, stmt, poolID)
		return err
	})
	if err != nil {
		return nil, err
	}
	return nodes, nil
}
func (c *Cluster) StoragePoolConfigGet(poolID int64) (map[string]string, error) {
	var key, value string
	query := "SELECT key, value FROM storage_pools_config WHERE storage_pool_id=? AND (node_id=? OR node_id IS NULL)"
	inargs := []interface{}{poolID, c.nodeID}
	outargs := []interface{}{key, value}

	results, err := queryScan(c.db, query, inargs, outargs)
	if err != nil {
		return nil, err
	}

	config := map[string]string{}

	for _, r := range results {
		key = r[0].(string)
		value = r[1].(string)

		config[key] = value
	}

	return config, nil
}
func (c *Cluster) StoragePoolCreate(poolName string, poolDescription string, poolDriver string, poolConfig map[string]string) (int64, error) {
	var id int64
	err := c.Transaction(func(tx *ClusterTx) error {
		result, err := tx.tx.Exec("INSERT INTO storage_pools (name, description, driver, state) VALUES (?, ?, ?, ?)", poolName, poolDescription, poolDriver, storagePoolCreated)
		if err != nil {
			return err
		}

		id, err = result.LastInsertId()
		if err != nil {
			return err
		}

		// Insert a node-specific entry pointing to ourselves.
		columns := []string{"storage_pool_id", "node_id"}
		values := []interface{}{id, c.nodeID}
		_, err = query.UpsertObject(tx.tx, "storage_pools_nodes", columns, values)
		if err != nil {
			return err
		}

		err = storagePoolConfigAdd(tx.tx, id, c.nodeID, poolConfig)
		if err != nil {
			return err
		}
		return nil
	})
	if err != nil {
		id = -1
	}

	return id, nil
}
func storagePoolConfigAdd(tx *sql.Tx, poolID, nodeID int64, poolConfig map[string]string) error {
	str := "INSERT INTO storage_pools_config (storage_pool_id, node_id, key, value) VALUES(?, ?, ?, ?)"
	stmt, err := tx.Prepare(str)
	defer stmt.Close()
	if err != nil {
		return err
	}

	for k, v := range poolConfig {
		if v == "" {
			continue
		}
		var nodeIDValue interface{}
		if !shared.StringInSlice(k, StoragePoolNodeConfigKeys) {
			nodeIDValue = nil
		} else {
			nodeIDValue = nodeID
		}

		_, err = stmt.Exec(poolID, nodeIDValue, k, v)
		if err != nil {
			return err
		}
	}

	return nil
}
func (c *Cluster) StoragePoolUpdate(poolName, description string, poolConfig map[string]string) error {
	poolID, _, err := c.StoragePoolGet(poolName)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		err = StoragePoolUpdateDescription(tx.tx, poolID, description)
		if err != nil {
			return err
		}

		err = StoragePoolConfigClear(tx.tx, poolID, c.nodeID)
		if err != nil {
			return err
		}

		err = storagePoolConfigAdd(tx.tx, poolID, c.nodeID, poolConfig)
		if err != nil {
			return err
		}
		return nil
	})

	return err
}
func StoragePoolConfigClear(tx *sql.Tx, poolID, nodeID int64) error {
	_, err := tx.Exec("DELETE FROM storage_pools_config WHERE storage_pool_id=? AND (node_id=? OR node_id IS NULL)", poolID, nodeID)
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) StoragePoolDelete(poolName string) (*api.StoragePool, error) {
	poolID, pool, err := c.StoragePoolGet(poolName)
	if err != nil {
		return nil, err
	}

	err = exec(c.db, "DELETE FROM storage_pools WHERE id=?", poolID)
	if err != nil {
		return nil, err
	}

	return pool, nil
}
func (c *Cluster) StoragePoolVolumesGetNames(poolID int64) ([]string, error) {
	var volumeName string
	query := "SELECT name FROM storage_volumes WHERE storage_pool_id=? AND node_id=?"
	inargs := []interface{}{poolID, c.nodeID}
	outargs := []interface{}{volumeName}

	result, err := queryScan(c.db, query, inargs, outargs)
	if err != nil {
		return []string{}, err
	}

	var out []string

	for _, r := range result {
		out = append(out, r[0].(string))
	}

	return out, nil
}
func (c *Cluster) StoragePoolVolumesGet(project string, poolID int64, volumeTypes []int) ([]*api.StorageVolume, error) {
	var nodeIDs []int

	err := c.Transaction(func(tx *ClusterTx) error {
		var err error
		nodeIDs, err = query.SelectIntegers(tx.tx, `
SELECT DISTINCT node_id
  FROM storage_volumes
  JOIN projects ON projects.id = storage_volumes.project_id
 WHERE (projects.name=? OR storage_volumes.type=?) AND storage_pool_id=?
`, project, StoragePoolVolumeTypeCustom, poolID)
		return err
	})
	if err != nil {
		return nil, err
	}
	volumes := []*api.StorageVolume{}

	for _, nodeID := range nodeIDs {
		nodeVolumes, err := c.storagePoolVolumesGet(project, poolID, int64(nodeID), volumeTypes)
		if err != nil {
			return nil, err
		}
		volumes = append(volumes, nodeVolumes...)
	}
	return volumes, nil
}
func (c *Cluster) StoragePoolNodeVolumesGet(poolID int64, volumeTypes []int) ([]*api.StorageVolume, error) {
	return c.storagePoolVolumesGet("default", poolID, c.nodeID, volumeTypes)
}
func (c *Cluster) storagePoolVolumesGet(project string, poolID, nodeID int64, volumeTypes []int) ([]*api.StorageVolume, error) {
	// Get all storage volumes of all types attached to a given storage
	// pool.
	result := []*api.StorageVolume{}
	for _, volumeType := range volumeTypes {
		volumeNames, err := c.StoragePoolVolumesGetType(project, volumeType, poolID, nodeID)
		if err != nil && err != sql.ErrNoRows {
			return nil, errors.Wrap(err, "failed to fetch volume types")
		}
		for _, volumeName := range volumeNames {
			_, volume, err := c.StoragePoolVolumeGetType(project, volumeName, volumeType, poolID, nodeID)
			if err != nil {
				return nil, errors.Wrap(err, "failed to fetch volume type")
			}
			result = append(result, volume)
		}
	}

	if len(result) == 0 {
		return result, ErrNoSuchObject
	}

	return result, nil
}
func (c *Cluster) StoragePoolVolumesGetType(project string, volumeType int, poolID, nodeID int64) ([]string, error) {
	var poolName string
	query := `
SELECT storage_volumes.name
  FROM storage_volumes
  JOIN projects ON projects.id=storage_volumes.project_id
 WHERE (projects.name=? OR storage_volumes.type=?) AND storage_pool_id=? AND node_id=? AND type=?
`
	inargs := []interface{}{project, StoragePoolVolumeTypeCustom, poolID, nodeID, volumeType}
	outargs := []interface{}{poolName}

	result, err := queryScan(c.db, query, inargs, outargs)
	if err != nil {
		return []string{}, err
	}

	response := []string{}
	for _, r := range result {
		response = append(response, r[0].(string))
	}

	return response, nil
}
func (c *Cluster) StoragePoolVolumeSnapshotsGetType(volumeName string, volumeType int, poolID int64) ([]string, error) {
	result := []string{}
	regexp := volumeName + shared.SnapshotDelimiter
	length := len(regexp)

	query := "SELECT name FROM storage_volumes WHERE storage_pool_id=? AND node_id=? AND type=? AND snapshot=? AND SUBSTR(name,1,?)=?"
	inargs := []interface{}{poolID, c.nodeID, volumeType, true, length, regexp}
	outfmt := []interface{}{volumeName}

	dbResults, err := queryScan(c.db, query, inargs, outfmt)
	if err != nil {
		return result, err
	}

	for _, r := range dbResults {
		result = append(result, r[0].(string))
	}

	return result, nil
}
func (c *Cluster) StoragePoolNodeVolumesGetType(volumeType int, poolID int64) ([]string, error) {
	return c.StoragePoolVolumesGetType("default", volumeType, poolID, c.nodeID)
}
func (c *Cluster) StoragePoolVolumeGetType(project string, volumeName string, volumeType int, poolID, nodeID int64) (int64, *api.StorageVolume, error) {
	// Custom volumes are "global", i.e. they are associated with the
	// default project.
	if volumeType == StoragePoolVolumeTypeCustom {
		project = "default"
	}

	volumeID, err := c.StoragePoolVolumeGetTypeID(project, volumeName, volumeType, poolID, nodeID)
	if err != nil {
		return -1, nil, err
	}

	volumeNode, err := c.StorageVolumeNodeGet(volumeID)
	if err != nil {
		return -1, nil, err
	}

	volumeConfig, err := c.StorageVolumeConfigGet(volumeID)
	if err != nil {
		return -1, nil, err
	}

	volumeDescription, err := c.StorageVolumeDescriptionGet(volumeID)
	if err != nil {
		return -1, nil, err
	}

	volumeTypeName, err := StoragePoolVolumeTypeToName(volumeType)
	if err != nil {
		return -1, nil, err
	}

	storageVolume := api.StorageVolume{
		Type: volumeTypeName,
	}
	storageVolume.Name = volumeName
	storageVolume.Description = volumeDescription
	storageVolume.Config = volumeConfig
	storageVolume.Location = volumeNode

	return volumeID, &storageVolume, nil
}
func (c *Cluster) StoragePoolNodeVolumeGetType(volumeName string, volumeType int, poolID int64) (int64, *api.StorageVolume, error) {
	return c.StoragePoolNodeVolumeGetTypeByProject("default", volumeName, volumeType, poolID)
}
func (c *Cluster) StoragePoolNodeVolumeGetTypeByProject(project, volumeName string, volumeType int, poolID int64) (int64, *api.StorageVolume, error) {
	return c.StoragePoolVolumeGetType(project, volumeName, volumeType, poolID, c.nodeID)
}
func (c *Cluster) StoragePoolVolumeUpdate(volumeName string, volumeType int, poolID int64, volumeDescription string, volumeConfig map[string]string) error {
	volumeID, _, err := c.StoragePoolNodeVolumeGetType(volumeName, volumeType, poolID)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		err = storagePoolVolumeReplicateIfCeph(tx.tx, volumeID, "default", volumeName, volumeType, poolID, func(volumeID int64) error {
			err = StorageVolumeConfigClear(tx.tx, volumeID)
			if err != nil {
				return err
			}

			err = StorageVolumeConfigAdd(tx.tx, volumeID, volumeConfig)
			if err != nil {
				return err
			}

			return StorageVolumeDescriptionUpdate(tx.tx, volumeID, volumeDescription)
		})
		if err != nil {
			return err
		}
		return nil
	})

	return err
}
func (c *Cluster) StoragePoolVolumeDelete(project, volumeName string, volumeType int, poolID int64) error {
	volumeID, _, err := c.StoragePoolNodeVolumeGetTypeByProject(project, volumeName, volumeType, poolID)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		err := storagePoolVolumeReplicateIfCeph(tx.tx, volumeID, project, volumeName, volumeType, poolID, func(volumeID int64) error {
			_, err := tx.tx.Exec("DELETE FROM storage_volumes WHERE id=?", volumeID)
			return err
		})
		return err
	})

	return err
}
func (c *Cluster) StoragePoolVolumeRename(project, oldVolumeName string, newVolumeName string, volumeType int, poolID int64) error {
	volumeID, _, err := c.StoragePoolNodeVolumeGetTypeByProject(project, oldVolumeName, volumeType, poolID)
	if err != nil {
		return err
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		err := storagePoolVolumeReplicateIfCeph(tx.tx, volumeID, project, oldVolumeName, volumeType, poolID, func(volumeID int64) error {
			_, err := tx.tx.Exec("UPDATE storage_volumes SET name=? WHERE id=? AND type=?", newVolumeName, volumeID, volumeType)
			return err
		})
		return err
	})

	return err
}
func storagePoolVolumeReplicateIfCeph(tx *sql.Tx, volumeID int64, project, volumeName string, volumeType int, poolID int64, f func(int64) error) error {
	driver, err := storagePoolDriverGet(tx, poolID)
	if err != nil {
		return err
	}
	volumeIDs := []int64{volumeID}

	// If this is a ceph volume, we want to duplicate the change across the
	// the rows for all other nodes.
	if driver == "ceph" {
		volumeIDs, err = storageVolumeIDsGet(tx, project, volumeName, volumeType, poolID)
		if err != nil {
			return err
		}
	}

	for _, volumeID := range volumeIDs {
		err := f(volumeID)
		if err != nil {
			return err
		}
	}

	return nil
}
func (c *Cluster) StoragePoolVolumeCreate(project, volumeName, volumeDescription string, volumeType int, snapshot bool, poolID int64, volumeConfig map[string]string) (int64, error) {
	var thisVolumeID int64

	err := c.Transaction(func(tx *ClusterTx) error {
		nodeIDs := []int{int(c.nodeID)}
		driver, err := storagePoolDriverGet(tx.tx, poolID)
		if err != nil {
			return err
		}
		// If the driver is ceph, create a volume entry for each node.
		if driver == "ceph" {
			nodeIDs, err = query.SelectIntegers(tx.tx, "SELECT id FROM nodes")
			if err != nil {
				return err
			}
		}

		for _, nodeID := range nodeIDs {
			result, err := tx.tx.Exec(`
INSERT INTO storage_volumes (storage_pool_id, node_id, type, snapshot, name, description, project_id) VALUES (?, ?, ?, ?, ?, ?, (SELECT id FROM projects WHERE name = ?))
`,
				poolID, nodeID, volumeType, snapshot, volumeName, volumeDescription, project)
			if err != nil {
				return err
			}

			volumeID, err := result.LastInsertId()
			if err != nil {
				return err
			}
			if int64(nodeID) == c.nodeID {
				// Return the ID of the volume created on this node.
				thisVolumeID = volumeID
			}

			err = StorageVolumeConfigAdd(tx.tx, volumeID, volumeConfig)
			if err != nil {
				tx.tx.Rollback()
				return err
			}
		}
		return nil
	})
	if err != nil {
		thisVolumeID = -1
	}

	return thisVolumeID, err
}
func (c *Cluster) StoragePoolVolumeGetTypeID(project string, volumeName string, volumeType int, poolID, nodeID int64) (int64, error) {
	volumeID := int64(-1)
	query := `SELECT storage_volumes.id
FROM storage_volumes
JOIN storage_pools ON storage_volumes.storage_pool_id = storage_pools.id
JOIN projects ON storage_volumes.project_id = projects.id
WHERE projects.name=? AND storage_volumes.storage_pool_id=? AND storage_volumes.node_id=?
AND storage_volumes.name=? AND storage_volumes.type=?`
	inargs := []interface{}{project, poolID, nodeID, volumeName, volumeType}
	outargs := []interface{}{&volumeID}

	err := dbQueryRowScan(c.db, query, inargs, outargs)
	if err != nil {
		if err == sql.ErrNoRows {
			return -1, ErrNoSuchObject
		}
		return -1, err
	}

	return volumeID, nil
}
func (c *Cluster) StoragePoolNodeVolumeGetTypeID(volumeName string, volumeType int, poolID int64) (int64, error) {
	return c.StoragePoolVolumeGetTypeID("default", volumeName, volumeType, poolID, c.nodeID)
}
func StoragePoolVolumeTypeToName(volumeType int) (string, error) {
	switch volumeType {
	case StoragePoolVolumeTypeContainer:
		return StoragePoolVolumeTypeNameContainer, nil
	case StoragePoolVolumeTypeImage:
		return StoragePoolVolumeTypeNameImage, nil
	case StoragePoolVolumeTypeCustom:
		return StoragePoolVolumeTypeNameCustom, nil
	}

	return "", fmt.Errorf("invalid storage volume type")
}
func DevicesAdd(tx *sql.Tx, w string, cID int64, devices types.Devices) error {
	// Prepare the devices entry SQL
	str1 := fmt.Sprintf("INSERT INTO %ss_devices (%s_id, name, type) VALUES (?, ?, ?)", w, w)
	stmt1, err := tx.Prepare(str1)
	if err != nil {
		return err
	}
	defer stmt1.Close()

	// Prepare the devices config entry SQL
	str2 := fmt.Sprintf("INSERT INTO %ss_devices_config (%s_device_id, key, value) VALUES (?, ?, ?)", w, w)
	stmt2, err := tx.Prepare(str2)
	if err != nil {
		return err
	}
	defer stmt2.Close()

	// Insert all the devices
	for k, v := range devices {
		t, err := dbDeviceTypeToInt(v["type"])
		if err != nil {
			return err
		}

		result, err := stmt1.Exec(cID, k, t)
		if err != nil {
			return err
		}

		id64, err := result.LastInsertId()
		if err != nil {
			return fmt.Errorf("Error inserting device %s into database", k)
		}
		id := int(id64)

		for ck, cv := range v {
			// The type is stored as int in the parent entry
			if ck == "type" || cv == "" {
				continue
			}

			_, err = stmt2.Exec(id, ck, cv)
			if err != nil {
				return err
			}
		}
	}

	return nil
}
func (c *Cluster) Devices(project, qName string, isprofile bool) (types.Devices, error) {
	err := c.Transaction(func(tx *ClusterTx) error {
		enabled, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return err
		}
		if !enabled {
			project = "default"
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	var q string
	if isprofile {
		q = `SELECT profiles_devices.id, profiles_devices.name, profiles_devices.type
			FROM profiles_devices
                        JOIN profiles ON profiles_devices.profile_id = profiles.id
                        JOIN projects ON projects.id=profiles.project_id
   		WHERE projects.name=? AND profiles.name=?`
	} else {
		q = `SELECT containers_devices.id, containers_devices.name, containers_devices.type
			FROM containers_devices
                        JOIN containers	ON containers_devices.container_id = containers.id
                        JOIN projects ON projects.id=containers.project_id
			WHERE projects.name=? AND containers.name=?`
	}
	var id, dtype int
	var name, stype string
	inargs := []interface{}{project, qName}
	outfmt := []interface{}{id, name, dtype}
	results, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return nil, err
	}

	devices := types.Devices{}
	for _, r := range results {
		id = r[0].(int)
		name = r[1].(string)
		stype, err = dbDeviceTypeToString(r[2].(int))
		if err != nil {
			return nil, err
		}
		newdev, err := dbDeviceConfig(c.db, id, isprofile)
		if err != nil {
			return nil, err
		}
		newdev["type"] = stype
		devices[name] = newdev
	}

	return devices, nil
}
func (n *Node) Patches() ([]string, error) {
	inargs := []interface{}{}
	outfmt := []interface{}{""}

	query := fmt.Sprintf("SELECT name FROM patches")
	result, err := queryScan(n.db, query, inargs, outfmt)
	if err != nil {
		return []string{}, err
	}

	response := []string{}
	for _, r := range result {
		response = append(response, r[0].(string))
	}

	return response, nil
}
func (n *Node) PatchesMarkApplied(patch string) error {
	stmt := `INSERT INTO patches (name, applied_at) VALUES (?, strftime("%s"));`
	_, err := n.db.Exec(stmt, patch)
	return err
}
func entityType(pkg string, entity string) string {
	typ := lex.Capital(entity)
	if pkg != "db" {
		typ = pkg + "." + typ
	}
	return typ
}
func entityPost(entity string) string {
	return fmt.Sprintf("%sPost", lex.Capital(lex.Plural(entity)))
}
func stmtCodeVar(entity string, kind string, filters ...string) string {
	name := fmt.Sprintf("%s%s", entity, lex.Camel(kind))

	if len(filters) > 0 {
		name += "By"
		name += strings.Join(filters, "And")
	}

	return name
}
func destFunc(slice string, typ string, fields []*Field) string {
	f := fmt.Sprintf(`func(i int) []interface{} {
                      %s = append(%s, %s{})
                      return []interface{}{
`, slice, slice, typ)

	for _, field := range fields {
		f += fmt.Sprintf("&%s[i].%s,\n", slice, field.Name)
	}

	f += "        }\n"
	f += "}"

	return f
}
func CompareConfigs(config1, config2 map[string]string, exclude []string) error {
	if exclude == nil {
		exclude = []string{}
	}

	delta := []string{}
	for key, value := range config1 {
		if shared.StringInSlice(key, exclude) {
			continue
		}
		if config2[key] != value {
			delta = append(delta, key)
		}
	}
	for key, value := range config2 {
		if shared.StringInSlice(key, exclude) {
			continue
		}
		if config1[key] != value {
			present := false
			for i := range delta {
				if delta[i] == key {
					present = true
				}
				break
			}
			if !present {
				delta = append(delta, key)
			}
		}
	}
	sort.Strings(delta)
	if len(delta) > 0 {
		return fmt.Errorf("different values for keys: %s", strings.Join(delta, ", "))
	}

	return nil
}
func CopyConfig(config map[string]string) map[string]string {
	copy := map[string]string{}
	for key, value := range config {
		copy[key] = value
	}
	return copy
}
func NewNotifier(state *state.State, cert *shared.CertInfo, policy NotifierPolicy) (Notifier, error) {
	address, err := node.ClusterAddress(state.Node)
	if err != nil {
		return nil, errors.Wrap(err, "failed to fetch node address")
	}

	// Fast-track the case where we're not clustered at all.
	if address == "" {
		nullNotifier := func(func(lxd.ContainerServer) error) error { return nil }
		return nullNotifier, nil
	}

	peers := []string{}
	err = state.Cluster.Transaction(func(tx *db.ClusterTx) error {
		offlineThreshold, err := tx.NodeOfflineThreshold()
		if err != nil {
			return err
		}

		nodes, err := tx.Nodes()
		if err != nil {
			return err
		}
		for _, node := range nodes {
			if node.Address == address || node.Address == "0.0.0.0" {
				continue // Exclude ourselves
			}
			if node.IsOffline(offlineThreshold) {
				switch policy {
				case NotifyAll:
					return fmt.Errorf("peer node %s is down", node.Address)
				case NotifyAlive:
					continue // Just skip this node
				}
			}
			peers = append(peers, node.Address)
		}
		return nil
	})
	if err != nil {
		return nil, err
	}

	notifier := func(hook func(lxd.ContainerServer) error) error {
		errs := make([]error, len(peers))
		wg := sync.WaitGroup{}
		wg.Add(len(peers))
		for i, address := range peers {
			logger.Debugf("Notify node %s of state changes", address)
			go func(i int, address string) {
				defer wg.Done()
				client, err := Connect(address, cert, true)
				if err != nil {
					errs[i] = errors.Wrapf(err, "failed to connect to peer %s", address)
					return
				}
				err = hook(client)
				if err != nil {
					errs[i] = errors.Wrapf(err, "failed to notify peer %s", address)
				}
			}(i, address)
		}
		wg.Wait()
		// TODO: aggregate all errors?
		for i, err := range errs {
			if err != nil {
				// FIXME: unfortunately the LXD client currently does not
				//        provide a way to differentiate between errors
				if isClientConnectionError(err) && policy == NotifyAlive {
					logger.Warnf("Could not notify node %s", peers[i])
					continue
				}
				return err
			}
		}
		return nil
	}

	return notifier, nil
}
func Events(endpoints *endpoints.Endpoints, cluster *db.Cluster, f func(int64, api.Event)) (task.Func, task.Schedule) {
	listeners := map[int64]*lxd.EventListener{}

	// Update our pool of event listeners. Since database queries are
	// blocking, we spawn the actual logic in a goroutine, to abort
	// immediately when we receive the stop signal.
	update := func(ctx context.Context) {
		ch := make(chan struct{})
		go func() {
			eventsUpdateListeners(endpoints, cluster, listeners, f)
			ch <- struct{}{}
		}()
		select {
		case <-ch:
		case <-ctx.Done():
		}

	}

	schedule := task.Every(time.Second)

	return update, schedule
}
func eventsConnect(address string, cert *shared.CertInfo) (*lxd.EventListener, error) {
	client, err := Connect(address, cert, true)
	if err != nil {
		return nil, err
	}

	// Set the project to the special wildcard in order to get notified
	// about all events across all projects.
	client = client.UseProject("*")

	return client.GetEvents()
}
func (s *storageDir) StoragePoolInit() error {
	err := s.StorageCoreInit()
	if err != nil {
		return err
	}

	return nil
}
func getAAProfileContent(c container) string {
	profile := strings.TrimLeft(AA_PROFILE_BASE, "\n")

	// Apply new features
	if aaParserSupports("unix") {
		profile += `
  ### Feature: unix
  # Allow receive via unix sockets from anywhere
  unix (receive),

  # Allow all unix in the container
  unix peer=(label=@{profile_name}),
`
	}

	// Apply cgns bits
	if shared.PathExists("/proc/self/ns/cgroup") {
		profile += "\n  ### Feature: cgroup namespace\n"
		profile += "  mount fstype=cgroup -> /sys/fs/cgroup/**,\n"
		profile += "  mount fstype=cgroup2 -> /sys/fs/cgroup/**,\n"
	}

	state := c.DaemonState()
	if state.OS.AppArmorStacking && !state.OS.AppArmorStacked {
		profile += "\n  ### Feature: apparmor stacking\n"
		profile += `  ### Configuration: apparmor profile loading (in namespace)
  deny /sys/k[^e]*{,/**} wklx,
  deny /sys/ke[^r]*{,/**} wklx,
  deny /sys/ker[^n]*{,/**} wklx,
  deny /sys/kern[^e]*{,/**} wklx,
  deny /sys/kerne[^l]*{,/**} wklx,
  deny /sys/kernel/[^s]*{,/**} wklx,
  deny /sys/kernel/s[^e]*{,/**} wklx,
  deny /sys/kernel/se[^c]*{,/**} wklx,
  deny /sys/kernel/sec[^u]*{,/**} wklx,
  deny /sys/kernel/secu[^r]*{,/**} wklx,
  deny /sys/kernel/secur[^i]*{,/**} wklx,
  deny /sys/kernel/securi[^t]*{,/**} wklx,
  deny /sys/kernel/securit[^y]*{,/**} wklx,
  deny /sys/kernel/security/[^a]*{,/**} wklx,
  deny /sys/kernel/security/a[^p]*{,/**} wklx,
  deny /sys/kernel/security/ap[^p]*{,/**} wklx,
  deny /sys/kernel/security/app[^a]*{,/**} wklx,
  deny /sys/kernel/security/appa[^r]*{,/**} wklx,
  deny /sys/kernel/security/appar[^m]*{,/**} wklx,
  deny /sys/kernel/security/apparm[^o]*{,/**} wklx,
  deny /sys/kernel/security/apparmo[^r]*{,/**} wklx,
  deny /sys/kernel/security/apparmor?*{,/**} wklx,
  deny /sys/kernel/security?*{,/**} wklx,
  deny /sys/kernel?*{,/**} wklx,
`
		profile += fmt.Sprintf("  change_profile -> \":%s:*\",\n", AANamespace(c))
		profile += fmt.Sprintf("  change_profile -> \":%s://*\",\n", AANamespace(c))
	} else {
		profile += "\n  ### Feature: apparmor stacking (not present)\n"
		profile += "  deny /sys/k*{,/**} wklx,\n"
	}

	if c.IsNesting() {
		// Apply nesting bits
		profile += "\n  ### Configuration: nesting\n"
		profile += strings.TrimLeft(AA_PROFILE_NESTING, "\n")
		if !state.OS.AppArmorStacking || state.OS.AppArmorStacked {
			profile += fmt.Sprintf("  change_profile -> \"%s\",\n", AAProfileFull(c))
		}
	}

	if !c.IsPrivileged() || state.OS.RunningInUserNS {
		// Apply unprivileged bits
		profile += "\n  ### Configuration: unprivileged containers\n"
		profile += strings.TrimLeft(AA_PROFILE_UNPRIVILEGED, "\n")
	}

	// Append raw.apparmor
	rawApparmor, ok := c.ExpandedConfig()["raw.apparmor"]
	if ok {
		profile += "\n  ### Configuration: raw.apparmor\n"
		for _, line := range strings.Split(strings.Trim(rawApparmor, "\n"), "\n") {
			profile += fmt.Sprintf("  %s\n", line)
		}
	}

	return fmt.Sprintf(`#include <tunables/global>
profile "%s" flags=(attach_disconnected,mediate_deleted) {
%s
}
`, AAProfileFull(c), strings.Trim(profile, "\n"))
}
func AALoadProfile(c container) error {
	state := c.DaemonState()
	if !state.OS.AppArmorAdmin {
		return nil
	}

	if err := mkApparmorNamespace(c, AANamespace(c)); err != nil {
		return err
	}

	/* In order to avoid forcing a profile parse (potentially slow) on
	 * every container start, let's use apparmor's binary policy cache,
	 * which checks mtime of the files to figure out if the policy needs to
	 * be regenerated.
	 *
	 * Since it uses mtimes, we shouldn't just always write out our local
	 * apparmor template; instead we should check to see whether the
	 * template is the same as ours. If it isn't we should write our
	 * version out so that the new changes are reflected and we definitely
	 * force a recompile.
	 */
	profile := path.Join(aaPath, "profiles", AAProfileShort(c))
	content, err := ioutil.ReadFile(profile)
	if err != nil && !os.IsNotExist(err) {
		return err
	}

	updated := getAAProfileContent(c)

	if string(content) != string(updated) {
		if err := os.MkdirAll(path.Join(aaPath, "cache"), 0700); err != nil {
			return err
		}

		if err := os.MkdirAll(path.Join(aaPath, "profiles"), 0700); err != nil {
			return err
		}

		if err := ioutil.WriteFile(profile, []byte(updated), 0600); err != nil {
			return err
		}
	}

	return runApparmor(APPARMOR_CMD_LOAD, c)
}
func AADestroy(c container) error {
	state := c.DaemonState()
	if !state.OS.AppArmorAdmin {
		return nil
	}

	if state.OS.AppArmorStacking && !state.OS.AppArmorStacked {
		p := path.Join("/sys/kernel/security/apparmor/policy/namespaces", AANamespace(c))
		if err := os.Remove(p); err != nil {
			logger.Error("Error removing apparmor namespace", log.Ctx{"err": err, "ns": p})
		}
	}

	return runApparmor(APPARMOR_CMD_UNLOAD, c)
}
func AAParseProfile(c container) error {
	state := c.DaemonState()
	if !state.OS.AppArmorAvailable {
		return nil
	}

	return runApparmor(APPARMOR_CMD_PARSE, c)
}
func getSystemHandler(syslog string, debug bool, format log.Format) log.Handler {
	return nil
}
func NotifyUpgradeCompleted(state *state.State, cert *shared.CertInfo) error {
	notifier, err := NewNotifier(state, cert, NotifyAll)
	if err != nil {
		return err
	}
	return notifier(func(client lxd.ContainerServer) error {
		info, err := client.GetConnectionInfo()
		if err != nil {
			return errors.Wrap(err, "failed to get connection info")
		}

		url := fmt.Sprintf("%s%s", info.Addresses[0], databaseEndpoint)
		request, err := http.NewRequest("PATCH", url, nil)
		if err != nil {
			return errors.Wrap(err, "failed to create database notify upgrade request")
		}

		httpClient, err := client.GetHTTPClient()
		if err != nil {
			return errors.Wrap(err, "failed to get HTTP client")
		}

		response, err := httpClient.Do(request)
		if err != nil {
			return errors.Wrap(err, "failed to notify node about completed upgrade")
		}

		if response.StatusCode != http.StatusOK {
			return fmt.Errorf("database upgrade notification failed: %s", response.Status)
		}

		return nil
	})
}
func KeepUpdated(state *state.State) (task.Func, task.Schedule) {
	f := func(ctx context.Context) {
		ch := make(chan struct{})
		go func() {
			maybeUpdate(state)
			close(ch)
		}()
		select {
		case <-ctx.Done():
		case <-ch:
		}
	}

	schedule := task.Every(5 * time.Minute)

	return f, schedule
}
func maybeUpdate(state *state.State) {
	shouldUpdate := false

	enabled, err := Enabled(state.Node)
	if err != nil {
		logger.Errorf("Failed to check clustering is enabled: %v", err)
		return
	}
	if !enabled {
		return
	}

	err = state.Cluster.Transaction(func(tx *db.ClusterTx) error {
		outdated, err := tx.NodeIsOutdated()
		if err != nil {
			return err
		}
		shouldUpdate = outdated
		return nil
	})

	if err != nil {
		// Just log the error and return.
		logger.Errorf("Failed to check if this node is out-of-date: %v", err)
		return
	}

	if !shouldUpdate {
		logger.Debugf("Cluster node is up-to-date")
		return
	}

	logger.Infof("Node is out-of-date with respect to other cluster nodes")

	updateExecutable := os.Getenv("LXD_CLUSTER_UPDATE")
	if updateExecutable == "" {
		logger.Debug("No LXD_CLUSTER_UPDATE variable set, skipping auto-update")
		return
	}

	logger.Infof("Triggering cluster update using: %s", updateExecutable)

	_, err = shared.RunCommand(updateExecutable)
	if err != nil {
		logger.Errorf("Cluster upgrade failed: '%v'", err.Error())
		return
	}
}
func NewServer(apiURL string, apiKey string, agentAuthURL string, agentUsername string, agentPrivateKey string, agentPublicKey string) (*Server, error) {
	r := Server{
		apiURL:          apiURL,
		apiKey:          apiKey,
		lastSyncID:      "",
		lastChange:      time.Time{},
		resources:       make(map[string]string),
		permissions:     make(map[string]map[string][]string),
		permissionsLock: &sync.Mutex{},
	}

	//
	var keyPair bakery.KeyPair
	keyPair.Private.UnmarshalText([]byte(agentPrivateKey))
	keyPair.Public.UnmarshalText([]byte(agentPublicKey))

	r.client = httpbakery.NewClient()
	authInfo := agent.AuthInfo{
		Key: &keyPair,
		Agents: []agent.Agent{
			{
				URL:      agentAuthURL,
				Username: agentUsername,
			},
		},
	}

	err := agent.SetUpAuth(r.client, &authInfo)
	if err != nil {
		return nil, err
	}

	r.client.Client.Jar, err = cookiejar.New(nil)
	if err != nil {
		return nil, err
	}

	return &r, nil
}
func (r *Server) StartStatusCheck() {
	// Initialize the last changed timestamp
	r.hasStatusChanged()

	r.statusDone = make(chan int)
	go func() {
		for {
			select {
			case <-r.statusDone:
				return
			case <-time.After(time.Minute):
				if r.hasStatusChanged() {
					r.flushCache()
				}
			}
		}
	}()
}
func (r *Server) SyncProjects() error {
	if r.ProjectsFunc == nil {
		return fmt.Errorf("ProjectsFunc isn't configured yet, cannot sync")
	}

	resources := []rbacResource{}
	resourcesMap := map[string]string{}

	// Get all projects
	projects, err := r.ProjectsFunc()
	if err != nil {
		return err
	}

	// Convert to RBAC format
	for id, name := range projects {
		resources = append(resources, rbacResource{
			Name:       name,
			Identifier: strconv.FormatInt(id, 10),
		})

		resourcesMap[name] = strconv.FormatInt(id, 10)
	}

	// Update RBAC
	err = r.postResources(resources, nil, true)
	if err != nil {
		return err
	}

	// Update project map
	r.resourcesLock.Lock()
	r.resources = resourcesMap
	r.resourcesLock.Unlock()

	return nil
}
func (r *Server) AddProject(id int64, name string) error {
	resource := rbacResource{
		Name:       name,
		Identifier: strconv.FormatInt(id, 10),
	}

	// Update RBAC
	err := r.postResources([]rbacResource{resource}, nil, false)
	if err != nil {
		return err
	}

	// Update project map
	r.resourcesLock.Lock()
	r.resources[name] = strconv.FormatInt(id, 10)
	r.resourcesLock.Unlock()

	return nil
}
func (r *Server) DeleteProject(id int64) error {
	// Update RBAC
	err := r.postResources(nil, []string{strconv.FormatInt(id, 10)}, false)
	if err != nil {
		return err
	}

	// Update project map
	r.resourcesLock.Lock()
	for k, v := range r.resources {
		if v == strconv.FormatInt(id, 10) {
			delete(r.resources, k)
			break
		}
	}
	r.resourcesLock.Unlock()

	return nil
}
func (r *Server) RenameProject(id int64, name string) error {
	return r.AddProject(id, name)
}
func (r *Server) IsAdmin(username string) bool {
	r.permissionsLock.Lock()
	defer r.permissionsLock.Unlock()

	// Check whether the permissions are cached
	_, cached := r.permissions[username]

	if !cached {
		r.syncPermissions(username)
	}

	return shared.StringInSlice("admin", r.permissions[username][""])
}
func (r *Server) HasPermission(username, project, permission string) bool {
	r.permissionsLock.Lock()
	defer r.permissionsLock.Unlock()

	// Check whether the permissions are cached
	_, cached := r.permissions[username]

	if !cached {
		r.syncPermissions(username)
	}

	r.resourcesLock.Lock()
	permissions := r.permissions[username][r.resources[project]]
	r.resourcesLock.Unlock()

	return shared.StringInSlice(permission, permissions)
}
func rsyncSend(conn *websocket.Conn, path string, rsyncArgs string) error {
	cmd, dataSocket, stderr, err := rsyncSendSetup(path, rsyncArgs)
	if err != nil {
		return err
	}

	if dataSocket != nil {
		defer dataSocket.Close()
	}

	readDone, writeDone := shared.WebsocketMirror(conn, dataSocket, io.ReadCloser(dataSocket), nil, nil)

	output, err := ioutil.ReadAll(stderr)
	if err != nil {
		cmd.Process.Kill()
		cmd.Wait()
		return fmt.Errorf("Failed to rsync: %v\n%s", err, output)
	}

	err = cmd.Wait()
	<-readDone
	<-writeDone

	if err != nil {
		return fmt.Errorf("Failed to rsync: %v\n%s", err, output)
	}

	return nil
}
func rsyncSendSetup(path string, rsyncArgs string) (*exec.Cmd, net.Conn, io.ReadCloser, error) {
	auds := fmt.Sprintf("@lxd-p2c/%s", uuid.NewRandom().String())
	if len(auds) > shared.ABSTRACT_UNIX_SOCK_LEN-1 {
		auds = auds[:shared.ABSTRACT_UNIX_SOCK_LEN-1]
	}

	l, err := net.Listen("unix", auds)
	if err != nil {
		return nil, nil, nil, err
	}

	execPath, err := os.Readlink("/proc/self/exe")
	if err != nil {
		return nil, nil, nil, err
	}

	if !shared.PathExists(execPath) {
		execPath = os.Args[0]
	}

	rsyncCmd := fmt.Sprintf("sh -c \"%s netcat %s\"", execPath, auds)

	args := []string{
		"-ar",
		"--devices",
		"--numeric-ids",
		"--partial",
		"--sparse",
		"--xattrs",
		"--delete",
		"--compress",
		"--compress-level=2",
	}

	// Ignore deletions (requires 3.1 or higher)
	rsyncCheckVersion := func(min string) bool {
		out, err := shared.RunCommand("rsync", "--version")
		if err != nil {
			return false
		}

		fields := strings.Split(out, " ")
		curVer, err := version.Parse(fields[3])
		if err != nil {
			return false
		}

		minVer, err := version.Parse(min)
		if err != nil {
			return false
		}

		return curVer.Compare(minVer) >= 0
	}

	if rsyncCheckVersion("3.1.0") {
		args = append(args, "--ignore-missing-args")
	}

	if rsyncArgs != "" {
		args = append(args, strings.Split(rsyncArgs, " ")...)
	}

	args = append(args, []string{path, "localhost:/tmp/foo"}...)
	args = append(args, []string{"-e", rsyncCmd}...)

	cmd := exec.Command("rsync", args...)
	cmd.Stdout = os.Stderr

	stderr, err := cmd.StderrPipe()
	if err != nil {
		return nil, nil, nil, err
	}

	if err := cmd.Start(); err != nil {
		return nil, nil, nil, err
	}

	conn, err := l.Accept()
	if err != nil {
		cmd.Process.Kill()
		cmd.Wait()
		return nil, nil, nil, err
	}
	l.Close()

	return cmd, conn, stderr, nil
}
func tlsClientConfig(info *shared.CertInfo) (*tls.Config, error) {
	keypair := info.KeyPair()
	ca := info.CA()
	config := shared.InitTLSConfig()
	config.Certificates = []tls.Certificate{keypair}
	config.RootCAs = x509.NewCertPool()
	if ca != nil {
		config.RootCAs.AddCert(ca)
	}
	// Since the same cluster keypair is used both as server and as client
	// cert, let's add it to the CA pool to make it trusted.
	cert, err := x509.ParseCertificate(keypair.Certificate[0])
	if err != nil {
		return nil, err
	}
	cert.IsCA = true
	cert.KeyUsage = x509.KeyUsageCertSign
	config.RootCAs.AddCert(cert)

	if cert.DNSNames != nil {
		config.ServerName = cert.DNSNames[0]
	}
	return config, nil
}
func tlsCheckCert(r *http.Request, info *shared.CertInfo) bool {
	cert, err := x509.ParseCertificate(info.KeyPair().Certificate[0])
	if err != nil {
		// Since we have already loaded this certificate, typically
		// using LoadX509KeyPair, an error should never happen, but
		// check for good measure.
		panic(fmt.Sprintf("invalid keypair material: %v", err))
	}
	trustedCerts := map[string]x509.Certificate{"0": *cert}

	trusted, _ := util.CheckTrustState(*r.TLS.PeerCertificates[0], trustedCerts)

	return r.TLS != nil && trusted
}
func internalClusterContainerMovedPost(d *Daemon, r *http.Request) Response {
	project := projectParam(r)
	containerName := mux.Vars(r)["name"]
	err := containerPostCreateContainerMountPoint(d, project, containerName)
	if err != nil {
		return SmartError(err)
	}
	return EmptySyncResponse
}
func containerPostCreateContainerMountPoint(d *Daemon, project, containerName string) error {
	c, err := containerLoadByProjectAndName(d.State(), project, containerName)
	if err != nil {
		return errors.Wrap(err, "Failed to load moved container on target node")
	}
	poolName, err := c.StoragePool()
	if err != nil {
		return errors.Wrap(err, "Failed get pool name of moved container on target node")
	}
	snapshotNames, err := d.cluster.ContainerGetSnapshots(project, containerName)
	if err != nil {
		return errors.Wrap(err, "Failed to create container snapshot names")
	}

	containerMntPoint := getContainerMountPoint(c.Project(), poolName, containerName)
	err = createContainerMountpoint(containerMntPoint, c.Path(), c.IsPrivileged())
	if err != nil {
		return errors.Wrap(err, "Failed to create container mount point on target node")
	}

	for _, snapshotName := range snapshotNames {
		mntPoint := getSnapshotMountPoint(project, poolName, snapshotName)
		snapshotsSymlinkTarget := shared.VarPath("storage-pools",
			poolName, "containers-snapshots", containerName)
		snapshotMntPointSymlink := shared.VarPath("snapshots", containerName)
		err := createSnapshotMountpoint(mntPoint, snapshotsSymlinkTarget, snapshotMntPointSymlink)
		if err != nil {
			return errors.Wrap(err, "Failed to create snapshot mount point on target node")
		}
	}

	return nil
}
func (list Devices) Contains(k string, d Device) bool {
	// If it didn't exist, it's different
	if list[k] == nil {
		return false
	}

	old := list[k]

	return deviceEquals(old, d)
}
func (list Devices) Update(newlist Devices) (map[string]Device, map[string]Device, map[string]Device, []string) {
	rmlist := map[string]Device{}
	addlist := map[string]Device{}
	updatelist := map[string]Device{}

	for key, d := range list {
		if !newlist.Contains(key, d) {
			rmlist[key] = d
		}
	}

	for key, d := range newlist {
		if !list.Contains(key, d) {
			addlist[key] = d
		}
	}

	updateDiff := []string{}
	for key, d := range addlist {
		srcOldDevice := rmlist[key]
		var oldDevice Device
		err := shared.DeepCopy(&srcOldDevice, &oldDevice)
		if err != nil {
			continue
		}

		srcNewDevice := newlist[key]
		var newDevice Device
		err = shared.DeepCopy(&srcNewDevice, &newDevice)
		if err != nil {
			continue
		}

		updateDiff = deviceEqualsDiffKeys(oldDevice, newDevice)

		for _, k := range []string{"limits.max", "limits.read", "limits.write", "limits.egress", "limits.ingress", "ipv4.address", "ipv6.address", "ipv4.routes", "ipv6.routes"} {
			delete(oldDevice, k)
			delete(newDevice, k)
		}

		if deviceEquals(oldDevice, newDevice) {
			delete(rmlist, key)
			delete(addlist, key)
			updatelist[key] = d
		}
	}

	return rmlist, addlist, updatelist, updateDiff
}
func (list Devices) DeviceNames() []string {
	sortable := sortableDevices{}
	for k, d := range list {
		sortable = append(sortable, namedDevice{k, d})
	}

	sort.Sort(sortable)
	return sortable.Names()
}
func Infof(format string, args ...interface{}) {
	if Log != nil {
		Log.Info(fmt.Sprintf(format, args...))
	}
}
func Debugf(format string, args ...interface{}) {
	if Log != nil {
		Log.Debug(fmt.Sprintf(format, args...))
	}
}
func Warnf(format string, args ...interface{}) {
	if Log != nil {
		Log.Warn(fmt.Sprintf(format, args...))
	}
}
func Errorf(format string, args ...interface{}) {
	if Log != nil {
		Log.Error(fmt.Sprintf(format, args...))
	}
}
func Critf(format string, args ...interface{}) {
	if Log != nil {
		Log.Crit(fmt.Sprintf(format, args...))
	}
}
func eventForward(id int64, event api.Event) {
	if event.Type == "logging" {
		// Parse the message
		logEntry := api.EventLogging{}
		err := json.Unmarshal(event.Metadata, &logEntry)
		if err != nil {
			return
		}

		if !debug && logEntry.Level == "dbug" {
			return
		}

		if !debug && !verbose && logEntry.Level == "info" {
			return
		}
	}

	err := eventBroadcast("", event, true)
	if err != nil {
		logger.Warnf("Failed to forward event from node %d: %v", id, err)
	}
}
func StorageProgressReader(op *operation, key string, description string) func(io.ReadCloser) io.ReadCloser {
	return func(reader io.ReadCloser) io.ReadCloser {
		if op == nil {
			return reader
		}

		progress := func(progressInt int64, speedInt int64) {
			progressWrapperRender(op, key, description, progressInt, speedInt)
		}

		readPipe := &ioprogress.ProgressReader{
			ReadCloser: reader,
			Tracker: &ioprogress.ProgressTracker{
				Handler: progress,
			},
		}

		return readPipe
	}
}
func StorageProgressWriter(op *operation, key string, description string) func(io.WriteCloser) io.WriteCloser {
	return func(writer io.WriteCloser) io.WriteCloser {
		if op == nil {
			return writer
		}

		progress := func(progressInt int64, speedInt int64) {
			progressWrapperRender(op, key, description, progressInt, speedInt)
		}

		writePipe := &ioprogress.ProgressWriter{
			WriteCloser: writer,
			Tracker: &ioprogress.ProgressTracker{
				Handler: progress,
			},
		}

		return writePipe
	}
}
func GetLSBRelease() (map[string]string, error) {
	osRelease, err := getLSBRelease("/etc/os-release")
	if os.IsNotExist(err) {
		return getLSBRelease("/usr/lib/os-release")
	}
	return osRelease, err
}
func Reset(path string, imports []string) error {
	content := fmt.Sprintf(`package %s

// The code below was generated by %s - DO NOT EDIT!

import (
`, os.Getenv("GOPACKAGE"), os.Args[0])

	for _, uri := range imports {
		content += fmt.Sprintf("\t%q\n", uri)
	}

	content += ")\n\n"

	// FIXME: we should only import what's needed.
	content += "var _ = api.ServerEnvironment{}\n"

	bytes := []byte(content)

	var err error

	if path == "-" {
		_, err = os.Stdout.Write(bytes)
	} else {
		err = ioutil.WriteFile(path, []byte(content), 0644)
	}

	if err != nil {
		errors.Wrapf(err, "Reset target source file '%s'", path)
	}

	return nil
}
func Append(path string, snippet Snippet) error {
	buffer := newBuffer()
	buffer.N()

	err := snippet.Generate(buffer)
	if err != nil {
		return errors.Wrap(err, "Generate code snippet")
	}

	var file *os.File

	if path == "-" {
		file = os.Stdout
	} else {
		file, err = os.OpenFile(path, os.O_APPEND|os.O_WRONLY, 0644)
		if err != nil {
			return errors.Wrapf(err, "Open target source code file '%s'", path)
		}
		defer file.Close()
	}

	bytes, err := buffer.code()
	if err != nil {
		return err
	}

	_, err = file.Write(bytes)
	if err != nil {
		return errors.Wrapf(err, "Append snippet to target source code file '%s'", path)
	}

	return nil
}
func ContainerToArgs(container *Container) ContainerArgs {
	args := ContainerArgs{
		ID:           container.ID,
		Project:      container.Project,
		Name:         container.Name,
		Node:         container.Node,
		Ctype:        ContainerType(container.Type),
		Architecture: container.Architecture,
		Ephemeral:    container.Ephemeral,
		CreationDate: container.CreationDate,
		Stateful:     container.Stateful,
		LastUsedDate: container.LastUseDate,
		Description:  container.Description,
		Config:       container.Config,
		Devices:      container.Devices,
		Profiles:     container.Profiles,
		ExpiryDate:   container.ExpiryDate,
	}

	if args.Devices == nil {
		args.Devices = types.Devices{}
	}

	return args
}
func (c *ClusterTx) ContainerNames(project string) ([]string, error) {
	stmt := `
SELECT containers.name FROM containers
  JOIN projects ON projects.id = containers.project_id
  WHERE projects.name = ? AND containers.type = ?
`
	return query.SelectStrings(c.tx, stmt, project, CTypeRegular)
}
func (c *ClusterTx) ContainerNodeAddress(project string, name string) (string, error) {
	stmt := `
SELECT nodes.id, nodes.address
  FROM nodes
  JOIN containers ON containers.node_id = nodes.id
  JOIN projects ON projects.id = containers.project_id
 WHERE projects.name = ? AND containers.name = ?
`
	var address string
	var id int64
	rows, err := c.tx.Query(stmt, project, name)
	if err != nil {
		return "", err
	}
	defer rows.Close()

	if !rows.Next() {
		return "", ErrNoSuchObject
	}

	err = rows.Scan(&id, &address)
	if err != nil {
		return "", err
	}

	if rows.Next() {
		return "", fmt.Errorf("more than one node associated with container")
	}

	err = rows.Err()
	if err != nil {
		return "", err
	}

	if id == c.nodeID {
		return "", nil
	}

	return address, nil
}
func (c *ClusterTx) ContainersListByNodeAddress(project string) (map[string][]string, error) {
	offlineThreshold, err := c.NodeOfflineThreshold()
	if err != nil {
		return nil, err
	}

	stmt := `
SELECT containers.name, nodes.id, nodes.address, nodes.heartbeat
  FROM containers
  JOIN nodes ON nodes.id = containers.node_id
  JOIN projects ON projects.id = containers.project_id
  WHERE containers.type=?
    AND projects.name = ?
  ORDER BY containers.id
`
	rows, err := c.tx.Query(stmt, CTypeRegular, project)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	result := map[string][]string{}

	for i := 0; rows.Next(); i++ {
		var name string
		var nodeAddress string
		var nodeID int64
		var nodeHeartbeat time.Time
		err := rows.Scan(&name, &nodeID, &nodeAddress, &nodeHeartbeat)
		if err != nil {
			return nil, err
		}
		if nodeID == c.nodeID {
			nodeAddress = ""
		} else if nodeIsOffline(offlineThreshold, nodeHeartbeat) {
			nodeAddress = "0.0.0.0"
		}
		result[nodeAddress] = append(result[nodeAddress], name)
	}

	err = rows.Err()
	if err != nil {
		return nil, err
	}
	return result, nil
}
func (c *ClusterTx) ContainerListExpanded() ([]Container, error) {
	containers, err := c.ContainerList(ContainerFilter{})
	if err != nil {
		return nil, errors.Wrap(err, "Load containers")
	}

	profiles, err := c.ProfileList(ProfileFilter{})
	if err != nil {
		return nil, errors.Wrap(err, "Load profiles")
	}

	// Index of all profiles by project and name.
	profilesByProjectAndName := map[string]map[string]Profile{}
	for _, profile := range profiles {
		profilesByName, ok := profilesByProjectAndName[profile.Project]
		if !ok {
			profilesByName = map[string]Profile{}
			profilesByProjectAndName[profile.Project] = profilesByName
		}
		profilesByName[profile.Name] = profile
	}

	for i, container := range containers {
		profiles := make([]api.Profile, len(container.Profiles))
		for j, name := range container.Profiles {
			profile := profilesByProjectAndName[container.Project][name]
			profiles[j] = *ProfileToAPI(&profile)
		}

		containers[i].Config = ProfilesExpandConfig(container.Config, profiles)
		containers[i].Devices = ProfilesExpandDevices(container.Devices, profiles)
	}

	return containers, nil
}
func (c *ClusterTx) ContainersByNodeName(project string) (map[string]string, error) {
	stmt := `
SELECT containers.name, nodes.name
  FROM containers
  JOIN nodes ON nodes.id = containers.node_id
  JOIN projects ON projects.id = containers.project_id
  WHERE containers.type=?
    AND projects.name = ?
`
	rows, err := c.tx.Query(stmt, CTypeRegular, project)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	result := map[string]string{}

	for i := 0; rows.Next(); i++ {
		var name string
		var nodeName string
		err := rows.Scan(&name, &nodeName)
		if err != nil {
			return nil, err
		}
		result[name] = nodeName
	}

	err = rows.Err()
	if err != nil {
		return nil, err
	}
	return result, nil
}
func (c *ClusterTx) SnapshotIDsAndNames(name string) (map[int]string, error) {
	prefix := name + shared.SnapshotDelimiter
	length := len(prefix)
	objects := make([]struct {
		ID   int
		Name string
	}, 0)
	dest := func(i int) []interface{} {
		objects = append(objects, struct {
			ID   int
			Name string
		}{})
		return []interface{}{&objects[i].ID, &objects[i].Name}
	}
	stmt, err := c.tx.Prepare("SELECT id, name FROM containers WHERE SUBSTR(name,1,?)=? AND type=?")
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, length, prefix, CTypeSnapshot)
	if err != nil {
		return nil, err
	}
	result := make(map[int]string)
	for i := range objects {
		result[objects[i].ID] = strings.Split(objects[i].Name, shared.SnapshotDelimiter)[1]
	}
	return result, nil
}
func (c *ClusterTx) ContainerNodeList() ([]Container, error) {
	node, err := c.NodeName()
	if err != nil {
		return nil, errors.Wrap(err, "Local node name")
	}
	filter := ContainerFilter{
		Node: node,
		Type: int(CTypeRegular),
	}

	return c.ContainerList(filter)
}
func (c *ClusterTx) ContainerNodeProjectList(project string) ([]Container, error) {
	node, err := c.NodeName()
	if err != nil {
		return nil, errors.Wrap(err, "Local node name")
	}
	filter := ContainerFilter{
		Project: project,
		Node:    node,
		Type:    int(CTypeRegular),
	}

	return c.ContainerList(filter)
}
func (c *Cluster) ContainerRemove(project, name string) error {
	return c.Transaction(func(tx *ClusterTx) error {
		return tx.ContainerDelete(project, name)
	})
}
func (c *Cluster) ContainerProjectAndName(id int) (string, string, error) {
	q := `
SELECT projects.name, containers.name
  FROM containers
  JOIN projects ON projects.id = containers.project_id
WHERE containers.id=?
`
	project := ""
	name := ""
	arg1 := []interface{}{id}
	arg2 := []interface{}{&project, &name}
	err := dbQueryRowScan(c.db, q, arg1, arg2)
	if err == sql.ErrNoRows {
		return "", "", ErrNoSuchObject
	}

	return project, name, err
}
func ContainerConfigClear(tx *sql.Tx, id int) error {
	_, err := tx.Exec("DELETE FROM containers_config WHERE container_id=?", id)
	if err != nil {
		return err
	}
	_, err = tx.Exec("DELETE FROM containers_profiles WHERE container_id=?", id)
	if err != nil {
		return err
	}
	_, err = tx.Exec(`DELETE FROM containers_devices_config WHERE id IN
		(SELECT containers_devices_config.id
		 FROM containers_devices_config JOIN containers_devices
		 ON containers_devices_config.container_device_id=containers_devices.id
		 WHERE containers_devices.container_id=?)`, id)
	if err != nil {
		return err
	}
	_, err = tx.Exec("DELETE FROM containers_devices WHERE container_id=?", id)
	return err
}
func (c *Cluster) ContainerConfigGet(id int, key string) (string, error) {
	q := "SELECT value FROM containers_config WHERE container_id=? AND key=?"
	value := ""
	arg1 := []interface{}{id, key}
	arg2 := []interface{}{&value}
	err := dbQueryRowScan(c.db, q, arg1, arg2)
	if err == sql.ErrNoRows {
		return "", ErrNoSuchObject
	}

	return value, err
}
func (c *Cluster) ContainerConfigRemove(id int, key string) error {
	err := exec(c.db, "DELETE FROM containers_config WHERE key=? AND container_id=?", key, id)
	return err
}
func (c *Cluster) ContainerSetStateful(id int, stateful bool) error {
	statefulInt := 0
	if stateful {
		statefulInt = 1
	}

	err := exec(c.db, "UPDATE containers SET stateful=? WHERE id=?", statefulInt, id)
	return err
}
func ContainerProfilesInsert(tx *sql.Tx, id int, project string, profiles []string) error {
	enabled, err := projectHasProfiles(tx, project)
	if err != nil {
		return errors.Wrap(err, "Check if project has profiles")
	}
	if !enabled {
		project = "default"
	}

	applyOrder := 1
	str := `
INSERT INTO containers_profiles (container_id, profile_id, apply_order)
  VALUES (
    ?,
    (SELECT profiles.id
     FROM profiles
     JOIN projects ON projects.id=profiles.project_id
     WHERE projects.name=? AND profiles.name=?),
    ?
  )
`
	stmt, err := tx.Prepare(str)
	if err != nil {
		return err
	}
	defer stmt.Close()
	for _, profile := range profiles {
		_, err = stmt.Exec(id, project, profile, applyOrder)
		if err != nil {
			logger.Debugf("Error adding profile %s to container: %s",
				profile, err)
			return err
		}
		applyOrder = applyOrder + 1
	}

	return nil
}
func (c *Cluster) ContainerProfiles(id int) ([]string, error) {
	var name string
	var profiles []string

	query := `
        SELECT name FROM containers_profiles
        JOIN profiles ON containers_profiles.profile_id=profiles.id
		WHERE container_id=?
        ORDER BY containers_profiles.apply_order`
	inargs := []interface{}{id}
	outfmt := []interface{}{name}

	results, err := queryScan(c.db, query, inargs, outfmt)
	if err != nil {
		return nil, err
	}

	for _, r := range results {
		name = r[0].(string)

		profiles = append(profiles, name)
	}

	return profiles, nil
}
func (c *Cluster) ContainerConfig(id int) (map[string]string, error) {
	var key, value string
	q := `SELECT key, value FROM containers_config WHERE container_id=?`

	inargs := []interface{}{id}
	outfmt := []interface{}{key, value}

	// Results is already a slice here, not db Rows anymore.
	results, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return nil, err //SmartError will wrap this and make "not found" errors pretty
	}

	config := map[string]string{}

	for _, r := range results {
		key = r[0].(string)
		value = r[1].(string)

		config[key] = value
	}

	return config, nil
}
func (c *Cluster) ContainerSetState(id int, state string) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		// Set the new value
		str := fmt.Sprintf("INSERT OR REPLACE INTO containers_config (container_id, key, value) VALUES (?, 'volatile.last_state.power', ?)")
		stmt, err := tx.tx.Prepare(str)
		if err != nil {
			return err
		}
		defer stmt.Close()

		if _, err = stmt.Exec(id, state); err != nil {
			return err
		}
		return nil
	})
	return err
}
func ContainerUpdate(tx *sql.Tx, id int, description string, architecture int, ephemeral bool,
	expiryDate time.Time) error {
	str := fmt.Sprintf("UPDATE containers SET description=?, architecture=?, ephemeral=?, expiry_date=? WHERE id=?")
	stmt, err := tx.Prepare(str)
	if err != nil {
		return err
	}
	defer stmt.Close()

	ephemeralInt := 0
	if ephemeral {
		ephemeralInt = 1
	}

	if expiryDate.IsZero() {
		_, err = stmt.Exec(description, architecture, ephemeralInt, "", id)
	} else {
		_, err = stmt.Exec(description, architecture, ephemeralInt, expiryDate, id)
	}
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) ContainerLastUsedUpdate(id int, date time.Time) error {
	stmt := `UPDATE containers SET last_use_date=? WHERE id=?`
	err := exec(c.db, stmt, date, id)
	return err
}
func (c *Cluster) ContainerGetSnapshots(project, name string) ([]string, error) {
	result := []string{}

	regexp := name + shared.SnapshotDelimiter
	length := len(regexp)
	q := `
SELECT containers.name
  FROM containers
  JOIN projects ON projects.id = containers.project_id
WHERE projects.name=? AND containers.type=? AND SUBSTR(containers.name,1,?)=?
`
	inargs := []interface{}{project, CTypeSnapshot, length, regexp}
	outfmt := []interface{}{name}
	dbResults, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return result, err
	}

	for _, r := range dbResults {
		result = append(result, r[0].(string))
	}

	return result, nil
}
func (c *ClusterTx) ContainerGetSnapshotsFull(project string, name string) ([]Container, error) {
	filter := ContainerFilter{
		Parent:  name,
		Project: project,
		Type:    int(CTypeSnapshot),
	}

	return c.ContainerList(filter)
}
func (c *Cluster) ContainerNextSnapshot(project string, name string, pattern string) int {
	base := name + shared.SnapshotDelimiter
	length := len(base)
	q := `
SELECT containers.name
  FROM containers
  JOIN projects ON projects.id = containers.project_id
 WHERE projects.name=? AND containers.type=? AND SUBSTR(containers.name,1,?)=?`
	var numstr string
	inargs := []interface{}{project, CTypeSnapshot, length, base}
	outfmt := []interface{}{numstr}
	results, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return 0
	}
	max := 0

	for _, r := range results {
		snapOnlyName := strings.SplitN(r[0].(string), shared.SnapshotDelimiter, 2)[1]
		fields := strings.SplitN(pattern, "%d", 2)

		var num int
		count, err := fmt.Sscanf(snapOnlyName, fmt.Sprintf("%s%%d%s", fields[0], fields[1]), &num)
		if err != nil || count != 1 {
			continue
		}
		if num >= max {
			max = num + 1
		}
	}

	return max
}
func (c *ClusterTx) ContainerPool(project, containerName string) (string, error) {
	// Get container storage volume. Since container names are globally
	// unique, and their storage volumes carry the same name, their storage
	// volumes are unique too.
	poolName := ""
	query := `
SELECT storage_pools.name FROM storage_pools
  JOIN storage_volumes ON storage_pools.id=storage_volumes.storage_pool_id
  JOIN containers ON containers.name=storage_volumes.name
  JOIN projects ON projects.id=containers.project_id
 WHERE projects.name=? AND storage_volumes.node_id=? AND storage_volumes.name=? AND storage_volumes.type=?
`
	inargs := []interface{}{project, c.nodeID, containerName, StoragePoolVolumeTypeContainer}
	outargs := []interface{}{&poolName}

	err := c.tx.QueryRow(query, inargs...).Scan(outargs...)
	if err != nil {
		if err == sql.ErrNoRows {
			return "", ErrNoSuchObject
		}

		return "", err
	}

	return poolName, nil
}
func (c *Cluster) ContainerGetBackup(project, name string) (ContainerBackupArgs, error) {
	args := ContainerBackupArgs{}
	args.Name = name

	containerOnlyInt := -1
	optimizedStorageInt := -1
	q := `
SELECT containers_backups.id, containers_backups.container_id,
       containers_backups.creation_date, containers_backups.expiry_date,
       containers_backups.container_only, containers_backups.optimized_storage
    FROM containers_backups
    JOIN containers ON containers.id=containers_backups.container_id
    JOIN projects ON projects.id=containers.project_id
    WHERE projects.name=? AND containers_backups.name=?
`
	arg1 := []interface{}{project, name}
	arg2 := []interface{}{&args.ID, &args.ContainerID, &args.CreationDate,
		&args.ExpiryDate, &containerOnlyInt, &optimizedStorageInt}
	err := dbQueryRowScan(c.db, q, arg1, arg2)
	if err != nil {
		if err == sql.ErrNoRows {
			return args, ErrNoSuchObject
		}

		return args, err
	}

	if containerOnlyInt == 1 {
		args.ContainerOnly = true
	}

	if optimizedStorageInt == 1 {
		args.OptimizedStorage = true
	}

	return args, nil
}
func (c *Cluster) ContainerGetBackups(project, name string) ([]string, error) {
	var result []string

	q := `SELECT containers_backups.name FROM containers_backups
JOIN containers ON containers_backups.container_id=containers.id
JOIN projects ON projects.id=containers.project_id
WHERE projects.name=? AND containers.name=?`
	inargs := []interface{}{project, name}
	outfmt := []interface{}{name}
	dbResults, err := queryScan(c.db, q, inargs, outfmt)
	if err != nil {
		return nil, err
	}

	for _, r := range dbResults {
		result = append(result, r[0].(string))
	}

	return result, nil
}
func (c *Cluster) ContainerBackupCreate(args ContainerBackupArgs) error {
	_, err := c.ContainerBackupID(args.Name)
	if err == nil {
		return ErrAlreadyDefined
	}

	err = c.Transaction(func(tx *ClusterTx) error {
		containerOnlyInt := 0
		if args.ContainerOnly {
			containerOnlyInt = 1
		}

		optimizedStorageInt := 0
		if args.OptimizedStorage {
			optimizedStorageInt = 1
		}

		str := fmt.Sprintf("INSERT INTO containers_backups (container_id, name, creation_date, expiry_date, container_only, optimized_storage) VALUES (?, ?, ?, ?, ?, ?)")
		stmt, err := tx.tx.Prepare(str)
		if err != nil {
			return err
		}
		defer stmt.Close()
		result, err := stmt.Exec(args.ContainerID, args.Name,
			args.CreationDate.Unix(), args.ExpiryDate.Unix(), containerOnlyInt,
			optimizedStorageInt)
		if err != nil {
			return err
		}

		_, err = result.LastInsertId()
		if err != nil {
			return fmt.Errorf("Error inserting %s into database", args.Name)
		}

		return nil
	})

	return err
}
func (c *Cluster) ContainerBackupRemove(name string) error {
	id, err := c.ContainerBackupID(name)
	if err != nil {
		return err
	}

	err = exec(c.db, "DELETE FROM containers_backups WHERE id=?", id)
	if err != nil {
		return err
	}

	return nil
}
func (c *Cluster) ContainerBackupRename(oldName, newName string) error {
	err := c.Transaction(func(tx *ClusterTx) error {
		str := fmt.Sprintf("UPDATE containers_backups SET name = ? WHERE name = ?")
		stmt, err := tx.tx.Prepare(str)
		if err != nil {
			return err
		}
		defer stmt.Close()

		logger.Debug(
			"Calling SQL Query",
			log.Ctx{
				"query":   "UPDATE containers_backups SET name = ? WHERE name = ?",
				"oldName": oldName,
				"newName": newName})
		if _, err := stmt.Exec(newName, oldName); err != nil {
			return err
		}

		return nil
	})
	return err
}
func (c *Cluster) ContainerBackupsGetExpired() ([]string, error) {
	var result []string
	var name string
	var expiryDate string

	q := `SELECT containers_backups.name, containers_backups.expiry_date FROM containers_backups`
	outfmt := []interface{}{name, expiryDate}
	dbResults, err := queryScan(c.db, q, nil, outfmt)
	if err != nil {
		return nil, err
	}

	for _, r := range dbResults {
		timestamp := r[1]

		var backupExpiry time.Time
		err = backupExpiry.UnmarshalText([]byte(timestamp.(string)))
		if err != nil {
			return []string{}, err
		}

		if backupExpiry.IsZero() {
			// Backup doesn't expire
			continue
		}

		// Backup has expired
		if time.Now().Unix()-backupExpiry.Unix() >= 0 {
			result = append(result, r[0].(string))
		}
	}

	return result, nil
}
func DefaultOS() *OS {
	newOS := &OS{
		VarDir:   shared.VarPath(),
		CacheDir: shared.CachePath(),
		LogDir:   shared.LogPath(),
	}
	newOS.InotifyWatch.Fd = -1
	newOS.InotifyWatch.Targets = make(map[string]*InotifyTargetInfo)
	return newOS
}
func (s *OS) Init() error {
	err := s.initDirs()
	if err != nil {
		return err
	}

	s.Architectures, err = util.GetArchitectures()
	if err != nil {
		return err
	}

	s.LxcPath = filepath.Join(s.VarDir, "containers")

	s.BackingFS, err = util.FilesystemDetect(s.LxcPath)
	if err != nil {
		logger.Error("Error detecting backing fs", log.Ctx{"err": err})
	}

	s.IdmapSet = util.GetIdmapSet()
	s.ExecPath = util.GetExecPath()
	s.RunningInUserNS = shared.RunningInUserNS()

	s.initAppArmor()
	s.initCGroup()

	return nil
}
func (op *operation) GetWebsocket(secret string) (*websocket.Conn, error) {
	return op.r.GetOperationWebsocket(op.ID, secret)
}
func (op *operation) Refresh() error {
	// Get the current version of the operation
	newOp, _, err := op.r.GetOperation(op.ID)
	if err != nil {
		return err
	}

	// Update the operation struct
	op.Operation = *newOp

	return nil
}
func (op *remoteOperation) CancelTarget() error {
	if op.targetOp == nil {
		return fmt.Errorf("No associated target operation")
	}

	return op.targetOp.Cancel()
}
func (op *remoteOperation) GetTarget() (*api.Operation, error) {
	if op.targetOp == nil {
		return nil, fmt.Errorf("No associated target operation")
	}

	opAPI := op.targetOp.Get()
	return &opAPI, nil
}
func (e *Endpoints) up(config *Config) error {
	e.mu.Lock()
	defer e.mu.Unlock()

	e.servers = map[kind]*http.Server{
		devlxd:  config.DevLxdServer,
		local:   config.RestServer,
		network: config.RestServer,
		cluster: config.RestServer,
		pprof:   pprofCreateServer(),
	}
	e.cert = config.Cert
	e.inherited = map[kind]bool{}

	var err error

	// Check for socket activation.
	systemdListeners := util.GetListeners(e.systemdListenFDsStart)
	if len(systemdListeners) > 0 {
		e.listeners = activatedListeners(systemdListeners, e.cert)
		for kind := range e.listeners {
			e.inherited[kind] = true
		}
	} else {
		e.listeners = map[kind]net.Listener{}

		e.listeners[local], err = localCreateListener(config.UnixSocket, config.LocalUnixSocketGroup)
		if err != nil {
			return fmt.Errorf("local endpoint: %v", err)
		}
	}

	// Start the devlxd listener
	e.listeners[devlxd], err = createDevLxdlListener(config.Dir)
	if err != nil {
		return err
	}

	if config.NetworkAddress != "" {
		listener, ok := e.listeners[network]
		if ok {
			logger.Infof("Replacing inherited TCP socket with configured one")
			listener.Close()
			e.inherited[network] = false
		}

		// Errors here are not fatal and are just logged.
		e.listeners[network] = networkCreateListener(config.NetworkAddress, e.cert)

		isCovered := util.IsAddressCovered(config.ClusterAddress, config.NetworkAddress)
		if config.ClusterAddress != "" && !isCovered {
			e.listeners[cluster], err = clusterCreateListener(config.ClusterAddress, e.cert)
			if err != nil {
				return err
			}

			logger.Infof("Starting cluster handler:")
			e.serveHTTP(cluster)
		}

	}

	if config.DebugAddress != "" {
		e.listeners[pprof], err = pprofCreateListener(config.DebugAddress)
		if err != nil {
			return err
		}

		logger.Infof("Starting pprof handler:")
		e.serveHTTP(pprof)
	}

	logger.Infof("Starting /dev/lxd handler:")
	e.serveHTTP(devlxd)

	logger.Infof("REST API daemon:")
	e.serveHTTP(local)
	e.serveHTTP(network)

	return nil
}
func (e *Endpoints) Down() error {
	e.mu.Lock()
	defer e.mu.Unlock()

	if e.listeners[network] != nil || e.listeners[local] != nil {
		logger.Infof("Stopping REST API handler:")
		err := e.closeListener(network)
		if err != nil {
			return err
		}

		err = e.closeListener(local)
		if err != nil {
			return err
		}
	}

	if e.listeners[cluster] != nil {
		logger.Infof("Stopping cluster handler:")
		err := e.closeListener(cluster)
		if err != nil {
			return err
		}
	}

	if e.listeners[devlxd] != nil {
		logger.Infof("Stopping /dev/lxd handler:")
		err := e.closeListener(devlxd)
		if err != nil {
			return err
		}
	}

	if e.listeners[pprof] != nil {
		logger.Infof("Stopping pprof handler:")
		err := e.closeListener(pprof)
		if err != nil {
			return err
		}
	}

	if e.tomb != nil {
		e.tomb.Kill(nil)
		e.tomb.Wait()
	}

	return nil
}
func (e *Endpoints) serveHTTP(kind kind) {
	listener := e.listeners[kind]

	if listener == nil {
		return
	}

	ctx := log.Ctx{"socket": listener.Addr()}
	if e.inherited[kind] {
		ctx["inherited"] = true
	}

	message := fmt.Sprintf(" - binding %s", descriptions[kind])
	logger.Info(message, ctx)

	server := e.servers[kind]

	// Defer the creation of the tomb, so Down() doesn't wait on it unless
	// we actually have spawned at least a server.
	if e.tomb == nil {
		e.tomb = &tomb.Tomb{}
	}

	e.tomb.Go(func() error {
		server.Serve(listener)
		return nil
	})
}
func (e *Endpoints) closeListener(kind kind) error {
	listener := e.listeners[kind]
	if listener == nil {
		return nil
	}
	delete(e.listeners, kind)

	logger.Info(" - closing socket", log.Ctx{"socket": listener.Addr()})

	return listener.Close()
}
func activatedListeners(systemdListeners []net.Listener, cert *shared.CertInfo) map[kind]net.Listener {
	listeners := map[kind]net.Listener{}
	for _, listener := range systemdListeners {
		var kind kind
		switch listener.(type) {
		case *net.UnixListener:
			kind = local
		case *net.TCPListener:
			kind = network
			listener = networkTLSListener(listener, cert)
		default:
			continue
		}
		listeners[kind] = listener
	}
	return listeners
}
func (c *Config) CandidServer() (string, string, int64, string) {
	return c.m.GetString("candid.api.url"),
		c.m.GetString("candid.api.key"),
		c.m.GetInt64("candid.expiry"),
		c.m.GetString("candid.domains")
}
func (c *Config) RBACServer() (string, string, int64, string, string, string, string) {
	return c.m.GetString("rbac.api.url"),
		c.m.GetString("rbac.api.key"),
		c.m.GetInt64("rbac.expiry"),
		c.m.GetString("rbac.agent.url"),
		c.m.GetString("rbac.agent.username"),
		c.m.GetString("rbac.agent.private_key"),
		c.m.GetString("rbac.agent.public_key")
}
func (c *Config) AutoUpdateInterval() time.Duration {
	n := c.m.GetInt64("images.auto_update_interval")
	return time.Duration(n) * time.Hour
}
func (c *Config) MAASController() (string, string) {
	url := c.m.GetString("maas.api.url")
	key := c.m.GetString("maas.api.key")
	return url, key
}
func (c *Config) OfflineThreshold() time.Duration {
	n := c.m.GetInt64("cluster.offline_threshold")
	return time.Duration(n) * time.Second
}
func ConfigGetString(cluster *db.Cluster, key string) (string, error) {
	config, err := configGet(cluster)
	if err != nil {
		return "", err
	}
	return config.m.GetString(key), nil
}
func ConfigGetBool(cluster *db.Cluster, key string) (bool, error) {
	config, err := configGet(cluster)
	if err != nil {
		return false, err
	}
	return config.m.GetBool(key), nil
}
func ConfigGetInt64(cluster *db.Cluster, key string) (int64, error) {
	config, err := configGet(cluster)
	if err != nil {
		return 0, err
	}
	return config.m.GetInt64(key), nil
}
func (e *Endpoints) ClusterAddress() string {
	e.mu.RLock()
	defer e.mu.RUnlock()

	listener := e.listeners[cluster]
	if listener == nil {
		return ""
	}
	return listener.Addr().String()
}
func Debug(msg string, ctx ...interface{}) {
	if Log != nil {
		pc, fn, line, _ := runtime.Caller(1)
		msg := fmt.Sprintf("%s: %d: %s: %s", fn, line, runtime.FuncForPC(pc).Name(), msg)
		Log.Debug(msg, ctx...)
	}
}
func RestServer(d *Daemon) *http.Server {
	/* Setup the web server */
	mux := mux.NewRouter()
	mux.StrictSlash(false)

	mux.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		SyncResponse(true, []string{"/1.0"}).Render(w)
	})

	for endpoint, f := range d.gateway.HandlerFuncs() {
		mux.HandleFunc(endpoint, f)
	}

	for _, c := range api10 {
		d.createCmd(mux, "1.0", c)
	}

	for _, c := range apiInternal {
		d.createCmd(mux, "internal", c)
	}

	mux.NotFoundHandler = http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		logger.Info("Sending top level 404", log.Ctx{"url": r.URL})
		w.Header().Set("Content-Type", "application/json")
		NotFound(nil).Render(w)
	})

	return &http.Server{Handler: &lxdHttpServer{r: mux, d: d}}
}
func projectParam(request *http.Request) string {
	project := queryParam(request, "project")
	if project == "" {
		project = "default"
	}
	return project
}
func queryParam(request *http.Request, key string) string {
	var values url.Values
	var err error

	if request.URL != nil {
		values, err = url.ParseQuery(request.URL.RawQuery)
		if err != nil {
			logger.Warnf("Failed to parse query string %q: %v", request.URL.RawQuery, err)
			return ""
		}
	}

	if values == nil {
		values = make(url.Values)
	}

	return values.Get(key)
}
func newDb() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "db [sub-command]",
		Short: "Database-related code generation.",
		RunE: func(cmd *cobra.Command, args []string) error {
			return fmt.Errorf("Not implemented")
		},
	}

	cmd.AddCommand(newDbSchema())
	cmd.AddCommand(newDbMapper())

	return cmd
}
func (t OperationType) Description() string {
	switch t {
	case OperationClusterBootstrap:
		return "Creating bootstrap node"
	case OperationClusterJoin:
		return "Joining cluster"
	case OperationBackupCreate:
		return "Backing up container"
	case OperationBackupRename:
		return "Renaming container backup"
	case OperationBackupRestore:
		return "Restoring backup"
	case OperationBackupRemove:
		return "Removing container backup"
	case OperationConsoleShow:
		return "Showing console"
	case OperationContainerCreate:
		return "Creating container"
	case OperationContainerUpdate:
		return "Updating container"
	case OperationContainerRename:
		return "Renaming container"
	case OperationContainerMigrate:
		return "Migrating container"
	case OperationContainerLiveMigrate:
		return "Live-migrating container"
	case OperationContainerFreeze:
		return "Freezing container"
	case OperationContainerUnfreeze:
		return "Unfreezing container"
	case OperationContainerDelete:
		return "Deleting container"
	case OperationContainerStart:
		return "Starting container"
	case OperationContainerStop:
		return "Stopping container"
	case OperationContainerRestart:
		return "Restarting container"
	case OperationCommandExec:
		return "Executing command"
	case OperationSnapshotCreate:
		return "Snapshotting container"
	case OperationSnapshotRename:
		return "Renaming snapshot"
	case OperationSnapshotRestore:
		return "Restoring snapshot"
	case OperationSnapshotTransfer:
		return "Transferring snapshot"
	case OperationSnapshotUpdate:
		return "Updating snapshot"
	case OperationSnapshotDelete:
		return "Deleting snapshot"
	case OperationImageDownload:
		return "Downloading image"
	case OperationImageDelete:
		return "Deleting image"
	case OperationImageToken:
		return "Image download token"
	case OperationImageRefresh:
		return "Refreshing image"
	case OperationVolumeCopy:
		return "Copying storage volume"
	case OperationVolumeCreate:
		return "Creating storage volume"
	case OperationVolumeMigrate:
		return "Migrating storage volume"
	case OperationVolumeMove:
		return "Moving storage volume"
	case OperationVolumeSnapshotCreate:
		return "Creating storage volume snapshot"
	case OperationVolumeSnapshotDelete:
		return "Deleting storage volume snapshot"
	case OperationVolumeSnapshotUpdate:
		return "Updating storage volume snapshot"
	case OperationProjectRename:
		return "Renaming project"
	case OperationImagesExpire:
		return "Cleaning up expired images"
	case OperationImagesPruneLeftover:
		return "Pruning leftover image files"
	case OperationImagesUpdate:
		return "Updating images"
	case OperationImagesSynchronize:
		return "Synchronizing images"
	case OperationLogsExpire:
		return "Expiring log files"
	case OperationInstanceTypesUpdate:
		return "Updating instance types"
	case OperationBackupsExpire:
		return "Cleaning up expired backups"
	case OperationSnapshotsExpire:
		return "Cleaning up expired snapshots"
	default:
		return "Executing operation"
	}
}
func (t OperationType) Permission() string {
	switch t {
	case OperationBackupCreate:
		return "operate-containers"
	case OperationBackupRename:
		return "operate-containers"
	case OperationBackupRestore:
		return "operate-containers"
	case OperationBackupRemove:
		return "operate-containers"
	case OperationConsoleShow:
		return "operate-containers"
	case OperationContainerFreeze:
		return "operate-containers"
	case OperationContainerUnfreeze:
		return "operate-containers"
	case OperationContainerStart:
		return "operate-containers"
	case OperationContainerStop:
		return "operate-containers"
	case OperationContainerRestart:
		return "operate-containers"
	case OperationCommandExec:
		return "operate-containers"
	case OperationSnapshotCreate:
		return "operate-containers"
	case OperationSnapshotRename:
		return "operate-containers"
	case OperationSnapshotTransfer:
		return "operate-containers"
	case OperationSnapshotUpdate:
		return "operate-containers"
	case OperationSnapshotDelete:
		return "operate-containers"

	case OperationContainerCreate:
		return "manage-containers"
	case OperationContainerUpdate:
		return "manage-containers"
	case OperationContainerRename:
		return "manage-containers"
	case OperationContainerMigrate:
		return "manage-containers"
	case OperationContainerLiveMigrate:
		return "manage-containers"
	case OperationContainerDelete:
		return "manage-containers"
	case OperationSnapshotRestore:
		return "manage-containers"

	case OperationImageDownload:
		return "manage-images"
	case OperationImageDelete:
		return "manage-images"
	case OperationImageToken:
		return "manage-images"
	case OperationImageRefresh:
		return "manage-images"
	case OperationImagesUpdate:
		return "manage-images"
	case OperationImagesSynchronize:
		return "manage-images"
	}

	return ""
}
func (c *ClusterTx) OperationsUUIDs() ([]string, error) {
	stmt := "SELECT uuid FROM operations WHERE node_id=?"
	return query.SelectStrings(c.tx, stmt, c.nodeID)
}
func (c *ClusterTx) OperationNodes(project string) ([]string, error) {
	stmt := `
SELECT DISTINCT nodes.address
  FROM operations
  LEFT OUTER JOIN projects ON projects.id = operations.project_id
  JOIN nodes ON nodes.id = operations.node_id
 WHERE projects.name = ? OR operations.project_id IS NULL
`
	return query.SelectStrings(c.tx, stmt, project)
}
func (c *ClusterTx) OperationByUUID(uuid string) (Operation, error) {
	null := Operation{}
	operations, err := c.operations("uuid=?", uuid)
	if err != nil {
		return null, err
	}
	switch len(operations) {
	case 0:
		return null, ErrNoSuchObject
	case 1:
		return operations[0], nil
	default:
		return null, fmt.Errorf("more than one node matches")
	}
}
func (c *ClusterTx) OperationAdd(project, uuid string, typ OperationType) (int64, error) {
	var projectID interface{}

	if project != "" {
		var err error
		projectID, err = c.ProjectID(project)
		if err != nil {
			return -1, errors.Wrap(err, "Fetch project ID")
		}
	} else {
		projectID = nil
	}

	columns := []string{"uuid", "node_id", "type", "project_id"}
	values := []interface{}{uuid, c.nodeID, typ, projectID}
	return query.UpsertObject(c.tx, "operations", columns, values)
}
func (c *ClusterTx) OperationRemove(uuid string) error {
	result, err := c.tx.Exec("DELETE FROM operations WHERE uuid=?", uuid)
	if err != nil {
		return err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if n != 1 {
		return fmt.Errorf("query deleted %d rows instead of 1", n)
	}
	return nil
}
func (c *ClusterTx) operations(where string, args ...interface{}) ([]Operation, error) {
	operations := []Operation{}
	dest := func(i int) []interface{} {
		operations = append(operations, Operation{})
		return []interface{}{
			&operations[i].ID,
			&operations[i].UUID,
			&operations[i].NodeAddress,
			&operations[i].Type,
		}
	}
	sql := `
SELECT operations.id, uuid, nodes.address, type FROM operations JOIN nodes ON nodes.id = node_id `
	if where != "" {
		sql += fmt.Sprintf("WHERE %s ", where)
	}
	sql += "ORDER BY operations.id"
	stmt, err := c.tx.Prepare(sql)
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, args...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch operations")
	}
	return operations, nil
}
func expireLogsTask(state *state.State) (task.Func, task.Schedule) {
	f := func(ctx context.Context) {
		opRun := func(op *operation) error {
			return expireLogs(ctx, state)
		}

		op, err := operationCreate(state.Cluster, "", operationClassTask, db.OperationLogsExpire, nil, nil, opRun, nil, nil)
		if err != nil {
			logger.Error("Failed to start log expiry operation", log.Ctx{"err": err})
			return
		}

		logger.Infof("Expiring log files")
		_, err = op.Run()
		if err != nil {
			logger.Error("Failed to expire logs", log.Ctx{"err": err})
		}
		logger.Infof("Done expiring log files")
	}

	return f, task.Daily()
}
func absPath(path string) string {
	// We expect to be called by code within the lxd package itself.
	_, filename, _, _ := runtime.Caller(1)

	elems := strings.Split(filename, string(filepath.Separator))
	for i := len(elems) - 1; i >= 0; i-- {
		if elems[i] == "lxd" {
			elems = append([]string{string(filepath.Separator)}, elems[:i]...)
			elems = append(elems, path)
			return filepath.Join(elems...)
		}
	}

	log.Fatalf("Could not found root dir of LXD tree source tree")

	return ""
}
func (s Schema) Keys() []string {
	keys := make([]string, len(s))
	i := 0
	for key := range s {
		keys[i] = key
		i++
	}
	sort.Strings(keys)
	return keys
}
func (s Schema) Defaults() map[string]interface{} {
	values := make(map[string]interface{}, len(s))
	for name, key := range s {
		values[name] = key.Default
	}
	return values
}
func (s Schema) mustGetKey(name string) Key {
	key, ok := s[name]
	if !ok {
		panic(fmt.Sprintf("attempt to access unknown key '%s'", name))
	}
	return key
}
func (s Schema) assertKeyType(name string, code Type) {
	key := s.mustGetKey(name)
	if key.Type != code {
		panic(fmt.Sprintf("key '%s' has type code %d, not %d", name, key.Type, code))
	}
}
func (v *Key) validate(value string) error {
	validator := v.Validator
	if validator == nil {
		// Dummy validator
		validator = func(string) error { return nil }
	}

	// Handle unsetting
	if value == "" {
		return validator(v.Default)
	}

	switch v.Type {
	case String:
	case Bool:
		if !shared.StringInSlice(strings.ToLower(value), booleans) {
			return fmt.Errorf("invalid boolean")
		}
	case Int64:
		_, err := strconv.ParseInt(value, 10, 64)
		if err != nil {
			return fmt.Errorf("invalid integer")
		}
	default:
		panic(fmt.Sprintf("unexpected value type: %d", v.Type))
	}

	if v.Deprecated != "" && value != v.Default {
		return fmt.Errorf("deprecated: %s", v.Deprecated)
	}

	// Run external validation function
	return validator(value)
}
func (r *ProtocolLXD) GetStoragePoolVolumes(pool string) ([]api.StorageVolume, error) {
	if !r.HasExtension("storage") {
		return nil, fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	volumes := []api.StorageVolume{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/storage-pools/%s/volumes?recursion=1", url.QueryEscape(pool)), nil, "", &volumes)
	if err != nil {
		return nil, err
	}

	return volumes, nil
}
func (r *ProtocolLXD) GetStoragePoolVolume(pool string, volType string, name string) (*api.StorageVolume, string, error) {
	if !r.HasExtension("storage") {
		return nil, "", fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	volume := api.StorageVolume{}

	// Fetch the raw value
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s", url.QueryEscape(pool), url.QueryEscape(volType), url.QueryEscape(name))
	etag, err := r.queryStruct("GET", path, nil, "", &volume)
	if err != nil {
		return nil, "", err
	}

	return &volume, etag, nil
}
func (r *ProtocolLXD) CreateStoragePoolVolume(pool string, volume api.StorageVolumesPost) error {
	if !r.HasExtension("storage") {
		return fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	// Send the request
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s", url.QueryEscape(pool), url.QueryEscape(volume.Type))
	_, _, err := r.query("POST", path, volume, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) CreateStoragePoolVolumeSnapshot(pool string, volumeType string, volumeName string, snapshot api.StorageVolumeSnapshotsPost) (Operation, error) {
	if !r.HasExtension("storage_api_volume_snapshots") {
		return nil, fmt.Errorf("The server is missing the required \"storage_api_volume_snapshots\" API extension")
	}

	// Send the request
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s/snapshots",
		url.QueryEscape(pool),
		url.QueryEscape(volumeType),
		url.QueryEscape(volumeName))
	op, _, err := r.queryOperation("POST", path, snapshot, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) GetStoragePoolVolumeSnapshots(pool string, volumeType string, volumeName string) ([]api.StorageVolumeSnapshot, error) {
	if !r.HasExtension("storage_api_volume_snapshots") {
		return nil, fmt.Errorf("The server is missing the required \"storage_api_volume_snapshots\" API extension")
	}

	snapshots := []api.StorageVolumeSnapshot{}

	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s/snapshots?recursion=1",
		url.QueryEscape(pool),
		url.QueryEscape(volumeType),
		url.QueryEscape(volumeName))
	_, err := r.queryStruct("GET", path, nil, "", &snapshots)
	if err != nil {
		return nil, err
	}

	return snapshots, nil
}
func (r *ProtocolLXD) GetStoragePoolVolumeSnapshot(pool string, volumeType string, volumeName string, snapshotName string) (*api.StorageVolumeSnapshot, string, error) {
	if !r.HasExtension("storage_api_volume_snapshots") {
		return nil, "", fmt.Errorf("The server is missing the required \"storage_api_volume_snapshots\" API extension")
	}

	snapshot := api.StorageVolumeSnapshot{}

	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s/snapshots/%s",
		url.QueryEscape(pool),
		url.QueryEscape(volumeType),
		url.QueryEscape(volumeName),
		url.QueryEscape(snapshotName))
	etag, err := r.queryStruct("GET", path, nil, "", &snapshot)
	if err != nil {
		return nil, "", err
	}

	return &snapshot, etag, nil
}
func (r *ProtocolLXD) UpdateStoragePoolVolumeSnapshot(pool string, volumeType string, volumeName string, snapshotName string, volume api.StorageVolumeSnapshotPut, ETag string) error {
	if !r.HasExtension("storage_api_volume_snapshots") {
		return fmt.Errorf("The server is missing the required \"storage_api_volume_snapshots\" API extension")
	}

	// Send the request
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s/snapshots/%s", url.QueryEscape(pool), url.QueryEscape(volumeType), url.QueryEscape(volumeName), url.QueryEscape(snapshotName))
	_, _, err := r.queryOperation("PUT", path, volume, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) MigrateStoragePoolVolume(pool string, volume api.StorageVolumePost) (Operation, error) {
	if !r.HasExtension("storage_api_remote_volume_handling") {
		return nil, fmt.Errorf("The server is missing the required \"storage_api_remote_volume_handling\" API extension")
	}

	// Sanity check
	if !volume.Migration {
		return nil, fmt.Errorf("Can't ask for a rename through MigrateStoragePoolVolume")
	}

	// Send the request
	path := fmt.Sprintf("/storage-pools/%s/volumes/custom/%s", url.QueryEscape(pool), volume.Name)
	op, _, err := r.queryOperation("POST", path, volume, "")
	if err != nil {
		return nil, err
	}

	return op, nil
}
func (r *ProtocolLXD) MoveStoragePoolVolume(pool string, source ContainerServer, sourcePool string, volume api.StorageVolume, args *StoragePoolVolumeMoveArgs) (RemoteOperation, error) {
	if !r.HasExtension("storage_api_local_volume_handling") {
		return nil, fmt.Errorf("The server is missing the required \"storage_api_local_volume_handling\" API extension")
	}

	if r != source {
		return nil, fmt.Errorf("Moving storage volumes between remotes is not implemented")
	}

	req := api.StorageVolumePost{
		Name: args.Name,
		Pool: pool,
	}

	// Send the request
	op, _, err := r.queryOperation("POST", fmt.Sprintf("/storage-pools/%s/volumes/%s/%s", url.QueryEscape(sourcePool), url.QueryEscape(volume.Type), volume.Name), req, "")
	if err != nil {
		return nil, err
	}

	rop := remoteOperation{
		targetOp: op,
		chDone:   make(chan bool),
	}

	// Forward targetOp to remote op
	go func() {
		rop.err = rop.targetOp.Wait()
		close(rop.chDone)
	}()

	return &rop, nil
}
func (r *ProtocolLXD) UpdateStoragePoolVolume(pool string, volType string, name string, volume api.StorageVolumePut, ETag string) error {
	if !r.HasExtension("storage") {
		return fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	if volume.Restore != "" && !r.HasExtension("storage_api_volume_snapshots") {
		return fmt.Errorf("The server is missing the required \"storage_api_volume_snapshots\" API extension")
	}

	// Send the request
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s", url.QueryEscape(pool), url.QueryEscape(volType), url.QueryEscape(name))
	_, _, err := r.query("PUT", path, volume, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) DeleteStoragePoolVolume(pool string, volType string, name string) error {
	if !r.HasExtension("storage") {
		return fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	// Send the request
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s", url.QueryEscape(pool), url.QueryEscape(volType), url.QueryEscape(name))
	_, _, err := r.query("DELETE", path, nil, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) RenameStoragePoolVolume(pool string, volType string, name string, volume api.StorageVolumePost) error {
	if !r.HasExtension("storage_api_volume_rename") {
		return fmt.Errorf("The server is missing the required \"storage_api_volume_rename\" API extension")
	}
	path := fmt.Sprintf("/storage-pools/%s/volumes/%s/%s", url.QueryEscape(pool), url.QueryEscape(volType), url.QueryEscape(name))

	// Send the request
	_, _, err := r.query("POST", path, volume, "")
	if err != nil {
		return err
	}

	return nil
}
func doStoragePoolCreateInternal(state *state.State, poolName, poolDescription string, driver string, config map[string]string, isNotification bool) error {
	tryUndo := true
	s, err := storagePoolInit(state, poolName)
	if err != nil {
		return err
	}

	// If this is a clustering notification for a ceph storage, we don't
	// want this node to actually create the pool, as it's already been
	// done by the node that triggered this notification. We just need to
	// create the storage pool directory.
	if s, ok := s.(*storageCeph); ok && isNotification {
		volumeMntPoint := getStoragePoolVolumeMountPoint(s.pool.Name, s.volume.Name)
		return os.MkdirAll(volumeMntPoint, 0711)

	}
	err = s.StoragePoolCreate()
	if err != nil {
		return err
	}
	defer func() {
		if !tryUndo {
			return
		}
		s.StoragePoolDelete()
	}()

	// In case the storage pool config was changed during the pool creation,
	// we need to update the database to reflect this change. This can e.g.
	// happen, when we create a loop file image. This means we append ".img"
	// to the path the user gave us and update the config in the storage
	// callback. So diff the config here to see if something like this has
	// happened.
	postCreateConfig := s.GetStoragePoolWritable().Config
	configDiff, _ := storageConfigDiff(config, postCreateConfig)
	if len(configDiff) > 0 {
		// Create the database entry for the storage pool.
		err = state.Cluster.StoragePoolUpdate(poolName, poolDescription, postCreateConfig)
		if err != nil {
			return fmt.Errorf("Error inserting %s into database: %s", poolName, err)
		}
	}

	// Success, update the closure to mark that the changes should be kept.
	tryUndo = false

	return nil
}
func containerGetParentAndSnapshotName(name string) (string, string, bool) {
	fields := strings.SplitN(name, shared.SnapshotDelimiter, 2)
	if len(fields) == 1 {
		return name, "", false
	}

	return fields[0], fields[1], true
}
func containerLoadFromAllProjects(s *state.State) ([]container, error) {
	var projects []string

	err := s.Cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		projects, err = tx.ProjectNames()
		return err
	})
	if err != nil {
		return nil, err
	}

	containers := []container{}
	for _, project := range projects {
		projectContainers, err := containerLoadByProject(s, project)
		if err != nil {
			return nil, errors.Wrapf(nil, "Load containers in project %s", project)
		}
		containers = append(containers, projectContainers...)
	}

	return containers, nil
}
func containerLoadNodeAll(s *state.State) ([]container, error) {
	// Get all the container arguments
	var cts []db.Container
	err := s.Cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		cts, err = tx.ContainerNodeList()
		if err != nil {
			return err
		}

		return nil
	})
	if err != nil {
		return nil, err
	}

	return containerLoadAllInternal(cts, s)
}
func containerLoadNodeProjectAll(s *state.State, project string) ([]container, error) {
	// Get all the container arguments
	var cts []db.Container
	err := s.Cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		cts, err = tx.ContainerNodeProjectList(project)
		if err != nil {
			return err
		}

		return nil
	})
	if err != nil {
		return nil, err
	}

	return containerLoadAllInternal(cts, s)
}
func heartbeatNode(taskCtx context.Context, address string, cert *shared.CertInfo, raftNodes []db.RaftNode) error {
	logger.Debugf("Sending heartbeat request to %s", address)

	config, err := tlsClientConfig(cert)
	if err != nil {
		return err
	}
	url := fmt.Sprintf("https://%s%s", address, databaseEndpoint)
	client := &http.Client{Transport: &http.Transport{TLSClientConfig: config}}

	buffer := bytes.Buffer{}
	err = json.NewEncoder(&buffer).Encode(raftNodes)
	if err != nil {
		return err
	}

	request, err := http.NewRequest("PUT", url, bytes.NewReader(buffer.Bytes()))
	if err != nil {
		return err
	}
	ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
	defer cancel()
	request = request.WithContext(ctx)
	request.Close = true // Immediately close the connection after the request is done

	// Perform the request asynchronously, so we can abort it if the task context is done.
	errCh := make(chan error)
	go func() {
		response, err := client.Do(request)
		if err != nil {
			errCh <- errors.Wrap(err, "failed to send HTTP request")
			return
		}
		defer response.Body.Close()
		if response.StatusCode != http.StatusOK {
			errCh <- fmt.Errorf("HTTP request failed: %s", response.Status)
			return
		}
		errCh <- nil
	}()

	select {
	case err := <-errCh:
		return err
	case <-taskCtx.Done():
		return taskCtx.Err()
	}
}
func (c *cmdList) dotPrefixMatch(short string, full string) bool {
	fullMembs := strings.Split(full, ".")
	shortMembs := strings.Split(short, ".")

	if len(fullMembs) != len(shortMembs) {
		return false
	}

	for i := range fullMembs {
		if !strings.HasPrefix(fullMembs[i], shortMembs[i]) {
			return false
		}
	}

	return true
}
func (s *storageZfs) ContainerMount(c container) (bool, error) {
	return s.doContainerMount(c.Project(), c.Name(), c.IsPrivileged())
}
func (s *storageZfs) ContainerStorageReady(container container) bool {
	volumeName := projectPrefix(container.Project(), container.Name())
	fs := fmt.Sprintf("containers/%s", volumeName)
	return zfsFilesystemEntityExists(s.getOnDiskPoolName(), fs)
}
func AskChoice(question string, choices []string, defaultAnswer string) string {
	for {
		answer := askQuestion(question, defaultAnswer)

		if shared.StringInSlice(answer, choices) {
			return answer
		}

		invalidInput()
	}
}
func AskInt(question string, min int64, max int64, defaultAnswer string) int64 {
	for {
		answer := askQuestion(question, defaultAnswer)

		result, err := strconv.ParseInt(answer, 10, 64)

		if err == nil && (min == -1 || result >= min) && (max == -1 || result <= max) {
			return result
		}

		invalidInput()
	}
}
func AskString(question string, defaultAnswer string, validate func(string) error) string {
	for {
		answer := askQuestion(question, defaultAnswer)

		if validate != nil {
			error := validate(answer)
			if error != nil {
				fmt.Fprintf(os.Stderr, "Invalid input: %s\n\n", error)
				continue
			}

			return answer
		}

		if len(answer) != 0 {
			return answer
		}

		invalidInput()
	}
}
func AskPassword(question string) string {
	for {
		fmt.Printf(question)

		pwd, _ := terminal.ReadPassword(0)
		fmt.Println("")
		inFirst := string(pwd)
		inFirst = strings.TrimSuffix(inFirst, "\n")

		fmt.Printf("Again: ")
		pwd, _ = terminal.ReadPassword(0)
		fmt.Println("")
		inSecond := string(pwd)
		inSecond = strings.TrimSuffix(inSecond, "\n")

		if inFirst == inSecond {
			return inFirst
		}

		invalidInput()
	}
}
func AskPasswordOnce(question string) string {
	fmt.Printf(question)
	pwd, _ := terminal.ReadPassword(0)
	fmt.Println("")

	return string(pwd)
}
func askQuestion(question, defaultAnswer string) string {
	fmt.Printf(question)

	return readAnswer(defaultAnswer)
}
func readAnswer(defaultAnswer string) string {
	answer, _ := stdin.ReadString('\n')
	answer = strings.TrimSuffix(answer, "\n")
	answer = strings.TrimSpace(answer)
	if answer == "" {
		answer = defaultAnswer
	}

	return answer
}
func profilePost(d *Daemon, r *http.Request) Response {
	project := projectParam(r)
	name := mux.Vars(r)["name"]

	if name == "default" {
		return Forbidden(errors.New("The 'default' profile cannot be renamed"))
	}

	req := api.ProfilePost{}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		return BadRequest(err)
	}

	// Sanity checks
	if req.Name == "" {
		return BadRequest(fmt.Errorf("No name provided"))
	}

	if strings.Contains(req.Name, "/") {
		return BadRequest(fmt.Errorf("Profile names may not contain slashes"))
	}

	if shared.StringInSlice(req.Name, []string{".", ".."}) {
		return BadRequest(fmt.Errorf("Invalid profile name '%s'", req.Name))
	}

	err := d.cluster.Transaction(func(tx *db.ClusterTx) error {
		hasProfiles, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check project features")
		}

		if !hasProfiles {
			project = "default"
		}

		// Check that the name isn't already in use
		_, err = tx.ProfileGet(project, req.Name)
		if err == nil {
			return fmt.Errorf("Name '%s' already in use", req.Name)
		}

		return tx.ProfileRename(project, name, req.Name)
	})
	if err != nil {
		return SmartError(err)
	}

	return SyncResponseLocation(true, nil, fmt.Sprintf("/%s/profiles/%s", version.APIVersion, req.Name))
}
func profileDelete(d *Daemon, r *http.Request) Response {
	project := projectParam(r)
	name := mux.Vars(r)["name"]

	if name == "default" {
		return Forbidden(errors.New("The 'default' profile cannot be deleted"))
	}

	err := d.cluster.Transaction(func(tx *db.ClusterTx) error {
		hasProfiles, err := tx.ProjectHasProfiles(project)
		if err != nil {
			return errors.Wrap(err, "Check project features")
		}

		if !hasProfiles {
			project = "default"
		}

		profile, err := tx.ProfileGet(project, name)
		if err != nil {
			return err
		}
		if len(profile.UsedBy) > 0 {
			return fmt.Errorf("Profile is currently in use")
		}

		return tx.ProfileDelete(project, name)
	})
	if err != nil {
		return SmartError(err)
	}

	return EmptySyncResponse
}
func IsRootDiskDevice(device map[string]string) bool {
	if device["type"] == "disk" && device["path"] == "/" && device["source"] == "" {
		return true
	}

	return false
}
func GetRootDiskDevice(devices map[string]map[string]string) (string, map[string]string, error) {
	var devName string
	var dev map[string]string

	for n, d := range devices {
		if IsRootDiskDevice(d) {
			if devName != "" {
				return "", nil, fmt.Errorf("More than one root device found")
			}

			devName = n
			dev = d
		}
	}

	if devName != "" {
		return devName, dev, nil
	}

	return "", nil, fmt.Errorf("No root device could be found")
}
func ForwardedResponse(client lxd.ContainerServer, request *http.Request) Response {
	return &forwardedResponse{
		client:  client,
		request: request,
	}
}
func ForwardedResponseIfTargetIsRemote(d *Daemon, request *http.Request) Response {
	targetNode := queryParam(request, "target")
	if targetNode == "" {
		return nil
	}

	// Figure out the address of the target node (which is possibly
	// this very same node).
	address, err := cluster.ResolveTarget(d.cluster, targetNode)
	if err != nil {
		return SmartError(err)
	}

	if address != "" {
		// Forward the response.
		cert := d.endpoints.NetworkCert()
		client, err := cluster.Connect(address, cert, false)
		if err != nil {
			return SmartError(err)
		}
		return ForwardedResponse(client, request)
	}

	return nil
}
func ForwardedResponseIfContainerIsRemote(d *Daemon, r *http.Request, project, name string) (Response, error) {
	cert := d.endpoints.NetworkCert()
	client, err := cluster.ConnectIfContainerIsRemote(d.cluster, project, name, cert)
	if err != nil {
		return nil, err
	}
	if client == nil {
		return nil, nil
	}
	return ForwardedResponse(client, r), nil
}
func ForwardedOperationResponse(project string, op *api.Operation) Response {
	return &forwardedOperationResponse{
		op:      op,
		project: project,
	}
}
func (p *ProgressRenderer) Done(msg string) {
	// Acquire rendering lock
	p.lock.Lock()
	defer p.lock.Unlock()

	// Check if we're already done
	if p.done {
		return
	}

	// Mark this renderer as done
	p.done = true

	// Handle quiet mode
	if p.Quiet {
		msg = ""
	}

	// Truncate msg to terminal length
	msg = p.truncate(msg)

	// If we're not printing a completion message and nothing was printed before just return
	if msg == "" && p.maxLength == 0 {
		return
	}

	// Print the new message
	if msg != "" {
		msg += "\n"
	}

	if len(msg) > p.maxLength {
		p.maxLength = len(msg)
	} else {
		fmt.Printf("\r%s", strings.Repeat(" ", p.maxLength))
	}

	fmt.Print("\r")
	fmt.Print(msg)
}
func (p *ProgressRenderer) Update(status string) {
	// Wait if needed
	timeout := p.wait.Sub(time.Now())
	if timeout.Seconds() > 0 {
		time.Sleep(timeout)
	}

	// Acquire rendering lock
	p.lock.Lock()
	defer p.lock.Unlock()

	// Check if we're already done
	if p.done {
		return
	}

	// Handle quiet mode
	if p.Quiet {
		return
	}

	// Skip status updates when not dealing with a terminal
	if p.terminal == 0 {
		if !termios.IsTerminal(int(os.Stdout.Fd())) {
			p.terminal = -1
		}

		p.terminal = 1
	}

	if p.terminal != 1 {
		return
	}

	// Print the new message
	msg := "%s"
	if p.Format != "" {
		msg = p.Format
	}

	msg = fmt.Sprintf(msg, status)

	// Truncate msg to terminal length
	msg = "\r" + p.truncate(msg)

	// Don't print if empty and never printed
	if len(msg) == 1 && p.maxLength == 0 {
		return
	}

	if len(msg) > p.maxLength {
		p.maxLength = len(msg)
	} else {
		fmt.Printf("\r%s", strings.Repeat(" ", p.maxLength))
	}

	fmt.Print(msg)
}
func (p *ProgressRenderer) Warn(status string, timeout time.Duration) {
	// Acquire rendering lock
	p.lock.Lock()
	defer p.lock.Unlock()

	// Check if we're already done
	if p.done {
		return
	}

	// Render the new message
	p.wait = time.Now().Add(timeout)
	msg := fmt.Sprintf("%s", status)

	// Truncate msg to terminal length
	msg = "\r" + p.truncate(msg)

	// Don't print if empty and never printed
	if len(msg) == 1 && p.maxLength == 0 {
		return
	}

	if len(msg) > p.maxLength {
		p.maxLength = len(msg)
	} else {
		fmt.Printf("\r%s", strings.Repeat(" ", p.maxLength))
	}

	fmt.Print(msg)
}
func (p *ProgressRenderer) UpdateProgress(progress ioprogress.ProgressData) {
	p.Update(progress.Text)
}
func (p *ProgressRenderer) UpdateOp(op api.Operation) {
	if op.Metadata == nil {
		return
	}

	for key, value := range op.Metadata {
		if !strings.HasSuffix(key, "_progress") {
			continue
		}

		p.Update(value.(string))
		break
	}
}
func updateFromV6(tx *sql.Tx) error {
	// Fetch the IDs of all existing nodes.
	nodeIDs, err := query.SelectIntegers(tx, "SELECT id FROM nodes")
	if err != nil {
		return errors.Wrap(err, "failed to get IDs of current nodes")
	}

	// Fetch the IDs of all existing zfs pools.
	poolIDs, err := query.SelectIntegers(tx, `
SELECT id FROM storage_pools WHERE driver='zfs'
`)
	if err != nil {
		return errors.Wrap(err, "failed to get IDs of current zfs pools")
	}

	for _, poolID := range poolIDs {
		// Fetch the config for this zfs pool and check if it has the zfs.pool_name key
		config, err := query.SelectConfig(
			tx, "storage_pools_config", "storage_pool_id=? AND node_id IS NULL", poolID)
		if err != nil {
			return errors.Wrap(err, "failed to fetch of zfs pool config")
		}
		poolName, ok := config["zfs.pool_name"]
		if !ok {
			continue // This zfs storage pool does not have a zfs.pool_name config
		}

		// Delete the current zfs.pool_name key
		_, err = tx.Exec(`
DELETE FROM storage_pools_config WHERE key='zfs.pool_name' AND storage_pool_id=? AND node_id IS NULL
`, poolID)
		if err != nil {
			return errors.Wrap(err, "failed to delete zfs.pool_name config")
		}

		// Add zfs.pool_name config entry for each node
		for _, nodeID := range nodeIDs {
			_, err := tx.Exec(`
INSERT INTO storage_pools_config(storage_pool_id, node_id, key, value)
  VALUES(?, ?, 'zfs.pool_name', ?)
`, poolID, nodeID, poolName)
			if err != nil {
				return errors.Wrap(err, "failed to create zfs.pool_name node config")
			}
		}
	}

	return nil
}
func localCreateListener(path string, group string) (net.Listener, error) {
	err := CheckAlreadyRunning(path)
	if err != nil {
		return nil, err
	}

	err = socketUnixRemoveStale(path)
	if err != nil {
		return nil, err
	}

	listener, err := socketUnixListen(path)
	if err != nil {
		return nil, err
	}

	err = localSetAccess(path, group)
	if err != nil {
		listener.Close()
		return nil, err
	}

	return listener, nil
}
func NewStmt(database, pkg, entity, kind string, config map[string]string) (*Stmt, error) {
	packages, err := Packages()
	if err != nil {
		return nil, err
	}

	stmt := &Stmt{
		db:       database,
		pkg:      pkg,
		entity:   entity,
		kind:     kind,
		config:   config,
		packages: packages,
	}

	return stmt, nil
}
func (s *Stmt) Generate(buf *file.Buffer) error {
	if strings.HasPrefix(s.kind, "objects") {
		return s.objects(buf)
	}

	if strings.HasPrefix(s.kind, "create") && strings.HasSuffix(s.kind, "-ref") {
		return s.createRef(buf)
	}

	if strings.HasSuffix(s.kind, "-ref") || strings.Contains(s.kind, "-ref-by-") {
		return s.ref(buf)
	}

	if strings.HasPrefix(s.kind, "names") {
		return s.names(buf)
	}

	switch s.kind {
	case "create":
		return s.create(buf)
	case "id":
		return s.id(buf)
	case "rename":
		return s.rename(buf)
	case "update":
		return s.update(buf)
	case "delete":
		return s.delete(buf)
	default:
		return fmt.Errorf("Unknown statement '%s'", s.kind)
	}
}
func (s *Stmt) register(buf *file.Buffer, sql string, filters ...string) {
	kind := strings.Replace(s.kind, "-", "_", -1)
	if kind == "id" {
		kind = "ID" // silence go lints
	}
	buf.L("var %s = %s.RegisterStmt(`\n%s\n`)", stmtCodeVar(s.entity, kind, filters...), s.db, sql)
}
func httpsLXD(url string, args *ConnectionArgs) (ContainerServer, error) {
	// Use empty args if not specified
	if args == nil {
		args = &ConnectionArgs{}
	}

	// Initialize the client struct
	server := ProtocolLXD{
		httpCertificate:  args.TLSServerCert,
		httpHost:         url,
		httpProtocol:     "https",
		httpUserAgent:    args.UserAgent,
		bakeryInteractor: args.AuthInteractor,
	}
	if args.AuthType == "candid" {
		server.RequireAuthenticated(true)
	}

	// Setup the HTTP client
	httpClient, err := tlsHTTPClient(args.HTTPClient, args.TLSClientCert, args.TLSClientKey, args.TLSCA, args.TLSServerCert, args.InsecureSkipVerify, args.Proxy)
	if err != nil {
		return nil, err
	}

	if args.CookieJar != nil {
		httpClient.Jar = args.CookieJar
	}

	server.http = httpClient
	if args.AuthType == "candid" {
		server.setupBakeryClient()
	}

	// Test the connection and seed the server information
	if !args.SkipGetServer {
		_, _, err := server.GetServer()
		if err != nil {
			return nil, err
		}
	}
	return &server, nil
}
func (c Container) IsActive() bool {
	switch c.StatusCode {
	case Stopped:
		return false
	case Error:
		return false
	default:
		return true
	}
}
func (n *NodeTx) RaftNodeAddress(id int64) (string, error) {
	stmt := "SELECT address FROM raft_nodes WHERE id=?"
	addresses, err := query.SelectStrings(n.tx, stmt, id)
	if err != nil {
		return "", err
	}
	switch len(addresses) {
	case 0:
		return "", ErrNoSuchObject
	case 1:
		return addresses[0], nil
	default:
		// This should never happen since we have a UNIQUE constraint
		// on the raft_nodes.id column.
		return "", fmt.Errorf("more than one match found")
	}
}
func (n *NodeTx) RaftNodeFirst(address string) error {
	columns := []string{"id", "address"}
	values := []interface{}{int64(1), address}
	id, err := query.UpsertObject(n.tx, "raft_nodes", columns, values)
	if err != nil {
		return err
	}
	if id != 1 {
		return fmt.Errorf("could not set raft node ID to 1")
	}
	return nil
}
func (n *NodeTx) RaftNodeAdd(address string) (int64, error) {
	columns := []string{"address"}
	values := []interface{}{address}
	return query.UpsertObject(n.tx, "raft_nodes", columns, values)
}
func (n *NodeTx) RaftNodeDelete(id int64) error {
	deleted, err := query.DeleteObject(n.tx, "raft_nodes", id)
	if err != nil {
		return err
	}
	if !deleted {
		return ErrNoSuchObject
	}
	return nil
}
func (n *NodeTx) RaftNodesReplace(nodes []RaftNode) error {
	_, err := n.tx.Exec("DELETE FROM raft_nodes")
	if err != nil {
		return err
	}

	columns := []string{"id", "address"}
	for _, node := range nodes {
		values := []interface{}{node.ID, node.Address}
		_, err := query.UpsertObject(n.tx, "raft_nodes", columns, values)
		if err != nil {
			return err
		}
	}
	return nil
}
func (s *OS) initCGroup() {
	flags := []*bool{
		&s.CGroupBlkioController,
		&s.CGroupCPUController,
		&s.CGroupCPUacctController,
		&s.CGroupCPUsetController,
		&s.CGroupDevicesController,
		&s.CGroupFreezerController,
		&s.CGroupMemoryController,
		&s.CGroupNetPrioController,
		&s.CGroupPidsController,
		&s.CGroupSwapAccounting,
	}
	for i, flag := range flags {
		*flag = shared.PathExists("/sys/fs/cgroup/" + cGroups[i].path)
		if !*flag {
			logger.Warnf(cGroups[i].warn)
		}
	}
}
func sqliteDirectAccess(conn *sqlite3.SQLiteConn) error {
	// Ensure journal mode is set to WAL, as this is a requirement for
	// replication.
	_, err := conn.Exec("PRAGMA journal_mode=wal", nil)
	if err != nil {
		return err
	}

	// Ensure we don't truncate or checkpoint the WAL on exit, as this
	// would bork replication which must be in full control of the WAL
	// file.
	_, err = conn.Exec("PRAGMA journal_size_limit=-1", nil)
	if err != nil {
		return err
	}

	// Ensure WAL autocheckpoint is disabled, since checkpoints are
	// triggered explicitly by dqlite.
	_, err = conn.Exec("PRAGMA wal_autocheckpoint=0", nil)
	if err != nil {
		return err
	}

	return nil
}
func (c *ClusterTx) ContainerGet(project string, name string) (*Container, error) {
	filter := ContainerFilter{}
	filter.Project = project
	filter.Name = name
	filter.Type = -1

	objects, err := c.ContainerList(filter)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch Container")
	}

	switch len(objects) {
	case 0:
		return nil, ErrNoSuchObject
	case 1:
		return &objects[0], nil
	default:
		return nil, fmt.Errorf("More than one container matches")
	}
}
func (c *ClusterTx) ContainerID(project string, name string) (int64, error) {
	stmt := c.stmt(containerID)
	rows, err := stmt.Query(project, name)
	if err != nil {
		return -1, errors.Wrap(err, "Failed to get container ID")
	}
	defer rows.Close()

	// For sanity, make sure we read one and only one row.
	if !rows.Next() {
		return -1, ErrNoSuchObject
	}
	var id int64
	err = rows.Scan(&id)
	if err != nil {
		return -1, errors.Wrap(err, "Failed to scan ID")
	}
	if rows.Next() {
		return -1, fmt.Errorf("More than one row returned")
	}
	err = rows.Err()
	if err != nil {
		return -1, errors.Wrap(err, "Result set failure")
	}

	return id, nil
}
func (c *ClusterTx) ContainerExists(project string, name string) (bool, error) {
	_, err := c.ContainerID(project, name)
	if err != nil {
		if err == ErrNoSuchObject {
			return false, nil
		}
		return false, err
	}

	return true, nil
}
func (m *Mapping) ContainsFields(fields []*Field) bool {
	matches := map[*Field]bool{}

	for _, field := range m.Fields {
		for _, other := range fields {
			if field.Name == other.Name && field.Type.Name == other.Type.Name {
				matches[field] = true
			}
		}
	}

	return len(matches) == len(fields)
}
func (m *Mapping) FieldByName(name string) *Field {
	for _, field := range m.Fields {
		if field.Name == name {
			return field
		}
	}

	return nil
}
func (m *Mapping) FieldColumnName(name string) string {
	field := m.FieldByName(name)
	return fmt.Sprintf("%s.%s", entityTable(m.Name), field.Column())
}
func (m *Mapping) FilterFieldByName(name string) (*Field, error) {
	field := m.FieldByName(name)
	if field == nil {
		return nil, fmt.Errorf("Unknown filter %q", name)
	}

	if field.Type.Code != TypeColumn {
		return nil, fmt.Errorf("Unknown filter %q not a column", name)
	}

	return field, nil
}
func (m *Mapping) ColumnFields(exclude ...string) []*Field {
	fields := []*Field{}

	for _, field := range m.Fields {
		if shared.StringInSlice(field.Name, exclude) {
			continue
		}
		if field.Type.Code == TypeColumn {
			fields = append(fields, field)
		}
	}

	return fields
}
func (m *Mapping) ScalarFields() []*Field {
	fields := []*Field{}

	for _, field := range m.Fields {
		if field.Config.Get("join") != "" {
			fields = append(fields, field)
		}
	}

	return fields
}
func (m *Mapping) RefFields() []*Field {
	fields := []*Field{}

	for _, field := range m.Fields {
		if field.Type.Code == TypeSlice || field.Type.Code == TypeMap {
			fields = append(fields, field)
		}
	}

	return fields
}
func (f *Field) Column() string {
	if f.Type.Code != TypeColumn {
		panic("attempt to get column name of non-column field")
	}

	column := lex.Snake(f.Name)

	join := f.Config.Get("join")
	if join != "" {
		column = fmt.Sprintf("%s AS %s", join, column)

	}

	return column
}
func (f *Field) ZeroValue() string {
	if f.Type.Code != TypeColumn {
		panic("attempt to get zero value of non-column field")
	}

	switch f.Type.Name {
	case "string":
		return `""`
	case "int":
		// FIXME: we use -1 since at the moment integer criteria are
		// required to be positive.
		return "-1"
	default:
		panic("unsupported zero value")
	}
}
func FieldColumns(fields []*Field) string {
	columns := make([]string, len(fields))

	for i, field := range fields {
		columns[i] = field.Column()
	}

	return strings.Join(columns, ", ")
}
func FieldArgs(fields []*Field) string {
	args := make([]string, len(fields))
	for i, field := range fields {
		args[i] = fmt.Sprintf("%s %s", lex.Minuscule(field.Name), field.Type.Name)
	}

	return strings.Join(args, ", ")
}
func FieldParams(fields []*Field) string {
	args := make([]string, len(fields))
	for i, field := range fields {
		args[i] = lex.Minuscule(field.Name)
	}

	return strings.Join(args, ", ")
}
func FieldCriteria(fields []*Field) string {
	criteria := make([]string, len(fields))

	for i, field := range fields {
		criteria[i] = fmt.Sprintf("%s = ?", field.Column())
	}

	return strings.Join(criteria, " AND ")
}
func initDataClusterApply(d lxd.ContainerServer, config *initDataCluster) error {
	if config == nil || !config.Enabled {
		return nil
	}

	// Get the current cluster configuration
	currentCluster, etag, err := d.GetCluster()
	if err != nil {
		return errors.Wrap(err, "Failed to retrieve current cluster config")
	}

	// Check if already enabled
	if !currentCluster.Enabled {
		// Configure the cluster
		op, err := d.UpdateCluster(config.ClusterPut, etag)
		if err != nil {
			return errors.Wrap(err, "Failed to configure cluster")
		}

		err = op.Wait()
		if err != nil {
			return errors.Wrap(err, "Failed to configure cluster")
		}
	}

	return nil
}
func JsonFormatEx(pretty, lineSeparated bool) Format {
	jsonMarshal := json.Marshal
	if pretty {
		jsonMarshal = func(v interface{}) ([]byte, error) {
			return json.MarshalIndent(v, "", "    ")
		}
	}

	return FormatFunc(func(r *Record) []byte {
		props := make(map[string]interface{})

		props[r.KeyNames.Time] = r.Time
		props[r.KeyNames.Lvl] = r.Lvl
		props[r.KeyNames.Msg] = r.Msg

		for i := 0; i < len(r.Ctx); i += 2 {
			k, ok := r.Ctx[i].(string)
			if !ok {
				props[errorKey] = fmt.Sprintf("%+v is not a string key", r.Ctx[i])
			}
			props[k] = formatJsonValue(r.Ctx[i+1])
		}

		b, err := jsonMarshal(props)
		if err != nil {
			b, _ = jsonMarshal(map[string]string{
				errorKey: err.Error(),
			})
			return b
		}

		if lineSeparated {
			b = append(b, '\n')
		}

		return b
	})
}
func formatLogfmtValue(value interface{}) string {
	if value == nil {
		return "nil"
	}

	value = formatShared(value)
	switch v := value.(type) {
	case bool:
		return strconv.FormatBool(v)
	case float32:
		return strconv.FormatFloat(float64(v), floatFormat, 3, 64)
	case float64:
		return strconv.FormatFloat(v, floatFormat, 3, 64)
	case int, int8, int16, int32, int64, uint, uint8, uint16, uint32, uint64:
		return fmt.Sprintf("%d", value)
	case string:
		return escapeString(v)
	default:
		return escapeString(fmt.Sprintf("%+v", value))
	}
}
func ResolveTarget(cluster *db.Cluster, target string) (string, error) {
	address := ""
	err := cluster.Transaction(func(tx *db.ClusterTx) error {
		name, err := tx.NodeName()
		if err != nil {
			return err
		}

		if target == name {
			return nil
		}

		node, err := tx.NodeByName(target)
		if err != nil {
			if err == db.ErrNoSuchObject {
				return fmt.Errorf("No cluster member called '%s'", target)
			}

			return err
		}

		if node.Name != name {
			address = node.Address
		}

		return nil
	})

	return address, err
}
func (pt *ProgressWriter) Write(p []byte) (int, error) {
	// Do normal writer tasks
	n, err := pt.WriteCloser.Write(p)

	// Do the actual progress tracking
	if pt.Tracker != nil {
		pt.Tracker.total += int64(n)
		pt.Tracker.update(n)
	}

	return n, err
}
func updateNodeVersion(tx *sql.Tx, address string, apiExtensions int) error {
	stmt := "UPDATE nodes SET schema=?, api_extensions=? WHERE address=?"
	result, err := tx.Exec(stmt, len(updates), apiExtensions, address)
	if err != nil {
		return err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if n != 1 {
		return fmt.Errorf("updated %d rows instead of 1", n)
	}
	return nil
}
func selectNodesVersions(tx *sql.Tx) ([][2]int, error) {
	versions := [][2]int{}

	dest := func(i int) []interface{} {
		versions = append(versions, [2]int{})
		return []interface{}{&versions[i][0], &versions[i][1]}
	}

	stmt, err := tx.Prepare("SELECT schema, api_extensions FROM nodes")
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest)
	if err != nil {
		return nil, err
	}
	return versions, nil
}
func GetArchitectures() ([]int, error) {
	architectures := []int{}

	architectureName, err := osarch.ArchitectureGetLocal()
	if err != nil {
		return nil, err
	}

	architecture, err := osarch.ArchitectureId(architectureName)
	if err != nil {
		return nil, err
	}
	architectures = append(architectures, architecture)

	personalities, err := osarch.ArchitecturePersonalities(architecture)
	if err != nil {
		return nil, err
	}
	for _, personality := range personalities {
		architectures = append(architectures, personality)
	}
	return architectures, nil
}
func RuntimeLiblxcVersionAtLeast(major int, minor int, micro int) bool {
	version := golxc.Version()
	version = strings.Replace(version, " (devel)", "-devel", 1)
	parts := strings.Split(version, ".")
	partsLen := len(parts)
	if partsLen == 0 {
		return false
	}

	develParts := strings.Split(parts[partsLen-1], "-")
	if len(develParts) == 2 && develParts[1] == "devel" {
		return true
	}

	maj := -1
	min := -1
	mic := -1

	for i, v := range parts {
		if i > 2 {
			break
		}

		num, err := strconv.Atoi(v)
		if err != nil {
			return false
		}

		switch i {
		case 0:
			maj = num
		case 1:
			min = num
		case 2:
			mic = num
		}
	}

	/* Major version is greater. */
	if maj > major {
		return true
	}

	if maj < major {
		return false
	}

	/* Minor number is greater.*/
	if min > minor {
		return true
	}

	if min < minor {
		return false
	}

	/* Patch number is greater. */
	if mic > micro {
		return true
	}

	if mic < micro {
		return false
	}

	return true
}
func GetExecPath() string {
	execPath := os.Getenv("LXD_EXEC_PATH")
	if execPath != "" {
		return execPath
	}

	execPath, err := os.Readlink("/proc/self/exe")
	if err != nil {
		execPath = "bad-exec-path"
	}
	return execPath
}
func Connect(address string, cert *shared.CertInfo, notify bool) (lxd.ContainerServer, error) {
	args := &lxd.ConnectionArgs{
		TLSServerCert: string(cert.PublicKey()),
		TLSClientCert: string(cert.PublicKey()),
		TLSClientKey:  string(cert.PrivateKey()),
		SkipGetServer: true,
	}
	if notify {
		args.UserAgent = "lxd-cluster-notifier"
	}

	url := fmt.Sprintf("https://%s", address)
	return lxd.ConnectLXD(url, args)
}
func ConnectIfContainerIsRemote(cluster *db.Cluster, project, name string, cert *shared.CertInfo) (lxd.ContainerServer, error) {
	var address string // Node address
	err := cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		address, err = tx.ContainerNodeAddress(project, name)
		return err
	})
	if err != nil {
		return nil, err
	}
	if address == "" {
		// The container is running right on this node, no need to connect.
		return nil, nil
	}
	return Connect(address, cert, false)
}
func ConnectIfVolumeIsRemote(cluster *db.Cluster, poolID int64, volumeName string, volumeType int, cert *shared.CertInfo) (lxd.ContainerServer, error) {
	var addresses []string // Node addresses
	err := cluster.Transaction(func(tx *db.ClusterTx) error {
		var err error
		addresses, err = tx.StorageVolumeNodeAddresses(poolID, "default", volumeName, volumeType)
		return err
	})
	if err != nil {
		return nil, err
	}

	if len(addresses) > 1 {
		var driver string
		err := cluster.Transaction(func(tx *db.ClusterTx) error {
			var err error
			driver, err = tx.StoragePoolDriver(poolID)
			return err
		})
		if err != nil {
			return nil, err
		}

		if driver == "ceph" {
			return nil, nil
		}

		return nil, fmt.Errorf("more than one node has a volume named %s", volumeName)
	}

	address := addresses[0]
	if address == "" {
		return nil, nil
	}

	return Connect(address, cert, false)
}
func SetupTrust(cert, targetAddress, targetCert, targetPassword string) error {
	// Connect to the target cluster node.
	args := &lxd.ConnectionArgs{
		TLSServerCert: targetCert,
	}
	target, err := lxd.ConnectLXD(fmt.Sprintf("https://%s", targetAddress), args)
	if err != nil {
		return errors.Wrap(err, "failed to connect to target cluster node")
	}
	block, _ := pem.Decode([]byte(cert))
	if block == nil {
		return errors.Wrap(err, "failed to decode certificate")
	}
	certificate := base64.StdEncoding.EncodeToString(block.Bytes)
	post := api.CertificatesPost{
		Password:    targetPassword,
		Certificate: certificate,
	}
	fingerprint, err := shared.CertFingerprintStr(cert)
	if err != nil {
		return errors.Wrap(err, "failed to calculate fingerprint")
	}
	post.Name = fmt.Sprintf("lxd.cluster.%s", fingerprint)
	post.Type = "client"
	err = target.CreateCertificate(post)
	if err != nil && err.Error() != "Certificate already in trust store" {
		return errors.Wrap(err, "Failed to add client cert to cluster")
	}
	return nil
}
func (r *ProtocolLXD) GetStoragePools() ([]api.StoragePool, error) {
	if !r.HasExtension("storage") {
		return nil, fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	pools := []api.StoragePool{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", "/storage-pools?recursion=1", nil, "", &pools)
	if err != nil {
		return nil, err
	}

	return pools, nil
}
func (r *ProtocolLXD) GetStoragePool(name string) (*api.StoragePool, string, error) {
	if !r.HasExtension("storage") {
		return nil, "", fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	pool := api.StoragePool{}

	// Fetch the raw value
	etag, err := r.queryStruct("GET", fmt.Sprintf("/storage-pools/%s", url.QueryEscape(name)), nil, "", &pool)
	if err != nil {
		return nil, "", err
	}

	return &pool, etag, nil
}
func (r *ProtocolLXD) CreateStoragePool(pool api.StoragePoolsPost) error {
	if !r.HasExtension("storage") {
		return fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	if pool.Driver == "ceph" && !r.HasExtension("storage_driver_ceph") {
		return fmt.Errorf("The server is missing the required \"storage_driver_ceph\" API extension")
	}

	// Send the request
	_, _, err := r.query("POST", "/storage-pools", pool, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) UpdateStoragePool(name string, pool api.StoragePoolPut, ETag string) error {
	if !r.HasExtension("storage") {
		return fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	// Send the request
	_, _, err := r.query("PUT", fmt.Sprintf("/storage-pools/%s", url.QueryEscape(name)), pool, ETag)
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) DeleteStoragePool(name string) error {
	if !r.HasExtension("storage") {
		return fmt.Errorf("The server is missing the required \"storage\" API extension")
	}

	// Send the request
	_, _, err := r.query("DELETE", fmt.Sprintf("/storage-pools/%s", url.QueryEscape(name)), nil, "")
	if err != nil {
		return err
	}

	return nil
}
func (r *ProtocolLXD) GetStoragePoolResources(name string) (*api.ResourcesStoragePool, error) {
	if !r.HasExtension("resources") {
		return nil, fmt.Errorf("The server is missing the required \"resources\" API extension")
	}

	res := api.ResourcesStoragePool{}

	// Fetch the raw value
	_, err := r.queryStruct("GET", fmt.Sprintf("/storage-pools/%s/resources", url.QueryEscape(name)), nil, "", &res)
	if err != nil {
		return nil, err
	}

	return &res, nil
}
func (s *OS) initDirs() error {
	dirs := []struct {
		path string
		mode os.FileMode
	}{
		{s.VarDir, 0711},
		{filepath.Join(s.VarDir, "backups"), 0700},
		{s.CacheDir, 0700},
		{filepath.Join(s.VarDir, "containers"), 0711},
		{filepath.Join(s.VarDir, "database"), 0700},
		{filepath.Join(s.VarDir, "devices"), 0711},
		{filepath.Join(s.VarDir, "devlxd"), 0755},
		{filepath.Join(s.VarDir, "disks"), 0700},
		{filepath.Join(s.VarDir, "images"), 0700},
		{s.LogDir, 0700},
		{filepath.Join(s.VarDir, "networks"), 0711},
		{filepath.Join(s.VarDir, "security"), 0700},
		{filepath.Join(s.VarDir, "shmounts"), 0711},
		{filepath.Join(s.VarDir, "snapshots"), 0700},
		{filepath.Join(s.VarDir, "storage-pools"), 0711},
	}

	for _, dir := range dirs {
		err := os.Mkdir(dir.path, dir.mode)
		if err != nil && !os.IsExist(err) {
			return errors.Wrapf(err, "failed to init dir %s", dir.path)
		}
	}

	return nil
}
func (n *NodeTx) Config() (map[string]string, error) {
	return query.SelectConfig(n.tx, "config", "")
}
func (n *NodeTx) UpdateConfig(values map[string]string) error {
	return query.UpdateConfig(n.tx, "config", values)
}
func (c *ClusterTx) Config() (map[string]string, error) {
	return query.SelectConfig(c.tx, "config", "")
}
func (c *ClusterTx) UpdateConfig(values map[string]string) error {
	return query.UpdateConfig(c.tx, "config", values)
}
func storagePoolClusterConfigForEtag(dbConfig map[string]string) map[string]string {
	config := util.CopyConfig(dbConfig)
	for _, key := range db.StoragePoolNodeConfigKeys {
		delete(config, key)
	}
	return config
}
func (r *ProtocolLXD) GetEvents() (*EventListener, error) {
	// Prevent anything else from interacting with the listeners
	r.eventListenersLock.Lock()
	defer r.eventListenersLock.Unlock()

	// Setup a new listener
	listener := EventListener{
		r:        r,
		chActive: make(chan bool),
	}

	if r.eventListeners != nil {
		// There is an existing Go routine setup, so just add another target
		r.eventListeners = append(r.eventListeners, &listener)
		return &listener, nil
	}

	// Initialize the list if needed
	r.eventListeners = []*EventListener{}

	// Setup a new connection with LXD
	url, err := r.setQueryAttributes("/events")
	if err != nil {
		return nil, err
	}

	conn, err := r.websocket(url)
	if err != nil {
		return nil, err
	}

	// Add the listener
	r.eventListeners = append(r.eventListeners, &listener)

	// Spawn a watcher that will close the websocket connection after all
	// listeners are gone.
	stopCh := make(chan struct{}, 0)
	go func() {
		for {
			select {
			case <-time.After(time.Minute):
			case <-stopCh:
				break
			}

			r.eventListenersLock.Lock()
			if len(r.eventListeners) == 0 {
				// We don't need the connection anymore, disconnect
				conn.Close()

				r.eventListeners = nil
				r.eventListenersLock.Unlock()
				break
			}
			r.eventListenersLock.Unlock()
		}
	}()

	// Spawn the listener
	go func() {
		for {
			_, data, err := conn.ReadMessage()
			if err != nil {
				// Prevent anything else from interacting with the listeners
				r.eventListenersLock.Lock()
				defer r.eventListenersLock.Unlock()

				// Tell all the current listeners about the failure
				for _, listener := range r.eventListeners {
					listener.err = err
					listener.disconnected = true
					close(listener.chActive)
				}

				// And remove them all from the list
				r.eventListeners = nil

				conn.Close()
				close(stopCh)

				return
			}

			// Attempt to unpack the message
			event := api.Event{}
			err = json.Unmarshal(data, &event)
			if err != nil {
				continue
			}

			// Extract the message type
			if event.Type == "" {
				continue
			}

			// Send the message to all handlers
			r.eventListenersLock.Lock()
			for _, listener := range r.eventListeners {
				listener.targetsLock.Lock()
				for _, target := range listener.targets {
					if target.types != nil && !shared.StringInSlice(event.Type, target.types) {
						continue
					}

					go target.function(event)
				}
				listener.targetsLock.Unlock()
			}
			r.eventListenersLock.Unlock()
		}
	}()

	return &listener, nil
}
func LogfmtFormat() log.Format {
	return log.FormatFunc(func(r *log.Record) []byte {
		common := []interface{}{r.KeyNames.Time, r.Time, r.KeyNames.Lvl, r.Lvl, r.KeyNames.Msg, r.Msg}
		buf := &bytes.Buffer{}

		logfmt(buf, common, 0, false)
		buf.Truncate(buf.Len() - 1)
		buf.WriteByte(' ')
		logfmt(buf, r.Ctx, 0, true)
		return buf.Bytes()
	})
}
func (c *ClusterTx) StorageVolumeNodeAddresses(poolID int64, project, name string, typ int) ([]string, error) {
	nodes := []struct {
		id      int64
		address string
	}{}
	dest := func(i int) []interface{} {
		nodes = append(nodes, struct {
			id      int64
			address string
		}{})
		return []interface{}{&nodes[i].id, &nodes[i].address}

	}
	sql := `
SELECT nodes.id, nodes.address
  FROM nodes
  JOIN storage_volumes ON storage_volumes.node_id=nodes.id
  JOIN projects ON projects.id = storage_volumes.project_id
 WHERE storage_volumes.storage_pool_id=? AND projects.name=? AND storage_volumes.name=? AND storage_volumes.type=?
`
	stmt, err := c.tx.Prepare(sql)
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, poolID, project, name, typ)
	if err != nil {
		return nil, err
	}

	addresses := []string{}
	for _, node := range nodes {
		address := node.address
		if node.id == c.nodeID {
			address = ""
		}
		addresses = append(addresses, address)
	}

	sort.Strings(addresses)

	if len(addresses) == 0 {
		return nil, ErrNoSuchObject
	}

	return addresses, nil
}
func (c *Cluster) StorageVolumeNodeGet(volumeID int64) (string, error) {
	name := ""
	query := `
SELECT nodes.name FROM storage_volumes
  JOIN nodes ON nodes.id=storage_volumes.node_id
   WHERE storage_volumes.id=?
`
	inargs := []interface{}{volumeID}
	outargs := []interface{}{&name}

	err := dbQueryRowScan(c.db, query, inargs, outargs)
	if err != nil {
		if err == sql.ErrNoRows {
			return "", ErrNoSuchObject
		}

		return "", err
	}

	return name, nil
}
func (c *Cluster) StorageVolumeConfigGet(volumeID int64) (map[string]string, error) {
	var key, value string
	query := "SELECT key, value FROM storage_volumes_config WHERE storage_volume_id=?"
	inargs := []interface{}{volumeID}
	outargs := []interface{}{key, value}

	results, err := queryScan(c.db, query, inargs, outargs)
	if err != nil {
		return nil, err
	}

	config := map[string]string{}

	for _, r := range results {
		key = r[0].(string)
		value = r[1].(string)

		config[key] = value
	}

	return config, nil
}
func (c *Cluster) StorageVolumeDescriptionGet(volumeID int64) (string, error) {
	description := sql.NullString{}
	query := "SELECT description FROM storage_volumes WHERE id=?"
	inargs := []interface{}{volumeID}
	outargs := []interface{}{&description}

	err := dbQueryRowScan(c.db, query, inargs, outargs)
	if err != nil {
		if err == sql.ErrNoRows {
			return "", ErrNoSuchObject
		}
		return "", err
	}

	return description.String, nil
}
func (c *Cluster) StorageVolumeIsAvailable(pool, volume string) (bool, error) {
	isAvailable := false

	err := c.Transaction(func(tx *ClusterTx) error {
		id, err := tx.StoragePoolID(pool)
		if err != nil {
			return errors.Wrapf(err, "Fetch storage pool ID for %q", pool)
		}

		driver, err := tx.StoragePoolDriver(id)
		if err != nil {
			return errors.Wrapf(err, "Fetch storage pool driver for %q", pool)
		}

		if driver != "ceph" {
			isAvailable = true
			return nil
		}

		node, err := tx.NodeName()
		if err != nil {
			return errors.Wrapf(err, "Fetch node name")
		}

		containers, err := tx.ContainerListExpanded()
		if err != nil {
			return errors.Wrapf(err, "Fetch containers")
		}

		for _, container := range containers {
			for _, device := range container.Devices {
				if device["type"] != "disk" {
					continue
				}
				if device["pool"] != pool {
					continue
				}
				if device["source"] != volume {
					continue
				}
				if container.Node != node {
					// This ceph volume is already attached
					// to a container on a different node.
					return nil
				}
			}
		}
		isAvailable = true

		return nil
	})
	if err != nil {
		return false, err
	}

	return isAvailable, nil
}
func StorageVolumeDescriptionUpdate(tx *sql.Tx, volumeID int64, description string) error {
	_, err := tx.Exec("UPDATE storage_volumes SET description=? WHERE id=?", description, volumeID)
	return err
}
func StorageVolumeConfigAdd(tx *sql.Tx, volumeID int64, volumeConfig map[string]string) error {
	str := "INSERT INTO storage_volumes_config (storage_volume_id, key, value) VALUES(?, ?, ?)"
	stmt, err := tx.Prepare(str)
	defer stmt.Close()
	if err != nil {
		return err
	}

	for k, v := range volumeConfig {
		if v == "" {
			continue
		}

		_, err = stmt.Exec(volumeID, k, v)
		if err != nil {
			return err
		}
	}

	return nil
}
func StorageVolumeConfigClear(tx *sql.Tx, volumeID int64) error {
	_, err := tx.Exec("DELETE FROM storage_volumes_config WHERE storage_volume_id=?", volumeID)
	if err != nil {
		return err
	}

	return nil
}
func storageVolumeIDsGet(tx *sql.Tx, project, volumeName string, volumeType int, poolID int64) ([]int64, error) {
	ids, err := query.SelectIntegers(tx, `
SELECT storage_volumes.id
  FROM storage_volumes
  JOIN projects ON projects.id = storage_volumes.project_id
 WHERE projects.name=? AND storage_volumes.name=? AND storage_volumes.type=? AND storage_pool_id=?
`, project, volumeName, volumeType, poolID)
	if err != nil {
		return nil, err
	}
	ids64 := make([]int64, len(ids))
	for i, id := range ids {
		ids64[i] = int64(id)
	}
	return ids64, nil
}
func (c *Cluster) StorageVolumeCleanupImages(fingerprints []string) error {
	stmt := fmt.Sprintf(
		"DELETE FROM storage_volumes WHERE type=? AND name NOT IN %s",
		query.Params(len(fingerprints)))
	args := []interface{}{StoragePoolVolumeTypeImage}
	for _, fingerprint := range fingerprints {
		args = append(args, fingerprint)
	}
	err := exec(c.db, stmt, args...)
	return err
}
func (c *Cluster) StorageVolumeMoveToLVMThinPoolNameKey() error {
	err := exec(c.db, "UPDATE storage_pools_config SET key='lvm.thinpool_name' WHERE key='volume.lvm.thinpool_name';")
	if err != nil {
		return err
	}

	err = exec(c.db, "DELETE FROM storage_volumes_config WHERE key='lvm.thinpool_name';")
	if err != nil {
		return err
	}

	return nil
}
func (b *Buffer) L(format string, a ...interface{}) {
	fmt.Fprintf(b.buf, format, a...)
	b.N()
}
func (b *Buffer) code() ([]byte, error) {
	code, err := format.Source(b.buf.Bytes())
	if err != nil {
		return nil, errors.Wrap(err, "Can't format generated source code")
	}
	return code, nil
}
func Pretty(input interface{}) string {
	pretty, err := json.MarshalIndent(input, "\t", "\t")
	if err != nil {
		return fmt.Sprintf("%v", input)
	}

	return fmt.Sprintf("\n\t%s", pretty)
}
func (e *Endpoints) NetworkPublicKey() []byte {
	e.mu.RLock()
	defer e.mu.RUnlock()

	return e.cert.PublicKey()
}
func (e *Endpoints) NetworkPrivateKey() []byte {
	e.mu.RLock()
	defer e.mu.RUnlock()

	return e.cert.PrivateKey()
}
func (e *Endpoints) NetworkCert() *shared.CertInfo {
	e.mu.RLock()
	defer e.mu.RUnlock()

	return e.cert
}
func (e *Endpoints) NetworkAddress() string {
	e.mu.RLock()
	defer e.mu.RUnlock()

	listener := e.listeners[network]
	if listener == nil {
		return ""
	}
	return listener.Addr().String()
}
func (e *Endpoints) NetworkUpdateAddress(address string) error {
	if address != "" {
		address = util.CanonicalNetworkAddress(address)
	}

	oldAddress := e.NetworkAddress()
	if address == oldAddress {
		return nil
	}

	clusterAddress := e.ClusterAddress()

	logger.Infof("Update network address")

	e.mu.Lock()
	defer e.mu.Unlock()

	// Close the previous socket
	e.closeListener(network)

	// If turning off listening, we're done.
	if address == "" {
		return nil
	}

	// If the new address covers the cluster one, turn off the cluster
	// listener.
	if clusterAddress != "" && util.IsAddressCovered(clusterAddress, address) {
		e.closeListener(cluster)
	}

	// Attempt to setup the new listening socket
	getListener := func(address string) (*net.Listener, error) {
		var err error
		var listener net.Listener

		for i := 0; i < 10; i++ { // Ten retries over a second seems reasonable.
			listener, err = net.Listen("tcp", address)
			if err == nil {
				break
			}

			time.Sleep(100 * time.Millisecond)
		}

		if err != nil {
			return nil, fmt.Errorf("cannot listen on https socket: %v", err)
		}

		return &listener, nil
	}

	// If setting a new address, setup the listener
	if address != "" {
		listener, err := getListener(address)
		if err != nil {
			// Attempt to revert to the previous address
			listener, err1 := getListener(oldAddress)
			if err1 == nil {
				e.listeners[network] = networkTLSListener(*listener, e.cert)
				e.serveHTTP(network)
			}

			return err
		}

		e.listeners[network] = networkTLSListener(*listener, e.cert)
		e.serveHTTP(network)
	}

	return nil
}
func (e *Endpoints) NetworkUpdateCert(cert *shared.CertInfo) {
	e.mu.Lock()
	defer e.mu.Unlock()
	e.cert = cert
	listener, ok := e.listeners[network]
	if !ok {
		return
	}
	listener.(*networkListener).Config(cert)

	// Update the cluster listener too, if enabled.
	listener, ok = e.listeners[cluster]
	if !ok {
		return
	}
	listener.(*networkListener).Config(cert)
}
func networkCreateListener(address string, cert *shared.CertInfo) net.Listener {
	listener, err := net.Listen("tcp", util.CanonicalNetworkAddress(address))
	if err != nil {
		logger.Error("Cannot listen on https socket, skipping...", log.Ctx{"err": err})
		return nil
	}
	return networkTLSListener(listener, cert)
}
func (l *networkListener) Accept() (net.Conn, error) {
	c, err := l.Listener.Accept()
	if err != nil {
		return nil, err
	}
	l.mu.RLock()
	defer l.mu.RUnlock()
	config := l.config
	return tls.Server(c, config), nil
}
func (l *networkListener) Config(cert *shared.CertInfo) {
	config := util.ServerTLSConfig(cert)

	l.mu.Lock()
	defer l.mu.Unlock()

	l.config = config
}
func (n NodeInfo) IsOffline(threshold time.Duration) bool {
	return nodeIsOffline(threshold, n.Heartbeat)
}
func (c *ClusterTx) NodeByAddress(address string) (NodeInfo, error) {
	null := NodeInfo{}
	nodes, err := c.nodes(false /* not pending */, "address=?", address)
	if err != nil {
		return null, err
	}
	switch len(nodes) {
	case 0:
		return null, ErrNoSuchObject
	case 1:
		return nodes[0], nil
	default:
		return null, fmt.Errorf("more than one node matches")
	}
}
func (c *ClusterTx) NodePendingByAddress(address string) (NodeInfo, error) {
	null := NodeInfo{}
	nodes, err := c.nodes(true /*pending */, "address=?", address)
	if err != nil {
		return null, err
	}
	switch len(nodes) {
	case 0:
		return null, ErrNoSuchObject
	case 1:
		return nodes[0], nil
	default:
		return null, fmt.Errorf("more than one node matches")
	}
}
func (c *ClusterTx) NodeByName(name string) (NodeInfo, error) {
	null := NodeInfo{}
	nodes, err := c.nodes(false /* not pending */, "name=?", name)
	if err != nil {
		return null, err
	}
	switch len(nodes) {
	case 0:
		return null, ErrNoSuchObject
	case 1:
		return nodes[0], nil
	default:
		return null, fmt.Errorf("more than one node matches")
	}
}
func (c *ClusterTx) NodeName() (string, error) {
	stmt := "SELECT name FROM nodes WHERE id=?"
	names, err := query.SelectStrings(c.tx, stmt, c.nodeID)
	if err != nil {
		return "", err
	}
	switch len(names) {
	case 0:
		return "", nil
	case 1:
		return names[0], nil
	default:
		return "", fmt.Errorf("inconsistency: non-unique node ID")
	}
}
func (c *ClusterTx) NodeAddress() (string, error) {
	stmt := "SELECT address FROM nodes WHERE id=?"
	addresses, err := query.SelectStrings(c.tx, stmt, c.nodeID)
	if err != nil {
		return "", err
	}
	switch len(addresses) {
	case 0:
		return "", nil
	case 1:
		return addresses[0], nil
	default:
		return "", fmt.Errorf("inconsistency: non-unique node ID")
	}
}
func (c *ClusterTx) NodeIsOutdated() (bool, error) {
	nodes, err := c.nodes(false /* not pending */, "")
	if err != nil {
		return false, errors.Wrap(err, "Failed to fetch nodes")
	}

	// Figure our own version.
	version := [2]int{}
	for _, node := range nodes {
		if node.ID == c.nodeID {
			version = node.Version()
		}
	}
	if version[0] == 0 || version[1] == 0 {
		return false, fmt.Errorf("Inconsistency: local node not found")
	}

	// Check if any of the other nodes is greater than us.
	for _, node := range nodes {
		if node.ID == c.nodeID {
			continue
		}
		n, err := util.CompareVersions(node.Version(), version)
		if err != nil {
			errors.Wrapf(err, "Failed to compare with version of node %s", node.Name)
		}

		if n == 1 {
			// The other node's version is greater than ours.
			return true, nil
		}
	}

	return false, nil
}
func (c *ClusterTx) NodesCount() (int, error) {
	count, err := query.Count(c.tx, "nodes", "")
	if err != nil {
		return 0, errors.Wrap(err, "failed to count existing nodes")
	}
	return count, nil
}
func (c *ClusterTx) NodeRename(old, new string) error {
	count, err := query.Count(c.tx, "nodes", "name=?", new)
	if err != nil {
		return errors.Wrap(err, "failed to check existing nodes")
	}
	if count != 0 {
		return ErrAlreadyDefined
	}
	stmt := `UPDATE nodes SET name=? WHERE name=?`
	result, err := c.tx.Exec(stmt, new, old)
	if err != nil {
		return errors.Wrap(err, "failed to update node name")
	}
	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "failed to get rows count")
	}
	if n != 1 {
		return fmt.Errorf("expected to update one row, not %d", n)
	}
	return nil
}
func (c *ClusterTx) nodes(pending bool, where string, args ...interface{}) ([]NodeInfo, error) {
	nodes := []NodeInfo{}
	dest := func(i int) []interface{} {
		nodes = append(nodes, NodeInfo{})
		return []interface{}{
			&nodes[i].ID,
			&nodes[i].Name,
			&nodes[i].Address,
			&nodes[i].Description,
			&nodes[i].Schema,
			&nodes[i].APIExtensions,
			&nodes[i].Heartbeat,
		}
	}
	if pending {
		args = append([]interface{}{1}, args...)
	} else {
		args = append([]interface{}{0}, args...)
	}
	sql := `
SELECT id, name, address, description, schema, api_extensions, heartbeat FROM nodes WHERE pending=? `
	if where != "" {
		sql += fmt.Sprintf("AND %s ", where)
	}
	sql += "ORDER BY id"
	stmt, err := c.tx.Prepare(sql)
	if err != nil {
		return nil, err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest, args...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch nodes")
	}
	return nodes, nil
}
func (c *ClusterTx) NodeAdd(name string, address string) (int64, error) {
	columns := []string{"name", "address", "schema", "api_extensions"}
	values := []interface{}{name, address, cluster.SchemaVersion, version.APIExtensionsCount()}
	return query.UpsertObject(c.tx, "nodes", columns, values)
}
func (c *ClusterTx) NodePending(id int64, pending bool) error {
	value := 0
	if pending {
		value = 1
	}
	result, err := c.tx.Exec("UPDATE nodes SET pending=? WHERE id=?", value, id)
	if err != nil {
		return err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if n != 1 {
		return fmt.Errorf("query updated %d rows instead of 1", n)
	}
	return nil
}
func (c *ClusterTx) NodeUpdate(id int64, name string, address string) error {
	result, err := c.tx.Exec("UPDATE nodes SET name=?, address=? WHERE id=?", name, address, id)
	if err != nil {
		return err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if n != 1 {
		return fmt.Errorf("query updated %d rows instead of 1", n)
	}
	return nil
}
func (c *ClusterTx) NodeRemove(id int64) error {
	result, err := c.tx.Exec("DELETE FROM nodes WHERE id=?", id)
	if err != nil {
		return err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if n != 1 {
		return fmt.Errorf("query deleted %d rows instead of 1", n)
	}
	return nil
}
func (c *ClusterTx) NodeHeartbeat(address string, heartbeat time.Time) error {
	stmt := "UPDATE nodes SET heartbeat=? WHERE address=?"
	result, err := c.tx.Exec(stmt, heartbeat, address)
	if err != nil {
		return err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return err
	}
	if n != 1 {
		return fmt.Errorf("expected to update one row and not %d", n)
	}
	return nil
}
func (c *ClusterTx) NodeIsEmpty(id int64) (string, error) {
	// Check if the node has any containers.
	containers, err := query.SelectStrings(c.tx, "SELECT name FROM containers WHERE node_id=?", id)
	if err != nil {
		return "", errors.Wrapf(err, "Failed to get containers for node %d", id)
	}
	if len(containers) > 0 {
		message := fmt.Sprintf(
			"Node still has the following containers: %s", strings.Join(containers, ", "))
		return message, nil
	}

	// Check if the node has any images available only in it.
	images := []struct {
		fingerprint string
		nodeID      int64
	}{}
	dest := func(i int) []interface{} {
		images = append(images, struct {
			fingerprint string
			nodeID      int64
		}{})
		return []interface{}{&images[i].fingerprint, &images[i].nodeID}

	}
	stmt, err := c.tx.Prepare(`
SELECT fingerprint, node_id FROM images JOIN images_nodes ON images.id=images_nodes.image_id`)
	if err != nil {
		return "", err
	}
	defer stmt.Close()
	err = query.SelectObjects(stmt, dest)
	if err != nil {
		return "", errors.Wrapf(err, "Failed to get image list for node %d", id)
	}
	index := map[string][]int64{} // Map fingerprints to IDs of nodes
	for _, image := range images {
		index[image.fingerprint] = append(index[image.fingerprint], image.nodeID)
	}

	fingerprints := []string{}
	for fingerprint, ids := range index {
		if len(ids) > 1 {
			continue
		}
		if ids[0] == id {
			fingerprints = append(fingerprints, fingerprint)
		}
	}

	if len(fingerprints) > 0 {
		message := fmt.Sprintf(
			"Node still has the following images: %s", strings.Join(fingerprints, ", "))
		return message, nil
	}

	// Check if the node has any custom volumes.
	volumes, err := query.SelectStrings(
		c.tx, "SELECT name FROM storage_volumes WHERE node_id=? AND type=?",
		id, StoragePoolVolumeTypeCustom)
	if err != nil {
		return "", errors.Wrapf(err, "Failed to get custom volumes for node %d", id)
	}
	if len(volumes) > 0 {
		message := fmt.Sprintf(
			"Node still has the following custom volumes: %s", strings.Join(volumes, ", "))
		return message, nil
	}

	return "", nil
}
func (c *ClusterTx) NodeClear(id int64) error {
	_, err := c.tx.Exec("DELETE FROM containers WHERE node_id=?", id)
	if err != nil {
		return err
	}

	// Get the IDs of the images this node is hosting.
	ids, err := query.SelectIntegers(c.tx, "SELECT image_id FROM images_nodes WHERE node_id=?", id)
	if err != nil {
		return err
	}

	// Delete the association
	_, err = c.tx.Exec("DELETE FROM images_nodes WHERE node_id=?", id)
	if err != nil {
		return err
	}

	// Delete the image as well if this was the only node with it.
	for _, id := range ids {
		count, err := query.Count(c.tx, "images_nodes", "image_id=?", id)
		if err != nil {
			return err
		}
		if count > 0 {
			continue
		}
		_, err = c.tx.Exec("DELETE FROM images WHERE id=?", id)
		if err != nil {
			return err
		}
	}

	return nil
}
func (c *ClusterTx) NodeOfflineThreshold() (time.Duration, error) {
	threshold := time.Duration(DefaultOfflineThreshold) * time.Second
	values, err := query.SelectStrings(
		c.tx, "SELECT value FROM config WHERE key='cluster.offline_threshold'")
	if err != nil {
		return -1, err
	}
	if len(values) > 0 {
		seconds, err := strconv.Atoi(values[0])
		if err != nil {
			return -1, err
		}
		threshold = time.Duration(seconds) * time.Second
	}
	return threshold, nil
}
func (c *ClusterTx) NodeUpdateVersion(id int64, version [2]int) error {
	stmt := "UPDATE nodes SET schema=?, api_extensions=? WHERE id=?"

	result, err := c.tx.Exec(stmt, version[0], version[1], id)
	if err != nil {
		return errors.Wrap(err, "Failed to update nodes table")
	}

	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "Failed to get affected rows")
	}

	if n != 1 {
		return fmt.Errorf("Expected exactly one row to be updated")
	}

	return nil
}
func Transaction(db *sql.DB, f func(*sql.Tx) error) error {
	tx, err := db.Begin()
	if err != nil {
		return errors.Wrap(err, "failed to begin transaction")
	}

	err = f(tx)
	if err != nil {
		return rollback(tx, err)
	}

	err = tx.Commit()
	if err == sql.ErrTxDone {
		err = nil // Ignore duplicate commits/rollbacks
	}
	return err
}
func rollback(tx *sql.Tx, reason error) error {
	err := tx.Rollback()
	if err != nil {
		logger.Warnf("Failed to rollback transaction after error (%v): %v", reason, err)
	}

	return reason
}
func (c *ClusterTx) ProfileURIs(filter ProfileFilter) ([]string, error) {
	// Check which filter criteria are active.
	criteria := map[string]interface{}{}
	if filter.Project != "" {
		criteria["Project"] = filter.Project
	}
	if filter.Name != "" {
		criteria["Name"] = filter.Name
	}

	// Pick the prepared statement and arguments to use based on active criteria.
	var stmt *sql.Stmt
	var args []interface{}

	if criteria["Project"] != nil && criteria["Name"] != nil {
		stmt = c.stmt(profileNamesByProjectAndName)
		args = []interface{}{
			filter.Project,
			filter.Name,
		}
	} else if criteria["Project"] != nil {
		stmt = c.stmt(profileNamesByProject)
		args = []interface{}{
			filter.Project,
		}
	} else {
		stmt = c.stmt(profileNames)
		args = []interface{}{}
	}

	code := cluster.EntityTypes["profile"]
	formatter := cluster.EntityFormatURIs[code]

	return query.SelectURIs(stmt, formatter, args...)
}
func (c *ClusterTx) ProfileGet(project string, name string) (*Profile, error) {
	filter := ProfileFilter{}
	filter.Project = project
	filter.Name = name

	objects, err := c.ProfileList(filter)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch Profile")
	}

	switch len(objects) {
	case 0:
		return nil, ErrNoSuchObject
	case 1:
		return &objects[0], nil
	default:
		return nil, fmt.Errorf("More than one profile matches")
	}
}
func (c *ClusterTx) ProfileExists(project string, name string) (bool, error) {
	_, err := c.ProfileID(project, name)
	if err != nil {
		if err == ErrNoSuchObject {
			return false, nil
		}
		return false, err
	}

	return true, nil
}
func (c *ClusterTx) ProfileConfigRef(filter ProfileFilter) (map[string]map[string]map[string]string, error) {
	// Result slice.
	objects := make([]struct {
		Project string
		Name    string
		Key     string
		Value   string
	}, 0)

	// Check which filter criteria are active.
	criteria := map[string]interface{}{}
	if filter.Project != "" {
		criteria["Project"] = filter.Project
	}
	if filter.Name != "" {
		criteria["Name"] = filter.Name
	}

	// Pick the prepared statement and arguments to use based on active criteria.
	var stmt *sql.Stmt
	var args []interface{}

	if criteria["Project"] != nil && criteria["Name"] != nil {
		stmt = c.stmt(profileConfigRefByProjectAndName)
		args = []interface{}{
			filter.Project,
			filter.Name,
		}
	} else if criteria["Project"] != nil {
		stmt = c.stmt(profileConfigRefByProject)
		args = []interface{}{
			filter.Project,
		}
	} else {
		stmt = c.stmt(profileConfigRef)
		args = []interface{}{}
	}

	// Dest function for scanning a row.
	dest := func(i int) []interface{} {
		objects = append(objects, struct {
			Project string
			Name    string
			Key     string
			Value   string
		}{})
		return []interface{}{
			&objects[i].Project,
			&objects[i].Name,
			&objects[i].Key,
			&objects[i].Value,
		}
	}

	// Select.
	err := query.SelectObjects(stmt, dest, args...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch  ref for profiles")
	}

	// Build index by primary name.
	index := map[string]map[string]map[string]string{}

	for _, object := range objects {
		_, ok := index[object.Project]
		if !ok {
			subIndex := map[string]map[string]string{}
			index[object.Project] = subIndex
		}

		item, ok := index[object.Project][object.Name]
		if !ok {
			item = map[string]string{}
		}

		index[object.Project][object.Name] = item
		item[object.Key] = object.Value
	}

	return index, nil
}
func (c *ClusterTx) ProfileUsedByRef(filter ProfileFilter) (map[string]map[string][]string, error) {
	// Result slice.
	objects := make([]struct {
		Project string
		Name    string
		Value   string
	}, 0)

	// Check which filter criteria are active.
	criteria := map[string]interface{}{}
	if filter.Project != "" {
		criteria["Project"] = filter.Project
	}
	if filter.Name != "" {
		criteria["Name"] = filter.Name
	}

	// Pick the prepared statement and arguments to use based on active criteria.
	var stmt *sql.Stmt
	var args []interface{}

	if criteria["Project"] != nil && criteria["Name"] != nil {
		stmt = c.stmt(profileUsedByRefByProjectAndName)
		args = []interface{}{
			filter.Project,
			filter.Name,
		}
	} else if criteria["Project"] != nil {
		stmt = c.stmt(profileUsedByRefByProject)
		args = []interface{}{
			filter.Project,
		}
	} else {
		stmt = c.stmt(profileUsedByRef)
		args = []interface{}{}
	}

	// Dest function for scanning a row.
	dest := func(i int) []interface{} {
		objects = append(objects, struct {
			Project string
			Name    string
			Value   string
		}{})
		return []interface{}{
			&objects[i].Project,
			&objects[i].Name,
			&objects[i].Value,
		}
	}

	// Select.
	err := query.SelectObjects(stmt, dest, args...)
	if err != nil {
		return nil, errors.Wrap(err, "Failed to fetch string ref for profiles")
	}

	// Build index by primary name.
	index := map[string]map[string][]string{}

	for _, object := range objects {
		_, ok := index[object.Project]
		if !ok {
			subIndex := map[string][]string{}
			index[object.Project] = subIndex
		}

		item, ok := index[object.Project][object.Name]
		if !ok {
			item = []string{}
		}

		index[object.Project][object.Name] = append(item, object.Value)
	}

	return index, nil
}
func (c *ClusterTx) ProfileCreate(object Profile) (int64, error) {
	// Check if a profile with the same key exists.
	exists, err := c.ProfileExists(object.Project, object.Name)
	if err != nil {
		return -1, errors.Wrap(err, "Failed to check for duplicates")
	}
	if exists {
		return -1, fmt.Errorf("This profile already exists")
	}

	args := make([]interface{}, 3)

	// Populate the statement arguments.
	args[0] = object.Project
	args[1] = object.Name
	args[2] = object.Description

	// Prepared statement to use.
	stmt := c.stmt(profileCreate)

	// Execute the statement.
	result, err := stmt.Exec(args...)
	if err != nil {
		return -1, errors.Wrap(err, "Failed to create profile")
	}

	id, err := result.LastInsertId()
	if err != nil {
		return -1, errors.Wrap(err, "Failed to fetch profile ID")
	}

	// Insert config reference.
	stmt = c.stmt(profileCreateConfigRef)
	for key, value := range object.Config {
		_, err := stmt.Exec(id, key, value)
		if err != nil {
			return -1, errors.Wrap(err, "Insert config for profile")
		}
	}

	// Insert devices reference.
	for name, config := range object.Devices {
		typ, ok := config["type"]
		if !ok {
			return -1, fmt.Errorf("No type for device %s", name)
		}
		typCode, err := dbDeviceTypeToInt(typ)
		if err != nil {
			return -1, errors.Wrapf(err, "Device type code for %s", typ)
		}
		stmt = c.stmt(profileCreateDevicesRef)
		result, err := stmt.Exec(id, name, typCode)
		if err != nil {
			return -1, errors.Wrapf(err, "Insert device %s", name)
		}
		deviceID, err := result.LastInsertId()
		if err != nil {
			return -1, errors.Wrap(err, "Failed to fetch device ID")
		}
		stmt = c.stmt(profileCreateDevicesConfigRef)
		for key, value := range config {
			_, err := stmt.Exec(deviceID, key, value)
			if err != nil {
				return -1, errors.Wrap(err, "Insert config for profile")
			}
		}
	}

	return id, nil
}
func (c *ClusterTx) ProfileRename(project string, name string, to string) error {
	stmt := c.stmt(profileRename)
	result, err := stmt.Exec(to, project, name)
	if err != nil {
		return errors.Wrap(err, "Rename profile")
	}

	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "Fetch affected rows")
	}
	if n != 1 {
		return fmt.Errorf("Query affected %d rows instead of 1", n)
	}
	return nil
}
func (c *ClusterTx) ProfileDelete(project string, name string) error {
	stmt := c.stmt(profileDelete)
	result, err := stmt.Exec(project, name)
	if err != nil {
		return errors.Wrap(err, "Delete profile")
	}

	n, err := result.RowsAffected()
	if err != nil {
		return errors.Wrap(err, "Fetch affected rows")
	}
	if n != 1 {
		return fmt.Errorf("Query deleted %d rows instead of 1", n)
	}

	return nil
}
func ServerTLSConfig(cert *shared.CertInfo) *tls.Config {
	config := shared.InitTLSConfig()
	config.ClientAuth = tls.RequestClientCert
	config.Certificates = []tls.Certificate{cert.KeyPair()}
	config.NextProtos = []string{"h2"} // Required by gRPC

	if cert.CA() != nil {
		pool := x509.NewCertPool()
		pool.AddCert(cert.CA())
		config.RootCAs = pool
		config.ClientCAs = pool

		logger.Infof("LXD is in CA mode, only CA-signed certificates will be allowed")
	}

	config.BuildNameToCertificate()
	return config
}
func NetworkInterfaceAddress() string {
	ifaces, err := net.Interfaces()
	if err != nil {
		return ""
	}
	for _, iface := range ifaces {
		if shared.IsLoopback(&iface) {
			continue
		}
		addrs, err := iface.Addrs()
		if err != nil {
			continue
		}
		if len(addrs) == 0 {
			continue
		}
		addr, ok := addrs[0].(*net.IPNet)
		if !ok {
			continue
		}
		return addr.IP.String()
	}
	return ""
}
func IsAddressCovered(address1, address2 string) bool {
	if address1 == address2 {
		return true
	}

	host1, port1, err := net.SplitHostPort(address1)
	if err != nil {
		return false
	}

	host2, port2, err := net.SplitHostPort(address2)
	if err != nil {
		return false
	}

	// If the ports are different, then address1 is clearly not covered by
	// address2.
	if port2 != port1 {
		return false
	}

	// If address2 is using an IPv4 wildcard for the host, then address2 is
	// only covered if it's an IPv4 address.
	if host2 == "0.0.0.0" {
		ip := net.ParseIP(host1)
		if ip != nil && ip.To4() != nil {
			return true
		}
		return false
	}

	// If address2 is using an IPv6 wildcard for the host, then address2 is
	// always covered.
	if host2 == "::" || host2 == "" {
		return true
	}

	return false
}
func SelectObjects(stmt *sql.Stmt, dest Dest, args ...interface{}) error {
	rows, err := stmt.Query(args...)
	if err != nil {
		return err
	}
	defer rows.Close()

	for i := 0; rows.Next(); i++ {
		err := rows.Scan(dest(i)...)
		if err != nil {
			return err
		}
	}

	err = rows.Err()
	if err != nil {
		return err
	}
	return nil
}
func DeleteObject(tx *sql.Tx, table string, id int64) (bool, error) {
	stmt := fmt.Sprintf("DELETE FROM %s WHERE id=?", table)
	result, err := tx.Exec(stmt, id)
	if err != nil {
		return false, err
	}
	n, err := result.RowsAffected()
	if err != nil {
		return false, err
	}
	if n > 1 {
		return true, fmt.Errorf("more than one row was deleted")
	}
	return n == 1, nil
}
func (t *Task) loop(ctx context.Context) {
	// Kick off the task immediately (as long as the the schedule is
	// greater than zero, see below).
	delay := immediately

	for {
		var timer <-chan time.Time

		schedule, err := t.schedule()
		switch err {
		case ErrSkip:
			// Reset the delay to be exactly the schedule, so we
			// rule out the case where it's set to immediately
			// because it's the first iteration or we got reset.
			delay = schedule
			fallthrough // Fall to case nil, to apply normal non-error logic
		case nil:
			// If the schedule is greater than zero, setup a timer
			// that will expire after 'delay' seconds (or after the
			// schedule in case of ErrSkip, to avoid triggering
			// immediately), otherwise setup a timer that will
			// never expire (hence the task function won't ever be
			// run, unless Reset() is called and schedule() starts
			// returning values greater than zero).
			if schedule > 0 {
				timer = time.After(delay)
			} else {
				timer = make(chan time.Time)
			}
		default:
			// If the schedule is not greater than zero, abort the
			// task and return immediately. Otherwise set up the
			// timer to retry after that amount of time.
			if schedule <= 0 {
				return
			}
			timer = time.After(schedule)

		}

		select {
		case <-timer:
			if err == nil {
				// Execute the task function synchronously. Consumers
				// are responsible for implementing proper cancellation
				// of the task function itself using the tomb's context.
				t.f(ctx)
				delay = schedule
			} else {
				// Don't execute the task function, and set the
				// delay to run it immediately whenever the
				// schedule function returns a nil error.
				delay = immediately
			}
		case <-ctx.Done():
			return

		case <-t.reset:
			delay = immediately
		}
	}
}
func IsTerminal(fd int) bool {
	_, err := GetState(fd)
	return err == nil
}
func socketUnixListen(path string) (net.Listener, error) {
	addr, err := net.ResolveUnixAddr("unix", path)
	if err != nil {
		return nil, fmt.Errorf("cannot resolve socket address: %v", err)
	}

	listener, err := net.ListenUnix("unix", addr)
	if err != nil {
		return nil, fmt.Errorf("cannot bind socket: %v", err)
	}

	return listener, err

}
func socketUnixRemoveStale(path string) error {
	// If there's no socket file at all, there's nothing to do.
	if !shared.PathExists(path) {
		return nil
	}

	logger.Debugf("Detected stale unix socket, deleting")
	err := os.Remove(path)
	if err != nil {
		return fmt.Errorf("could not delete stale local socket: %v", err)
	}

	return nil
}
func socketUnixSetPermissions(path string, mode os.FileMode) error {
	err := os.Chmod(path, mode)
	if err != nil {
		return fmt.Errorf("cannot set permissions on local socket: %v", err)
	}
	return nil
}
func socketUnixSetOwnership(path string, group string) error {
	var gid int
	var err error

	if group != "" {
		gid, err = shared.GroupId(group)
		if err != nil {
			return fmt.Errorf("cannot get group ID of '%s': %v", group, err)
		}
	} else {
		gid = os.Getgid()
	}

	err = os.Chown(path, os.Getuid(), gid)
	if err != nil {
		return fmt.Errorf("cannot change ownership on local socket: %v", err)

	}

	return nil
}
func cephOSDPoolExists(ClusterName string, poolName string, userName string) bool {
	_, err := shared.RunCommand(
		"ceph",
		"--name", fmt.Sprintf("client.%s", userName),
		"--cluster", ClusterName,
		"osd",
		"pool",
		"get",
		poolName,
		"size")
	if err != nil {
		return false
	}

	return true
}
func cephOSDPoolDestroy(clusterName string, poolName string, userName string) error {
	_, err := shared.RunCommand("ceph",
		"--name", fmt.Sprintf("client.%s", userName),
		"--cluster", clusterName,
		"osd",
		"pool",
		"delete",
		poolName,
		poolName,
		"--yes-i-really-really-mean-it")
	if err != nil {
		return err
	}

	return nil
}
func cephRBDVolumeExists(clusterName string, poolName string, volumeName string,
	volumeType string, userName string) bool {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"--pool", poolName,
		"image-meta",
		"list",
		fmt.Sprintf("%s_%s", volumeType, volumeName))
	if err != nil {
		return false
	}
	return true
}
func cephRBDSnapshotProtect(clusterName string, poolName string,
	volumeName string, volumeType string, snapshotName string,
	userName string) error {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"--pool", poolName,
		"snap",
		"protect",
		"--snap", snapshotName,
		fmt.Sprintf("%s_%s", volumeType, volumeName))
	if err != nil {
		runError, ok := err.(shared.RunError)
		if ok {
			exitError, ok := runError.Err.(*exec.ExitError)
			if ok {
				waitStatus := exitError.Sys().(syscall.WaitStatus)
				if waitStatus.ExitStatus() == 16 {
					// EBUSY (snapshot already protected)
					return nil
				}
			}
		}
		return err
	}

	return nil
}
func cephRBDCloneCreate(sourceClusterName string, sourcePoolName string,
	sourceVolumeName string, sourceVolumeType string,
	sourceSnapshotName string, targetPoolName string,
	targetVolumeName string, targetVolumeType string,
	userName string) error {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", sourceClusterName,
		"--image-feature", "layering",
		"clone",
		fmt.Sprintf("%s/%s_%s@%s", sourcePoolName, sourceVolumeType,
			sourceVolumeName, sourceSnapshotName),
		fmt.Sprintf("%s/%s_%s", targetPoolName, targetVolumeType,
			targetVolumeName))
	if err != nil {
		return err
	}

	return nil
}
func cephRBDSnapshotListClones(clusterName string, poolName string,
	volumeName string, volumeType string,
	snapshotName string, userName string) ([]string, error) {
	msg, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"--pool", poolName,
		"children",
		"--image", fmt.Sprintf("%s_%s", volumeType, volumeName),
		"--snap", snapshotName)
	if err != nil {
		return nil, err
	}

	msg = strings.TrimSpace(msg)
	clones := strings.Fields(msg)
	if len(clones) == 0 {
		return nil, db.ErrNoSuchObject
	}

	return clones, nil
}
func cephRBDVolumeMarkDeleted(clusterName string, poolName string,
	volumeType string, oldVolumeName string, newVolumeName string,
	userName string, suffix string) error {
	deletedName := fmt.Sprintf("%s/zombie_%s_%s", poolName, volumeType,
		newVolumeName)
	if suffix != "" {
		deletedName = fmt.Sprintf("%s_%s", deletedName, suffix)
	}
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"mv",
		fmt.Sprintf("%s/%s_%s", poolName, volumeType, oldVolumeName),
		deletedName)
	if err != nil {
		return err
	}

	return nil
}
func cephRBDVolumeUnmarkDeleted(clusterName string, poolName string,
	volumeName string, volumeType string, userName string, oldSuffix string,
	newSuffix string) error {
	oldName := fmt.Sprintf("%s/zombie_%s_%s", poolName, volumeType, volumeName)
	if oldSuffix != "" {
		oldName = fmt.Sprintf("%s_%s", oldName, oldSuffix)
	}

	newName := fmt.Sprintf("%s/%s_%s", poolName, volumeType, volumeName)
	if newSuffix != "" {
		newName = fmt.Sprintf("%s_%s", newName, newSuffix)
	}

	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"mv",
		oldName,
		newName)
	if err != nil {
		return err
	}

	return nil
}
func cephRBDVolumeRename(clusterName string, poolName string, volumeType string,
	oldVolumeName string, newVolumeName string, userName string) error {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"mv",
		fmt.Sprintf("%s/%s_%s", poolName, volumeType, oldVolumeName),
		fmt.Sprintf("%s/%s_%s", poolName, volumeType, newVolumeName))
	if err != nil {
		return err
	}

	return nil
}
func cephRBDVolumeSnapshotRename(clusterName string, poolName string,
	volumeName string, volumeType string, oldSnapshotName string,
	newSnapshotName string, userName string) error {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"snap",
		"rename",
		fmt.Sprintf("%s/%s_%s@%s", poolName, volumeType, volumeName,
			oldSnapshotName),
		fmt.Sprintf("%s/%s_%s@%s", poolName, volumeType, volumeName,
			newSnapshotName))
	if err != nil {
		return err
	}

	return nil
}
func cephRBDSnapshotDelete(clusterName string, poolName string,
	volumeName string, volumeType string, snapshotName string,
	userName string) error {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"--pool", poolName,
		"snap",
		"rm",
		fmt.Sprintf("%s_%s@%s", volumeType, volumeName, snapshotName))
	if err != nil {
		return err
	}

	return nil
}
func cephRBDVolumeCopy(clusterName string, oldVolumeName string,
	newVolumeName string, userName string) error {
	_, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--cluster", clusterName,
		"cp",
		oldVolumeName,
		newVolumeName)
	if err != nil {
		return err
	}

	return nil
}
func cephRBDVolumeListSnapshots(clusterName string, poolName string,
	volumeName string, volumeType string,
	userName string) ([]string, error) {
	msg, err := shared.RunCommand(
		"rbd",
		"--id", userName,
		"--format", "json",
		"--cluster", clusterName,
		"--pool", poolName,
		"snap",
		"ls", fmt.Sprintf("%s_%s", volumeType, volumeName))
	if err != nil {
		return []string{}, err
	}

	var data []map[string]interface{}
	err = json.Unmarshal([]byte(msg), &data)
	if err != nil {
		return []string{}, err
	}

	snapshots := []string{}
	for _, v := range data {
		_, ok := v["name"]
		if !ok {
			return []string{}, fmt.Errorf("No \"name\" property found")
		}

		name, ok := v["name"].(string)
		if !ok {
			return []string{}, fmt.Errorf("\"name\" property did not have string type")
		}

		name = strings.TrimSpace(name)
		snapshots = append(snapshots, name)
	}

	if len(snapshots) == 0 {
		return []string{}, db.ErrNoSuchObject
	}

	return snapshots, nil
}
func (s *storageCeph) getRBDSize() (string, error) {
	sz, err := shared.ParseByteSizeString(s.volume.Config["size"])
	if err != nil {
		return "", err
	}

	// Safety net: Set to default value.
	if sz == 0 {
		sz, _ = shared.ParseByteSizeString("10GB")
	}

	return fmt.Sprintf("%dB", sz), nil
}
func (s *storageCeph) getRBDFilesystem() string {
	if s.volume.Config["block.filesystem"] != "" {
		return s.volume.Config["block.filesystem"]
	}

	if s.pool.Config["volume.block.filesystem"] != "" {
		return s.pool.Config["volume.block.filesystem"]
	}

	return "ext4"
}
func (s *storageCeph) copyWithoutSnapshotsFull(target container,
	source container) error {
	logger.Debugf(`Creating non-sparse copy of RBD storage volume for container "%s" to "%s" without snapshots`, source.Name(), target.Name())

	sourceIsSnapshot := source.IsSnapshot()
	sourceContainerName := projectPrefix(source.Project(), source.Name())
	targetContainerName := projectPrefix(target.Project(), target.Name())
	oldVolumeName := fmt.Sprintf("%s/container_%s", s.OSDPoolName,
		sourceContainerName)
	newVolumeName := fmt.Sprintf("%s/container_%s", s.OSDPoolName,
		targetContainerName)
	if sourceIsSnapshot {
		sourceContainerOnlyName, sourceSnapshotOnlyName, _ :=
			containerGetParentAndSnapshotName(sourceContainerName)
		oldVolumeName = fmt.Sprintf("%s/container_%s@snapshot_%s",
			s.OSDPoolName, sourceContainerOnlyName,
			sourceSnapshotOnlyName)
	}

	err := cephRBDVolumeCopy(s.ClusterName, oldVolumeName, newVolumeName,
		s.UserName)
	if err != nil {
		logger.Debugf(`Failed to create full RBD copy "%s" to "%s": %s`, source.Name(), target.Name(), err)
		return err
	}

	_, err = cephRBDVolumeMap(s.ClusterName, s.OSDPoolName, targetContainerName,
		storagePoolVolumeTypeNameContainer, s.UserName)
	if err != nil {
		logger.Errorf(`Failed to map RBD storage volume for image "%s" on storage pool "%s": %s`, targetContainerName, s.pool.Name, err)
		return err
	}

	targetContainerMountPoint := getContainerMountPoint(target.Project(), s.pool.Name, target.Name())
	err = createContainerMountpoint(targetContainerMountPoint, target.Path(), target.IsPrivileged())
	if err != nil {
		return err
	}

	ourMount, err := target.StorageStart()
	if err != nil {
		return err
	}
	if ourMount {
		defer target.StorageStop()
	}

	err = target.TemplateApply("copy")
	if err != nil {
		logger.Errorf(`Failed to apply copy template for container "%s": %s`, target.Name(), err)
		return err
	}
	logger.Debugf(`Applied copy template for container "%s"`, target.Name())

	logger.Debugf(`Created non-sparse copy of RBD storage volume for container "%s" to "%s" without snapshots`, source.Name(),
		target.Name())
	return nil
}
func (s *storageCeph) copyWithoutSnapshotsSparse(target container,
	source container) error {
	logger.Debugf(`Creating sparse copy of RBD storage volume for container "%s" to "%s" without snapshots`, source.Name(),
		target.Name())

	sourceIsSnapshot := source.IsSnapshot()
	sourceContainerName := projectPrefix(source.Project(), source.Name())
	targetContainerName := projectPrefix(target.Project(), target.Name())
	sourceContainerOnlyName := sourceContainerName
	sourceSnapshotOnlyName := ""
	snapshotName := fmt.Sprintf("zombie_snapshot_%s",
		uuid.NewRandom().String())
	if sourceIsSnapshot {
		sourceContainerOnlyName, sourceSnapshotOnlyName, _ =
			containerGetParentAndSnapshotName(sourceContainerName)
		snapshotName = fmt.Sprintf("snapshot_%s", sourceSnapshotOnlyName)
	} else {
		// create snapshot
		err := cephRBDSnapshotCreate(s.ClusterName, s.OSDPoolName,
			sourceContainerName, storagePoolVolumeTypeNameContainer,
			snapshotName, s.UserName)
		if err != nil {
			logger.Errorf(`Failed to create snapshot for RBD storage volume for image "%s" on storage pool "%s": %s`, targetContainerName, s.pool.Name, err)
			return err
		}
	}

	// protect volume so we can create clones of it
	err := cephRBDSnapshotProtect(s.ClusterName, s.OSDPoolName,
		sourceContainerOnlyName, storagePoolVolumeTypeNameContainer,
		snapshotName, s.UserName)
	if err != nil {
		logger.Errorf(`Failed to protect snapshot for RBD storage volume for image "%s" on storage pool "%s": %s`, snapshotName, s.pool.Name, err)
		return err
	}

	err = cephRBDCloneCreate(s.ClusterName, s.OSDPoolName,
		sourceContainerOnlyName, storagePoolVolumeTypeNameContainer,
		snapshotName, s.OSDPoolName, targetContainerName,
		storagePoolVolumeTypeNameContainer, s.UserName)
	if err != nil {
		logger.Errorf(`Failed to clone new RBD storage volume for container "%s": %s`, targetContainerName, err)
		return err
	}

	// Re-generate the UUID
	err = s.cephRBDGenerateUUID(projectPrefix(target.Project(), target.Name()), storagePoolVolumeTypeNameContainer)
	if err != nil {
		return err
	}

	// Create mountpoint
	targetContainerMountPoint := getContainerMountPoint(target.Project(), s.pool.Name, target.Name())
	err = createContainerMountpoint(targetContainerMountPoint, target.Path(), target.IsPrivileged())
	if err != nil {
		return err
	}

	ourMount, err := target.StorageStart()
	if err != nil {
		return err
	}
	if ourMount {
		defer target.StorageStop()
	}

	err = target.TemplateApply("copy")
	if err != nil {
		logger.Errorf(`Failed to apply copy template for container "%s": %s`, target.Name(), err)
		return err
	}
	logger.Debugf(`Applied copy template for container "%s"`, target.Name())

	logger.Debugf(`Created sparse copy of RBD storage volume for container "%s" to "%s" without snapshots`, source.Name(),
		target.Name())
	return nil
}
func GetConfigCmd(noPortForwarding *bool) *cobra.Command {
	var format string
	getConfig := &cobra.Command{
		Short: "Retrieve Pachyderm's current auth configuration",
		Long:  "Retrieve Pachyderm's current auth configuration",
		Run: cmdutil.RunFixedArgs(0, func(args []string) error {
			c, err := client.NewOnUserMachine(true, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			resp, err := c.GetConfiguration(c.Ctx(), &auth.GetConfigurationRequest{})
			if err != nil {
				return grpcutil.ScrubGRPC(err)
			}
			if resp.Configuration == nil {
				fmt.Println("no auth config set")
				return nil
			}
			output, err := json.MarshalIndent(resp.Configuration, "", "  ")
			if err != nil {
				return fmt.Errorf("could not marshal response:\n%v\ndue to: %v", resp.Configuration, err)
			}
			switch format {
			case "json":
				// already done
			case "yaml":
				output, err = yaml.JSONToYAML(output)
				if err != nil {
					return fmt.Errorf("could not convert json to yaml: %v", err)
				}
			default:
				return fmt.Errorf("invalid output format: %v", format)
			}
			fmt.Println(string(output))
			return nil
		}),
	}
	getConfig.Flags().StringVarP(&format, "output-format", "o", "json", "output "+
		"format (\"json\" or \"yaml\")")
	return cmdutil.CreateAlias(getConfig, "auth get-config")
}
func SetConfigCmd(noPortForwarding *bool) *cobra.Command {
	var file string
	setConfig := &cobra.Command{
		Short: "Set Pachyderm's current auth configuration",
		Long:  "Set Pachyderm's current auth configuration",
		Run: cmdutil.RunFixedArgs(0, func(args []string) error {
			c, err := client.NewOnUserMachine(true, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			var configBytes []byte
			if file == "-" {
				var err error
				configBytes, err = ioutil.ReadAll(os.Stdin)
				if err != nil {
					return fmt.Errorf("could not read config from stdin: %v", err)
				}
			} else if file != "" {
				var err error
				configBytes, err = ioutil.ReadFile(file)
				if err != nil {
					return fmt.Errorf("could not read config from %q: %v", file, err)
				}
			} else {
				return errors.New("must set input file (use \"-\" to read from stdin)")
			}

			// Try to parse config as YAML (JSON is a subset of YAML)
			var config auth.AuthConfig
			if err := yaml.Unmarshal(configBytes, &config); err != nil {
				return fmt.Errorf("could not parse config: %v", err)
			}
			// TODO(msteffen): try to handle empty config?
			_, err = c.SetConfiguration(c.Ctx(), &auth.SetConfigurationRequest{
				Configuration: &config,
			})
			return grpcutil.ScrubGRPC(err)
		}),
	}
	setConfig.Flags().StringVarP(&file, "file", "f", "-", "input file (to use "+
		"as the new config")
	return cmdutil.CreateAlias(setConfig, "auth set-config")
}
func NewSharder(discoveryClient discovery.Client, numShards uint64, namespace string) Sharder {
	return newSharder(discoveryClient, numShards, namespace)
}
func NewRouter(
	sharder Sharder,
	dialer grpcutil.Dialer,
	localAddress string,
) Router {
	return newRouter(
		sharder,
		dialer,
		localAddress,
	)
}
func renewUserCredentials(ctx context.Context, pachdAddress string, adminToken string, userToken string, ttl time.Duration) error {
	// Setup a single use client w the given admin token / address
	client, err := pclient.NewFromAddress(pachdAddress)
	if err != nil {
		return err
	}
	defer client.Close() // avoid leaking connections

	client = client.WithCtx(ctx)
	client.SetAuthToken(adminToken)

	_, err = client.AuthAPIClient.ExtendAuthToken(client.Ctx(), &auth.ExtendAuthTokenRequest{
		Token: userToken,
		TTL:   int64(ttl.Seconds()),
	})

	if err != nil {
		return err
	}

	return nil
}
func NewLocalClient(root string) (Client, error) {
	if err := os.MkdirAll(root, 0755); err != nil {
		return nil, err
	}
	return &localClient{root}, nil
}
func AddSpanToAnyExisting(ctx context.Context, operation string, kvs ...interface{}) (opentracing.Span, context.Context) {
	if parentSpan := opentracing.SpanFromContext(ctx); parentSpan != nil {
		span := opentracing.StartSpan(operation, opentracing.ChildOf(parentSpan.Context()))
		tagSpan(span, kvs)
		return span, opentracing.ContextWithSpan(ctx, span)
	}
	return nil, ctx
}
func InstallJaegerTracerFromEnv() {
	jaegerOnce.Do(func() {
		jaegerEndpoint, onUserMachine := os.LookupEnv(jaegerEndpointEnvVar)
		if !onUserMachine {
			if host, ok := os.LookupEnv("JAEGER_COLLECTOR_SERVICE_HOST"); ok {
				port := os.Getenv("JAEGER_COLLECTOR_SERVICE_PORT_JAEGER_COLLECTOR_HTTP")
				jaegerEndpoint = fmt.Sprintf("%s:%s", host, port)
			}
		}
		if jaegerEndpoint == "" {
			return // break early -- not using Jaeger
		}

		// canonicalize jaegerEndpoint as http://<hostport>/api/traces
		jaegerEndpoint = strings.TrimPrefix(jaegerEndpoint, "http://")
		jaegerEndpoint = strings.TrimSuffix(jaegerEndpoint, "/api/traces")
		jaegerEndpoint = fmt.Sprintf("http://%s/api/traces", jaegerEndpoint)
		cfg := jaegercfg.Configuration{
			// Configure Jaeger to sample every call, but use the SpanInclusionFunc
			// addTraceIfTracingEnabled (defined below) to skip sampling every RPC
			// unless the PACH_ENABLE_TRACING environment variable is set
			Sampler: &jaegercfg.SamplerConfig{
				Type:  "const",
				Param: 1,
			},
			Reporter: &jaegercfg.ReporterConfig{
				LogSpans:            true,
				BufferFlushInterval: 1 * time.Second,
				CollectorEndpoint:   jaegerEndpoint,
			},
		}

		// configure jaeger logger
		logger := jaeger.Logger(jaeger.NullLogger)
		if !onUserMachine {
			logger = jaeger.StdLogger
		}

		// Hack: ignore second argument (io.Closer) because the Jaeger
		// implementation of opentracing.Tracer also implements io.Closer (i.e. the
		// first and second return values from cfg.New(), here, are two interfaces
		// that wrap the same underlying type). Instead of storing the second return
		// value here, just cast the tracer to io.Closer in CloseAndReportTraces()
		// (below) and call 'Close()' on it there.
		tracer, _, err := cfg.New(JaegerServiceName, jaegercfg.Logger(logger))
		if err != nil {
			panic(fmt.Sprintf("could not install Jaeger tracer: %v", err))
		}
		opentracing.SetGlobalTracer(tracer)
	})
}
func UnaryClientInterceptor() grpc.UnaryClientInterceptor {
	return otgrpc.OpenTracingClientInterceptor(opentracing.GlobalTracer(),
		otgrpc.IncludingSpans(addTraceIfTracingEnabled))
}
func StreamClientInterceptor() grpc.StreamClientInterceptor {
	return otgrpc.OpenTracingStreamClientInterceptor(opentracing.GlobalTracer(),
		otgrpc.IncludingSpans(addTraceIfTracingEnabled))
}
func UnaryServerInterceptor() grpc.UnaryServerInterceptor {
	return otgrpc.OpenTracingServerInterceptor(opentracing.GlobalTracer(),
		otgrpc.IncludingSpans(addTraceIfTracingEnabled))
}
func StreamServerInterceptor() grpc.StreamServerInterceptor {
	return otgrpc.OpenTracingStreamServerInterceptor(opentracing.GlobalTracer(),
		otgrpc.IncludingSpans(addTraceIfTracingEnabled))
}
func CloseAndReportTraces() {
	if c, ok := opentracing.GlobalTracer().(io.Closer); ok {
		c.Close()
	}
}
func newWriter(ctx context.Context, objC obj.Client, prefix string) *Writer {
	// Initialize buzhash64 with WindowSize window.
	hash := buzhash64.New()
	hash.Write(make([]byte, WindowSize))
	return &Writer{
		ctx:       ctx,
		objC:      objC,
		prefix:    prefix,
		cbs:       []func([]*DataRef) error{},
		buf:       &bytes.Buffer{},
		hash:      hash,
		splitMask: (1 << uint64(AverageBits)) - 1,
	}
}
func (b *ConstantBackOff) For(maxElapsed time.Duration) *ConstantBackOff {
	b.MaxElapsedTime = maxElapsed
	return b
}
func (l *logger) Log(request interface{}, response interface{}, err error, duration time.Duration) {
	if err != nil {
		l.LogAtLevelFromDepth(request, response, err, duration, logrus.ErrorLevel, 4)
	} else {
		l.LogAtLevelFromDepth(request, response, err, duration, logrus.InfoLevel, 4)
	}
	// We have to grab the method's name here before we
	// enter the goro's stack
	go l.ReportMetric(getMethodName(), duration, err)
}
func (f FormatterFunc) Format(entry *logrus.Entry) ([]byte, error) {
	return f(entry)
}
func NewGRPCLogWriter(logger *logrus.Logger, source string) *GRPCLogWriter {
	return &GRPCLogWriter{
		logger: logger,
		source: source,
	}
}
func Read() (*Config, error) {
	var c *Config

	// Read json file
	p := configPath()
	if raw, err := ioutil.ReadFile(p); err == nil {
		err = json.Unmarshal(raw, &c)
		if err != nil {
			return nil, err
		}
	} else if os.IsNotExist(err) {
		// File doesn't exist, so create a new config
		fmt.Println("no config detected at %q. Generating new config...", p)
		c = &Config{}
	} else {
		return nil, fmt.Errorf("fatal: could not read config at %q: %v", p, err)
	}
	if c.UserID == "" {
		fmt.Printf("No UserID present in config. Generating new UserID and "+
			"updating config at %s\n", p)
		uuid, err := uuid.NewV4()
		if err != nil {
			return nil, err
		}
		c.UserID = uuid.String()
		if err := c.Write(); err != nil {
			return nil, err
		}
	}
	return c, nil
}
func (c *Config) Write() error {
	rawConfig, err := json.MarshalIndent(c, "", "  ")
	if err != nil {
		return err
	}

	// If we're not using a custom config path, create the default config path
	p := configPath()
	if _, ok := os.LookupEnv(configEnvVar); ok {
		// using overridden config path -- just make sure the parent dir exists
		d := filepath.Dir(p)
		if _, err := os.Stat(d); err != nil {
			return fmt.Errorf("cannot use config at %s: could not stat parent directory (%v)", p, err)
		}
	} else {
		// using the default config path, create the config directory
		err = os.MkdirAll(defaultConfigDir, 0755)
		if err != nil {
			return err
		}
	}
	return ioutil.WriteFile(p, rawConfig, 0644)
}
func (r *readWriter) Read(val proto.Message) error {
	buf, err := r.ReadBytes()
	if err != nil {
		return err
	}
	return proto.Unmarshal(buf, val)
}
func (r *readWriter) Write(val proto.Message) (int64, error) {
	bytes, err := proto.Marshal(val)
	if err != nil {
		return 0, err
	}
	return r.WriteBytes(bytes)
}
func NewReadWriter(rw io.ReadWriter) ReadWriter {
	return &readWriter{r: rw, w: rw}
}
func RunGitHookServer(address string, etcdAddress string, etcdPrefix string) error {
	c, err := client.NewFromAddress(address)
	if err != nil {
		return err
	}
	etcdClient, err := etcd.New(etcd.Config{
		Endpoints:   []string{etcdAddress},
		DialOptions: client.DefaultDialOptions(),
	})
	if err != nil {
		return err
	}
	hook, err := github.New()
	if err != nil {
		return err
	}
	s := &gitHookServer{
		hook,
		c,
		etcdClient,
		ppsdb.Pipelines(etcdClient, etcdPrefix),
	}
	return http.ListenAndServe(fmt.Sprintf(":%d", GitHookPort), s)
}
func newLoggingPipe() *loggingPipe {
	p := &loggingPipe{}
	p.clientReader, p.clientWriter = io.Pipe()
	p.clientReader = io.TeeReader(p.clientReader, &p.ServerToClientBuf)
	p.serverReader, p.serverWriter = io.Pipe()
	p.serverReader = io.TeeReader(p.serverReader, &p.ClientToServerBuf)
	return p
}
func (l *loggingConn) Read(b []byte) (n int, err error) {
	return l.r.Read(b)
}
func (l *loggingConn) Write(b []byte) (n int, err error) {
	return l.w.Write(b)
}
func (l *TestListener) Accept() (net.Conn, error) {
	conn := <-l.connCh
	if conn == nil {
		return nil, errors.New("Accept() has already been called on this TestListener")
	}
	return conn, nil
}
func (l *TestListener) Close() error {
	l.connMu.Lock()
	defer l.connMu.Unlock()
	c := <-l.connCh
	if c != nil {
		close(l.connCh)
	}
	return nil
}
func errorf(c ErrCode, fmtStr string, args ...interface{}) error {
	return &hashTreeError{
		code: c,
		s:    fmt.Sprintf(fmtStr, args...),
	}
}
func InitWithKube(config *Configuration) *ServiceEnv {
	env := InitServiceEnv(config)
	env.kubeEg.Go(env.initKubeClient)
	return env // env is not ready yet
}
func (env *ServiceEnv) GetEtcdClient() *etcd.Client {
	if err := env.etcdEg.Wait(); err != nil {
		panic(err) // If env can't connect, there's no sensible way to recover
	}
	if env.etcdClient == nil {
		panic("service env never connected to etcd")
	}
	return env.etcdClient
}
func (env *ServiceEnv) GetKubeClient() *kube.Clientset {
	if err := env.kubeEg.Wait(); err != nil {
		panic(err) // If env can't connect, there's no sensible way to recover
	}
	if env.kubeClient == nil {
		panic("service env never connected to kubernetes")
	}
	return env.kubeClient
}
func NewHasher(jobModulus uint64, pipelineModulus uint64) *Hasher {
	return &Hasher{
		JobModulus:      jobModulus,
		PipelineModulus: pipelineModulus,
	}
}
func (s *Hasher) HashJob(jobID string) uint64 {
	return uint64(adler32.Checksum([]byte(jobID))) % s.JobModulus
}
func (s *Hasher) HashPipeline(pipelineName string) uint64 {
	return uint64(adler32.Checksum([]byte(pipelineName))) % s.PipelineModulus
}
func Status(ctx context.Context, pipelineRcName string, etcdClient *etcd.Client, etcdPrefix string, workerGrpcPort uint16) ([]*pps.WorkerStatus, error) {
	workerClients, err := Clients(ctx, pipelineRcName, etcdClient, etcdPrefix, workerGrpcPort)
	if err != nil {
		return nil, err
	}
	var result []*pps.WorkerStatus
	for _, workerClient := range workerClients {
		status, err := workerClient.Status(ctx, &types.Empty{})
		if err != nil {
			return nil, err
		}
		result = append(result, status)
	}
	return result, nil
}
func Cancel(ctx context.Context, pipelineRcName string, etcdClient *etcd.Client,
	etcdPrefix string, workerGrpcPort uint16, jobID string, dataFilter []string) error {
	workerClients, err := Clients(ctx, pipelineRcName, etcdClient, etcdPrefix, workerGrpcPort)
	if err != nil {
		return err
	}
	success := false
	for _, workerClient := range workerClients {
		resp, err := workerClient.Cancel(ctx, &CancelRequest{
			JobID:       jobID,
			DataFilters: dataFilter,
		})
		if err != nil {
			return err
		}
		if resp.Success {
			success = true
		}
	}
	if !success {
		return fmt.Errorf("datum matching filter %+v could not be found for jobID %s", dataFilter, jobID)
	}
	return nil
}
func Conns(ctx context.Context, pipelineRcName string, etcdClient *etcd.Client, etcdPrefix string, workerGrpcPort uint16) ([]*grpc.ClientConn, error) {
	resp, err := etcdClient.Get(ctx, path.Join(etcdPrefix, WorkerEtcdPrefix, pipelineRcName), etcd.WithPrefix())
	if err != nil {
		return nil, err
	}
	var result []*grpc.ClientConn
	for _, kv := range resp.Kvs {
		conn, err := grpc.Dial(fmt.Sprintf("%s:%d", path.Base(string(kv.Key)), workerGrpcPort),
			append(client.DefaultDialOptions(), grpc.WithInsecure())...)
		if err != nil {
			return nil, err
		}
		result = append(result, conn)
	}
	return result, nil
}
func Clients(ctx context.Context, pipelineRcName string, etcdClient *etcd.Client, etcdPrefix string, workerGrpcPort uint16) ([]Client, error) {
	conns, err := Conns(ctx, pipelineRcName, etcdClient, etcdPrefix, workerGrpcPort)
	if err != nil {
		return nil, err
	}
	var result []Client
	for _, conn := range conns {
		result = append(result, newClient(conn))
	}
	return result, nil
}
func NewClient(address string) (Client, error) {
	port, err := strconv.Atoi(os.Getenv(client.PPSWorkerPortEnv))
	if err != nil {
		return Client{}, err
	}
	conn, err := grpc.Dial(fmt.Sprintf("%s:%d", address, port),
		append(client.DefaultDialOptions(), grpc.WithInsecure())...)
	if err != nil {
		return Client{}, err
	}
	return newClient(conn), nil
}
func RunFixedArgs(numArgs int, run func([]string) error) func(*cobra.Command, []string) {
	return func(cmd *cobra.Command, args []string) {
		if len(args) != numArgs {
			fmt.Printf("expected %d arguments, got %d\n\n", numArgs, len(args))
			cmd.Usage()
		} else {
			if err := run(args); err != nil {
				ErrorAndExit("%v", err)
			}
		}
	}
}
func RunBoundedArgs(min int, max int, run func([]string) error) func(*cobra.Command, []string) {
	return func(cmd *cobra.Command, args []string) {
		if len(args) < min || len(args) > max {
			fmt.Printf("expected %d to %d arguments, got %d\n\n", min, max, len(args))
			cmd.Usage()
		} else {
			if err := run(args); err != nil {
				ErrorAndExit("%v", err)
			}
		}
	}
}
func Run(run func(args []string) error) func(*cobra.Command, []string) {
	return func(_ *cobra.Command, args []string) {
		if err := run(args); err != nil {
			ErrorAndExit(err.Error())
		}
	}
}
func ErrorAndExit(format string, args ...interface{}) {
	if errString := strings.TrimSpace(fmt.Sprintf(format, args...)); errString != "" {
		fmt.Fprintf(os.Stderr, "%s\n", errString)
	}
	os.Exit(1)
}
func ParseCommit(arg string) (*pfs.Commit, error) {
	parts := strings.SplitN(arg, "@", 2)
	if parts[0] == "" {
		return nil, fmt.Errorf("invalid format \"%s\": repo cannot be empty", arg)
	}
	commit := &pfs.Commit{
		Repo: &pfs.Repo{
			Name: parts[0],
		},
		ID: "",
	}
	if len(parts) == 2 {
		commit.ID = parts[1]
	}
	return commit, nil
}
func ParseBranch(arg string) (*pfs.Branch, error) {
	commit, err := ParseCommit(arg)
	if err != nil {
		return nil, err
	}
	return &pfs.Branch{Repo: commit.Repo, Name: commit.ID}, nil
}
func ParseFile(arg string) (*pfs.File, error) {
	repoAndRest := strings.SplitN(arg, "@", 2)
	if repoAndRest[0] == "" {
		return nil, fmt.Errorf("invalid format \"%s\": repo cannot be empty", arg)
	}
	file := &pfs.File{
		Commit: &pfs.Commit{
			Repo: &pfs.Repo{
				Name: repoAndRest[0],
			},
			ID: "",
		},
		Path: "",
	}
	if len(repoAndRest) > 1 {
		commitAndPath := strings.SplitN(repoAndRest[1], ":", 2)
		if commitAndPath[0] == "" {
			return nil, fmt.Errorf("invalid format \"%s\": commit cannot be empty", arg)
		}
		file.Commit.ID = commitAndPath[0]
		if len(commitAndPath) > 1 {
			file.Path = commitAndPath[1]
		}
	}
	return file, nil
}
func (r *RepeatedStringArg) Set(s string) error {
	*r = append(*r, s)
	return nil
}
func SetDocsUsage(command *cobra.Command) {
	command.SetHelpTemplate(`{{or .Long .Short}}

{{.UsageString}}
`)

	command.SetUsageFunc(func(cmd *cobra.Command) error {
		rootCmd := cmd.Root()

		// Walk the command tree, finding commands with the documented word
		var associated []*cobra.Command
		var walk func(*cobra.Command)
		walk = func(cursor *cobra.Command) {
			if cursor.Name() == cmd.Name() && cursor.CommandPath() != cmd.CommandPath() {
				associated = append(associated, cursor)
			}
			for _, subcmd := range cursor.Commands() {
				walk(subcmd)
			}
		}
		walk(rootCmd)

		var maxCommandPath int
		for _, x := range associated {
			commandPathLen := len(x.CommandPath())
			if commandPathLen > maxCommandPath {
				maxCommandPath = commandPathLen
			}
		}

		templateFuncs := template.FuncMap{
			"pad": func(s string) string {
				format := fmt.Sprintf("%%-%ds", maxCommandPath+1)
				return fmt.Sprintf(format, s)
			},
			"associated": func() []*cobra.Command {
				return associated
			},
		}

		text := `Associated Commands:{{range associated}}{{if .IsAvailableCommand}}
  {{pad .CommandPath}} {{.Short}}{{end}}{{end}}`

		t := template.New("top")
		t.Funcs(templateFuncs)
		template.Must(t.Parse(text))
		return t.Execute(cmd.Out(), cmd)
	})
}
func (a *apiServer) makeCronCommits(pachClient *client.APIClient, in *pps.Input) error {
	schedule, err := cron.ParseStandard(in.Cron.Spec)
	if err != nil {
		return err // Shouldn't happen, as the input is validated in CreatePipeline
	}
	// make sure there isn't an unfinished commit on the branch
	commitInfo, err := pachClient.InspectCommit(in.Cron.Repo, "master")
	if err != nil && !isNilBranchErr(err) {
		return err
	} else if commitInfo != nil && commitInfo.Finished == nil {
		// and if there is, delete it
		if err = pachClient.DeleteCommit(in.Cron.Repo, "master"); err != nil {
			return err
		}
	}

	var latestTime time.Time
	files, err := pachClient.ListFile(in.Cron.Repo, "master", "")
	if err != nil && !isNilBranchErr(err) {
		return err
	} else if err != nil || len(files) == 0 {
		// File not found, this happens the first time the pipeline is run
		latestTime, err = types.TimestampFromProto(in.Cron.Start)
		if err != nil {
			return err
		}
	} else {
		// Take the name of the most recent file as the latest timestamp
		// ListFile returns the files in lexicographical order, and the RFC3339 format goes
		// from largest unit of time to smallest, so the most recent file will be the last one
		latestTime, err = time.Parse(time.RFC3339, path.Base(files[len(files)-1].File.Path))
		if err != nil {
			return err
		}
	}

	for {
		// get the time of the next time from the latest time using the cron schedule
		next := schedule.Next(latestTime)
		// and wait until then to make the next commit
		time.Sleep(time.Until(next))
		if err != nil {
			return err
		}

		// We need the DeleteFile and the PutFile to happen in the same commit
		_, err = pachClient.StartCommit(in.Cron.Repo, "master")
		if err != nil {
			return err
		}
		if in.Cron.Overwrite {
			// If we want to "overwrite" the file, we need to delete the file with the previous time
			err := pachClient.DeleteFile(in.Cron.Repo, "master", latestTime.Format(time.RFC3339))
			if err != nil && !isNotFoundErr(err) && !isNilBranchErr(err) {
				return fmt.Errorf("delete error %v", err)
			}
		}

		// Put in an empty file named by the timestamp
		_, err = pachClient.PutFile(in.Cron.Repo, "master", next.Format(time.RFC3339), strings.NewReader(""))
		if err != nil {
			return fmt.Errorf("put error %v", err)
		}

		err = pachClient.FinishCommit(in.Cron.Repo, "master")
		if err != nil {
			return err
		}

		// set latestTime to the next time
		latestTime = next
	}
}
func (o *tracingObjClient) Writer(ctx context.Context, name string) (io.WriteCloser, error) {
	span, ctx := tracing.AddSpanToAnyExisting(ctx, o.provider+".Writer", "name", name)
	if span != nil {
		defer span.Finish()
	}
	return o.Client.Writer(ctx, name)
}
func (o *tracingObjClient) Reader(ctx context.Context, name string, offset uint64, size uint64) (io.ReadCloser, error) {
	span, ctx := tracing.AddSpanToAnyExisting(ctx, o.provider+".Reader",
		"name", name,
		"offset", fmt.Sprintf("%d", offset),
		"size", fmt.Sprintf("%d", size))
	defer tracing.FinishAnySpan(span)
	return o.Client.Reader(ctx, name, offset, size)
}
func (o *tracingObjClient) Delete(ctx context.Context, name string) error {
	span, ctx := tracing.AddSpanToAnyExisting(ctx, o.provider+".Delete",
		"name", name)
	defer tracing.FinishAnySpan(span)
	return o.Client.Delete(ctx, name)
}
func (o *tracingObjClient) Walk(ctx context.Context, prefix string, fn func(name string) error) error {
	span, ctx := tracing.AddSpanToAnyExisting(ctx, o.provider+".Walk",
		"prefix", prefix)
	defer tracing.FinishAnySpan(span)
	return o.Client.Walk(ctx, prefix, fn)
}
func (o *tracingObjClient) Exists(ctx context.Context, name string) bool {
	span, ctx := tracing.AddSpanToAnyExisting(ctx, o.provider+".Exists",
		"name", name)
	defer tracing.FinishAnySpan(span)
	return o.Client.Exists(ctx, name)
}
func GetBlock(hash hash.Hash) *Block {
	return &Block{
		Hash: base64.URLEncoding.EncodeToString(hash.Sum(nil)),
	}
}
func (h *healthServer) Health(context.Context, *types.Empty) (*types.Empty, error) {
	if !h.ready {
		return nil, fmt.Errorf("server not ready")
	}
	return &types.Empty{}, nil
}
func split(p string) (string, string) {
	return clean(path.Dir(p)), base(p)
}
func ValidatePath(path string) error {
	path = clean(path)
	match, _ := regexp.MatchString("^[ -~]+$", path)

	if !match {
		return fmt.Errorf("path (%v) invalid: only printable ASCII characters allowed", path)
	}

	if IsGlob(path) {
		return fmt.Errorf("path (%v) invalid: globbing character (%v) not allowed in path", path, globRegex.FindString(path))
	}

	return nil
}
func MatchDatum(filter []string, data []*pps.InputFile) bool {
	// All paths in request.DataFilters must appear somewhere in the log
	// line's inputs, or it's filtered
	matchesData := true
dataFilters:
	for _, dataFilter := range filter {
		for _, datum := range data {
			if dataFilter == datum.Path ||
				dataFilter == base64.StdEncoding.EncodeToString(datum.Hash) ||
				dataFilter == hex.EncodeToString(datum.Hash) {
				continue dataFilters // Found, move to next filter
			}
		}
		matchesData = false
		break
	}
	return matchesData
}
func NewCacheServer(router shard.Router, shards uint64) CacheServer {
	server := &groupCacheServer{
		Logger:      log.NewLogger("CacheServer"),
		router:      router,
		localShards: make(map[uint64]bool),
		shards:      shards,
	}
	groupcache.RegisterPeerPicker(func() groupcache.PeerPicker { return server })
	return server
}
func (a *apiServer) authorizePipelineOp(pachClient *client.APIClient, operation pipelineOperation, input *pps.Input, output string) error {
	ctx := pachClient.Ctx()
	me, err := pachClient.WhoAmI(ctx, &auth.WhoAmIRequest{})
	if auth.IsErrNotActivated(err) {
		return nil // Auth isn't activated, skip authorization completely
	} else if err != nil {
		return err
	}

	if input != nil {
		// Check that the user is authorized to read all input repos, and write to the
		// output repo (which the pipeline needs to be able to do on the user's
		// behalf)
		var eg errgroup.Group
		done := make(map[string]struct{}) // don't double-authorize repos
		pps.VisitInput(input, func(in *pps.Input) {
			var repo string

			if in.Pfs != nil {
				repo = in.Pfs.Repo
			} else {
				return
			}

			if _, ok := done[repo]; ok {
				return
			}
			done[repo] = struct{}{}
			eg.Go(func() error {
				resp, err := pachClient.Authorize(ctx, &auth.AuthorizeRequest{
					Repo:  repo,
					Scope: auth.Scope_READER,
				})
				if err != nil {
					return err
				}
				if !resp.Authorized {
					return &auth.ErrNotAuthorized{
						Subject:  me.Username,
						Repo:     repo,
						Required: auth.Scope_READER,
					}
				}
				return nil
			})
		})
		if err := eg.Wait(); err != nil {
			return err
		}
	}

	// Check that the user is authorized to write to the output repo.
	// Note: authorizePipelineOp is called before CreateRepo creates a
	// PipelineInfo proto in etcd, so PipelineManager won't have created an output
	// repo yet, and it's possible to check that the output repo doesn't exist
	// (if it did exist, we'd have to check that the user has permission to write
	// to it, and this is simpler)
	var required auth.Scope
	switch operation {
	case pipelineOpCreate:
		if _, err := pachClient.InspectRepo(output); err == nil {
			return fmt.Errorf("cannot overwrite repo \"%s\" with new output repo", output)
		} else if !isNotFoundErr(err) {
			return err
		}
	case pipelineOpListDatum, pipelineOpGetLogs:
		required = auth.Scope_READER
	case pipelineOpUpdate:
		required = auth.Scope_WRITER
	case pipelineOpDelete:
		required = auth.Scope_OWNER
	default:
		return fmt.Errorf("internal error, unrecognized operation %v", operation)
	}
	if required != auth.Scope_NONE {
		resp, err := pachClient.Authorize(ctx, &auth.AuthorizeRequest{
			Repo:  output,
			Scope: required,
		})
		if err != nil {
			return err
		}
		if !resp.Authorized {
			return &auth.ErrNotAuthorized{
				Subject:  me.Username,
				Repo:     output,
				Required: required,
			}
		}
	}
	return nil
}
func (a *apiServer) sudo(pachClient *client.APIClient, f func(*client.APIClient) error) error {
	// Get PPS auth token
	superUserTokenOnce.Do(func() {
		b := backoff.NewExponentialBackOff()
		b.MaxElapsedTime = 60 * time.Second
		b.MaxInterval = 5 * time.Second
		if err := backoff.Retry(func() error {
			superUserTokenCol := col.NewCollection(a.env.GetEtcdClient(), ppsconsts.PPSTokenKey, nil, &types.StringValue{}, nil, nil).ReadOnly(pachClient.Ctx())
			var result types.StringValue
			if err := superUserTokenCol.Get("", &result); err != nil {
				return err
			}
			superUserToken = result.Value
			return nil
		}, b); err != nil {
			panic(fmt.Sprintf("couldn't get PPS superuser token: %v", err))
		}
	})

	// Copy pach client, but keep ctx (to propagate cancellation). Replace token
	// with superUserToken
	superUserClient := pachClient.WithCtx(pachClient.Ctx())
	superUserClient.SetAuthToken(superUserToken)
	return f(superUserClient)
}
func setPipelineDefaults(pipelineInfo *pps.PipelineInfo) {
	now := time.Now()
	if pipelineInfo.Transform.Image == "" {
		pipelineInfo.Transform.Image = DefaultUserImage
	}
	pps.VisitInput(pipelineInfo.Input, func(input *pps.Input) {
		if input.Pfs != nil {
			if input.Pfs.Branch == "" {
				input.Pfs.Branch = "master"
			}
			if input.Pfs.Name == "" {
				input.Pfs.Name = input.Pfs.Repo
			}
		}
		if input.Cron != nil {
			if input.Cron.Start == nil {
				start, _ := types.TimestampProto(now)
				input.Cron.Start = start
			}
			if input.Cron.Repo == "" {
				input.Cron.Repo = fmt.Sprintf("%s_%s", pipelineInfo.Pipeline.Name, input.Cron.Name)
			}
		}
		if input.Git != nil {
			if input.Git.Branch == "" {
				input.Git.Branch = "master"
			}
			if input.Git.Name == "" {
				// We know URL looks like:
				// "https://github.com/sjezewski/testgithook.git",
				tokens := strings.Split(path.Base(input.Git.URL), ".")
				input.Git.Name = tokens[0]
			}
		}
	})
	if pipelineInfo.OutputBranch == "" {
		// Output branches default to master
		pipelineInfo.OutputBranch = "master"
	}
	if pipelineInfo.CacheSize == "" {
		pipelineInfo.CacheSize = "64M"
	}
	if pipelineInfo.ResourceRequests == nil && pipelineInfo.CacheSize != "" {
		pipelineInfo.ResourceRequests = &pps.ResourceSpec{
			Memory: pipelineInfo.CacheSize,
		}
	}
	if pipelineInfo.MaxQueueSize < 1 {
		pipelineInfo.MaxQueueSize = 1
	}
	if pipelineInfo.DatumTries == 0 {
		pipelineInfo.DatumTries = DefaultDatumTries
	}
}
func (a *apiServer) incrementGCGeneration(ctx context.Context) error {
	resp, err := a.env.GetEtcdClient().Get(ctx, client.GCGenerationKey)
	if err != nil {
		return err
	}

	if resp.Count == 0 {
		// If the generation number does not exist, create it.
		// It's important that the new generation is 1, as the first
		// generation is assumed to be 0.
		if _, err := a.env.GetEtcdClient().Put(ctx, client.GCGenerationKey, "1"); err != nil {
			return err
		}
	} else {
		oldGen, err := strconv.Atoi(string(resp.Kvs[0].Value))
		if err != nil {
			return err
		}
		newGen := oldGen + 1
		if _, err := a.env.GetEtcdClient().Put(ctx, client.GCGenerationKey, strconv.Itoa(newGen)); err != nil {
			return err
		}
	}
	return nil
}
func NewDebugServer(name string, etcdClient *etcd.Client, etcdPrefix string, workerGrpcPort uint16) debug.DebugServer {
	return &debugServer{
		name:           name,
		etcdClient:     etcdClient,
		etcdPrefix:     etcdPrefix,
		workerGrpcPort: workerGrpcPort,
	}
}
func (c APIClient) Health() error {
	_, err := c.healthClient.Health(c.Ctx(), &types.Empty{})
	return grpcutil.ScrubGRPC(err)
}
func newObjBlockAPIServer(dir string, cacheBytes int64, etcdAddress string, objClient obj.Client, test bool) (*objBlockAPIServer, error) {
	// defensive measure to make sure storage is working and error early if it's not
	// this is where we'll find out if the credentials have been misconfigured
	if err := obj.TestStorage(context.Background(), objClient); err != nil {
		return nil, err
	}
	oneCacheShare := cacheBytes / (objectCacheShares + tagCacheShares + objectInfoCacheShares + blockCacheShares)
	s := &objBlockAPIServer{
		Logger:           log.NewLogger("pfs.BlockAPI.Obj"),
		dir:              dir,
		objClient:        objClient,
		objectIndexes:    make(map[string]*pfsclient.ObjectIndex),
		objectCacheBytes: oneCacheShare * objectCacheShares,
	}

	objectGroupName := "object"
	tagGroupName := "tag"
	objectInfoGroupName := "objectInfo"
	blockGroupName := "block"

	if test {
		uuid := uuid.New()
		objectGroupName += uuid
		tagGroupName += uuid
		objectInfoGroupName += uuid
		blockGroupName += uuid
	}

	s.objectCache = groupcache.NewGroup(objectGroupName, oneCacheShare*objectCacheShares, groupcache.GetterFunc(s.objectGetter))
	s.tagCache = groupcache.NewGroup(tagGroupName, oneCacheShare*tagCacheShares, groupcache.GetterFunc(s.tagGetter))
	s.objectInfoCache = groupcache.NewGroup(objectInfoGroupName, oneCacheShare*objectInfoCacheShares, groupcache.GetterFunc(s.objectInfoGetter))
	s.blockCache = groupcache.NewGroup(blockGroupName, oneCacheShare*blockCacheShares, groupcache.GetterFunc(s.blockGetter))

	if !test {
		RegisterCacheStats("tag", &s.tagCache.Stats)
		RegisterCacheStats("object", &s.objectCache.Stats)
		RegisterCacheStats("object_info", &s.objectInfoCache.Stats)
	}

	go s.watchGC(etcdAddress)
	return s, nil
}
func (s *objBlockAPIServer) watchGC(etcdAddress string) {
	b := backoff.NewInfiniteBackOff()
	backoff.RetryNotify(func() error {
		etcdClient, err := etcd.New(etcd.Config{
			Endpoints:   []string{etcdAddress},
			DialOptions: client.DefaultDialOptions(),
		})
		if err != nil {
			return fmt.Errorf("error instantiating etcd client: %v", err)
		}

		watcher, err := watch.NewWatcher(context.Background(), etcdClient, "", client.GCGenerationKey, nil)
		if err != nil {
			return fmt.Errorf("error instantiating watch stream from generation number: %v", err)
		}
		defer watcher.Close()

		for {
			ev, ok := <-watcher.Watch()
			if ev.Err != nil {
				return fmt.Errorf("error from generation number watch: %v", ev.Err)
			}
			if !ok {
				return fmt.Errorf("generation number watch stream closed unexpectedly")
			}
			newGen, err := strconv.Atoi(string(ev.Value))
			if err != nil {
				return fmt.Errorf("error converting the generation number: %v", err)
			}
			s.setGeneration(newGen)
		}
	}, b, func(err error, d time.Duration) error {
		logrus.Errorf("error running GC watcher in block server: %v; retrying in %s", err, d)
		return nil
	})
}
func (s *objBlockAPIServer) splitKey(key string) string {
	gen := s.getGeneration()
	if len(key) < prefixLength {
		return fmt.Sprintf("%s.%d", key, gen)
	}
	return fmt.Sprintf("%s.%s.%d", key[:prefixLength], key[prefixLength:], gen)
}
func NewWriter(w io.Writer, header string) *Writer {
	if header[len(header)-1] != '\n' {
		panic("header must end in a new line")
	}
	tabwriter := ansiterm.NewTabWriter(w, 0, 1, 1, ' ', 0)
	tabwriter.Write([]byte(header))
	return &Writer{
		w:      tabwriter,
		lines:  1, // 1 because we just printed the header
		header: []byte(header),
	}
}
func (w *Writer) Write(buf []byte) (int, error) {
	if w.lines >= termHeight {
		if err := w.Flush(); err != nil {
			return 0, err
		}
		if _, err := w.w.Write(w.header); err != nil {
			return 0, err
		}
		w.lines++
	}
	w.lines += bytes.Count(buf, []byte{'\n'})
	return w.w.Write(buf)
}
func PrintRepoHeader(w io.Writer, printAuth bool) {
	if printAuth {
		fmt.Fprint(w, RepoAuthHeader)
		return
	}
	fmt.Fprint(w, RepoHeader)
}
func PrintRepoInfo(w io.Writer, repoInfo *pfs.RepoInfo, fullTimestamps bool) {
	fmt.Fprintf(w, "%s\t", repoInfo.Repo.Name)
	if fullTimestamps {
		fmt.Fprintf(w, "%s\t", repoInfo.Created.String())
	} else {
		fmt.Fprintf(w, "%s\t", pretty.Ago(repoInfo.Created))
	}
	fmt.Fprintf(w, "%s\t", units.BytesSize(float64(repoInfo.SizeBytes)))
	if repoInfo.AuthInfo != nil {
		fmt.Fprintf(w, "%s\t", repoInfo.AuthInfo.AccessLevel.String())
	}
	fmt.Fprintln(w)
}
func PrintDetailedRepoInfo(repoInfo *PrintableRepoInfo) error {
	template, err := template.New("RepoInfo").Funcs(funcMap).Parse(
		`Name: {{.Repo.Name}}{{if .Description}}
Description: {{.Description}}{{end}}{{if .FullTimestamps}}
Created: {{.Created}}{{else}}
Created: {{prettyAgo .Created}}{{end}}
Size of HEAD on master: {{prettySize .SizeBytes}}{{if .AuthInfo}}
Access level: {{ .AuthInfo.AccessLevel.String }}{{end}}
`)
	if err != nil {
		return err
	}
	err = template.Execute(os.Stdout, repoInfo)
	if err != nil {
		return err
	}
	return nil
}
func PrintBranch(w io.Writer, branchInfo *pfs.BranchInfo) {
	fmt.Fprintf(w, "%s\t", branchInfo.Branch.Name)
	if branchInfo.Head != nil {
		fmt.Fprintf(w, "%s\t\n", branchInfo.Head.ID)
	} else {
		fmt.Fprintf(w, "-\t\n")
	}
}
func PrintCommitInfo(w io.Writer, commitInfo *pfs.CommitInfo, fullTimestamps bool) {
	fmt.Fprintf(w, "%s\t", commitInfo.Commit.Repo.Name)
	fmt.Fprintf(w, "%s\t", commitInfo.Branch.Name)
	fmt.Fprintf(w, "%s\t", commitInfo.Commit.ID)
	if commitInfo.ParentCommit != nil {
		fmt.Fprintf(w, "%s\t", commitInfo.ParentCommit.ID)
	} else {
		fmt.Fprint(w, "<none>\t")
	}
	if fullTimestamps {
		fmt.Fprintf(w, "%s\t", commitInfo.Started.String())
	} else {
		fmt.Fprintf(w, "%s\t", pretty.Ago(commitInfo.Started))
	}
	if commitInfo.Finished != nil {
		fmt.Fprintf(w, fmt.Sprintf("%s\t", pretty.TimeDifference(commitInfo.Started, commitInfo.Finished)))
		fmt.Fprintf(w, "%s\t\n", units.BytesSize(float64(commitInfo.SizeBytes)))
	} else {
		fmt.Fprintf(w, "-\t")
		// Open commits don't have meaningful size information
		fmt.Fprintf(w, "-\t\n")
	}
}
func PrintDetailedCommitInfo(commitInfo *PrintableCommitInfo) error {
	template, err := template.New("CommitInfo").Funcs(funcMap).Parse(
		`Commit: {{.Commit.Repo.Name}}@{{.Commit.ID}}{{if .Branch}}
Original Branch: {{.Branch.Name}}{{end}}{{if .Description}}
Description: {{.Description}}{{end}}{{if .ParentCommit}}
Parent: {{.ParentCommit.ID}}{{end}}{{if .FullTimestamps}}
Started: {{.Started}}{{else}}
Started: {{prettyAgo .Started}}{{end}}{{if .Finished}}{{if .FullTimestamps}}
Finished: {{.Finished}}{{else}}
Finished: {{prettyAgo .Finished}}{{end}}{{end}}
Size: {{prettySize .SizeBytes}}{{if .Provenance}}
Provenance: {{range .Provenance}} {{.Commit.Repo.Name}}@{{.Commit.ID}} ({{.Branch.Name}}) {{end}} {{end}}
`)
	if err != nil {
		return err
	}
	err = template.Execute(os.Stdout, commitInfo)
	if err != nil {
		return err
	}
	return nil
}
func PrintFileInfo(w io.Writer, fileInfo *pfs.FileInfo, fullTimestamps bool) {
	fmt.Fprintf(w, "%s\t", fileInfo.File.Commit.ID)
	fmt.Fprintf(w, "%s\t", fileInfo.File.Path)
	if fileInfo.FileType == pfs.FileType_FILE {
		fmt.Fprint(w, "file\t")
	} else {
		fmt.Fprint(w, "dir\t")
	}
	if fileInfo.Committed == nil {
		fmt.Fprintf(w, "-\t")
	} else if fullTimestamps {
		fmt.Fprintf(w, "%s\t", fileInfo.Committed.String())
	} else {
		fmt.Fprintf(w, "%s\t", pretty.Ago(fileInfo.Committed))
	}
	fmt.Fprintf(w, "%s\t\n", units.BytesSize(float64(fileInfo.SizeBytes)))
}
func PrintDetailedFileInfo(fileInfo *pfs.FileInfo) error {
	template, err := template.New("FileInfo").Funcs(funcMap).Parse(
		`Path: {{.File.Path}}
Type: {{fileType .FileType}}
Size: {{prettySize .SizeBytes}}
Children: {{range .Children}} {{.}} {{end}}
`)
	if err != nil {
		return err
	}
	return template.Execute(os.Stdout, fileInfo)
}
func Add(s string, ancestors int) string {
	return fmt.Sprintf("%s~%d", s, ancestors)
}
func RetryNotify(operation Operation, b BackOff, notify Notify) error {
	var err error
	var next time.Duration

	b.Reset()
	for {
		if err = operation(); err == nil {
			return nil
		}

		if next = b.NextBackOff(); next == Stop {
			return err
		}

		if notify != nil {
			if err := notify(err, next); err != nil {
				return err
			}
		}

		time.Sleep(next)
	}
}
func (c *MergeCache) Get(id int64, w io.Writer, filter Filter) (retErr error) {
	r, err := c.Cache.Get(fmt.Sprint(id))
	if err != nil {
		return err
	}
	defer func() {
		if err := r.Close(); err != nil && retErr == nil {
			retErr = err
		}
	}()
	return NewWriter(w).Copy(NewReader(r, filter))
}
func (c *MergeCache) Delete(id int64) error {
	return c.Cache.Delete(fmt.Sprint(id))
}
func PrintJobInfo(w io.Writer, jobInfo *ppsclient.JobInfo, fullTimestamps bool) {
	fmt.Fprintf(w, "%s\t", jobInfo.Job.ID)
	fmt.Fprintf(w, "%s\t", jobInfo.Pipeline.Name)
	if fullTimestamps {
		fmt.Fprintf(w, "%s\t", jobInfo.Started.String())
	} else {
		fmt.Fprintf(w, "%s\t", pretty.Ago(jobInfo.Started))
	}
	if jobInfo.Finished != nil {
		fmt.Fprintf(w, "%s\t", pretty.TimeDifference(jobInfo.Started, jobInfo.Finished))
	} else {
		fmt.Fprintf(w, "-\t")
	}
	fmt.Fprintf(w, "%d\t", jobInfo.Restart)
	if jobInfo.DataRecovered != 0 {
		fmt.Fprintf(w, "%d + %d + %d / %d\t", jobInfo.DataProcessed, jobInfo.DataSkipped, jobInfo.DataRecovered, jobInfo.DataTotal)
	} else {
		fmt.Fprintf(w, "%d + %d / %d\t", jobInfo.DataProcessed, jobInfo.DataSkipped, jobInfo.DataTotal)
	}
	fmt.Fprintf(w, "%s\t", pretty.Size(jobInfo.Stats.DownloadBytes))
	fmt.Fprintf(w, "%s\t", pretty.Size(jobInfo.Stats.UploadBytes))
	if jobInfo.State == ppsclient.JobState_JOB_FAILURE {
		fmt.Fprintf(w, "%s: %s\t\n", jobState(jobInfo.State), safeTrim(jobInfo.Reason, jobReasonLen))
	} else {
		fmt.Fprintf(w, "%s\t\n", jobState(jobInfo.State))
	}
}
func PrintPipelineInfo(w io.Writer, pipelineInfo *ppsclient.PipelineInfo, fullTimestamps bool) {
	fmt.Fprintf(w, "%s\t", pipelineInfo.Pipeline.Name)
	fmt.Fprintf(w, "%s\t", ShorthandInput(pipelineInfo.Input))
	if fullTimestamps {
		fmt.Fprintf(w, "%s\t", pipelineInfo.CreatedAt.String())
	} else {
		fmt.Fprintf(w, "%s\t", pretty.Ago(pipelineInfo.CreatedAt))
	}
	fmt.Fprintf(w, "%s / %s\t\n", pipelineState(pipelineInfo.State), jobState(pipelineInfo.LastJobState))
}
func PrintWorkerStatus(w io.Writer, workerStatus *ppsclient.WorkerStatus, fullTimestamps bool) {
	fmt.Fprintf(w, "%s\t", workerStatus.WorkerID)
	fmt.Fprintf(w, "%s\t", workerStatus.JobID)
	for _, datum := range workerStatus.Data {
		fmt.Fprintf(w, datum.Path)
	}
	fmt.Fprintf(w, "\t")
	if fullTimestamps {
		fmt.Fprintf(w, "%s\t", workerStatus.Started.String())
	} else {
		fmt.Fprintf(w, "%s\t", pretty.Ago(workerStatus.Started))
	}
	fmt.Fprintf(w, "%d\t\n", workerStatus.QueueSize)
}
func PrintDetailedJobInfo(jobInfo *PrintableJobInfo) error {
	template, err := template.New("JobInfo").Funcs(funcMap).Parse(
		`ID: {{.Job.ID}} {{if .Pipeline}}
Pipeline: {{.Pipeline.Name}} {{end}} {{if .ParentJob}}
Parent: {{.ParentJob.ID}} {{end}}{{if .FullTimestamps}}
Started: {{.Started}}{{else}}
Started: {{prettyAgo .Started}} {{end}}{{if .Finished}}
Duration: {{prettyTimeDifference .Started .Finished}} {{end}}
State: {{jobState .State}}
Reason: {{.Reason}}
Processed: {{.DataProcessed}}
Failed: {{.DataFailed}}
Skipped: {{.DataSkipped}}
Recovered: {{.DataRecovered}}
Total: {{.DataTotal}}
Data Downloaded: {{prettySize .Stats.DownloadBytes}}
Data Uploaded: {{prettySize .Stats.UploadBytes}}
Download Time: {{prettyDuration .Stats.DownloadTime}}
Process Time: {{prettyDuration .Stats.ProcessTime}}
Upload Time: {{prettyDuration .Stats.UploadTime}}
Datum Timeout: {{.DatumTimeout}}
Job Timeout: {{.JobTimeout}}
Worker Status:
{{workerStatus .}}Restarts: {{.Restart}}
ParallelismSpec: {{.ParallelismSpec}}
{{ if .ResourceRequests }}ResourceRequests:
  CPU: {{ .ResourceRequests.Cpu }}
  Memory: {{ .ResourceRequests.Memory }} {{end}}
{{ if .ResourceLimits }}ResourceLimits:
  CPU: {{ .ResourceLimits.Cpu }}
  Memory: {{ .ResourceLimits.Memory }}
  {{ if .ResourceLimits.Gpu }}GPU:
    Type: {{ .ResourceLimits.Gpu.Type }}
    Number: {{ .ResourceLimits.Gpu.Number }} {{end}} {{end}}
{{ if .Service }}Service:
	{{ if .Service.InternalPort }}InternalPort: {{ .Service.InternalPort }} {{end}}
	{{ if .Service.ExternalPort }}ExternalPort: {{ .Service.ExternalPort }} {{end}} {{end}}Input:
{{jobInput .}}
Transform:
{{prettyTransform .Transform}} {{if .OutputCommit}}
Output Commit: {{.OutputCommit.ID}} {{end}} {{ if .StatsCommit }}
Stats Commit: {{.StatsCommit.ID}} {{end}} {{ if .Egress }}
Egress: {{.Egress.URL}} {{end}}
`)
	if err != nil {
		return err
	}
	err = template.Execute(os.Stdout, jobInfo)
	if err != nil {
		return err
	}
	return nil
}
func PrintDetailedPipelineInfo(pipelineInfo *PrintablePipelineInfo) error {
	template, err := template.New("PipelineInfo").Funcs(funcMap).Parse(
		`Name: {{.Pipeline.Name}}{{if .Description}}
Description: {{.Description}}{{end}}{{if .FullTimestamps }}
Created: {{.CreatedAt}}{{ else }}
Created: {{prettyAgo .CreatedAt}} {{end}}
State: {{pipelineState .State}}
Stopped: {{ .Stopped }}
Reason: {{.Reason}}
Parallelism Spec: {{.ParallelismSpec}}
{{ if .ResourceRequests }}ResourceRequests:
  CPU: {{ .ResourceRequests.Cpu }}
  Memory: {{ .ResourceRequests.Memory }} {{end}}
{{ if .ResourceLimits }}ResourceLimits:
  CPU: {{ .ResourceLimits.Cpu }}
  Memory: {{ .ResourceLimits.Memory }}
  {{ if .ResourceLimits.Gpu }}GPU:
    Type: {{ .ResourceLimits.Gpu.Type }} 
    Number: {{ .ResourceLimits.Gpu.Number }} {{end}} {{end}}
Datum Timeout: {{.DatumTimeout}}
Job Timeout: {{.JobTimeout}}
Input:
{{pipelineInput .PipelineInfo}}
{{ if .GithookURL }}Githook URL: {{.GithookURL}} {{end}}
Output Branch: {{.OutputBranch}}
Transform:
{{prettyTransform .Transform}}
{{ if .Egress }}Egress: {{.Egress.URL}} {{end}}
{{if .RecentError}} Recent Error: {{.RecentError}} {{end}}
Job Counts:
{{jobCounts .JobCounts}}
`)
	if err != nil {
		return err
	}
	err = template.Execute(os.Stdout, pipelineInfo)
	if err != nil {
		return err
	}
	return nil
}
func PrintDatumInfo(w io.Writer, datumInfo *ppsclient.DatumInfo) {
	totalTime := "-"
	if datumInfo.Stats != nil {
		totalTime = units.HumanDuration(client.GetDatumTotalTime(datumInfo.Stats))
	}
	fmt.Fprintf(w, "%s\t%s\t%s\n", datumInfo.Datum.ID, datumState(datumInfo.State), totalTime)
}
func PrintDetailedDatumInfo(w io.Writer, datumInfo *ppsclient.DatumInfo) {
	fmt.Fprintf(w, "ID\t%s\n", datumInfo.Datum.ID)
	fmt.Fprintf(w, "Job ID\t%s\n", datumInfo.Datum.Job.ID)
	fmt.Fprintf(w, "State\t%s\n", datumInfo.State)
	fmt.Fprintf(w, "Data Downloaded\t%s\n", pretty.Size(datumInfo.Stats.DownloadBytes))
	fmt.Fprintf(w, "Data Uploaded\t%s\n", pretty.Size(datumInfo.Stats.UploadBytes))

	totalTime := client.GetDatumTotalTime(datumInfo.Stats).String()
	fmt.Fprintf(w, "Total Time\t%s\n", totalTime)

	var downloadTime string
	dl, err := types.DurationFromProto(datumInfo.Stats.DownloadTime)
	if err != nil {
		downloadTime = err.Error()
	}
	downloadTime = dl.String()
	fmt.Fprintf(w, "Download Time\t%s\n", downloadTime)

	var procTime string
	proc, err := types.DurationFromProto(datumInfo.Stats.ProcessTime)
	if err != nil {
		procTime = err.Error()
	}
	procTime = proc.String()
	fmt.Fprintf(w, "Process Time\t%s\n", procTime)

	var uploadTime string
	ul, err := types.DurationFromProto(datumInfo.Stats.UploadTime)
	if err != nil {
		uploadTime = err.Error()
	}
	uploadTime = ul.String()
	fmt.Fprintf(w, "Upload Time\t%s\n", uploadTime)

	fmt.Fprintf(w, "PFS State:\n")
	tw := ansiterm.NewTabWriter(w, 10, 1, 3, ' ', 0)
	PrintFileHeader(tw)
	PrintFile(tw, datumInfo.PfsState)
	tw.Flush()
	fmt.Fprintf(w, "Inputs:\n")
	tw = ansiterm.NewTabWriter(w, 10, 1, 3, ' ', 0)
	PrintFileHeader(tw)
	for _, d := range datumInfo.Data {
		PrintFile(tw, d.File)
	}
	tw.Flush()
}
func PrintFile(w io.Writer, file *pfsclient.File) {
	fmt.Fprintf(w, "  %s\t%s\t%s\t\n", file.Commit.Repo.Name, file.Commit.ID, file.Path)
}
func ShorthandInput(input *ppsclient.Input) string {
	switch {
	case input == nil:
		return "none"
	case input.Pfs != nil:
		return fmt.Sprintf("%s:%s", input.Pfs.Repo, input.Pfs.Glob)
	case input.Cross != nil:
		var subInput []string
		for _, input := range input.Cross {
			subInput = append(subInput, ShorthandInput(input))
		}
		return "(" + strings.Join(subInput, " ⨯ ") + ")"
	case input.Union != nil:
		var subInput []string
		for _, input := range input.Union {
			subInput = append(subInput, ShorthandInput(input))
		}
		return "(" + strings.Join(subInput, " ∪ ") + ")"
	case input.Cron != nil:
		return fmt.Sprintf("%s:%s", input.Cron.Name, input.Cron.Spec)
	}
	return ""
}
func (v *vaultCredentialsProvider) Retrieve() (credentials.Value, error) {
	var emptyCreds, result credentials.Value // result

	// retrieve AWS creds from vault
	vaultSecret, err := v.vaultClient.Logical().Read(path.Join("aws", "creds", v.vaultRole))
	if err != nil {
		return emptyCreds, fmt.Errorf("could not retrieve creds from vault: %v", err)
	}
	accessKeyIface, accessKeyOk := vaultSecret.Data["access_key"]
	awsSecretIface, awsSecretOk := vaultSecret.Data["secret_key"]
	if !accessKeyOk || !awsSecretOk {
		return emptyCreds, fmt.Errorf("aws creds not present in vault response")
	}

	// Convert access key & secret in response to strings
	result.AccessKeyID, accessKeyOk = accessKeyIface.(string)
	result.SecretAccessKey, awsSecretOk = awsSecretIface.(string)
	if !accessKeyOk || !awsSecretOk {
		return emptyCreds, fmt.Errorf("aws creds in vault response were not both strings (%T and %T)", accessKeyIface, awsSecretIface)
	}

	// update the lease values in 'v', and spawn a goroutine to renew the lease
	v.updateLease(vaultSecret)
	go func() {
		for {
			// renew at half the lease duration or one day, whichever is greater
			// (lease must expire eventually)
			renewInterval := v.getLeaseDuration()
			if renewInterval.Seconds() < oneDayInSeconds {
				renewInterval = oneDayInSeconds * time.Second
			}

			// Wait until 'renewInterval' has elapsed, then renew the lease
			time.Sleep(renewInterval)
			backoff.RetryNotify(func() error {
				// every two days, renew the lease for this node's AWS credentials
				vaultSecret, err := v.vaultClient.Sys().Renew(v.leaseID, twoDaysInSeconds)
				if err != nil {
					return err
				}
				v.updateLease(vaultSecret)
				return nil
			}, backoff.NewExponentialBackOff(), func(err error, _ time.Duration) error {
				log.Errorf("could not renew vault lease: %v", err)
				return nil
			})
		}
	}()

	// Per https://www.vaultproject.io/docs/secrets/aws/index.html#usage, wait
	// until token is usable
	time.Sleep(10 * time.Second)
	return result, nil
}
func (v *vaultCredentialsProvider) IsExpired() bool {
	v.leaseMu.Lock()
	defer v.leaseMu.Unlock()
	return time.Now().After(v.leaseLastRenew.Add(v.leaseDuration))
}
func NewBranch(repoName string, branchName string) *pfs.Branch {
	return &pfs.Branch{
		Repo: NewRepo(repoName),
		Name: branchName,
	}
}
func NewCommit(repoName string, commitID string) *pfs.Commit {
	return &pfs.Commit{
		Repo: NewRepo(repoName),
		ID:   commitID,
	}
}
func NewCommitProvenance(repoName string, branchName string, commitID string) *pfs.CommitProvenance {
	return &pfs.CommitProvenance{
		Commit: NewCommit(repoName, commitID),
		Branch: NewBranch(repoName, branchName),
	}
}
func NewFile(repoName string, commitID string, path string) *pfs.File {
	return &pfs.File{
		Commit: NewCommit(repoName, commitID),
		Path:   path,
	}
}
func (c APIClient) CreateRepo(repoName string) error {
	_, err := c.PfsAPIClient.CreateRepo(
		c.Ctx(),
		&pfs.CreateRepoRequest{
			Repo: NewRepo(repoName),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) InspectRepo(repoName string) (*pfs.RepoInfo, error) {
	resp, err := c.PfsAPIClient.InspectRepo(
		c.Ctx(),
		&pfs.InspectRepoRequest{
			Repo: NewRepo(repoName),
		},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return resp, nil
}
func (c APIClient) ListRepo() ([]*pfs.RepoInfo, error) {
	request := &pfs.ListRepoRequest{}
	repoInfos, err := c.PfsAPIClient.ListRepo(
		c.Ctx(),
		request,
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return repoInfos.RepoInfo, nil
}
func (c APIClient) DeleteRepo(repoName string, force bool) error {
	_, err := c.PfsAPIClient.DeleteRepo(
		c.Ctx(),
		&pfs.DeleteRepoRequest{
			Repo:  NewRepo(repoName),
			Force: force,
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) BuildCommit(repoName string, branch string, parent string, treeObject string) (*pfs.Commit, error) {
	commit, err := c.PfsAPIClient.BuildCommit(
		c.Ctx(),
		&pfs.BuildCommitRequest{
			Parent: NewCommit(repoName, parent),
			Branch: branch,
			Tree:   &pfs.Object{Hash: treeObject},
		},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return commit, nil
}
func (c APIClient) StartCommitParent(repoName string, branch string, parentCommit string) (*pfs.Commit, error) {
	commit, err := c.PfsAPIClient.StartCommit(
		c.Ctx(),
		&pfs.StartCommitRequest{
			Parent: &pfs.Commit{
				Repo: &pfs.Repo{
					Name: repoName,
				},
				ID: parentCommit,
			},
			Branch: branch,
		},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return commit, nil
}
func (c APIClient) FinishCommit(repoName string, commitID string) error {
	_, err := c.PfsAPIClient.FinishCommit(
		c.Ctx(),
		&pfs.FinishCommitRequest{
			Commit: NewCommit(repoName, commitID),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) InspectCommit(repoName string, commitID string) (*pfs.CommitInfo, error) {
	return c.inspectCommit(repoName, commitID, pfs.CommitState_STARTED)
}
func (c APIClient) BlockCommit(repoName string, commitID string) (*pfs.CommitInfo, error) {
	return c.inspectCommit(repoName, commitID, pfs.CommitState_FINISHED)
}
func (c APIClient) ListCommit(repoName string, to string, from string, number uint64) ([]*pfs.CommitInfo, error) {
	var result []*pfs.CommitInfo
	if err := c.ListCommitF(repoName, to, from, number, func(ci *pfs.CommitInfo) error {
		result = append(result, ci)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) ListCommitF(repoName string, to string, from string, number uint64, f func(*pfs.CommitInfo) error) error {
	req := &pfs.ListCommitRequest{
		Repo:   NewRepo(repoName),
		Number: number,
	}
	if from != "" {
		req.From = NewCommit(repoName, from)
	}
	if to != "" {
		req.To = NewCommit(repoName, to)
	}
	stream, err := c.PfsAPIClient.ListCommitStream(c.Ctx(), req)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		ci, err := stream.Recv()
		if err == io.EOF {
			break
		} else if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(ci); err != nil {
			if err == errutil.ErrBreak {
				return nil
			}
			return err
		}
	}
	return nil
}
func (c APIClient) ListCommitByRepo(repoName string) ([]*pfs.CommitInfo, error) {
	return c.ListCommit(repoName, "", "", 0)
}
func (c APIClient) CreateBranch(repoName string, branch string, commit string, provenance []*pfs.Branch) error {
	var head *pfs.Commit
	if commit != "" {
		head = NewCommit(repoName, commit)
	}
	_, err := c.PfsAPIClient.CreateBranch(
		c.Ctx(),
		&pfs.CreateBranchRequest{
			Branch:     NewBranch(repoName, branch),
			Head:       head,
			Provenance: provenance,
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) InspectBranch(repoName string, branch string) (*pfs.BranchInfo, error) {
	branchInfo, err := c.PfsAPIClient.InspectBranch(
		c.Ctx(),
		&pfs.InspectBranchRequest{
			Branch: NewBranch(repoName, branch),
		},
	)
	return branchInfo, grpcutil.ScrubGRPC(err)
}
func (c APIClient) ListBranch(repoName string) ([]*pfs.BranchInfo, error) {
	branchInfos, err := c.PfsAPIClient.ListBranch(
		c.Ctx(),
		&pfs.ListBranchRequest{
			Repo: NewRepo(repoName),
		},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return branchInfos.BranchInfo, nil
}
func (c APIClient) SetBranch(repoName string, commit string, branch string) error {
	return c.CreateBranch(repoName, branch, commit, nil)
}
func (c APIClient) DeleteBranch(repoName string, branch string, force bool) error {
	_, err := c.PfsAPIClient.DeleteBranch(
		c.Ctx(),
		&pfs.DeleteBranchRequest{
			Branch: NewBranch(repoName, branch),
			Force:  force,
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) DeleteCommit(repoName string, commitID string) error {
	_, err := c.PfsAPIClient.DeleteCommit(
		c.Ctx(),
		&pfs.DeleteCommitRequest{
			Commit: NewCommit(repoName, commitID),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) FlushCommit(commits []*pfs.Commit, toRepos []*pfs.Repo) (CommitInfoIterator, error) {
	ctx, cancel := context.WithCancel(c.Ctx())
	stream, err := c.PfsAPIClient.FlushCommit(
		ctx,
		&pfs.FlushCommitRequest{
			Commits: commits,
			ToRepos: toRepos,
		},
	)
	if err != nil {
		cancel()
		return nil, grpcutil.ScrubGRPC(err)
	}
	return &commitInfoIterator{stream, cancel}, nil
}
func (c APIClient) FlushCommitF(commits []*pfs.Commit, toRepos []*pfs.Repo, f func(*pfs.CommitInfo) error) error {
	stream, err := c.PfsAPIClient.FlushCommit(
		c.Ctx(),
		&pfs.FlushCommitRequest{
			Commits: commits,
			ToRepos: toRepos,
		},
	)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		ci, err := stream.Recv()
		if err != nil {
			if err == io.EOF {
				return nil
			}
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(ci); err != nil {
			return err
		}
	}
}
func (c APIClient) FlushCommitAll(commits []*pfs.Commit, toRepos []*pfs.Repo) ([]*pfs.CommitInfo, error) {
	var result []*pfs.CommitInfo
	if err := c.FlushCommitF(commits, toRepos, func(ci *pfs.CommitInfo) error {
		result = append(result, ci)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) SubscribeCommit(repo string, branch string, from string, state pfs.CommitState) (CommitInfoIterator, error) {
	ctx, cancel := context.WithCancel(c.Ctx())
	req := &pfs.SubscribeCommitRequest{
		Repo:   NewRepo(repo),
		Branch: branch,
		State:  state,
	}
	if from != "" {
		req.From = NewCommit(repo, from)
	}
	stream, err := c.PfsAPIClient.SubscribeCommit(ctx, req)
	if err != nil {
		cancel()
		return nil, grpcutil.ScrubGRPC(err)
	}
	return &commitInfoIterator{stream, cancel}, nil
}
func (c APIClient) SubscribeCommitF(repo, branch, from string, state pfs.CommitState, f func(*pfs.CommitInfo) error) error {
	req := &pfs.SubscribeCommitRequest{
		Repo:   NewRepo(repo),
		Branch: branch,
		State:  state,
	}
	if from != "" {
		req.From = NewCommit(repo, from)
	}
	stream, err := c.PfsAPIClient.SubscribeCommit(c.Ctx(), req)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		ci, err := stream.Recv()
		if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(ci); err != nil {
			return grpcutil.ScrubGRPC(err)
		}
	}
}
func (c APIClient) PutObjectAsync(tags []*pfs.Tag) (*PutObjectWriteCloserAsync, error) {
	w, err := c.newPutObjectWriteCloserAsync(tags)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return w, nil
}
func (c APIClient) PutObject(_r io.Reader, tags ...string) (object *pfs.Object, _ int64, retErr error) {
	r := grpcutil.ReaderWrapper{_r}
	w, err := c.newPutObjectWriteCloser(tags...)
	if err != nil {
		return nil, 0, grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if err := w.Close(); err != nil && retErr == nil {
			retErr = grpcutil.ScrubGRPC(err)
		}
		if retErr == nil {
			object = w.object
		}
	}()
	buf := grpcutil.GetBuffer()
	defer grpcutil.PutBuffer(buf)
	written, err := io.CopyBuffer(w, r, buf)
	if err != nil {
		return nil, 0, grpcutil.ScrubGRPC(err)
	}
	// return value set by deferred function
	return nil, written, nil
}
func (c APIClient) PutObjectSplit(_r io.Reader) (objects []*pfs.Object, _ int64, retErr error) {
	r := grpcutil.ReaderWrapper{_r}
	w, err := c.newPutObjectSplitWriteCloser()
	if err != nil {
		return nil, 0, grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if err := w.Close(); err != nil && retErr == nil {
			retErr = grpcutil.ScrubGRPC(err)
		}
		if retErr == nil {
			objects = w.objects
		}
	}()
	buf := grpcutil.GetBuffer()
	defer grpcutil.PutBuffer(buf)
	written, err := io.CopyBuffer(w, r, buf)
	if err != nil {
		return nil, 0, grpcutil.ScrubGRPC(err)
	}
	// return value set by deferred function
	return nil, written, nil
}
func (c APIClient) GetObject(hash string, writer io.Writer) error {
	getObjectClient, err := c.ObjectAPIClient.GetObject(
		c.Ctx(),
		&pfs.Object{Hash: hash},
	)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	if err := grpcutil.WriteFromStreamingBytesClient(getObjectClient, writer); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) GetObjectReader(hash string) (io.ReadCloser, error) {
	ctx, cancel := context.WithCancel(c.Ctx())
	getObjectClient, err := c.ObjectAPIClient.GetObject(
		ctx,
		&pfs.Object{Hash: hash},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return grpcutil.NewStreamingBytesReader(getObjectClient, cancel), nil
}
func (c APIClient) GetObjects(hashes []string, offset uint64, size uint64, totalSize uint64, writer io.Writer) error {
	var objects []*pfs.Object
	for _, hash := range hashes {
		objects = append(objects, &pfs.Object{Hash: hash})
	}
	getObjectsClient, err := c.ObjectAPIClient.GetObjects(
		c.Ctx(),
		&pfs.GetObjectsRequest{
			Objects:     objects,
			OffsetBytes: offset,
			SizeBytes:   size,
			TotalSize:   totalSize,
		},
	)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	if err := grpcutil.WriteFromStreamingBytesClient(getObjectsClient, writer); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) TagObject(hash string, tags ...string) error {
	var _tags []*pfs.Tag
	for _, tag := range tags {
		_tags = append(_tags, &pfs.Tag{Name: tag})
	}
	if _, err := c.ObjectAPIClient.TagObject(
		c.Ctx(),
		&pfs.TagObjectRequest{
			Object: &pfs.Object{Hash: hash},
			Tags:   _tags,
		},
	); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) ListObject(f func(*pfs.Object) error) error {
	listObjectClient, err := c.ObjectAPIClient.ListObjects(c.Ctx(), &pfs.ListObjectsRequest{})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		object, err := listObjectClient.Recv()
		if err != nil {
			if err == io.EOF {
				return nil
			}
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(object); err != nil {
			return err
		}
	}
}
func (c APIClient) InspectObject(hash string) (*pfs.ObjectInfo, error) {
	value, err := c.ObjectAPIClient.InspectObject(
		c.Ctx(),
		&pfs.Object{Hash: hash},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return value, nil
}
func (c APIClient) GetTag(tag string, writer io.Writer) error {
	getTagClient, err := c.ObjectAPIClient.GetTag(
		c.Ctx(),
		&pfs.Tag{Name: tag},
	)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	if err := grpcutil.WriteFromStreamingBytesClient(getTagClient, writer); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) GetTagReader(tag string) (io.ReadCloser, error) {
	ctx, cancel := context.WithCancel(c.Ctx())
	getTagClient, err := c.ObjectAPIClient.GetTag(
		ctx,
		&pfs.Tag{Name: tag},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return grpcutil.NewStreamingBytesReader(getTagClient, cancel), nil
}
func (c APIClient) ListTag(f func(*pfs.ListTagsResponse) error) error {
	listTagClient, err := c.ObjectAPIClient.ListTags(c.Ctx(), &pfs.ListTagsRequest{IncludeObject: true})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		listTagResponse, err := listTagClient.Recv()
		if err != nil {
			if err == io.EOF {
				return nil
			}
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(listTagResponse); err != nil {
			return err
		}
	}
}
func (c APIClient) Compact() error {
	_, err := c.ObjectAPIClient.Compact(
		c.Ctx(),
		&types.Empty{},
	)
	return err
}
func (c APIClient) NewPutFileClient() (PutFileClient, error) {
	pfc, err := c.PfsAPIClient.PutFile(c.Ctx())
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return &putFileClient{c: pfc}, nil
}
func (c *putFileClient) PutFileOverwrite(repoName string, commitID string, path string, reader io.Reader, overwriteIndex int64) (_ int, retErr error) {
	writer, err := c.newPutFileWriteCloser(repoName, commitID, path, pfs.Delimiter_NONE, 0, 0, 0, &pfs.OverwriteIndex{Index: overwriteIndex})
	if err != nil {
		return 0, grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if err := writer.Close(); err != nil && retErr == nil {
			retErr = err
		}
	}()
	written, err := io.Copy(writer, reader)
	return int(written), grpcutil.ScrubGRPC(err)
}
func (c *putFileClient) Close() error {
	_, err := c.c.CloseAndRecv()
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) CopyFile(srcRepo, srcCommit, srcPath, dstRepo, dstCommit, dstPath string, overwrite bool) error {
	if _, err := c.PfsAPIClient.CopyFile(c.Ctx(),
		&pfs.CopyFileRequest{
			Src:       NewFile(srcRepo, srcCommit, srcPath),
			Dst:       NewFile(dstRepo, dstCommit, dstPath),
			Overwrite: overwrite,
		}); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) GetFile(repoName string, commitID string, path string, offset int64, size int64, writer io.Writer) error {
	if c.limiter != nil {
		c.limiter.Acquire()
		defer c.limiter.Release()
	}
	apiGetFileClient, err := c.getFile(repoName, commitID, path, offset, size)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	if err := grpcutil.WriteFromStreamingBytesClient(apiGetFileClient, writer); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) GetFileReader(repoName string, commitID string, path string, offset int64, size int64) (io.Reader, error) {
	apiGetFileClient, err := c.getFile(repoName, commitID, path, offset, size)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return grpcutil.NewStreamingBytesReader(apiGetFileClient, nil), nil
}
func (c APIClient) GetFileReadSeeker(repoName string, commitID string, path string) (io.ReadSeeker, error) {
	fileInfo, err := c.InspectFile(repoName, commitID, path)
	if err != nil {
		return nil, err
	}
	reader, err := c.GetFileReader(repoName, commitID, path, 0, 0)
	if err != nil {
		return nil, err
	}
	return &getFileReadSeeker{
		Reader: reader,
		file:   NewFile(repoName, commitID, path),
		offset: 0,
		size:   int64(fileInfo.SizeBytes),
		c:      c,
	}, nil
}
func (c APIClient) InspectFile(repoName string, commitID string, path string) (*pfs.FileInfo, error) {
	return c.inspectFile(repoName, commitID, path)
}
func (c APIClient) ListFile(repoName string, commitID string, path string) ([]*pfs.FileInfo, error) {
	var result []*pfs.FileInfo
	if err := c.ListFileF(repoName, commitID, path, 0, func(fi *pfs.FileInfo) error {
		result = append(result, fi)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) ListFileHistory(repoName string, commitID string, path string, history int64) ([]*pfs.FileInfo, error) {
	var result []*pfs.FileInfo
	if err := c.ListFileF(repoName, commitID, path, history, func(fi *pfs.FileInfo) error {
		result = append(result, fi)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) ListFileF(repoName string, commitID string, path string, history int64, f func(fi *pfs.FileInfo) error) error {
	fs, err := c.PfsAPIClient.ListFileStream(
		c.Ctx(),
		&pfs.ListFileRequest{
			File:    NewFile(repoName, commitID, path),
			History: history,
		},
	)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		fi, err := fs.Recv()
		if err == io.EOF {
			return nil
		} else if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(fi); err != nil {
			if err == errutil.ErrBreak {
				return nil
			}
			return err
		}
	}
}
func (c APIClient) Walk(repoName string, commitID string, path string, f WalkFn) error {
	fs, err := c.PfsAPIClient.WalkFile(
		c.Ctx(),
		&pfs.WalkFileRequest{File: NewFile(repoName, commitID, path)})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		fi, err := fs.Recv()
		if err == io.EOF {
			return nil
		} else if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(fi); err != nil {
			if err == errutil.ErrBreak {
				return nil
			}
			return err
		}
	}
}
func (c APIClient) DeleteFile(repoName string, commitID string, path string) error {
	_, err := c.PfsAPIClient.DeleteFile(
		c.Ctx(),
		&pfs.DeleteFileRequest{
			File: NewFile(repoName, commitID, path),
		},
	)
	return err
}
func (w *PutObjectWriteCloserAsync) Write(p []byte) (int, error) {
	select {
	case err := <-w.errChan:
		if err != nil {
			return 0, grpcutil.ScrubGRPC(err)
		}
	default:
		for len(w.buf)+len(p) > cap(w.buf) {
			// Write the bytes that fit into w.buf, then
			// remove those bytes from p.
			i := cap(w.buf) - len(w.buf)
			w.buf = append(w.buf, p[:i]...)
			p = p[i:]
			w.writeChan <- w.buf
			w.buf = grpcutil.GetBuffer()[:0]
		}
		w.buf = append(w.buf, p...)
	}
	return len(p), nil
}
func (w *PutObjectWriteCloserAsync) Close() error {
	w.writeChan <- w.buf
	close(w.writeChan)
	err := <-w.errChan
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	w.object, err = w.client.CloseAndRecv()
	return grpcutil.ScrubGRPC(err)
}
func PrettyPrintVersionNoAdditional(version *pb.Version) string {
	return fmt.Sprintf("%d.%d.%d", version.Major, version.Minor, version.Micro)
}
func recursiveBlockQuoteExamples(parent *cobra.Command) {
	if parent.Example != "" {
		parent.Example = fmt.Sprintf("```sh\n%s\n```", parent.Example)
	}

	for _, cmd := range parent.Commands() {
		recursiveBlockQuoteExamples(cmd)
	}
}
func errMissingField(field string) *logical.Response {
	return logical.ErrorResponse(fmt.Sprintf("missing required field '%s'", field))
}
func validateFields(req *logical.Request, data *framework.FieldData) error {
	var unknownFields []string
	for k := range req.Data {
		if _, ok := data.Schema[k]; !ok {
			unknownFields = append(unknownFields, k)
		}
	}

	if len(unknownFields) > 0 {
		return fmt.Errorf("unknown fields: %q", unknownFields)
	}

	return nil
}
func putConfig(ctx context.Context, s logical.Storage, cfg *config) error {
	entry, err := logical.StorageEntryJSON("config", cfg)
	if err != nil {
		return fmt.Errorf("%v: failed to generate storage entry", err)
	}
	if err := s.Put(ctx, entry); err != nil {
		return fmt.Errorf("%v: failed to write configuration to storage", err)
	}
	return nil
}
func getConfig(ctx context.Context, s logical.Storage) (*config, error) {
	entry, err := s.Get(ctx, "config")
	if err != nil {
		return nil, fmt.Errorf("%v: failed to get config from storage", err)
	}
	if entry == nil || len(entry.Value) == 0 {
		return nil, errors.New("no configuration in storage")
	}

	var result config
	if err := entry.DecodeJSON(&result); err != nil {
		return nil, fmt.Errorf("%v: failed to decode configuration", err)
	}

	return &result, nil
}
func Serve(
	servers ...ServerOptions,
) (retErr error) {
	for _, server := range servers {
		if server.RegisterFunc == nil {
			return ErrMustSpecifyRegisterFunc
		}
		if server.Port == 0 {
			return ErrMustSpecifyPort
		}
		opts := []grpc.ServerOption{
			grpc.MaxConcurrentStreams(math.MaxUint32),
			grpc.MaxRecvMsgSize(server.MaxMsgSize),
			grpc.MaxSendMsgSize(server.MaxMsgSize),
			grpc.KeepaliveEnforcementPolicy(keepalive.EnforcementPolicy{
				MinTime:             5 * time.Second,
				PermitWithoutStream: true,
			}),
			grpc.UnaryInterceptor(tracing.UnaryServerInterceptor()),
			grpc.StreamInterceptor(tracing.StreamServerInterceptor()),
		}
		if server.PublicPortTLSAllowed {
			// Validate environment
			certPath := path.Join(TLSVolumePath, TLSCertFile)
			keyPath := path.Join(TLSVolumePath, TLSKeyFile)
			_, certPathStatErr := os.Stat(certPath)
			_, keyPathStatErr := os.Stat(keyPath)
			if certPathStatErr != nil {
				log.Warnf("TLS disabled: could not stat public cert at %s: %v", certPath, certPathStatErr)
			}
			if keyPathStatErr != nil {
				log.Warnf("TLS disabled: could not stat private key at %s: %v", keyPath, keyPathStatErr)
			}
			if certPathStatErr == nil && keyPathStatErr == nil {
				// Read TLS cert and key
				transportCreds, err := credentials.NewServerTLSFromFile(certPath, keyPath)
				if err != nil {
					return fmt.Errorf("couldn't build transport creds: %v", err)
				}
				opts = append(opts, grpc.Creds(transportCreds))
			}
		}

		grpcServer := grpc.NewServer(opts...)
		if err := server.RegisterFunc(grpcServer); err != nil {
			return err
		}
		listener, err := net.Listen("tcp", fmt.Sprintf(":%d", server.Port))
		if err != nil {
			return err
		}
		if server.Cancel != nil {
			go func() {
				<-server.Cancel
				if err := listener.Close(); err != nil {
					fmt.Printf("listener.Close(): %v\n", err)
				}
			}()
		}
		if err := grpcServer.Serve(listener); err != nil {
			return err
		}
	}
	return nil
}
func NewPuller() *Puller {
	return &Puller{
		errCh: make(chan error, 1),
		pipes: make(map[string]bool),
	}
}
func (p *Puller) PullTree(client *pachclient.APIClient, root string, tree hashtree.HashTree, pipes bool, concurrency int) error {
	limiter := limit.New(concurrency)
	var eg errgroup.Group
	if err := tree.Walk("/", func(path string, node *hashtree.NodeProto) error {
		if node.FileNode != nil {
			path := filepath.Join(root, path)
			var hashes []string
			for _, object := range node.FileNode.Objects {
				hashes = append(hashes, object.Hash)
			}
			if pipes {
				return p.makePipe(path, func(w io.Writer) error {
					return client.GetObjects(hashes, 0, 0, uint64(node.SubtreeSize), w)
				})
			}
			limiter.Acquire()
			eg.Go(func() (retErr error) {
				defer limiter.Release()
				return p.makeFile(path, func(w io.Writer) error {
					return client.GetObjects(hashes, 0, 0, uint64(node.SubtreeSize), w)
				})
			})
		}
		return nil
	}); err != nil {
		return err
	}
	return eg.Wait()
}
func Push(client *pachclient.APIClient, root string, commit *pfs.Commit, overwrite bool) error {
	var g errgroup.Group
	if err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
		g.Go(func() (retErr error) {
			if path == root || info.IsDir() {
				return nil
			}

			f, err := os.Open(path)
			if err != nil {
				return err
			}
			defer func() {
				if err := f.Close(); err != nil && retErr == nil {
					retErr = err
				}
			}()

			relPath, err := filepath.Rel(root, path)
			if err != nil {
				return err
			}

			if overwrite {
				if err := client.DeleteFile(commit.Repo.Name, commit.ID, relPath); err != nil {
					return err
				}
			}

			_, err = client.PutFile(commit.Repo.Name, commit.ID, relPath, f)
			return err
		})
		return nil
	}); err != nil {
		return err
	}

	return g.Wait()
}
func PushObj(pachClient *pachclient.APIClient, commit *pfs.Commit, objClient obj.Client, root string) error {
	var eg errgroup.Group
	sem := make(chan struct{}, 200)
	if err := pachClient.Walk(commit.Repo.Name, commit.ID, "", func(fileInfo *pfs.FileInfo) error {
		if fileInfo.FileType != pfs.FileType_FILE {
			return nil
		}
		eg.Go(func() (retErr error) {
			sem <- struct{}{}
			defer func() { <-sem }()
			w, err := objClient.Writer(pachClient.Ctx(), filepath.Join(root, fileInfo.File.Path))
			if err != nil {
				return err
			}
			defer func() {
				if err := w.Close(); err != nil && retErr == nil {
					retErr = err
				}
			}()
			return pachClient.GetFile(commit.Repo.Name, commit.ID, fileInfo.File.Path, 0, 0, w)
		})
		return nil
	}); err != nil {
		return err
	}
	return eg.Wait()
}
func PushFile(c *pachclient.APIClient, pfc pachclient.PutFileClient, pfsFile *pfs.File, osFile io.ReadSeeker) error {
	fileInfo, err := c.InspectFile(pfsFile.Commit.Repo.Name, pfsFile.Commit.ID, pfsFile.Path)
	if err != nil && !isNotExist(err) {
		return err
	}

	var i int
	var object *pfs.Object
	if fileInfo != nil {
		for i, object = range fileInfo.Objects {
			hash := pfs.NewHash()
			if _, err := io.CopyN(hash, osFile, pfs.ChunkSize); err != nil {
				if err == io.EOF {
					break
				}
				return err
			}

			if object.Hash != pfs.EncodeHash(hash.Sum(nil)) {
				break
			}
		}
	}

	if _, err := osFile.Seek(int64(i)*pfs.ChunkSize, 0); err != nil {
		return err
	}

	_, err = pfc.PutFileOverwrite(pfsFile.Commit.Repo.Name, pfsFile.Commit.ID, pfsFile.Path, osFile, int64(i))
	return err
}
func (c APIClient) Dump(w io.Writer) error {
	goroClient, err := c.DebugClient.Dump(c.Ctx(), &debug.DumpRequest{})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return grpcutil.ScrubGRPC(grpcutil.WriteFromStreamingBytesClient(goroClient, w))
}
func (c APIClient) Profile(profile string, duration time.Duration, w io.Writer) error {
	var d *types.Duration
	if duration != 0 {
		d = types.DurationProto(duration)
	}
	profileClient, err := c.DebugClient.Profile(c.Ctx(), &debug.ProfileRequest{
		Profile:  profile,
		Duration: d,
	})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return grpcutil.ScrubGRPC(grpcutil.WriteFromStreamingBytesClient(profileClient, w))
}
func (c APIClient) Binary(w io.Writer) error {
	binaryClient, err := c.DebugClient.Binary(c.Ctx(), &debug.BinaryRequest{})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return grpcutil.ScrubGRPC(grpcutil.WriteFromStreamingBytesClient(binaryClient, w))
}
func RegisterCacheStats(cacheName string, groupCacheStats *groupcache.Stats) {
	c := &cacheStats{
		cacheName:    cacheName,
		descriptions: make(map[string]*prometheus.Desc),
		stats:        groupCacheStats,
	}
	if err := prometheus.Register(c); err != nil {
		// metrics may be redundantly registered; ignore these errors
		if _, ok := err.(prometheus.AlreadyRegisteredError); !ok {
			logrus.Infof("error registering prometheus metric: %v", err)
		}
	}
}
func (c *counter) wait(n int64) {
	c.mu.Lock()
	defer c.mu.Unlock()
	for c.n < n {
		c.cond.Wait()
	}
}
func RunWorkload(
	client *client.APIClient,
	rand *rand.Rand,
	size int,
) error {
	worker := newWorker(rand)
	for i := 0; i < size; i++ {
		if err := worker.work(client); err != nil {
			return err
		}
	}
	for _, job := range worker.startedJobs {
		jobInfo, err := client.InspectJob(job.ID, true)
		if err != nil {
			return err
		}
		if jobInfo.State != pps.JobState_JOB_SUCCESS {
			return fmt.Errorf("job %s failed", job.ID)
		}
	}
	return nil
}
func (w *worker) createRepo(c *client.APIClient) error {
	repoName := w.randString(10)
	if err := c.CreateRepo(repoName); err != nil {
		return err
	}
	w.repos = append(w.repos, &pfs.Repo{Name: repoName})

	// Start the first commit in the repo (no parent). This is critical to
	// advanceCommit(), which will try to finish a commit the first time it's
	// called, and therefore must have an open commit to finish.
	commit, err := c.StartCommit(repoName, "")
	if err != nil {
		return err
	}
	w.started = append(w.started, commit)
	return nil
}
func (w *worker) advanceCommit(c *client.APIClient) error {
	if len(w.started) >= maxStartedCommits || len(w.finished) == 0 {
		// Randomly select a commit that has been started and finish it
		if len(w.started) == 0 {
			return nil
		}
		i := w.rand.Intn(len(w.started))
		commit := w.started[i]

		// Before we finish a commit, we add a file. This assures that there
		// won't be any empty commits which will later crash jobs
		if _, err := c.PutFile(commit.Repo.Name, commit.ID, w.randString(10), w.reader()); err != nil {
			return err
		}
		if err := c.FinishCommit(commit.Repo.Name, commit.ID); err != nil {
			return err
		}
		// remove commit[i] from 'started' and add it to 'finished'
		w.started = append(w.started[:i], w.started[i+1:]...)
		w.finished = append(w.finished, commit)
	} else {
		// Start a new commmit (parented off of a commit that we've finished)
		commit := w.finished[w.rand.Intn(len(w.finished))]
		commit, err := c.StartCommitParent(commit.Repo.Name, "", commit.ID)
		if err != nil {
			return err
		}
		w.started = append(w.started, commit)
	}
	return nil
}
func RandString(r *rand.Rand, n int) string {
	b := make([]byte, n)
	for i := range b {
		b[i] = letters[r.Intn(len(letters))]
	}
	return string(b)
}
func NewReader(rand *rand.Rand, bytes int64) io.Reader {
	return &reader{
		rand:  rand,
		bytes: bytes,
	}
}
func iterDir(tx *bolt.Tx, path string, f func(k, v []byte, c *bolt.Cursor) error) error {
	node, err := get(tx, path)
	if err != nil {
		return err
	}
	if node.DirNode == nil {
		return errorf(PathConflict, "the file at \"%s\" is not a directory",
			path)
	}
	c := NewChildCursor(tx, path)
	for k, v := c.K(), c.V(); k != nil; k, v = c.Next() {
		if err := f(k, v, c.c); err != nil {
			if err == errutil.ErrBreak {
				return nil
			}
			return err
		}
	}
	return nil
}
func (h *dbHashTree) FSSize() int64 {
	rootNode, err := h.Get("/")
	if err != nil {
		return 0
	}
	return rootNode.SubtreeSize
}
func (h *dbHashTree) Diff(oldHashTree HashTree, newPath string, oldPath string, recursiveDepth int64, f func(path string, node *NodeProto, new bool) error) (retErr error) {
	// Setup a txn for each hashtree, this is a bit complicated because we don't want to make 2 read tx to the same tree, if we did then should someone start a write tx inbetween them we would have a deadlock
	old := oldHashTree.(*dbHashTree)
	if old == nil {
		return fmt.Errorf("unrecognized HashTree type")
	}
	rollback := func(tx *bolt.Tx) {
		if err := tx.Rollback(); err != nil && retErr == nil {
			retErr = err
		}
	}
	var newTx *bolt.Tx
	var oldTx *bolt.Tx
	if h == oldHashTree {
		tx, err := h.Begin(false)
		if err != nil {
			return err
		}
		newTx = tx
		oldTx = tx
		defer rollback(tx)
	} else {
		var err error
		newTx, err = h.Begin(false)
		if err != nil {
			return err
		}
		defer rollback(newTx)
		oldTx, err = old.Begin(false)
		if err != nil {
			return err
		}
		defer rollback(oldTx)
	}
	return diff(newTx, oldTx, newPath, oldPath, recursiveDepth, f)
}
func (h *dbHashTree) Serialize(_w io.Writer) error {
	w := pbutil.NewWriter(_w)
	return h.View(func(tx *bolt.Tx) error {
		for _, bucket := range buckets {
			b := tx.Bucket(b(bucket))
			if _, err := w.Write(
				&BucketHeader{
					Bucket: bucket,
				}); err != nil {
				return err
			}
			if err := b.ForEach(func(k, v []byte) error {
				if _, err := w.WriteBytes(k); err != nil {
					return err
				}
				_, err := w.WriteBytes(v)
				return err
			}); err != nil {
				return err
			}
			if _, err := w.WriteBytes(SentinelByte); err != nil {
				return err
			}
		}
		return nil
	})
}
func (h *dbHashTree) Deserialize(_r io.Reader) error {
	r := pbutil.NewReader(_r)
	hdr := &BucketHeader{}
	batchSize := 10000

	kvs := make(chan *keyValue, batchSize/10)
	// create cancellable ctx in case bolt writer encounters error
	eg, copyCtx := errgroup.WithContext(context.Background())
	eg.Go(func() error {
		var bucket []byte
		for {
			count := 0
			if err := h.Update(func(tx *bolt.Tx) error {
				if bucket != nil {
					tx.Bucket(bucket).FillPercent = 1
				}
				for kv := range kvs {
					if kv.k == nil {
						bucket = kv.v
						continue
					}
					if err := tx.Bucket(bucket).Put(kv.k, kv.v); err != nil {
						return err
					}
					count++
					if count >= batchSize {
						return nil
					}
				}
				return nil
			}); err != nil || copyCtx.Err() != nil {
				return err // may return nil if copyCtx was closed
			}
			if count <= 0 {
				return nil
			}
		}
	})
	eg.Go(func() error {
		defer close(kvs)
		for {
			hdr.Reset()
			// TODO(msteffen): don't block on Read if copyCtx() is cancelled?
			if err := r.Read(hdr); err != nil {
				if err == io.EOF {
					break
				}
				return err
			}
			bucket := b(hdr.Bucket)
			select {
			case kvs <- &keyValue{nil, bucket}:
			case <-copyCtx.Done():
				return nil
			}
			for {
				_k, err := r.ReadBytes()
				if err != nil {
					return err
				}
				if bytes.Equal(_k, SentinelByte) {
					break
				}
				// we need to make copies of k and v because the memory will be reused
				k := make([]byte, len(_k))
				copy(k, _k)
				_v, err := r.ReadBytes()
				if err != nil {
					return err
				}
				v := make([]byte, len(_v))
				copy(v, _v)
				select {
				case kvs <- &keyValue{k, v}:
				case <-copyCtx.Done():
					return nil
				}
			}
		}
		return nil
	})
	return eg.Wait()
}
func (h *dbHashTree) Copy() (HashTree, error) {
	if err := h.Hash(); err != nil {
		return nil, err
	}
	r, w := io.Pipe()
	var eg errgroup.Group
	eg.Go(func() (retErr error) {
		defer func() {
			if err := w.Close(); err != nil && retErr == nil {
				retErr = err
			}
		}()
		return h.Serialize(w)
	})
	var result HashTree
	eg.Go(func() error {
		var err error
		result, err = DeserializeDBHashTree(pathlib.Dir(h.Path()), r)
		return err
	})
	if err := eg.Wait(); err != nil {
		return nil, err
	}
	return result, nil
}
func (h *dbHashTree) Destroy() error {
	path := h.Path()
	if err := h.Close(); err != nil {
		return err
	}
	return os.Remove(path)
}
func (h *dbHashTree) PutFileOverwrite(path string, objects []*pfs.Object, overwriteIndex *pfs.OverwriteIndex, sizeDelta int64) error {
	return h.putFile(path, objects, overwriteIndex, sizeDelta, false)
}
func (h *dbHashTree) PutDirHeaderFooter(path string, header, footer *pfs.Object, headerSize, footerSize int64) error {
	path = clean(path)
	return h.Batch(func(tx *bolt.Tx) error {
		// validation: 'path' must point to directory (or nothing--may not be
		// created yet)
		node, err := get(tx, path)
		if err != nil && Code(err) != PathNotFound {
			return errorf(Internal, "could not get node at %q: %v", path, err)
		}
		if node != nil && node.nodetype() != directory {
			return errorf(PathConflict, "cannot add header to non-directory file "+
				"at %q; a file of type %s is already there", path, node.nodetype())
		}

		// Upsert directory at 'path' with 'Shared' field
		var newNode bool
		if node == nil {
			newNode = true
			node = &NodeProto{
				Name: base(path),
				DirNode: &DirectoryNodeProto{
					Shared: &Shared{},
				},
				// header/footer size are also stored in Shared (for CopyFile) but
				// adding it here makes size calculation in canonicalize() work
				SubtreeSize: headerSize + footerSize,
			}
		}

		// only write node to db if the node is new, or the header or footer
		// changed
		headerSame := (node.DirNode.Shared.Header == nil && header == nil) ||
			(node.DirNode.Shared.Header != nil && node.DirNode.Shared.Header.Hash == header.Hash)
		footerSame := (node.DirNode.Shared.Footer == nil && footer == nil) ||
			(node.DirNode.Shared.Footer != nil && node.DirNode.Shared.Footer.Hash == footer.Hash)
		if newNode || !headerSame || !footerSame {
			node.DirNode.Shared = &Shared{
				Header:     header,
				Footer:     footer,
				HeaderSize: headerSize,
				FooterSize: footerSize,
			}
			return put(tx, path, node)
		}
		return nil
	})
}
func (h *dbHashTree) PutFileHeaderFooter(path string, objects []*pfs.Object, size int64) error {
	return h.putFile(path, objects, nil, size, true)
}
func deleteDir(tx *bolt.Tx, path string) error {
	c := fs(tx).Cursor()
	prefix := append(b(path), nullByte[0])
	for k, _ := c.Seek(prefix); bytes.HasPrefix(k, prefix); k, _ = c.Next() {
		if err := c.Delete(); err != nil {
			return err
		}
	}
	return fs(tx).Delete(b(path))
}
func NewReader(r io.Reader, filter Filter) *Reader {
	return &Reader{
		pbr:    pbutil.NewReader(r),
		filter: filter,
	}
}
func (r *Reader) Read() (*MergeNode, error) {
	_k, err := r.pbr.ReadBytes()
	if err != nil {
		return nil, err
	}
	if r.filter != nil {
		for {
			if r.filter(_k) {
				break
			}
			_, err = r.pbr.ReadBytes()
			if err != nil {
				return nil, err
			}
			_k, err = r.pbr.ReadBytes()
			if err != nil {
				return nil, err
			}
		}

	}
	k := make([]byte, len(_k))
	copy(k, _k)
	_v, err := r.pbr.ReadBytes()
	if err != nil {
		return nil, err
	}
	v := make([]byte, len(_v))
	copy(v, _v)
	return &MergeNode{
		k: k,
		v: v,
	}, nil
}
func NewWriter(w io.Writer) *Writer {
	return &Writer{
		pbw: pbutil.NewWriter(w),
	}
}
func (w *Writer) Write(n *MergeNode) error {
	// Marshal node if it was merged
	if n.nodeProto != nil {
		var err error
		n.v, err = n.nodeProto.Marshal()
		if err != nil {
			return err
		}
	}
	// Get size info from root node
	if bytes.Equal(n.k, nullByte) {
		if n.nodeProto == nil {
			n.nodeProto = &NodeProto{}
			if err := n.nodeProto.Unmarshal(n.v); err != nil {
				return err
			}
		}
		w.size = uint64(n.nodeProto.SubtreeSize)
	}
	// Write index for every index size bytes
	if w.offset > uint64(len(w.idxs)+1)*IndexSize {
		w.idxs = append(w.idxs, &Index{
			K:      n.k,
			Offset: w.offset,
		})
	}
	b, err := w.pbw.WriteBytes(n.k)
	if err != nil {
		return err
	}
	w.offset += uint64(b)
	b, err = w.pbw.WriteBytes(n.v)
	if err != nil {
		return err
	}
	w.offset += uint64(b)
	return nil
}
func (w *Writer) Copy(r *Reader) error {
	for {
		n, err := r.Read()
		if err != nil {
			if err == io.EOF {
				return nil
			}
			return err
		}
		if err := w.Write(n); err != nil {
			return err
		}
	}
}
func (w *Writer) Index() ([]byte, error) {
	buf := &bytes.Buffer{}
	pbw := pbutil.NewWriter(buf)
	for _, idx := range w.idxs {
		if _, err := pbw.Write(idx); err != nil {
			return nil, err
		}
	}
	return buf.Bytes(), nil
}
func GetRangeFromIndex(r io.Reader, prefix string) (uint64, uint64, error) {
	prefix = clean(prefix)
	pbr := pbutil.NewReader(r)
	idx := &Index{}
	k := b(prefix)
	var lower, upper uint64
	iter := func(f func(int) bool) error {
		for {
			if err := pbr.Read(idx); err != nil {
				if err == io.EOF {
					break
				}
				return err
			}
			var cmp int
			if len(k) < len(idx.K) {
				cmp = bytes.Compare(k, idx.K[:len(k)])
			} else {
				cmp = bytes.Compare(k[:len(idx.K)], idx.K)
			}
			if f(cmp) {
				break
			}
		}
		return nil
	}
	low := func(cmp int) bool {
		if cmp > 0 {
			lower = idx.Offset
			return false
		} else if cmp < 0 {
			// Handles the case where a prefix fits within one range
			upper = idx.Offset
		}
		return true
	}
	up := func(cmp int) bool {
		if cmp < 0 {
			upper = idx.Offset
			return true
		}
		return false
	}
	// Find lower
	iter(low)
	// Find upper
	if upper <= 0 {
		iter(up)
	}
	// Handles the case when at the end of the indexes
	if upper <= 0 {
		return lower, 0, nil
	}
	// Return offset and size
	return lower, upper - lower, nil
}
func NewFilter(numTrees int64, tree int64) Filter {
	return func(k []byte) bool {
		if pathToTree(k, numTrees) == uint64(tree) {
			return true
		}
		return false
	}
}
func PathToTree(path string, numTrees int64) uint64 {
	path = clean(path)
	return pathToTree(b(path), numTrees)
}
func Merge(w *Writer, rs []*Reader) error {
	if len(rs) == 0 {
		return nil
	}
	mq := &mergePQ{q: make([]*nodeStream, len(rs)+1)}
	// Setup first set of nodes
	for _, r := range rs {
		if err := mq.insert(&nodeStream{r: r}); err != nil {
			return err
		}
	}
	for mq.q[1] != nil {
		// Get next nodes to merge
		ns, err := mq.next()
		if err != nil {
			return err
		}
		// Merge nodes
		n, err := merge(ns)
		if err != nil {
			return err
		}
		// Write out result
		if err := w.Write(n); err != nil {
			return err
		}
	}
	return nil
}
func (h *dbHashTree) Hash() error {
	return h.Batch(func(tx *bolt.Tx) error {
		return canonicalize(tx, "")
	})
}
func IsGlob(pattern string) bool {
	pattern = clean(pattern)
	return globRegex.Match([]byte(pattern))
}
func GlobLiteralPrefix(pattern string) string {
	pattern = clean(pattern)
	idx := globRegex.FindStringIndex(pattern)
	if idx == nil {
		return pattern
	}
	return pattern[:idx[0]]
}
func GetHashTreeObject(pachClient *client.APIClient, storageRoot string, treeRef *pfs.Object) (HashTree, error) {
	return getHashTree(storageRoot, func(w io.Writer) error {
		return pachClient.GetObject(treeRef.Hash, w)
	})
}
func GetHashTreeTag(pachClient *client.APIClient, storageRoot string, treeRef *pfs.Tag) (HashTree, error) {
	return getHashTree(storageRoot, func(w io.Writer) error {
		return pachClient.GetTag(treeRef.Name, w)
	})
}
func PutHashTree(pachClient *client.APIClient, tree HashTree, tags ...string) (*pfs.Object, error) {
	r, w := io.Pipe()
	var eg errgroup.Group
	eg.Go(func() (retErr error) {
		defer func() {
			if err := w.Close(); err != nil && retErr == nil {
				retErr = err
			}
		}()
		return tree.Serialize(w)
	})
	var treeRef *pfs.Object
	eg.Go(func() error {
		var err error
		treeRef, _, err = pachClient.PutObject(r, tags...)
		return err
	})
	if err := eg.Wait(); err != nil {
		return nil, err
	}
	return treeRef, nil
}
func NewChildCursor(tx *bolt.Tx, path string) *ChildCursor {
	path = clean(path)
	c := fs(tx).Cursor()
	dir := b(path)
	k, v := c.Seek(append(dir, nullByte[0]))
	if !bytes.Equal(dir, nullByte) {
		dir = append(dir, nullByte[0])
	}
	if !bytes.HasPrefix(k, dir) {
		k, v = nil, nil
	}
	return &ChildCursor{
		c:   c,
		dir: dir,
		k:   k,
		v:   v,
	}
}
func (d *ChildCursor) Next() ([]byte, []byte) {
	if d.k == nil {
		return nil, nil
	}
	k, v := d.c.Seek(append(d.k, 1))
	if !bytes.HasPrefix(k, d.dir) {
		k, v = nil, nil
	}
	d.k, d.v = k, v
	return k, v
}
func NewOrdered(root string) *Ordered {
	root = clean(root)
	o := &Ordered{}
	n := &node{
		path: "",
		nodeProto: &NodeProto{
			Name:    "",
			DirNode: &DirectoryNodeProto{},
		},
		hash: sha256.New(),
	}
	o.fs = append(o.fs, n)
	o.dirStack = append(o.dirStack, n)
	o.MkdirAll(root)
	o.root = root
	return o
}
func (o *Ordered) MkdirAll(path string) {
	var paths []string
	for path != "" {
		paths = append(paths, path)
		path, _ = split(path)
	}
	for i := len(paths) - 1; i >= 0; i-- {
		o.PutDir(paths[i])
	}
}
func (o *Ordered) PutDir(path string) {
	path = clean(path)
	if path == "" {
		return
	}
	nodeProto := &NodeProto{
		Name:    base(path),
		DirNode: &DirectoryNodeProto{},
	}
	o.putDir(path, nodeProto)
}
func (o *Ordered) Serialize(_w io.Writer) error {
	w := NewWriter(_w)
	// Unwind directory stack
	for len(o.dirStack) > 1 {
		child := o.dirStack[len(o.dirStack)-1]
		child.nodeProto.Hash = child.hash.Sum(nil)
		o.dirStack = o.dirStack[:len(o.dirStack)-1]
		parent := o.dirStack[len(o.dirStack)-1]
		parent.hash.Write([]byte(fmt.Sprintf("%s:%s:", child.nodeProto.Name, child.nodeProto.Hash)))
		parent.nodeProto.SubtreeSize += child.nodeProto.SubtreeSize
	}
	o.fs[0].nodeProto.Hash = o.fs[0].hash.Sum(nil)
	for _, n := range o.fs {
		if err := w.Write(&MergeNode{
			k:         b(n.path),
			nodeProto: n.nodeProto,
		}); err != nil {
			return err
		}
	}
	return nil
}
func NewUnordered(root string) *Unordered {
	return &Unordered{
		fs:   make(map[string]*NodeProto),
		root: clean(root),
	}
}
func (u *Unordered) Ordered() *Ordered {
	paths := make([]string, len(u.fs))
	i := 0
	for path := range u.fs {
		paths[i] = path
		i++
	}
	sort.Strings(paths)
	o := NewOrdered("")
	for i := 1; i < len(paths); i++ {
		path := paths[i]
		n := u.fs[path]
		if n.DirNode != nil {
			o.putDir(path, n)
		} else {
			o.putFile(path, n)
		}
	}
	return o
}
func revokeUserCredentials(ctx context.Context, pachdAddress string, userToken string, adminToken string) error {
	// Setup a single use client w the given admin token / address
	client, err := pclient.NewFromAddress(pachdAddress)
	if err != nil {
		return err
	}
	defer client.Close() // avoid leaking connections

	client = client.WithCtx(ctx)
	client.SetAuthToken(adminToken)
	_, err = client.AuthAPIClient.RevokeAuthToken(client.Ctx(), &auth.RevokeAuthTokenRequest{
		Token: userToken,
	})
	return err
}
func NewAPIServer(version *pb.Version, options APIServerOptions) pb.APIServer {
	return newAPIServer(version, options)
}
func String(v *pb.Version) string {
	return fmt.Sprintf("%d.%d.%d%s", v.Major, v.Minor, v.Micro, v.Additional)
}
func getPipelineInfo(pachClient *client.APIClient, env *serviceenv.ServiceEnv) (*pps.PipelineInfo, error) {
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()
	resp, err := env.GetEtcdClient().Get(ctx, path.Join(env.PPSEtcdPrefix, "pipelines", env.PPSPipelineName))
	if err != nil {
		return nil, err
	}
	if len(resp.Kvs) != 1 {
		return nil, fmt.Errorf("expected to find 1 pipeline (%s), got %d: %v", env.PPSPipelineName, len(resp.Kvs), resp)
	}
	var pipelinePtr pps.EtcdPipelineInfo
	if err := pipelinePtr.Unmarshal(resp.Kvs[0].Value); err != nil {
		return nil, err
	}
	pachClient.SetAuthToken(pipelinePtr.AuthToken)
	// Notice we use the SpecCommitID from our env, not from etcd. This is
	// because the value in etcd might get updated while the worker pod is
	// being created and we don't want to run the transform of one version of
	// the pipeline in the image of a different verison.
	pipelinePtr.SpecCommit.ID = env.PPSSpecCommitID
	return ppsutil.GetPipelineInfo(pachClient, &pipelinePtr, true)
}
func removeStr(ss *[]string, s string) bool {
	idx := sort.SearchStrings(*ss, s)
	if idx == len(*ss) {
		return false
	}
	copy((*ss)[idx:], (*ss)[idx+1:])
	*ss = (*ss)[:len(*ss)-1]
	return true
}
func PublicCertToPEM(cert *tls.Certificate) []byte {
	return pem.EncodeToMemory(&pem.Block{
		Type:  "CERTIFICATE",
		Bytes: cert.Certificate[0],
	})
}
func GenerateSelfSignedCert(address string, name *pkix.Name, ipAddresses ...string) (*tls.Certificate, error) {
	// Generate Subject Distinguished Name
	if name == nil {
		name = &pkix.Name{}
	}
	switch {
	case address == "" && name.CommonName == "":
		return nil, errors.New("must set either \"address\" or \"name.CommonName\"")
	case address != "" && name.CommonName == "":
		name.CommonName = address
	case address != "" && name.CommonName != "" && name.CommonName != address:
		return nil, fmt.Errorf("set address to \"%s\" but name.CommonName to \"%s\"", address, name.CommonName)
	default:
		// name.CommonName is already valid--nothing to do
	}

	// Parse IPs in ipAddresses
	parsedIPs := []net.IP{}
	for _, strIP := range ipAddresses {
		nextParsedIP := net.ParseIP(strIP)
		if nextParsedIP == nil {
			return nil, fmt.Errorf("invalid IP: %s", strIP)
		}
		parsedIPs = append(parsedIPs, nextParsedIP)
	}
	// Generate key pair. According to
	// https://security.stackexchange.com/questions/5096/rsa-vs-dsa-for-ssh-authentication-keys
	// RSA is likely to be faster and more secure in practice than DSA/ECDSA, so
	// this only generates RSA keys
	key, err := rsa.GenerateKey(rand.Reader, rsaKeySize)
	if err != nil {
		return nil, fmt.Errorf("could not generate RSA private key: %v", err)
	}

	// Generate unsigned cert
	cert := x509.Certificate{
		// the x509 spec requires every x509 cert must have a serial number that is
		// unique for the signing CA. All of the certs generated by this package
		// are self-signed, so this just starts at 1 and counts up
		SerialNumber: big.NewInt(atomic.AddInt64(&serialNumber, 1)),
		Subject:      *name,
		NotBefore:    time.Now().Add(-1 * time.Second),
		NotAfter:     time.Now().Add(validDur),

		KeyUsage: x509.KeyUsageCertSign | // can sign certs (need for self-signing)
			x509.KeyUsageKeyEncipherment | // can encrypt other keys (need for TLS in symmetric mode)
			x509.KeyUsageKeyAgreement, // can establish keys (need for TLS in Diffie-Hellman mode)
		ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth}, // can authenticate server (for TLS)

		IsCA:                  true, // must be set b/c KeyUsageCertSign is set
		BasicConstraintsValid: true, // mark "Basic Constraints" extn critical(?)
		MaxPathLenZero:        true, // must directly sign all end entity certs
		IPAddresses:           parsedIPs,
		DNSNames:              []string{address},
	}

	// Sign 'cert' (cert is both 'template' and 'parent' b/c it's self-signed)
	signedCertDER, err := x509.CreateCertificate(rand.Reader, &cert, &cert, &key.PublicKey, key)
	if err != nil {
		return nil, fmt.Errorf("could not self-sign certificate: %v", err)
	}
	signedCert, err := x509.ParseCertificate(signedCertDER)
	if err != nil {
		return nil, fmt.Errorf("could not parse the just-generated signed certificate: %v", err)
	}
	return &tls.Certificate{
		Certificate: [][]byte{signedCertDER},
		Leaf:        signedCert,
		PrivateKey:  key,
	}, nil
}
func ActivateCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	var initialAdmin string
	activate := &cobra.Command{
		Short: "Activate Pachyderm's auth system",
		Long: `
Activate Pachyderm's auth system, and restrict access to existing data to the
user running the command (or the argument to --initial-admin), who will be the
first cluster admin`[1:],
		Run: cmdutil.Run(func(args []string) error {
			var token string
			var err error
			if !strings.HasPrefix(initialAdmin, auth.RobotPrefix) {
				token, err = githubLogin()
				if err != nil {
					return err
				}
			}
			fmt.Println("Retrieving Pachyderm token...")

			// Exchange GitHub token for Pachyderm token
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			resp, err := c.Activate(c.Ctx(),
				&auth.ActivateRequest{
					GitHubToken: token,
					Subject:     initialAdmin,
				})
			if err != nil {
				return fmt.Errorf("error activating Pachyderm auth: %v",
					grpcutil.ScrubGRPC(err))
			}
			if err := writePachTokenToCfg(resp.PachToken); err != nil {
				return err
			}
			if strings.HasPrefix(initialAdmin, auth.RobotPrefix) {
				fmt.Println("WARNING: DO NOT LOSE THE ROBOT TOKEN BELOW WITHOUT " +
					"ADDING OTHER ADMINS.\nIF YOU DO, YOU WILL BE PERMANENTLY LOCKED OUT " +
					"OF YOUR CLUSTER!")
				fmt.Printf("Pachyderm token for \"%s\":\n%s\n", initialAdmin, resp.PachToken)
			}
			return nil
		}),
	}
	activate.PersistentFlags().StringVar(&initialAdmin, "initial-admin", "", `
The subject (robot user or github user) who
will be the first cluster admin; the user running 'activate' will identify as
this user once auth is active.  If you set 'initial-admin' to a robot
user, pachctl will print that robot user's Pachyderm token; this token is
effectively a root token, and if it's lost you will be locked out of your
cluster`[1:])
	return cmdutil.CreateAlias(activate, "auth activate")
}
func DeactivateCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	deactivate := &cobra.Command{
		Short: "Delete all ACLs, tokens, and admins, and deactivate Pachyderm auth",
		Long: "Deactivate Pachyderm's auth system, which will delete ALL auth " +
			"tokens, ACLs and admins, and expose all data in the cluster to any " +
			"user with cluster access. Use with caution.",
		Run: cmdutil.Run(func(args []string) error {
			fmt.Println("Are you sure you want to delete ALL auth information " +
				"(ACLs, tokens, and admins) in this cluster, and expose ALL data? yN")
			confirm, err := bufio.NewReader(os.Stdin).ReadString('\n')
			if !strings.Contains("yY", confirm[:1]) {
				return fmt.Errorf("operation aborted")
			}
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			_, err = c.Deactivate(c.Ctx(), &auth.DeactivateRequest{})
			return grpcutil.ScrubGRPC(err)
		}),
	}
	return cmdutil.CreateAlias(deactivate, "auth deactivate")
}
func LoginCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	var useOTP bool
	login := &cobra.Command{
		Short: "Log in to Pachyderm",
		Long: "Login to Pachyderm. Any resources that have been restricted to " +
			"the account you have with your ID provider (e.g. GitHub, Okta) " +
			"account will subsequently be accessible.",
		Run: cmdutil.Run(func([]string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()

			// Issue authentication request to Pachyderm and get response
			var resp *auth.AuthenticateResponse
			var authErr error
			if useOTP {
				// Exhange short-lived Pachyderm auth code for long-lived Pachyderm token
				fmt.Println("Please enter your Pachyderm One-Time Password:")
				code, err := bufio.NewReader(os.Stdin).ReadString('\n')
				if err != nil {
					return fmt.Errorf("error reading One-Time Password: %v", err)
				}
				code = strings.TrimSpace(code) // drop trailing newline
				resp, authErr = c.Authenticate(
					c.Ctx(),
					&auth.AuthenticateRequest{OneTimePassword: code})
			} else {
				// Exchange GitHub token for Pachyderm token
				token, err := githubLogin()
				if err != nil {
					return err
				}
				fmt.Println("Retrieving Pachyderm token...")
				resp, authErr = c.Authenticate(
					c.Ctx(),
					&auth.AuthenticateRequest{GitHubToken: token})
			}

			// Write new Pachyderm token to config
			if authErr != nil {
				if auth.IsErrPartiallyActivated(authErr) {
					return fmt.Errorf("%v: if pachyderm is stuck in this state, you "+
						"can revert by running 'pachctl auth deactivate' or retry by "+
						"running 'pachctl auth activate' again", authErr)
				}
				return fmt.Errorf("error authenticating with Pachyderm cluster: %v",
					grpcutil.ScrubGRPC(authErr))
			}
			return writePachTokenToCfg(resp.PachToken)
		}),
	}
	login.PersistentFlags().BoolVarP(&useOTP, "one-time-password", "o", false,
		"If set, authenticate with a Dash-provided One-Time Password, rather than "+
			"via GitHub")
	return cmdutil.CreateAlias(login, "auth login")
}
func LogoutCmd() *cobra.Command {
	logout := &cobra.Command{
		Short: "Log out of Pachyderm by deleting your local credential",
		Long: "Log out of Pachyderm by deleting your local credential. Note that " +
			"it's not necessary to log out before logging in with another account " +
			"(simply run 'pachctl auth login' twice) but 'logout' can be useful on " +
			"shared workstations.",
		Run: cmdutil.Run(func([]string) error {
			cfg, err := config.Read()
			if err != nil {
				return fmt.Errorf("error reading Pachyderm config (for cluster "+
					"address): %v", err)
			}
			if cfg.V1 == nil {
				return nil
			}
			cfg.V1.SessionToken = ""
			return cfg.Write()
		}),
	}
	return cmdutil.CreateAlias(logout, "auth logout")
}
func WhoamiCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	whoami := &cobra.Command{
		Short: "Print your Pachyderm identity",
		Long:  "Print your Pachyderm identity.",
		Run: cmdutil.Run(func([]string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			resp, err := c.WhoAmI(c.Ctx(), &auth.WhoAmIRequest{})
			if err != nil {
				return fmt.Errorf("error: %v", grpcutil.ScrubGRPC(err))
			}
			fmt.Printf("You are \"%s\"\n", resp.Username)
			if resp.TTL > 0 {
				fmt.Printf("session expires: %v\n", time.Now().Add(time.Duration(resp.TTL)*time.Second).Format(time.RFC822))
			}
			if resp.IsAdmin {
				fmt.Println("You are an administrator of this Pachyderm cluster")
			}
			return nil
		}),
	}
	return cmdutil.CreateAlias(whoami, "auth whoami")
}
func CheckCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	check := &cobra.Command{
		Use:   "{{alias}} (none|reader|writer|owner) <repo>",
		Short: "Check whether you have reader/writer/etc-level access to 'repo'",
		Long: "Check whether you have reader/writer/etc-level access to 'repo'. " +
			"For example, 'pachctl auth check reader private-data' prints \"true\" " +
			"if the you have at least \"reader\" access to the repo " +
			"\"private-data\" (you could be a reader, writer, or owner). Unlike " +
			"`pachctl auth get`, you do not need to have access to 'repo' to " +
			"discover your own access level.",
		Run: cmdutil.RunFixedArgs(2, func(args []string) error {
			scope, err := auth.ParseScope(args[0])
			if err != nil {
				return err
			}
			repo := args[1]
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			resp, err := c.Authorize(c.Ctx(), &auth.AuthorizeRequest{
				Repo:  repo,
				Scope: scope,
			})
			if err != nil {
				return grpcutil.ScrubGRPC(err)
			}
			fmt.Printf("%t\n", resp.Authorized)
			return nil
		}),
	}
	return cmdutil.CreateAlias(check, "auth check")
}
func GetCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	get := &cobra.Command{
		Use:   "{{alias}} [<username>] <repo>",
		Short: "Get the ACL for 'repo' or the access that 'username' has to 'repo'",
		Long: "Get the ACL for 'repo' or the access that 'username' has to " +
			"'repo'. For example, 'pachctl auth get github-alice private-data' " +
			"prints \"reader\", \"writer\", \"owner\", or \"none\", depending on " +
			"the privileges that \"github-alice\" has in \"repo\". Currently all " +
			"Pachyderm authentication uses GitHub OAuth, so 'username' must be a " +
			"GitHub username",
		Run: cmdutil.RunBoundedArgs(1, 2, func(args []string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			if len(args) == 1 {
				// Get ACL for a repo
				repo := args[0]
				resp, err := c.GetACL(c.Ctx(), &auth.GetACLRequest{
					Repo: repo,
				})
				if err != nil {
					return grpcutil.ScrubGRPC(err)
				}
				t := template.Must(template.New("ACLEntries").Parse(
					"{{range .}}{{.Username }}: {{.Scope}}\n{{end}}"))
				return t.Execute(os.Stdout, resp.Entries)
			}
			// Get User's scope on an acl
			username, repo := args[0], args[1]
			resp, err := c.GetScope(c.Ctx(), &auth.GetScopeRequest{
				Repos:    []string{repo},
				Username: username,
			})
			if err != nil {
				return grpcutil.ScrubGRPC(err)
			}
			fmt.Println(resp.Scopes[0].String())
			return nil
		}),
	}
	return cmdutil.CreateAlias(get, "auth get")
}
func SetScopeCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	setScope := &cobra.Command{
		Use:   "{{alias}} <username> (none|reader|writer|owner) <repo>",
		Short: "Set the scope of access that 'username' has to 'repo'",
		Long: "Set the scope of access that 'username' has to 'repo'. For " +
			"example, 'pachctl auth set github-alice none private-data' prevents " +
			"\"github-alice\" from interacting with the \"private-data\" repo in any " +
			"way (the default). Similarly, 'pachctl auth set github-alice reader " +
			"private-data' would let \"github-alice\" read from \"private-data\" but " +
			"not create commits (writer) or modify the repo's access permissions " +
			"(owner). Currently all Pachyderm authentication uses GitHub OAuth, so " +
			"'username' must be a GitHub username",
		Run: cmdutil.RunFixedArgs(3, func(args []string) error {
			scope, err := auth.ParseScope(args[1])
			if err != nil {
				return err
			}
			username, repo := args[0], args[2]
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			_, err = c.SetScope(c.Ctx(), &auth.SetScopeRequest{
				Repo:     repo,
				Scope:    scope,
				Username: username,
			})
			return grpcutil.ScrubGRPC(err)
		}),
	}
	return cmdutil.CreateAlias(setScope, "auth set")
}
func ListAdminsCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	listAdmins := &cobra.Command{
		Short: "List the current cluster admins",
		Long:  "List the current cluster admins",
		Run: cmdutil.Run(func([]string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return err
			}
			defer c.Close()
			resp, err := c.GetAdmins(c.Ctx(), &auth.GetAdminsRequest{})
			if err != nil {
				return grpcutil.ScrubGRPC(err)
			}
			for _, user := range resp.Admins {
				fmt.Println(user)
			}
			return nil
		}),
	}
	return cmdutil.CreateAlias(listAdmins, "auth list-admins")
}
func ModifyAdminsCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	var add []string
	var remove []string
	modifyAdmins := &cobra.Command{
		Short: "Modify the current cluster admins",
		Long: "Modify the current cluster admins. --add accepts a comma-" +
			"separated list of users to grant admin status, and --remove accepts a " +
			"comma-separated list of users to revoke admin status",
		Run: cmdutil.Run(func([]string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return err
			}
			defer c.Close()
			_, err = c.ModifyAdmins(c.Ctx(), &auth.ModifyAdminsRequest{
				Add:    add,
				Remove: remove,
			})
			if auth.IsErrPartiallyActivated(err) {
				return fmt.Errorf("%v: if pachyderm is stuck in this state, you "+
					"can revert by running 'pachctl auth deactivate' or retry by "+
					"running 'pachctl auth activate' again", err)
			}
			return grpcutil.ScrubGRPC(err)
		}),
	}
	modifyAdmins.PersistentFlags().StringSliceVar(&add, "add", []string{},
		"Comma-separated list of users to grant admin status")
	modifyAdmins.PersistentFlags().StringSliceVar(&remove, "remove", []string{},
		"Comma-separated list of users revoke admin status")
	return cmdutil.CreateAlias(modifyAdmins, "auth modify-admins")
}
func GetAuthTokenCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	var quiet bool
	getAuthToken := &cobra.Command{
		Use:   "{{alias}} <username>",
		Short: "Get an auth token that authenticates the holder as \"username\"",
		Long: "Get an auth token that authenticates the holder as \"username\"; " +
			"this can only be called by cluster admins",
		Run: cmdutil.RunFixedArgs(1, func(args []string) error {
			subject := args[0]
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %v", err)
			}
			defer c.Close()
			resp, err := c.GetAuthToken(c.Ctx(), &auth.GetAuthTokenRequest{
				Subject: subject,
			})
			if err != nil {
				return grpcutil.ScrubGRPC(err)
			}
			if quiet {
				fmt.Println(resp.Token)
			} else {
				fmt.Printf("New credentials:\n  Subject: %s\n  Token: %s\n", resp.Subject, resp.Token)
			}
			return nil
		}),
	}
	getAuthToken.PersistentFlags().BoolVarP(&quiet, "quiet", "q", false, "if "+
		"set, only print the resulting token (if successful). This is useful for "+
		"scripting, as the output can be piped to use-auth-token")
	return cmdutil.CreateAlias(getAuthToken, "auth get-auth-token")
}
func UseAuthTokenCmd() *cobra.Command {
	useAuthToken := &cobra.Command{
		Short: "Read a Pachyderm auth token from stdin, and write it to the " +
			"current user's Pachyderm config file",
		Long: "Read a Pachyderm auth token from stdin, and write it to the " +
			"current user's Pachyderm config file",
		Run: cmdutil.RunFixedArgs(0, func(args []string) error {
			fmt.Println("Please paste your Pachyderm auth token:")
			token, err := bufio.NewReader(os.Stdin).ReadString('\n')
			if err != nil {
				return fmt.Errorf("error reading token: %v", err)
			}
			writePachTokenToCfg(strings.TrimSpace(token)) // drop trailing newline
			return nil
		}),
	}
	return cmdutil.CreateAlias(useAuthToken, "auth use-auth-token")
}
func Cmds(noMetrics, noPortForwarding *bool) []*cobra.Command {
	var commands []*cobra.Command

	auth := &cobra.Command{
		Short: "Auth commands manage access to data in a Pachyderm cluster",
		Long:  "Auth commands manage access to data in a Pachyderm cluster",
	}

	commands = append(commands, cmdutil.CreateAlias(auth, "auth"))
	commands = append(commands, ActivateCmd(noMetrics, noPortForwarding))
	commands = append(commands, DeactivateCmd(noMetrics, noPortForwarding))
	commands = append(commands, LoginCmd(noMetrics, noPortForwarding))
	commands = append(commands, LogoutCmd())
	commands = append(commands, WhoamiCmd(noMetrics, noPortForwarding))
	commands = append(commands, CheckCmd(noMetrics, noPortForwarding))
	commands = append(commands, SetScopeCmd(noMetrics, noPortForwarding))
	commands = append(commands, GetCmd(noMetrics, noPortForwarding))
	commands = append(commands, ListAdminsCmd(noMetrics, noPortForwarding))
	commands = append(commands, ModifyAdminsCmd(noMetrics, noPortForwarding))
	commands = append(commands, GetAuthTokenCmd(noMetrics, noPortForwarding))
	commands = append(commands, UseAuthTokenCmd())
	commands = append(commands, GetConfigCmd(noPortForwarding))
	commands = append(commands, SetConfigCmd(noPortForwarding))

	return commands
}
func ParseScope(s string) (Scope, error) {
	for name, value := range Scope_value {
		if strings.EqualFold(s, name) {
			return Scope(value), nil
		}
	}
	return Scope_NONE, fmt.Errorf("unrecognized scope: %s", s)
}
func IsErrNotActivated(err error) bool {
	if err == nil {
		return false
	}
	// TODO(msteffen) This is unstructured because we have no way to propagate
	// structured errors across GRPC boundaries. Fix
	return strings.Contains(err.Error(), status.Convert(ErrNotActivated).Message())
}
func IsErrPartiallyActivated(err error) bool {
	if err == nil {
		return false
	}
	// TODO(msteffen) This is unstructured because we have no way to propagate
	// structured errors across GRPC boundaries. Fix
	return strings.Contains(err.Error(), status.Convert(ErrPartiallyActivated).Message())
}
func IsErrNotSignedIn(err error) bool {
	if err == nil {
		return false
	}
	// TODO(msteffen) This is unstructured because we have no way to propagate
	// structured errors across GRPC boundaries. Fix
	return strings.Contains(err.Error(), status.Convert(ErrNotSignedIn).Message())
}
func IsErrBadToken(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), status.Convert(ErrBadToken).Message())
}
func IsErrNotAuthorized(err error) bool {
	if err == nil {
		return false
	}
	// TODO(msteffen) This is unstructured because we have no way to propagate
	// structured errors across GRPC boundaries. Fix
	return strings.Contains(err.Error(), errNotAuthorizedMsg)
}
func IsErrInvalidPrincipal(err error) bool {
	if err == nil {
		return false
	}
	return strings.Contains(err.Error(), "invalid principal \"") &&
		strings.Contains(err.Error(), "\"; must start with one of \"pipeline:\", \"github:\", or \"robot:\", or have no \":\"")
}
func IsErrTooShortTTL(err error) bool {
	if err == nil {
		return false
	}
	errMsg := err.Error()
	return strings.Contains(errMsg, "provided TTL (") &&
		strings.Contains(errMsg, ") is shorter than token's existing TTL (") &&
		strings.Contains(errMsg, ")")
}
func NewDatumFactory(pachClient *client.APIClient, input *pps.Input) (DatumFactory, error) {
	switch {
	case input.Pfs != nil:
		return newPFSDatumFactory(pachClient, input.Pfs)
	case input.Union != nil:
		return newUnionDatumFactory(pachClient, input.Union)
	case input.Cross != nil:
		return newCrossDatumFactory(pachClient, input.Cross)
	case input.Cron != nil:
		return newCronDatumFactory(pachClient, input.Cron)
	case input.Git != nil:
		return newGitDatumFactory(pachClient, input.Git)
	}
	return nil, fmt.Errorf("unrecognized input type")
}
func NewCollection(etcdClient *etcd.Client, prefix string, indexes []*Index, template proto.Message, keyCheck func(string) error, valCheck func(proto.Message) error) Collection {
	// We want to ensure that the prefix always ends with a trailing
	// slash.  Otherwise, when you list the items under a collection
	// such as `foo`, you might end up listing items under `foobar`
	// as well.
	if len(prefix) > 0 && prefix[len(prefix)-1] != '/' {
		prefix = prefix + "/"
	}

	return &collection{
		prefix:     prefix,
		etcdClient: etcdClient,
		indexes:    indexes,
		limit:      defaultLimit,
		template:   template,
		keyCheck:   keyCheck,
		valCheck:   valCheck,
	}
}
func (c *collection) Path(key string) string {
	return path.Join(c.prefix, key)
}
func (c *readWriteCollection) getIndexPath(val interface{}, index *Index, key string) string {
	reflVal := reflect.ValueOf(val)
	field := reflect.Indirect(reflVal).FieldByName(index.Field).Interface()
	return c.indexPath(index, field, key)
}
func (c *readWriteCollection) getMultiIndexPaths(val interface{}, index *Index, key string) []string {
	var indexPaths []string
	field := reflect.Indirect(reflect.ValueOf(val)).FieldByName(index.Field)
	for i := 0; i < field.Len(); i++ {
		indexPaths = append(indexPaths, c.indexPath(index, field.Index(i).Interface(), key))
	}
	return indexPaths
}
func (c *readWriteCollection) Upsert(key string, val proto.Message, f func() error) error {
	if err := watch.CheckType(c.template, val); err != nil {
		return err
	}
	if err := c.Get(key, val); err != nil && !IsErrNotFound(err) {
		return err
	}
	if err := f(); err != nil {
		return err
	}
	return c.Put(key, val)
}
func (c *readonlyCollection) get(key string, opts ...etcd.OpOption) (*etcd.GetResponse, error) {
	span, ctx := tracing.AddSpanToAnyExisting(c.ctx, "etcd.Get")
	defer tracing.FinishAnySpan(span)
	resp, err := c.etcdClient.Get(ctx, key, opts...)
	return resp, err
}
func (c *readonlyCollection) List(val proto.Message, opts *Options, f func(string) error) error {
	if err := watch.CheckType(c.template, val); err != nil {
		return err
	}
	return c.list(c.prefix, &c.limit, opts, func(kv *mvccpb.KeyValue) error {
		if err := proto.Unmarshal(kv.Value, val); err != nil {
			return err
		}
		return f(strings.TrimPrefix(string(kv.Key), c.prefix))
	})
}
func (c *readonlyCollection) Watch(opts ...watch.OpOption) (watch.Watcher, error) {
	return watch.NewWatcher(c.ctx, c.etcdClient, c.prefix, c.prefix, c.template, opts...)
}
func (c *readonlyCollection) WatchByIndex(index *Index, val interface{}) (watch.Watcher, error) {
	eventCh := make(chan *watch.Event)
	done := make(chan struct{})
	watcher, err := watch.NewWatcher(c.ctx, c.etcdClient, c.prefix, c.indexDir(index, val), c.template)
	if err != nil {
		return nil, err
	}
	go func() (retErr error) {
		defer func() {
			if retErr != nil {
				eventCh <- &watch.Event{
					Type: watch.EventError,
					Err:  retErr,
				}
				watcher.Close()
			}
			close(eventCh)
		}()
		for {
			var ev *watch.Event
			var ok bool
			select {
			case ev, ok = <-watcher.Watch():
			case <-done:
				watcher.Close()
				return nil
			}
			if !ok {
				watcher.Close()
				return nil
			}

			var directEv *watch.Event
			switch ev.Type {
			case watch.EventError:
				// pass along the error
				return ev.Err
			case watch.EventPut:
				resp, err := c.get(c.Path(path.Base(string(ev.Key))))
				if err != nil {
					return err
				}
				if len(resp.Kvs) == 0 {
					// this happens only if the item was deleted shortly after
					// we receive this event.
					continue
				}
				directEv = &watch.Event{
					Key:      []byte(path.Base(string(ev.Key))),
					Value:    resp.Kvs[0].Value,
					Type:     ev.Type,
					Template: c.template,
				}
			case watch.EventDelete:
				directEv = &watch.Event{
					Key:      []byte(path.Base(string(ev.Key))),
					Type:     ev.Type,
					Template: c.template,
				}
			}
			eventCh <- directEv
		}
	}()
	return watch.MakeWatcher(eventCh, done), nil
}
func (c *readonlyCollection) WatchOne(key string) (watch.Watcher, error) {
	return watch.NewWatcher(c.ctx, c.etcdClient, c.prefix, c.Path(key), c.template)
}
func (c *readonlyCollection) WatchOneF(key string, f func(e *watch.Event) error) error {
	watcher, err := watch.NewWatcher(c.ctx, c.etcdClient, c.prefix, c.Path(key), c.template)
	if err != nil {
		return err
	}
	defer watcher.Close()
	for {
		select {
		case e := <-watcher.Watch():
			if err := f(e); err != nil {
				if err == errutil.ErrBreak {
					return nil
				}
				return err
			}
		case <-c.ctx.Done():
			return c.ctx.Err()
		}
	}
}
func (c *Cache) Get(key string) (io.ReadCloser, error) {
	c.mu.Lock()
	defer c.mu.Unlock()
	if !c.keys[key] {
		return nil, fmt.Errorf("key %v not found in cache", key)
	}
	f, err := os.Open(filepath.Join(c.root, key))
	if err != nil {
		return nil, err
	}
	return f, nil
}
func (c *Cache) Keys() []string {
	c.mu.Lock()
	defer c.mu.Unlock()
	var keys []string
	for key := range c.keys {
		keys = append(keys, key)
	}
	sort.Strings(keys)
	return keys
}
func (c *Cache) Clear() error {
	c.mu.Lock()
	defer c.mu.Unlock()
	defer func() {
		c.keys = make(map[string]bool)
	}()
	for key := range c.keys {
		if err := os.Remove(filepath.Join(c.root, key)); err != nil {
			return err
		}
	}
	return nil
}
func NewHTTPServer(address string) (http.Handler, error) {
	router := httprouter.New()
	s := &server{
		router:     router,
		address:    address,
		httpClient: &http.Client{},
	}

	router.GET(getFilePath, s.getFileHandler)
	router.GET(servicePath, s.serviceHandler)

	router.POST(loginPath, s.authLoginHandler)
	router.POST(logoutPath, s.authLogoutHandler)
	router.POST(servicePath, s.serviceHandler)

	router.NotFound = http.HandlerFunc(notFound)
	return s, nil
}
func NewDeployServer(kubeClient *kube.Clientset, kubeNamespace string) deploy.APIServer {
	return &apiServer{
		kubeClient:    kubeClient,
		kubeNamespace: kubeNamespace,
	}
}
func Export(opts *assets.AssetOpts, out io.Writer) error {
	client, err := docker.NewClientFromEnv()
	if err != nil {
		return err
	}
	authConfigs, err := docker.NewAuthConfigurationsFromDockerCfg()
	if err != nil {
		return fmt.Errorf("error parsing auth: %s, try running `docker login`", err.Error())
	}
	if len(authConfigs.Configs) == 0 {
		return fmt.Errorf("didn't find any valid auth configurations")
	}
	images := assets.Images(opts)
	for _, image := range images {
		repository, tag := docker.ParseRepositoryTag(image)
		pulled := false
		var loopErr []error
		for registry, authConfig := range authConfigs.Configs {
			if err := client.PullImage(
				docker.PullImageOptions{
					Repository:        repository,
					Tag:               tag,
					InactivityTimeout: 5 * time.Second,
				},
				authConfig,
			); err != nil {
				loopErr = append(loopErr, fmt.Errorf("error pulling from %s: %v", registry, err))
				continue
			}
			pulled = true
			break
		}
		if !pulled {
			errStr := ""
			for _, err := range loopErr {
				errStr += err.Error() + "\n"
			}
			return fmt.Errorf("errors pulling image %s:%s:\n%s", repository, tag, errStr)
		}
	}
	return client.ExportImages(docker.ExportImagesOptions{
		Names:        images,
		OutputStream: out,
	})
}
func Import(opts *assets.AssetOpts, in io.Reader) error {
	client, err := docker.NewClientFromEnv()
	if err != nil {
		return err
	}
	authConfigs, err := docker.NewAuthConfigurationsFromDockerCfg()
	if err != nil {
		return fmt.Errorf("error parsing auth: %s, try running `docker login`", err.Error())
	}
	if len(authConfigs.Configs) == 0 {
		return fmt.Errorf("didn't find any valid auth configurations")
	}
	if err := client.LoadImage(docker.LoadImageOptions{
		InputStream: in,
	}); err != nil {
		return err
	}
	registry := opts.Registry
	opts.Registry = "" // pretend we're using default images so we can get targets to tag
	images := assets.Images(opts)
	opts.Registry = registry
	for _, image := range images {
		repository, tag := docker.ParseRepositoryTag(image)
		registryRepo := assets.AddRegistry(opts.Registry, repository)
		if err := client.TagImage(image, docker.TagImageOptions{
			Repo: registryRepo,
			Tag:  tag,
		},
		); err != nil {
			return fmt.Errorf("error tagging image: %v", err)
		}
		pushed := false
		var loopErr []error
		for registry, authConfig := range authConfigs.Configs {
			if err := client.PushImage(
				docker.PushImageOptions{
					Name:              registryRepo,
					Tag:               tag,
					Registry:          opts.Registry,
					InactivityTimeout: 5 * time.Second,
				},
				authConfig,
			); err != nil {
				loopErr = append(loopErr, fmt.Errorf("error pushing to %s: %v", registry, err))
				continue
			}
			pushed = true
			break
		}
		if !pushed {
			errStr := ""
			for _, err := range loopErr {
				errStr += err.Error() + "\n"
			}
			return fmt.Errorf("errors pushing image %s:%s:\n%s", registryRepo, tag, errStr)
		}
	}
	return nil
}
func DatumTagPrefix(salt string) string {
	// We need to hash the salt because UUIDs are not necessarily
	// random in every bit.
	h := sha256.New()
	h.Write([]byte(salt))
	return hex.EncodeToString(h.Sum(nil))[:4]
}
func NewPFSInput(repo string, glob string) *pps.Input {
	return &pps.Input{
		Pfs: &pps.PFSInput{
			Repo: repo,
			Glob: glob,
		},
	}
}
func NewPFSInputOpts(name string, repo string, branch string, glob string, lazy bool) *pps.Input {
	return &pps.Input{
		Pfs: &pps.PFSInput{
			Name:   name,
			Repo:   repo,
			Branch: branch,
			Glob:   glob,
			Lazy:   lazy,
		},
	}
}
func NewJobInput(repoName string, commitID string, glob string) *pps.JobInput {
	return &pps.JobInput{
		Commit: NewCommit(repoName, commitID),
		Glob:   glob,
	}
}
func NewPipelineInput(repoName string, glob string) *pps.PipelineInput {
	return &pps.PipelineInput{
		Repo: NewRepo(repoName),
		Glob: glob,
	}
}
func (c APIClient) CreateJob(pipeline string, outputCommit *pfs.Commit) (*pps.Job, error) {
	job, err := c.PpsAPIClient.CreateJob(
		c.Ctx(),
		&pps.CreateJobRequest{
			Pipeline:     NewPipeline(pipeline),
			OutputCommit: outputCommit,
		},
	)
	return job, grpcutil.ScrubGRPC(err)
}
func (c APIClient) ListJob(pipelineName string, inputCommit []*pfs.Commit, outputCommit *pfs.Commit) ([]*pps.JobInfo, error) {
	var result []*pps.JobInfo
	if err := c.ListJobF(pipelineName, inputCommit, outputCommit, func(ji *pps.JobInfo) error {
		result = append(result, ji)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) ListJobF(pipelineName string, inputCommit []*pfs.Commit, outputCommit *pfs.Commit, f func(*pps.JobInfo) error) error {
	var pipeline *pps.Pipeline
	if pipelineName != "" {
		pipeline = NewPipeline(pipelineName)
	}
	client, err := c.PpsAPIClient.ListJobStream(
		c.Ctx(),
		&pps.ListJobRequest{
			Pipeline:     pipeline,
			InputCommit:  inputCommit,
			OutputCommit: outputCommit,
		})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		ji, err := client.Recv()
		if err == io.EOF {
			return nil
		} else if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(ji); err != nil {
			if err == errutil.ErrBreak {
				return nil
			}
			return err
		}
	}
}
func (c APIClient) FlushJob(commits []*pfs.Commit, toPipelines []string, f func(*pps.JobInfo) error) error {
	req := &pps.FlushJobRequest{
		Commits: commits,
	}
	for _, pipeline := range toPipelines {
		req.ToPipelines = append(req.ToPipelines, NewPipeline(pipeline))
	}
	client, err := c.PpsAPIClient.FlushJob(c.Ctx(), req)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		jobInfo, err := client.Recv()
		if err != nil {
			if err == io.EOF {
				return nil
			}
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(jobInfo); err != nil {
			return err
		}
	}
}
func (c APIClient) FlushJobAll(commits []*pfs.Commit, toPipelines []string) ([]*pps.JobInfo, error) {
	var result []*pps.JobInfo
	if err := c.FlushJob(commits, toPipelines, func(ji *pps.JobInfo) error {
		result = append(result, ji)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) DeleteJob(jobID string) error {
	_, err := c.PpsAPIClient.DeleteJob(
		c.Ctx(),
		&pps.DeleteJobRequest{
			Job: NewJob(jobID),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) StopJob(jobID string) error {
	_, err := c.PpsAPIClient.StopJob(
		c.Ctx(),
		&pps.StopJobRequest{
			Job: NewJob(jobID),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) RestartDatum(jobID string, datumFilter []string) error {
	_, err := c.PpsAPIClient.RestartDatum(
		c.Ctx(),
		&pps.RestartDatumRequest{
			Job:         NewJob(jobID),
			DataFilters: datumFilter,
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) ListDatum(jobID string, pageSize int64, page int64) (*pps.ListDatumResponse, error) {
	client, err := c.PpsAPIClient.ListDatumStream(
		c.Ctx(),
		&pps.ListDatumRequest{
			Job:      NewJob(jobID),
			PageSize: pageSize,
			Page:     page,
		},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	resp := &pps.ListDatumResponse{}
	first := true
	for {
		r, err := client.Recv()
		if err == io.EOF {
			break
		} else if err != nil {
			return nil, grpcutil.ScrubGRPC(err)
		}
		if first {
			resp.TotalPages = r.TotalPages
			resp.Page = r.Page
			first = false
		}
		resp.DatumInfos = append(resp.DatumInfos, r.DatumInfo)
	}
	return resp, nil
}
func (c APIClient) ListDatumF(jobID string, pageSize int64, page int64, f func(di *pps.DatumInfo) error) error {
	client, err := c.PpsAPIClient.ListDatumStream(
		c.Ctx(),
		&pps.ListDatumRequest{
			Job:      NewJob(jobID),
			PageSize: pageSize,
			Page:     page,
		},
	)
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		resp, err := client.Recv()
		if err == io.EOF {
			return nil
		} else if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(resp.DatumInfo); err != nil {
			if err == errutil.ErrBreak {
				return nil
			}
			return err
		}
	}
}
func (c APIClient) InspectDatum(jobID string, datumID string) (*pps.DatumInfo, error) {
	datumInfo, err := c.PpsAPIClient.InspectDatum(
		c.Ctx(),
		&pps.InspectDatumRequest{
			Datum: &pps.Datum{
				ID:  datumID,
				Job: NewJob(jobID),
			},
		},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return datumInfo, nil
}
func (l *LogsIter) Next() bool {
	if l.err != nil {
		l.msg = nil
		return false
	}
	l.msg, l.err = l.logsClient.Recv()
	if l.err != nil {
		return false
	}
	return true
}
func (c APIClient) InspectPipeline(pipelineName string) (*pps.PipelineInfo, error) {
	pipelineInfo, err := c.PpsAPIClient.InspectPipeline(
		c.Ctx(),
		&pps.InspectPipelineRequest{
			Pipeline: NewPipeline(pipelineName),
		},
	)
	return pipelineInfo, grpcutil.ScrubGRPC(err)
}
func (c APIClient) ListPipeline() ([]*pps.PipelineInfo, error) {
	pipelineInfos, err := c.PpsAPIClient.ListPipeline(
		c.Ctx(),
		&pps.ListPipelineRequest{},
	)
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return pipelineInfos.PipelineInfo, nil
}
func (c APIClient) DeletePipeline(name string, force bool) error {
	_, err := c.PpsAPIClient.DeletePipeline(
		c.Ctx(),
		&pps.DeletePipelineRequest{
			Pipeline: NewPipeline(name),
			Force:    force,
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) StartPipeline(name string) error {
	_, err := c.PpsAPIClient.StartPipeline(
		c.Ctx(),
		&pps.StartPipelineRequest{
			Pipeline: NewPipeline(name),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) StopPipeline(name string) error {
	_, err := c.PpsAPIClient.StopPipeline(
		c.Ctx(),
		&pps.StopPipelineRequest{
			Pipeline: NewPipeline(name),
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) RerunPipeline(name string, include []*pfs.Commit, exclude []*pfs.Commit) error {
	_, err := c.PpsAPIClient.RerunPipeline(
		c.Ctx(),
		&pps.RerunPipelineRequest{
			Pipeline: NewPipeline(name),
			Include:  include,
			Exclude:  exclude,
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func (c APIClient) CreatePipelineService(
	name string,
	image string,
	cmd []string,
	stdin []string,
	parallelismSpec *pps.ParallelismSpec,
	input *pps.Input,
	update bool,
	internalPort int32,
	externalPort int32,
) error {
	_, err := c.PpsAPIClient.CreatePipeline(
		c.Ctx(),
		&pps.CreatePipelineRequest{
			Pipeline: NewPipeline(name),
			Transform: &pps.Transform{
				Image: image,
				Cmd:   cmd,
				Stdin: stdin,
			},
			ParallelismSpec: parallelismSpec,
			Input:           input,
			Update:          update,
			Service: &pps.Service{
				InternalPort: internalPort,
				ExternalPort: externalPort,
			},
		},
	)
	return grpcutil.ScrubGRPC(err)
}
func GetDatumTotalTime(s *pps.ProcessStats) time.Duration {
	totalDuration := time.Duration(0)
	duration, _ := types.DurationFromProto(s.DownloadTime)
	totalDuration += duration
	duration, _ = types.DurationFromProto(s.ProcessTime)
	totalDuration += duration
	duration, _ = types.DurationFromProto(s.UploadTime)
	totalDuration += duration
	return totalDuration
}
func Mount(c *client.APIClient, mountPoint string, opts *Options) error {
	nfs := pathfs.NewPathNodeFs(newFileSystem(c, opts.getCommits()), nil)
	server, _, err := nodefs.MountRoot(mountPoint, nfs.Root(), opts.getFuse())
	if err != nil {
		return fmt.Errorf("nodefs.MountRoot: %v", err)
	}
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt)
	go func() {
		select {
		case <-sigChan:
		case <-opts.getUnmount():
		}
		server.Unmount()
	}()
	server.Serve()
	return nil
}
func NewBufPool(size int) *BufPool {
	return &BufPool{sync.Pool{
		New: func() interface{} { return make([]byte, size) },
	}}
}
func StorageRootFromEnv() (string, error) {
	storageRoot, ok := os.LookupEnv(PachRootEnvVar)
	if !ok {
		return "", fmt.Errorf("%s not found", PachRootEnvVar)
	}
	storageBackend, ok := os.LookupEnv(StorageBackendEnvVar)
	if !ok {
		return "", fmt.Errorf("%s not found", StorageBackendEnvVar)
	}
	// These storage backends do not like leading slashes
	switch storageBackend {
	case Amazon:
		fallthrough
	case Minio:
		if len(storageRoot) > 0 && storageRoot[0] == '/' {
			storageRoot = storageRoot[1:]
		}
	}
	return storageRoot, nil
}
func BlockPathFromEnv(block *pfs.Block) (string, error) {
	storageRoot, err := StorageRootFromEnv()
	if err != nil {
		return "", err
	}
	return filepath.Join(storageRoot, "block", block.Hash), nil
}
func NewGoogleClient(bucket string, opts []option.ClientOption) (Client, error) {
	return newGoogleClient(bucket, opts)
}
func NewGoogleClientFromSecret(bucket string) (Client, error) {
	var err error
	if bucket == "" {
		bucket, err = readSecretFile("/google-bucket")
		if err != nil {
			return nil, fmt.Errorf("google-bucket not found")
		}
	}
	cred, err := readSecretFile("/google-cred")
	if err != nil {
		return nil, fmt.Errorf("google-cred not found")
	}
	var opts []option.ClientOption
	if cred != "" {
		opts = append(opts, option.WithCredentialsFile(secretFile("/google-cred")))
	} else {
		opts = append(opts, option.WithTokenSource(google.ComputeTokenSource("")))
	}
	return NewGoogleClient(bucket, opts)
}
func NewGoogleClientFromEnv() (Client, error) {
	bucket, ok := os.LookupEnv(GoogleBucketEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", GoogleBucketEnvVar)
	}
	creds, ok := os.LookupEnv(GoogleCredEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", GoogleCredEnvVar)
	}
	opts := []option.ClientOption{option.WithCredentialsJSON([]byte(creds))}
	return NewGoogleClient(bucket, opts)
}
func NewMicrosoftClientFromSecret(container string) (Client, error) {
	var err error
	if container == "" {
		container, err = readSecretFile("/microsoft-container")
		if err != nil {
			return nil, fmt.Errorf("microsoft-container not found")
		}
	}
	id, err := readSecretFile("/microsoft-id")
	if err != nil {
		return nil, fmt.Errorf("microsoft-id not found")
	}
	secret, err := readSecretFile("/microsoft-secret")
	if err != nil {
		return nil, fmt.Errorf("microsoft-secret not found")
	}
	return NewMicrosoftClient(container, id, secret)
}
func NewMicrosoftClientFromEnv() (Client, error) {
	container, ok := os.LookupEnv(MicrosoftContainerEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MicrosoftContainerEnvVar)
	}
	id, ok := os.LookupEnv(MicrosoftIDEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MicrosoftIDEnvVar)
	}
	secret, ok := os.LookupEnv(MicrosoftSecretEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MicrosoftSecretEnvVar)
	}
	return NewMicrosoftClient(container, id, secret)
}
func NewMinioClientFromSecret(bucket string) (Client, error) {
	var err error
	if bucket == "" {
		bucket, err = readSecretFile("/minio-bucket")
		if err != nil {
			return nil, err
		}
	}
	endpoint, err := readSecretFile("/minio-endpoint")
	if err != nil {
		return nil, err
	}
	id, err := readSecretFile("/minio-id")
	if err != nil {
		return nil, err
	}
	secret, err := readSecretFile("/minio-secret")
	if err != nil {
		return nil, err
	}
	secure, err := readSecretFile("/minio-secure")
	if err != nil {
		return nil, err
	}
	isS3V2, err := readSecretFile("/minio-signature")
	if err != nil {
		return nil, err
	}
	return NewMinioClient(endpoint, bucket, id, secret, secure == "1", isS3V2 == "1")
}
func NewMinioClientFromEnv() (Client, error) {
	bucket, ok := os.LookupEnv(MinioBucketEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MinioBucketEnvVar)
	}
	endpoint, ok := os.LookupEnv(MinioEndpointEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MinioEndpointEnvVar)
	}
	id, ok := os.LookupEnv(MinioIDEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MinioIDEnvVar)
	}
	secret, ok := os.LookupEnv(MinioSecretEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MinioSecretEnvVar)
	}
	secure, ok := os.LookupEnv(MinioSecureEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MinioSecureEnvVar)
	}
	isS3V2, ok := os.LookupEnv(MinioSignatureEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", MinioSignatureEnvVar)
	}
	return NewMinioClient(endpoint, bucket, id, secret, secure == "1", isS3V2 == "1")
}
func NewAmazonClientFromSecret(bucket string, reversed ...bool) (Client, error) {
	// Get AWS region (required for constructing an AWS client)
	region, err := readSecretFile("/amazon-region")
	if err != nil {
		return nil, fmt.Errorf("amazon-region not found")
	}

	// Use or retrieve S3 bucket
	if bucket == "" {
		bucket, err = readSecretFile("/amazon-bucket")
		if err != nil {
			return nil, err
		}
	}

	// Retrieve either static or vault credentials; if neither are found, we will
	// use IAM roles (i.e. the EC2 metadata service)
	var creds AmazonCreds
	creds.ID, err = readSecretFile("/amazon-id")
	if err != nil && !os.IsNotExist(err) {
		return nil, err
	}
	creds.Secret, err = readSecretFile("/amazon-secret")
	if err != nil && !os.IsNotExist(err) {
		return nil, err
	}
	creds.Token, err = readSecretFile("/amazon-token")
	if err != nil && !os.IsNotExist(err) {
		return nil, err
	}
	creds.VaultAddress, err = readSecretFile("/amazon-vault-addr")
	if err != nil && !os.IsNotExist(err) {
		return nil, err
	}
	creds.VaultRole, err = readSecretFile("/amazon-vault-role")
	if err != nil && !os.IsNotExist(err) {
		return nil, err
	}
	creds.VaultToken, err = readSecretFile("/amazon-vault-token")
	if err != nil && !os.IsNotExist(err) {
		return nil, err
	}

	// Get Cloudfront distribution (not required, though we can log a warning)
	distribution, err := readSecretFile("/amazon-distribution")
	return NewAmazonClient(region, bucket, &creds, distribution, reversed...)
}
func NewAmazonClientFromEnv() (Client, error) {
	region, ok := os.LookupEnv(AmazonRegionEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", AmazonRegionEnvVar)
	}
	bucket, ok := os.LookupEnv(AmazonBucketEnvVar)
	if !ok {
		return nil, fmt.Errorf("%s not found", AmazonBucketEnvVar)
	}

	var creds AmazonCreds
	creds.ID, _ = os.LookupEnv(AmazonIDEnvVar)
	creds.Secret, _ = os.LookupEnv(AmazonSecretEnvVar)
	creds.Token, _ = os.LookupEnv(AmazonTokenEnvVar)
	creds.VaultAddress, _ = os.LookupEnv(AmazonVaultAddrEnvVar)
	creds.VaultRole, _ = os.LookupEnv(AmazonVaultRoleEnvVar)
	creds.VaultToken, _ = os.LookupEnv(AmazonVaultTokenEnvVar)

	distribution, _ := os.LookupEnv(AmazonDistributionEnvVar)
	return NewAmazonClient(region, bucket, &creds, distribution)
}
func NewClientFromURLAndSecret(url *ObjectStoreURL, reversed ...bool) (c Client, err error) {
	switch url.Store {
	case "s3":
		c, err = NewAmazonClientFromSecret(url.Bucket, reversed...)
	case "gcs":
		fallthrough
	case "gs":
		c, err = NewGoogleClientFromSecret(url.Bucket)
	case "as":
		fallthrough
	case "wasb":
		// In Azure, the first part of the path is the container name.
		c, err = NewMicrosoftClientFromSecret(url.Bucket)
	case "local":
		c, err = NewLocalClient("/" + url.Bucket)
	}
	switch {
	case err != nil:
		return nil, err
	case c != nil:
		return TracingObjClient(url.Store, c), nil
	default:
		return nil, fmt.Errorf("unrecognized object store: %s", url.Bucket)
	}
}
func ParseURL(urlStr string) (*ObjectStoreURL, error) {
	url, err := url.Parse(urlStr)
	if err != nil {
		return nil, fmt.Errorf("error parsing url %v: %v", urlStr, err)
	}
	switch url.Scheme {
	case "s3", "gcs", "gs", "local":
		return &ObjectStoreURL{
			Store:  url.Scheme,
			Bucket: url.Host,
			Object: strings.Trim(url.Path, "/"),
		}, nil
	case "as", "wasb":
		// In Azure, the first part of the path is the container name.
		parts := strings.Split(strings.Trim(url.Path, "/"), "/")
		if len(parts) < 1 {
			return nil, fmt.Errorf("malformed Azure URI: %v", urlStr)
		}
		return &ObjectStoreURL{
			Store:  url.Scheme,
			Bucket: parts[0],
			Object: strings.Trim(path.Join(parts[1:]...), "/"),
		}, nil
	}
	return nil, fmt.Errorf("unrecognized object store: %s", url.Scheme)
}
func NewClientFromEnv(storageRoot string) (c Client, err error) {
	storageBackend, ok := os.LookupEnv(StorageBackendEnvVar)
	if !ok {
		return nil, fmt.Errorf("storage backend environment variable not found")
	}
	switch storageBackend {
	case Amazon:
		c, err = NewAmazonClientFromEnv()
	case Google:
		c, err = NewGoogleClientFromEnv()
	case Microsoft:
		c, err = NewMicrosoftClientFromEnv()
	case Minio:
		c, err = NewMinioClientFromEnv()
	case Local:
		c, err = NewLocalClient(storageRoot)
	}
	switch {
	case err != nil:
		return nil, err
	case c != nil:
		return TracingObjClient(storageBackend, c), nil
	default:
		return nil, fmt.Errorf("unrecognized storage backend: %s", storageBackend)
	}
}
func NewExponentialBackOffConfig() *backoff.ExponentialBackOff {
	config := backoff.NewExponentialBackOff()
	// We want to backoff more aggressively (i.e. wait longer) than the default
	config.InitialInterval = 1 * time.Second
	config.Multiplier = 2
	config.MaxInterval = 15 * time.Minute
	return config
}
func (b *BackoffReadCloser) Close() error {
	span, _ := tracing.AddSpanToAnyExisting(b.ctx, "obj/BackoffReadCloser.Close")
	defer tracing.FinishAnySpan(span)
	return b.reader.Close()
}
func (b *BackoffWriteCloser) Close() error {
	span, _ := tracing.AddSpanToAnyExisting(b.ctx, "obj/BackoffWriteCloser.Close")
	defer tracing.FinishAnySpan(span)
	err := b.writer.Close()
	if b.client.IsIgnorable(err) {
		return nil
	}
	return err
}
func IsRetryable(client Client, err error) bool {
	return isNetRetryable(err) || client.IsRetryable(err)
}
func RunStdin(stdin io.Reader, args ...string) error {
	return RunIO(IO{Stdin: stdin}, args...)
}
func RunIODirPath(ioObj IO, dirPath string, args ...string) error {
	var debugStderr io.ReadWriter = bytes.NewBuffer(nil)
	var stderr io.Writer = debugStderr
	if ioObj.Stderr != nil {
		stderr = io.MultiWriter(debugStderr, ioObj.Stderr)
	}
	cmd := exec.Command(args[0], args[1:]...)
	cmd.Stdin = ioObj.Stdin
	cmd.Stdout = ioObj.Stdout
	cmd.Stderr = stderr
	cmd.Dir = dirPath
	if err := cmd.Run(); err != nil {
		data, _ := ioutil.ReadAll(debugStderr)
		if data != nil && len(data) > 0 {
			return fmt.Errorf("%s: %s\n%s", strings.Join(args, " "), err.Error(), string(data))
		}
		return fmt.Errorf("%s: %s", strings.Join(args, " "), err.Error())
	}
	return nil
}
func NewAuthServer(env *serviceenv.ServiceEnv, etcdPrefix string, public bool) (authclient.APIServer, error) {
	s := &apiServer{
		env:        env,
		pachLogger: log.NewLogger("authclient.API"),
		adminCache: make(map[string]struct{}),
		tokens: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, tokensPrefix),
			nil,
			&authclient.TokenInfo{},
			nil,
			nil,
		),
		authenticationCodes: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, authenticationCodesPrefix),
			nil,
			&authclient.OTPInfo{},
			nil,
			nil,
		),
		acls: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, aclsPrefix),
			nil,
			&authclient.ACL{},
			nil,
			nil,
		),
		admins: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, adminsPrefix),
			nil,
			&types.BoolValue{}, // smallest value that etcd actually stores
			nil,
			nil,
		),
		members: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, membersPrefix),
			nil,
			&authclient.Groups{},
			nil,
			nil,
		),
		groups: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, groupsPrefix),
			nil,
			&authclient.Users{},
			nil,
			nil,
		),
		authConfig: col.NewCollection(
			env.GetEtcdClient(),
			path.Join(etcdPrefix, configKey),
			nil,
			&authclient.AuthConfig{},
			nil,
			nil,
		),
		public: public,
	}
	go s.retrieveOrGeneratePPSToken()
	go s.watchAdmins(path.Join(etcdPrefix, adminsPrefix))

	if public {
		// start SAML service (won't respond to
		// anything until config is set)
		go s.serveSAML()
	}
	// Watch for new auth config options
	go s.watchConfig()
	return s, nil
}
func (a *apiServer) expiredClusterAdminCheck(ctx context.Context, username string) error {
	state, err := a.getEnterpriseTokenState()
	if err != nil {
		return fmt.Errorf("error confirming Pachyderm Enterprise token: %v", err)
	}

	isAdmin, err := a.isAdmin(ctx, username)
	if err != nil {
		return err
	}
	if state != enterpriseclient.State_ACTIVE && !isAdmin {
		return errors.New("Pachyderm Enterprise is not active in this " +
			"cluster (until Pachyderm Enterprise is re-activated or Pachyderm " +
			"auth is deactivated, only cluster admins can perform any operations)")
	}
	return nil
}
func (a *apiServer) getOneTimePassword(ctx context.Context, username string, expiration time.Time) (code string, err error) {
	// Create OTPInfo that will be stored
	otpInfo := &authclient.OTPInfo{
		Subject: username,
	}
	if !expiration.IsZero() {
		expirationProto, err := types.TimestampProto(expiration)
		if err != nil {
			return "", fmt.Errorf("could not create OTP with expiration time %s: %v",
				expiration.String(), err)
		}
		otpInfo.SessionExpiration = expirationProto
	}

	// Generate and store new OTP
	code = "auth_code:" + uuid.NewWithoutDashes()
	if _, err = col.NewSTM(ctx, a.env.GetEtcdClient(), func(stm col.STM) error {
		return a.authenticationCodes.ReadWrite(stm).PutTTL(hashToken(code),
			otpInfo, defaultAuthCodeTTLSecs)
	}); err != nil {
		return "", err
	}
	return code, nil
}
func hashToken(token string) string {
	sum := sha256.Sum256([]byte(token))
	return fmt.Sprintf("%x", sum)
}
func getAuthToken(ctx context.Context) (string, error) {
	md, ok := metadata.FromIncomingContext(ctx)
	if !ok {
		return "", authclient.ErrNoMetadata
	}
	if len(md[authclient.ContextTokenKey]) > 1 {
		return "", fmt.Errorf("multiple authentication token keys found in context")
	} else if len(md[authclient.ContextTokenKey]) == 0 {
		return "", authclient.ErrNotSignedIn
	}
	return md[authclient.ContextTokenKey][0], nil
}
func (a *apiServer) canonicalizeSubjects(ctx context.Context, subjects []string) ([]string, error) {
	if subjects == nil {
		return []string{}, nil
	}

	eg := &errgroup.Group{}
	canonicalizedSubjects := make([]string, len(subjects))
	for i, subject := range subjects {
		i, subject := i, subject
		eg.Go(func() error {
			subject, err := a.canonicalizeSubject(ctx, subject)
			if err != nil {
				return err
			}
			canonicalizedSubjects[i] = subject
			return nil
		})
	}
	if err := eg.Wait(); err != nil {
		return nil, err
	}

	return canonicalizedSubjects, nil
}
func Matches(tb testing.TB, expectedMatch string, actual string, msgAndArgs ...interface{}) {
	tb.Helper()
	r, err := regexp.Compile(expectedMatch)
	if err != nil {
		fatal(tb, msgAndArgs, "Match string provided (%v) is invalid", expectedMatch)
	}
	if !r.MatchString(actual) {
		fatal(tb, msgAndArgs, "Actual string (%v) does not match pattern (%v)", actual, expectedMatch)
	}
}
func OneOfMatches(tb testing.TB, expectedMatch string, actuals []string, msgAndArgs ...interface{}) {
	tb.Helper()
	r, err := regexp.Compile(expectedMatch)
	if err != nil {
		fatal(tb, msgAndArgs, "Match string provided (%v) is invalid", expectedMatch)
	}
	for _, actual := range actuals {
		if r.MatchString(actual) {
			return
		}
	}
	fatal(tb, msgAndArgs, "None of actual strings (%v) match pattern (%v)", actuals, expectedMatch)

}
func Equal(tb testing.TB, expected interface{}, actual interface{}, msgAndArgs ...interface{}) {
	tb.Helper()
	eV, aV := reflect.ValueOf(expected), reflect.ValueOf(actual)
	if eV.Type() != aV.Type() {
		fatal(
			tb,
			msgAndArgs,
			"Not equal: %T(%#v) (expected)\n"+
				"        != %T(%#v) (actual)", expected, expected, actual, actual)
	}
	if !reflect.DeepEqual(expected, actual) {
		fatal(
			tb,
			msgAndArgs,
			"Not equal: %#v (expected)\n"+
				"        != %#v (actual)", expected, actual)
	}
}
func NotEqual(tb testing.TB, expected interface{}, actual interface{}, msgAndArgs ...interface{}) {
	tb.Helper()
	if reflect.DeepEqual(expected, actual) {
		fatal(
			tb,
			msgAndArgs,
			"Equal: %#v (expected)\n"+
				"    == %#v (actual)", expected, actual)
	}
}
func oneOfEquals(sliceName string, slice interface{}, elem interface{}) (bool, error) {
	e := reflect.ValueOf(elem)
	sl := reflect.ValueOf(slice)
	if slice == nil || sl.IsNil() {
		sl = reflect.MakeSlice(reflect.SliceOf(e.Type()), 0, 0)
	}
	if sl.Kind() != reflect.Slice {
		return false, fmt.Errorf("\"%s\" must a be a slice, but instead was %s", sliceName, sl.Type().String())
	}
	if e.Type() != sl.Type().Elem() {
		return false, nil
	}
	arePtrs := e.Kind() == reflect.Ptr
	for i := 0; i < sl.Len(); i++ {
		if !arePtrs && reflect.DeepEqual(e.Interface(), sl.Index(i).Interface()) {
			return true, nil
		} else if arePtrs && reflect.DeepEqual(e.Elem().Interface(), sl.Index(i).Elem().Interface()) {
			return true, nil
		}
	}
	return false, nil
}
func NoneEquals(tb testing.TB, expected interface{}, actuals interface{}, msgAndArgs ...interface{}) {
	tb.Helper()
	equal, err := oneOfEquals("actuals", actuals, expected)
	if err != nil {
		fatal(tb, msgAndArgs, err.Error())
	}
	if equal {
		fatal(tb, msgAndArgs,
			"Equal : %#v (expected)\n == one of %#v (actuals)", expected, actuals)
	}
}
func NoError(tb testing.TB, err error, msgAndArgs ...interface{}) {
	tb.Helper()
	if err != nil {
		fatal(tb, msgAndArgs, "No error is expected but got %s", err.Error())
	}
}
func NoErrorWithinT(tb testing.TB, t time.Duration, f func() error, msgAndArgs ...interface{}) {
	tb.Helper()
	errCh := make(chan error)
	go func() {
		// This goro will leak if the timeout is exceeded, but it's okay because the
		// test is failing anyway
		errCh <- f()
	}()
	select {
	case err := <-errCh:
		if err != nil {
			fatal(tb, msgAndArgs, "No error is expected but got %s", err.Error())
		}
	case <-time.After(t):
		fatal(tb, msgAndArgs, "operation did not finish within %s", t.String())
	}
}
func NoErrorWithinTRetry(tb testing.TB, t time.Duration, f func() error, msgAndArgs ...interface{}) {
	tb.Helper()
	doneCh := make(chan struct{})
	timeout := false
	var err error
	go func() {
		for !timeout {
			if err = f(); err == nil {
				close(doneCh)
				break
			}
		}
	}()
	select {
	case <-doneCh:
	case <-time.After(t):
		timeout = true
		fatal(tb, msgAndArgs, "operation did not finish within %s - last error: %v", t.String(), err)
	}
}
func YesError(tb testing.TB, err error, msgAndArgs ...interface{}) {
	tb.Helper()
	if err == nil {
		fatal(tb, msgAndArgs, "Error is expected but got %v", err)
	}
}
func NotNil(tb testing.TB, object interface{}, msgAndArgs ...interface{}) {
	tb.Helper()
	success := true

	if object == nil {
		success = false
	} else {
		value := reflect.ValueOf(object)
		kind := value.Kind()
		if kind >= reflect.Chan && kind <= reflect.Slice && value.IsNil() {
			success = false
		}
	}

	if !success {
		fatal(tb, msgAndArgs, "Expected value not to be nil.")
	}
}
func Nil(tb testing.TB, object interface{}, msgAndArgs ...interface{}) {
	tb.Helper()
	if object == nil {
		return
	}
	value := reflect.ValueOf(object)
	kind := value.Kind()
	if kind >= reflect.Chan && kind <= reflect.Slice && value.IsNil() {
		return
	}

	fatal(tb, msgAndArgs, "Expected value to be nil, but was %v", object)
}
func False(tb testing.TB, value bool, msgAndArgs ...interface{}) {
	tb.Helper()
	if value {
		fatal(tb, msgAndArgs, "Should be false.")
	}
}
func NewSTM(ctx context.Context, c *v3.Client, apply func(STM) error) (*v3.TxnResponse, error) {
	return newSTMSerializable(ctx, c, apply, false)
}
func NewDryrunSTM(ctx context.Context, c *v3.Client, apply func(STM) error) error {
	_, err := newSTMSerializable(ctx, c, apply, true)
	return err
}
func newSTMRepeatable(ctx context.Context, c *v3.Client, apply func(STM) error) (*v3.TxnResponse, error) {
	s := &stm{client: c, ctx: ctx, getOpts: []v3.OpOption{v3.WithSerializable()}}
	return runSTM(s, apply, false)
}
func newSTMSerializable(ctx context.Context, c *v3.Client, apply func(STM) error, dryrun bool) (*v3.TxnResponse, error) {
	s := &stmSerializable{
		stm:      stm{client: c, ctx: ctx},
		prefetch: make(map[string]*v3.GetResponse),
	}
	return runSTM(s, apply, dryrun)
}
func newSTMReadCommitted(ctx context.Context, c *v3.Client, apply func(STM) error) (*v3.TxnResponse, error) {
	s := &stmReadCommitted{stm{client: c, ctx: ctx, getOpts: []v3.OpOption{v3.WithSerializable()}}}
	return runSTM(s, apply, true)
}
func (s *stmReadCommitted) commit() *v3.TxnResponse {
	s.rset = nil
	return s.stm.commit()
}
func Pipelines(etcdClient *etcd.Client, etcdPrefix string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, pipelinesPrefix),
		nil,
		&pps.EtcdPipelineInfo{},
		nil,
		nil,
	)
}
func Jobs(etcdClient *etcd.Client, etcdPrefix string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, jobsPrefix),
		[]*col.Index{JobsPipelineIndex, JobsOutputIndex},
		&pps.EtcdJobInfo{},
		nil,
		nil,
	)
}
func NewTicker(b BackOff) *Ticker {
	c := make(chan time.Time)
	t := &Ticker{
		C:    c,
		c:    c,
		b:    b,
		stop: make(chan struct{}),
	}
	go t.run()
	runtime.SetFinalizer(t, (*Ticker).Stop)
	return t
}
func nodeToMap(node *etcd.Node, out map[string]string) bool {
	key := strings.TrimPrefix(node.Key, "/")
	if !node.Dir {
		if node.Value == "" {
			if _, ok := out[key]; ok {
				delete(out, key)
				return true
			}
			return false
		}
		if value, ok := out[key]; !ok || value != node.Value {
			out[key] = node.Value
			return true
		}
		return false
	}
	changed := false
	for _, node := range node.Nodes {
		changed = nodeToMap(node, out) || changed
	}
	return changed
}
func ServiceAccount(opts *AssetOpts) *v1.ServiceAccount {
	return &v1.ServiceAccount{
		TypeMeta: metav1.TypeMeta{
			Kind:       "ServiceAccount",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(ServiceAccountName, labels(""), nil, opts.Namespace),
	}
}
func ClusterRole(opts *AssetOpts) *rbacv1.ClusterRole {
	return &rbacv1.ClusterRole{
		TypeMeta: metav1.TypeMeta{
			Kind:       "ClusterRole",
			APIVersion: "rbac.authorization.k8s.io/v1",
		},
		ObjectMeta: objectMeta(roleName, labels(""), nil, opts.Namespace),
		Rules:      rolePolicyRules,
	}
}
func RoleBinding(opts *AssetOpts) *rbacv1.RoleBinding {
	return &rbacv1.RoleBinding{
		TypeMeta: metav1.TypeMeta{
			Kind:       "RoleBinding",
			APIVersion: "rbac.authorization.k8s.io/v1",
		},
		ObjectMeta: objectMeta(roleBindingName, labels(""), nil, opts.Namespace),
		Subjects: []rbacv1.Subject{{
			Kind:      "ServiceAccount",
			Name:      ServiceAccountName,
			Namespace: opts.Namespace,
		}},
		RoleRef: rbacv1.RoleRef{
			Kind: "Role",
			Name: roleName,
		},
	}
}
func GetSecretEnvVars(storageBackend string) []v1.EnvVar {
	var envVars []v1.EnvVar
	if storageBackend != "" {
		envVars = append(envVars, v1.EnvVar{
			Name:  obj.StorageBackendEnvVar,
			Value: storageBackend,
		})
	}
	trueVal := true
	for envVar, secretKey := range obj.EnvVarToSecretKey {
		envVars = append(envVars, v1.EnvVar{
			Name: envVar,
			ValueFrom: &v1.EnvVarSource{
				SecretKeyRef: &v1.SecretKeySelector{
					LocalObjectReference: v1.LocalObjectReference{
						Name: client.StorageSecretName,
					},
					Key:      secretKey,
					Optional: &trueVal,
				},
			},
		})
	}
	return envVars
}
func PachdService(opts *AssetOpts) *v1.Service {
	prometheusAnnotations := map[string]string{
		"prometheus.io/scrape": "true",
		"prometheus.io/port":   strconv.Itoa(PrometheusPort),
	}
	return &v1.Service{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Service",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(pachdName, labels(pachdName), prometheusAnnotations, opts.Namespace),
		Spec: v1.ServiceSpec{
			Type: v1.ServiceTypeNodePort,
			Selector: map[string]string{
				"app": pachdName,
			},
			Ports: []v1.ServicePort{
				{
					Port:     600, // also set in cmd/pachd/main.go
					Name:     "s3gateway-port",
					NodePort: 30600,
				},
				{
					Port:     650, // also set in cmd/pachd/main.go
					Name:     "api-grpc-port",
					NodePort: 30650,
				},
				{
					Port:     651, // also set in cmd/pachd/main.go
					Name:     "trace-port",
					NodePort: 30651,
				},
				{
					Port:     652, // also set in cmd/pachd/main.go
					Name:     "api-http-port",
					NodePort: 30652,
				},
				{
					Port:     auth.SamlPort,
					Name:     "saml-port",
					NodePort: 30000 + auth.SamlPort,
				},
				{
					Port:     githook.GitHookPort,
					Name:     "api-git-port",
					NodePort: githook.NodePort(),
				},
			},
		},
	}
}
func GithookService(namespace string) *v1.Service {
	name := "githook"
	return &v1.Service{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Service",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(name, labels(name), nil, namespace),
		Spec: v1.ServiceSpec{
			Type: v1.ServiceTypeLoadBalancer,
			Selector: map[string]string{
				"app": pachdName,
			},
			Ports: []v1.ServicePort{
				{
					TargetPort: intstr.FromInt(githook.GitHookPort),
					Name:       "api-git-port",
					Port:       githook.ExternalPort(),
				},
			},
		},
	}
}
func EtcdDeployment(opts *AssetOpts, hostPath string) *apps.Deployment {
	cpu := resource.MustParse(opts.EtcdCPURequest)
	mem := resource.MustParse(opts.EtcdMemRequest)
	var volumes []v1.Volume
	if hostPath == "" {
		volumes = []v1.Volume{
			{
				Name: "etcd-storage",
				VolumeSource: v1.VolumeSource{
					PersistentVolumeClaim: &v1.PersistentVolumeClaimVolumeSource{
						ClaimName: etcdVolumeClaimName,
					},
				},
			},
		}
	} else {
		volumes = []v1.Volume{
			{
				Name: "etcd-storage",
				VolumeSource: v1.VolumeSource{
					HostPath: &v1.HostPathVolumeSource{
						Path: filepath.Join(hostPath, "etcd"),
					},
				},
			},
		}
	}
	resourceRequirements := v1.ResourceRequirements{
		Requests: v1.ResourceList{
			v1.ResourceCPU:    cpu,
			v1.ResourceMemory: mem,
		},
	}
	if !opts.NoGuaranteed {
		resourceRequirements.Limits = v1.ResourceList{
			v1.ResourceCPU:    cpu,
			v1.ResourceMemory: mem,
		}
	}
	// Don't want to strip the registry out of etcdImage since it's from quay
	// not docker hub.
	image := etcdImage
	if opts.Registry != "" {
		image = AddRegistry(opts.Registry, etcdImage)
	}
	return &apps.Deployment{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Deployment",
			APIVersion: "apps/v1beta1",
		},
		ObjectMeta: objectMeta(etcdName, labels(etcdName), nil, opts.Namespace),
		Spec: apps.DeploymentSpec{
			Replicas: replicas(1),
			Selector: &metav1.LabelSelector{
				MatchLabels: labels(etcdName),
			},
			Template: v1.PodTemplateSpec{
				ObjectMeta: objectMeta(etcdName, labels(etcdName), nil, opts.Namespace),
				Spec: v1.PodSpec{
					Containers: []v1.Container{
						{
							Name:  etcdName,
							Image: image,
							//TODO figure out how to get a cluster of these to talk to each other
							Command: etcdCmd,
							Ports: []v1.ContainerPort{
								{
									ContainerPort: 2379,
									Name:          "client-port",
								},
								{
									ContainerPort: 2380,
									Name:          "peer-port",
								},
							},
							VolumeMounts: []v1.VolumeMount{
								{
									Name:      "etcd-storage",
									MountPath: "/var/data/etcd",
								},
							},
							ImagePullPolicy: "IfNotPresent",
							Resources:       resourceRequirements,
						},
					},
					Volumes:          volumes,
					ImagePullSecrets: imagePullSecrets(opts),
				},
			},
		},
	}
}
func EtcdStorageClass(opts *AssetOpts, backend backend) (interface{}, error) {
	sc := map[string]interface{}{
		"apiVersion": "storage.k8s.io/v1beta1",
		"kind":       "StorageClass",
		"metadata": map[string]interface{}{
			"name":      defaultEtcdStorageClassName,
			"labels":    labels(etcdName),
			"namespace": opts.Namespace,
		},
	}
	switch backend {
	case googleBackend:
		sc["provisioner"] = "kubernetes.io/gce-pd"
		sc["parameters"] = map[string]string{
			"type": "pd-ssd",
		}
	case amazonBackend:
		sc["provisioner"] = "kubernetes.io/aws-ebs"
		sc["parameters"] = map[string]string{
			"type": "gp2",
		}
	default:
		return nil, nil
	}
	return sc, nil
}
func EtcdVolume(persistentDiskBackend backend, opts *AssetOpts,
	hostPath string, name string, size int) (*v1.PersistentVolume, error) {
	spec := &v1.PersistentVolume{
		TypeMeta: metav1.TypeMeta{
			Kind:       "PersistentVolume",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(etcdVolumeName, labels(etcdName), nil, opts.Namespace),
		Spec: v1.PersistentVolumeSpec{
			Capacity: map[v1.ResourceName]resource.Quantity{
				"storage": resource.MustParse(fmt.Sprintf("%vGi", size)),
			},
			AccessModes:                   []v1.PersistentVolumeAccessMode{v1.ReadWriteOnce},
			PersistentVolumeReclaimPolicy: v1.PersistentVolumeReclaimRetain,
		},
	}

	switch persistentDiskBackend {
	case amazonBackend:
		spec.Spec.PersistentVolumeSource = v1.PersistentVolumeSource{
			AWSElasticBlockStore: &v1.AWSElasticBlockStoreVolumeSource{
				FSType:   "ext4",
				VolumeID: name,
			},
		}
	case googleBackend:
		spec.Spec.PersistentVolumeSource = v1.PersistentVolumeSource{
			GCEPersistentDisk: &v1.GCEPersistentDiskVolumeSource{
				FSType: "ext4",
				PDName: name,
			},
		}
	case microsoftBackend:
		dataDiskURI := name
		split := strings.Split(name, "/")
		diskName := split[len(split)-1]

		spec.Spec.PersistentVolumeSource = v1.PersistentVolumeSource{
			AzureDisk: &v1.AzureDiskVolumeSource{
				DiskName:    diskName,
				DataDiskURI: dataDiskURI,
			},
		}
	case minioBackend:
		fallthrough
	case localBackend:
		spec.Spec.PersistentVolumeSource = v1.PersistentVolumeSource{
			HostPath: &v1.HostPathVolumeSource{
				Path: filepath.Join(hostPath, "etcd"),
			},
		}
	default:
		return nil, fmt.Errorf("cannot generate volume spec for unknown backend \"%v\"", persistentDiskBackend)
	}
	return spec, nil
}
func EtcdNodePortService(local bool, opts *AssetOpts) *v1.Service {
	var clientNodePort int32
	if local {
		clientNodePort = 32379
	}
	return &v1.Service{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Service",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(etcdName, labels(etcdName), nil, opts.Namespace),
		Spec: v1.ServiceSpec{
			Type: v1.ServiceTypeNodePort,
			Selector: map[string]string{
				"app": etcdName,
			},
			Ports: []v1.ServicePort{
				{
					Port:     2379,
					Name:     "client-port",
					NodePort: clientNodePort,
				},
			},
		},
	}
}
func EtcdHeadlessService(opts *AssetOpts) *v1.Service {
	return &v1.Service{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Service",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(etcdHeadlessServiceName, labels(etcdName), nil, opts.Namespace),
		Spec: v1.ServiceSpec{
			Selector: map[string]string{
				"app": etcdName,
			},
			ClusterIP: "None",
			Ports: []v1.ServicePort{
				{
					Name: "peer-port",
					Port: 2380,
				},
			},
		},
	}
}
func EtcdStatefulSet(opts *AssetOpts, backend backend, diskSpace int) interface{} {
	mem := resource.MustParse(opts.EtcdMemRequest)
	cpu := resource.MustParse(opts.EtcdCPURequest)
	initialCluster := make([]string, 0, opts.EtcdNodes)
	for i := 0; i < opts.EtcdNodes; i++ {
		url := fmt.Sprintf("http://etcd-%d.etcd-headless.${NAMESPACE}.svc.cluster.local:2380", i)
		initialCluster = append(initialCluster, fmt.Sprintf("etcd-%d=%s", i, url))
	}
	// Because we need to refer to some environment variables set the by the
	// k8s downward API, we define the command for running etcd here, and then
	// actually run it below via '/bin/sh -c ${CMD}'
	etcdCmd := append(etcdCmd,
		"--listen-peer-urls=http://0.0.0.0:2380",
		"--initial-cluster-token=pach-cluster", // unique ID
		"--initial-advertise-peer-urls=http://${ETCD_NAME}.etcd-headless.${NAMESPACE}.svc.cluster.local:2380",
		"--initial-cluster="+strings.Join(initialCluster, ","),
	)
	for i, str := range etcdCmd {
		etcdCmd[i] = fmt.Sprintf("\"%s\"", str) // quote all arguments, for shell
	}

	var pvcTemplates []interface{}
	switch backend {
	case googleBackend, amazonBackend:
		storageClassName := opts.EtcdStorageClassName
		if storageClassName == "" {
			storageClassName = defaultEtcdStorageClassName
		}
		pvcTemplates = []interface{}{
			map[string]interface{}{
				"metadata": map[string]interface{}{
					"name":   etcdVolumeClaimName,
					"labels": labels(etcdName),
					"annotations": map[string]string{
						"volume.beta.kubernetes.io/storage-class": storageClassName,
					},
					"namespace": opts.Namespace,
				},
				"spec": map[string]interface{}{
					"resources": map[string]interface{}{
						"requests": map[string]interface{}{
							"storage": resource.MustParse(fmt.Sprintf("%vGi", diskSpace)),
						},
					},
					"accessModes": []string{"ReadWriteOnce"},
				},
			},
		}
	default:
		pvcTemplates = []interface{}{
			map[string]interface{}{
				"metadata": map[string]interface{}{
					"name":      etcdVolumeClaimName,
					"labels":    labels(etcdName),
					"namespace": opts.Namespace,
				},
				"spec": map[string]interface{}{
					"resources": map[string]interface{}{
						"requests": map[string]interface{}{
							"storage": resource.MustParse(fmt.Sprintf("%vGi", diskSpace)),
						},
					},
					"accessModes": []string{"ReadWriteOnce"},
				},
			},
		}
	}
	var imagePullSecrets []map[string]string
	if opts.ImagePullSecret != "" {
		imagePullSecrets = append(imagePullSecrets, map[string]string{"name": opts.ImagePullSecret})
	}
	// As of March 17, 2017, the Kubernetes client does not include structs for
	// Stateful Set, so we generate the kubernetes manifest using raw json.
	// TODO(msteffen): we're now upgrading our kubernetes client, so we should be
	// abe to rewrite this spec using k8s client structs
	image := etcdImage
	if opts.Registry != "" {
		image = AddRegistry(opts.Registry, etcdImage)
	}
	return map[string]interface{}{
		"apiVersion": "apps/v1beta1",
		"kind":       "StatefulSet",
		"metadata": map[string]interface{}{
			"name":      etcdName,
			"labels":    labels(etcdName),
			"namespace": opts.Namespace,
		},
		"spec": map[string]interface{}{
			// Effectively configures a RC
			"serviceName": etcdHeadlessServiceName,
			"replicas":    int(opts.EtcdNodes),
			"selector": map[string]interface{}{
				"matchLabels": labels(etcdName),
			},

			// pod template
			"template": map[string]interface{}{
				"metadata": map[string]interface{}{
					"name":      etcdName,
					"labels":    labels(etcdName),
					"namespace": opts.Namespace,
				},
				"spec": map[string]interface{}{
					"imagePullSecrets": imagePullSecrets,
					"containers": []interface{}{
						map[string]interface{}{
							"name":    etcdName,
							"image":   image,
							"command": []string{"/bin/sh", "-c"},
							"args":    []string{strings.Join(etcdCmd, " ")},
							// Use the downward API to pass the pod name to etcd. This sets
							// the etcd-internal name of each node to its pod name.
							"env": []map[string]interface{}{{
								"name": "ETCD_NAME",
								"valueFrom": map[string]interface{}{
									"fieldRef": map[string]interface{}{
										"apiVersion": "v1",
										"fieldPath":  "metadata.name",
									},
								},
							}, {
								"name": "NAMESPACE",
								"valueFrom": map[string]interface{}{
									"fieldRef": map[string]interface{}{
										"apiVersion": "v1",
										"fieldPath":  "metadata.namespace",
									},
								},
							}},
							"ports": []interface{}{
								map[string]interface{}{
									"containerPort": 2379,
									"name":          "client-port",
								},
								map[string]interface{}{
									"containerPort": 2380,
									"name":          "peer-port",
								},
							},
							"volumeMounts": []interface{}{
								map[string]interface{}{
									"name":      etcdVolumeClaimName,
									"mountPath": "/var/data/etcd",
								},
							},
							"imagePullPolicy": "IfNotPresent",
							"resources": map[string]interface{}{
								"requests": map[string]interface{}{
									string(v1.ResourceCPU):    cpu.String(),
									string(v1.ResourceMemory): mem.String(),
								},
							},
						},
					},
				},
			},
			"volumeClaimTemplates": pvcTemplates,
		},
	}
}
func DashDeployment(opts *AssetOpts) *apps.Deployment {
	return &apps.Deployment{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Deployment",
			APIVersion: "apps/v1beta1",
		},
		ObjectMeta: objectMeta(dashName, labels(dashName), nil, opts.Namespace),
		Spec: apps.DeploymentSpec{
			Selector: &metav1.LabelSelector{
				MatchLabels: labels(dashName),
			},
			Template: v1.PodTemplateSpec{
				ObjectMeta: objectMeta(dashName, labels(dashName), nil, opts.Namespace),
				Spec: v1.PodSpec{
					Containers: []v1.Container{
						{
							Name:  dashName,
							Image: AddRegistry(opts.Registry, opts.DashImage),
							Ports: []v1.ContainerPort{
								{
									ContainerPort: 8080,
									Name:          "dash-http",
								},
							},
							ImagePullPolicy: "IfNotPresent",
						},
						{
							Name:  grpcProxyName,
							Image: AddRegistry(opts.Registry, grpcProxyImage),
							Ports: []v1.ContainerPort{
								{
									ContainerPort: 8081,
									Name:          "grpc-proxy-http",
								},
							},
							ImagePullPolicy: "IfNotPresent",
						},
					},
					ImagePullSecrets: imagePullSecrets(opts),
				},
			},
		},
	}
}
func DashService(opts *AssetOpts) *v1.Service {
	return &v1.Service{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Service",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(dashName, labels(dashName), nil, opts.Namespace),
		Spec: v1.ServiceSpec{
			Type:     v1.ServiceTypeNodePort,
			Selector: labels(dashName),
			Ports: []v1.ServicePort{
				{
					Port:     8080,
					Name:     "dash-http",
					NodePort: 30080,
				},
				{
					Port:     8081,
					Name:     "grpc-proxy-http",
					NodePort: 30081,
				},
			},
		},
	}
}
func WriteSecret(encoder Encoder, data map[string][]byte, opts *AssetOpts) error {
	if opts.DashOnly {
		return nil
	}
	secret := &v1.Secret{
		TypeMeta: metav1.TypeMeta{
			Kind:       "Secret",
			APIVersion: "v1",
		},
		ObjectMeta: objectMeta(client.StorageSecretName, labels(client.StorageSecretName), nil, opts.Namespace),
		Data:       data,
	}
	return encoder.Encode(secret)
}
func GoogleSecret(bucket string, cred string) map[string][]byte {
	return map[string][]byte{
		"google-bucket": []byte(bucket),
		"google-cred":   []byte(cred),
	}
}
func WriteDashboardAssets(encoder Encoder, opts *AssetOpts) error {
	if err := encoder.Encode(DashService(opts)); err != nil {
		return err
	}
	return encoder.Encode(DashDeployment(opts))
}
func WriteLocalAssets(encoder Encoder, opts *AssetOpts, hostPath string) error {
	if err := WriteAssets(encoder, opts, localBackend, localBackend, 1 /* = volume size (gb) */, hostPath); err != nil {
		return err
	}
	if secretErr := WriteSecret(encoder, LocalSecret(), opts); secretErr != nil {
		return secretErr
	}
	return nil
}
func WriteCustomAssets(encoder Encoder, opts *AssetOpts, args []string, objectStoreBackend string,
	persistentDiskBackend string, secure, isS3V2 bool) error {
	switch objectStoreBackend {
	case "s3":
		if len(args) != s3CustomArgs {
			return fmt.Errorf("Expected %d arguments for disk+s3 backend", s3CustomArgs)
		}
		volumeSize, err := strconv.Atoi(args[1])
		if err != nil {
			return fmt.Errorf("volume size needs to be an integer; instead got %v", args[1])
		}
		switch persistentDiskBackend {
		case "aws":
			if err := WriteAssets(encoder, opts, minioBackend, amazonBackend, volumeSize, ""); err != nil {
				return err
			}
		case "google":
			if err := WriteAssets(encoder, opts, minioBackend, googleBackend, volumeSize, ""); err != nil {
				return err
			}
		case "azure":
			if err := WriteAssets(encoder, opts, minioBackend, microsoftBackend, volumeSize, ""); err != nil {
				return err
			}
		default:
			return fmt.Errorf("Did not recognize the choice of persistent-disk")
		}
		return WriteSecret(encoder, MinioSecret(args[2], args[3], args[4], args[5], secure, isS3V2), opts)
	default:
		return fmt.Errorf("Did not recognize the choice of object-store")
	}
}
func WriteAmazonAssets(encoder Encoder, opts *AssetOpts, region string, bucket string, volumeSize int, creds *AmazonCreds, cloudfrontDistro string) error {
	if err := WriteAssets(encoder, opts, amazonBackend, amazonBackend, volumeSize, ""); err != nil {
		return err
	}
	var secret map[string][]byte
	if creds == nil {
		secret = AmazonIAMRoleSecret(region, bucket, cloudfrontDistro)
	} else if creds.ID != "" {
		secret = AmazonSecret(region, bucket, creds.ID, creds.Secret, creds.Token, cloudfrontDistro)
	} else if creds.VaultAddress != "" {
		secret = AmazonVaultSecret(region, bucket, creds.VaultAddress, creds.VaultRole, creds.VaultToken, cloudfrontDistro)
	}
	return WriteSecret(encoder, secret, opts)
}
func WriteGoogleAssets(encoder Encoder, opts *AssetOpts, bucket string, cred string, volumeSize int) error {
	if err := WriteAssets(encoder, opts, googleBackend, googleBackend, volumeSize, ""); err != nil {
		return err
	}
	return WriteSecret(encoder, GoogleSecret(bucket, cred), opts)
}
func WriteMicrosoftAssets(encoder Encoder, opts *AssetOpts, container string, id string, secret string, volumeSize int) error {
	if err := WriteAssets(encoder, opts, microsoftBackend, microsoftBackend, volumeSize, ""); err != nil {
		return err
	}
	return WriteSecret(encoder, MicrosoftSecret(container, id, secret), opts)
}
func Images(opts *AssetOpts) []string {
	return []string{
		versionedWorkerImage(opts),
		etcdImage,
		grpcProxyImage,
		pauseImage,
		versionedPachdImage(opts),
		opts.DashImage,
	}
}
func AddRegistry(registry string, imageName string) string {
	if registry == "" {
		return imageName
	}
	parts := strings.Split(imageName, "/")
	if len(parts) == 3 {
		parts = parts[1:]
	}
	return path.Join(registry, parts[0], parts[1])
}
func (b *ExponentialBackOff) withCanonicalRandomizationFactor() *ExponentialBackOff {
	if b.RandomizationFactor < 0 {
		b.RandomizationFactor = 0
	} else if b.RandomizationFactor > 1 {
		b.RandomizationFactor = 1
	}
	return b
}
func (b *ExponentialBackOff) Reset() {
	b.currentInterval = b.InitialInterval
	b.startTime = b.Clock.Now()
}
func (b *ExponentialBackOff) incrementCurrentInterval() {
	// Check for overflow, if overflow is detected set the current interval to the max interval.
	if float64(b.currentInterval) >= float64(b.MaxInterval)/b.Multiplier {
		b.currentInterval = b.MaxInterval
	} else {
		b.currentInterval = time.Duration(float64(b.currentInterval) * b.Multiplier)
	}
}
func NewBlockAPIServer(dir string, cacheBytes int64, backend string, etcdAddress string) (BlockAPIServer, error) {
	switch backend {
	case MinioBackendEnvVar:
		// S3 compatible doesn't like leading slashes
		if len(dir) > 0 && dir[0] == '/' {
			dir = dir[1:]
		}
		blockAPIServer, err := newMinioBlockAPIServer(dir, cacheBytes, etcdAddress)
		if err != nil {
			return nil, err
		}
		return blockAPIServer, nil
	case AmazonBackendEnvVar:
		// amazon doesn't like leading slashes
		if len(dir) > 0 && dir[0] == '/' {
			dir = dir[1:]
		}
		blockAPIServer, err := newAmazonBlockAPIServer(dir, cacheBytes, etcdAddress)
		if err != nil {
			return nil, err
		}
		return blockAPIServer, nil
	case GoogleBackendEnvVar:
		// TODO figure out if google likes leading slashses
		blockAPIServer, err := newGoogleBlockAPIServer(dir, cacheBytes, etcdAddress)
		if err != nil {
			return nil, err
		}
		return blockAPIServer, nil
	case MicrosoftBackendEnvVar:
		blockAPIServer, err := newMicrosoftBlockAPIServer(dir, cacheBytes, etcdAddress)
		if err != nil {
			return nil, err
		}
		return blockAPIServer, nil
	case LocalBackendEnvVar:
		fallthrough
	default:
		blockAPIServer, err := newLocalBlockAPIServer(dir, cacheBytes, etcdAddress)
		if err != nil {
			return nil, err
		}
		return blockAPIServer, nil
	}
}
func LocalStorage(tb testing.TB) (obj.Client, *Storage) {
	wd, err := os.Getwd()
	require.NoError(tb, err)
	objC, err := obj.NewLocalClient(wd)
	require.NoError(tb, err)
	return objC, NewStorage(objC, Prefix)
}
func (a *APIServer) deleteJob(stm col.STM, jobPtr *pps.EtcdJobInfo) error {
	pipelinePtr := &pps.EtcdPipelineInfo{}
	if err := a.pipelines.ReadWrite(stm).Update(jobPtr.Pipeline.Name, pipelinePtr, func() error {
		if pipelinePtr.JobCounts == nil {
			pipelinePtr.JobCounts = make(map[int32]int32)
		}
		if pipelinePtr.JobCounts[int32(jobPtr.State)] != 0 {
			pipelinePtr.JobCounts[int32(jobPtr.State)]--
		}
		return nil
	}); err != nil {
		return err
	}
	return a.jobs.ReadWrite(stm).Delete(jobPtr.Job.ID)
}
func writeXML(w http.ResponseWriter, r *http.Request, code int, v interface{}) {
	w.Header().Set("Content-Type", "application/xml")
	w.WriteHeader(code)
	encoder := xml.NewEncoder(w)
	if err := encoder.Encode(v); err != nil {
		// just log a message since a response has already been partially
		// written
		requestLogger(r).Errorf("could not encode xml response: %v", err)
	}
}
func clean1_7HashtreePath(p string) string {
	if !strings.HasPrefix(p, "/") {
		p = "/" + p
	}
	return default1_7HashtreeRoot(pathlib.Clean(p))
}
func NewFromAddress(addr string, options ...Option) (*APIClient, error) {
	// Apply creation options
	settings := clientSettings{
		maxConcurrentStreams: DefaultMaxConcurrentStreams,
		dialTimeout:          DefaultDialTimeout,
	}
	for _, option := range options {
		if err := option(&settings); err != nil {
			return nil, err
		}
	}
	c := &APIClient{
		addr:    addr,
		caCerts: settings.caCerts,
		limiter: limit.New(settings.maxConcurrentStreams),
	}
	if err := c.connect(settings.dialTimeout); err != nil {
		return nil, err
	}
	return c, nil
}
func getUserMachineAddrAndOpts(cfg *config.Config) (string, []Option, error) {
	// 1) PACHD_ADDRESS environment variable (shell-local) overrides global config
	if envAddr, ok := os.LookupEnv("PACHD_ADDRESS"); ok {
		if !strings.Contains(envAddr, ":") {
			envAddr = fmt.Sprintf("%s:%s", envAddr, DefaultPachdNodePort)
		}
		options, err := getCertOptionsFromEnv()
		if err != nil {
			return "", nil, err
		}
		return envAddr, options, nil
	}

	// 2) Get target address from global config if possible
	if cfg != nil && cfg.V1 != nil && cfg.V1.PachdAddress != "" {
		// Also get cert info from config (if set)
		if cfg.V1.ServerCAs != "" {
			pemBytes, err := base64.StdEncoding.DecodeString(cfg.V1.ServerCAs)
			if err != nil {
				return "", nil, fmt.Errorf("could not decode server CA certs in config: %v", err)
			}
			return cfg.V1.PachdAddress, []Option{WithAdditionalRootCAs(pemBytes)}, nil
		}
		return cfg.V1.PachdAddress, nil, nil
	}

	// 3) Use default address (broadcast) if nothing else works
	options, err := getCertOptionsFromEnv()
	if err != nil {
		return "", nil, err
	}
	return "", options, nil
}
func NewInCluster(options ...Option) (*APIClient, error) {
	host, ok := os.LookupEnv("PACHD_SERVICE_HOST")
	if !ok {
		return nil, fmt.Errorf("PACHD_SERVICE_HOST not set")
	}
	port, ok := os.LookupEnv("PACHD_SERVICE_PORT")
	if !ok {
		return nil, fmt.Errorf("PACHD_SERVICE_PORT not set")
	}
	// create new pachctl client
	return NewFromAddress(fmt.Sprintf("%s:%s", host, port), options...)
}
func (c *APIClient) Close() error {
	if err := c.clientConn.Close(); err != nil {
		return err
	}

	if c.portForwarder != nil {
		c.portForwarder.Close()
	}

	return nil
}
func (c APIClient) DeleteAll() error {
	if _, err := c.AuthAPIClient.Deactivate(
		c.Ctx(),
		&auth.DeactivateRequest{},
	); err != nil && !auth.IsErrNotActivated(err) {
		return grpcutil.ScrubGRPC(err)
	}
	if _, err := c.PpsAPIClient.DeleteAll(
		c.Ctx(),
		&types.Empty{},
	); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	if _, err := c.PfsAPIClient.DeleteAll(
		c.Ctx(),
		&types.Empty{},
	); err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	return nil
}
func (c APIClient) SetMaxConcurrentStreams(n int) {
	c.limiter = limit.New(n)
}
func (c *APIClient) WithCtx(ctx context.Context) *APIClient {
	result := *c // copy c
	result.ctx = ctx
	return &result
}
func NewDLock(client *etcd.Client, prefix string) DLock {
	return &etcdImpl{
		client: client,
		prefix: prefix,
	}
}
func (a *APIServer) DatumID(data []*Input) string {
	hash := sha256.New()
	for _, d := range data {
		hash.Write([]byte(d.FileInfo.File.Path))
		hash.Write(d.FileInfo.Hash)
	}
	// InputFileID is a single string id for the data from this input, it's used in logs and in
	// the statsTree
	return hex.EncodeToString(hash.Sum(nil))
}
func (a *APIServer) runUserErrorHandlingCode(ctx context.Context, logger *taggedLogger, environ []string, stats *pps.ProcessStats, rawDatumTimeout *types.Duration) (retErr error) {
	logger.Logf("beginning to run user error handling code")
	defer func(start time.Time) {
		if retErr != nil {
			logger.Logf("errored running user error handling code after %v: %v", time.Since(start), retErr)
		} else {
			logger.Logf("finished running user error handling code after %v", time.Since(start))
		}
	}(time.Now())

	cmd := exec.CommandContext(ctx, a.pipelineInfo.Transform.ErrCmd[0], a.pipelineInfo.Transform.ErrCmd[1:]...)
	if a.pipelineInfo.Transform.ErrStdin != nil {
		cmd.Stdin = strings.NewReader(strings.Join(a.pipelineInfo.Transform.ErrStdin, "\n") + "\n")
	}
	cmd.Stdout = logger.userLogger()
	cmd.Stderr = logger.userLogger()
	cmd.Env = environ
	if a.uid != nil && a.gid != nil {
		cmd.SysProcAttr = &syscall.SysProcAttr{
			Credential: &syscall.Credential{
				Uid: *a.uid,
				Gid: *a.gid,
			},
		}
	}
	cmd.Dir = a.pipelineInfo.Transform.WorkingDir
	err := cmd.Start()
	if err != nil {
		return fmt.Errorf("error cmd.Start: %v", err)
	}
	// A context w a deadline will successfully cancel/kill
	// the running process (minus zombies)
	state, err := cmd.Process.Wait()
	if err != nil {
		return fmt.Errorf("error cmd.Wait: %v", err)
	}
	if isDone(ctx) {
		if err = ctx.Err(); err != nil {
			return err
		}
	}
	// Because of this issue: https://github.com/golang/go/issues/18874
	// We forked os/exec so that we can call just the part of cmd.Wait() that
	// happens after blocking on the process. Unfortunately calling
	// cmd.Process.Wait() then cmd.Wait() will produce an error. So instead we
	// close the IO using this helper
	err = cmd.WaitIO(state, err)
	// We ignore broken pipe errors, these occur very occasionally if a user
	// specifies Stdin but their process doesn't actually read everything from
	// Stdin. This is a fairly common thing to do, bash by default ignores
	// broken pipe errors.
	if err != nil && !strings.Contains(err.Error(), "broken pipe") {
		// (if err is an acceptable return code, don't return err)
		if exiterr, ok := err.(*exec.ExitError); ok {
			if status, ok := exiterr.Sys().(syscall.WaitStatus); ok {
				for _, returnCode := range a.pipelineInfo.Transform.AcceptReturnCode {
					if int(returnCode) == status.ExitStatus() {
						return nil
					}
				}
			}
		}
		return fmt.Errorf("error cmd.WaitIO: %v", err)
	}
	return nil
}
func HashDatum(pipelineName string, pipelineSalt string, data []*Input) string {
	hash := sha256.New()
	for _, datum := range data {
		hash.Write([]byte(datum.Name))
		hash.Write([]byte(datum.FileInfo.File.Path))
		hash.Write(datum.FileInfo.Hash)
	}

	hash.Write([]byte(pipelineName))
	hash.Write([]byte(pipelineSalt))

	return client.DatumTagPrefix(pipelineSalt) + hex.EncodeToString(hash.Sum(nil))
}
func HashDatum15(pipelineInfo *pps.PipelineInfo, data []*Input) (string, error) {
	hash := sha256.New()
	for _, datum := range data {
		hash.Write([]byte(datum.Name))
		hash.Write([]byte(datum.FileInfo.File.Path))
		hash.Write(datum.FileInfo.Hash)
	}

	// We set env to nil because if env contains more than one elements,
	// since it's a map, the output of Marshal() can be non-deterministic.
	env := pipelineInfo.Transform.Env
	pipelineInfo.Transform.Env = nil
	defer func() {
		pipelineInfo.Transform.Env = env
	}()
	bytes, err := pipelineInfo.Transform.Marshal()
	if err != nil {
		return "", err
	}
	hash.Write(bytes)
	hash.Write([]byte(pipelineInfo.Pipeline.Name))
	hash.Write([]byte(pipelineInfo.ID))
	hash.Write([]byte(strconv.Itoa(int(pipelineInfo.Version))))

	// Note in 1.5.0 this function was called HashPipelineID, it's now called
	// HashPipelineName but it has the same implementation.
	return client.DatumTagPrefix(pipelineInfo.ID) + hex.EncodeToString(hash.Sum(nil)), nil
}
func (a *APIServer) Status(ctx context.Context, _ *types.Empty) (*pps.WorkerStatus, error) {
	a.statusMu.Lock()
	defer a.statusMu.Unlock()
	started, err := types.TimestampProto(a.started)
	if err != nil {
		return nil, err
	}
	result := &pps.WorkerStatus{
		JobID:     a.jobID,
		WorkerID:  a.workerName,
		Started:   started,
		Data:      a.datum(),
		QueueSize: atomic.LoadInt64(&a.queueSize),
	}
	return result, nil
}
func (a *APIServer) Cancel(ctx context.Context, request *CancelRequest) (*CancelResponse, error) {
	a.statusMu.Lock()
	defer a.statusMu.Unlock()
	if request.JobID != a.jobID {
		return &CancelResponse{Success: false}, nil
	}
	if !MatchDatum(request.DataFilters, a.datum()) {
		return &CancelResponse{Success: false}, nil
	}
	a.cancel()
	// clear the status since we're no longer processing this datum
	a.jobID = ""
	a.data = nil
	a.started = time.Time{}
	a.cancel = nil
	return &CancelResponse{Success: true}, nil
}
func mergeStats(x, y *pps.ProcessStats) error {
	var err error
	if x.DownloadTime, err = plusDuration(x.DownloadTime, y.DownloadTime); err != nil {
		return err
	}
	if x.ProcessTime, err = plusDuration(x.ProcessTime, y.ProcessTime); err != nil {
		return err
	}
	if x.UploadTime, err = plusDuration(x.UploadTime, y.UploadTime); err != nil {
		return err
	}
	x.DownloadBytes += y.DownloadBytes
	x.UploadBytes += y.UploadBytes
	return nil
}
func (a *APIServer) mergeChunk(logger *taggedLogger, high int64, result *processResult) (retErr error) {
	logger.Logf("starting to merge chunk")
	defer func(start time.Time) {
		if retErr != nil {
			logger.Logf("errored merging chunk after %v: %v", time.Since(start), retErr)
		} else {
			logger.Logf("finished merging chunk after %v", time.Since(start))
		}
	}(time.Now())
	buf := &bytes.Buffer{}
	if result.datumsFailed <= 0 {
		if err := a.datumCache.Merge(hashtree.NewWriter(buf), nil, nil); err != nil {
			return err
		}
	}
	if err := a.chunkCache.Put(high, buf); err != nil {
		return err
	}
	if a.pipelineInfo.EnableStats {
		buf.Reset()
		if err := a.datumStatsCache.Merge(hashtree.NewWriter(buf), nil, nil); err != nil {
			return err
		}
		return a.chunkStatsCache.Put(high, buf)
	}
	return nil
}
func IsCommitNotFoundErr(err error) bool {
	if err == nil {
		return false
	}
	return commitNotFoundRe.MatchString(grpcutil.ScrubGRPC(err).Error())
}
func IsCommitDeletedErr(err error) bool {
	if err == nil {
		return false
	}
	return commitDeletedRe.MatchString(grpcutil.ScrubGRPC(err).Error())
}
func IsCommitFinishedErr(err error) bool {
	if err == nil {
		return false
	}
	return commitFinishedRe.MatchString(grpcutil.ScrubGRPC(err).Error())
}
func IsRepoNotFoundErr(err error) bool {
	if err == nil {
		return false
	}
	return repoNotFoundRe.MatchString(err.Error())
}
func IsBranchNotFoundErr(err error) bool {
	if err == nil {
		return false
	}
	return branchNotFoundRe.MatchString(err.Error())
}
func IsFileNotFoundErr(err error) bool {
	if err == nil {
		return false
	}
	return fileNotFoundRe.MatchString(err.Error())
}
func (c APIClient) Version() (string, error) {
	v, err := c.VersionAPIClient.GetVersion(c.Ctx(), &types.Empty{})
	if err != nil {
		return "", grpcutil.ScrubGRPC(err)
	}
	return version.PrettyPrintVersion(v), nil
}
func validateRepoName(name string) error {
	match, _ := regexp.MatchString("^[a-zA-Z0-9_-]+$", name)
	if !match {
		return fmt.Errorf("repo name (%v) invalid: only alphanumeric characters, underscores, and dashes are allowed", name)
	}
	return nil
}
func newDriver(env *serviceenv.ServiceEnv, etcdPrefix string, treeCache *hashtree.Cache, storageRoot string, memoryRequest int64) (*driver, error) {
	// Validate arguments
	if treeCache == nil {
		return nil, fmt.Errorf("cannot initialize driver with nil treeCache")
	}
	// Initialize driver
	etcdClient := env.GetEtcdClient()
	d := &driver{
		etcdClient:     etcdClient,
		prefix:         etcdPrefix,
		repos:          pfsdb.Repos(etcdClient, etcdPrefix),
		putFileRecords: pfsdb.PutFileRecords(etcdClient, etcdPrefix),
		commits: func(repo string) col.Collection {
			return pfsdb.Commits(etcdClient, etcdPrefix, repo)
		},
		branches: func(repo string) col.Collection {
			return pfsdb.Branches(etcdClient, etcdPrefix, repo)
		},
		openCommits: pfsdb.OpenCommits(etcdClient, etcdPrefix),
		treeCache:   treeCache,
		storageRoot: storageRoot,
		// Allow up to a third of the requested memory to be used for memory intensive operations
		memoryLimiter: semaphore.NewWeighted(memoryRequest / 3),
	}
	// Create spec repo (default repo)
	repo := client.NewRepo(ppsconsts.SpecRepo)
	repoInfo := &pfs.RepoInfo{
		Repo:    repo,
		Created: now(),
	}
	if _, err := col.NewSTM(context.Background(), etcdClient, func(stm col.STM) error {
		repos := d.repos.ReadWrite(stm)
		return repos.Create(repo.Name, repoInfo)
	}); err != nil && !col.IsErrExists(err) {
		return nil, err
	}
	return d, nil
}
func (d *driver) inspectCommit(pachClient *client.APIClient, commit *pfs.Commit, blockState pfs.CommitState) (*pfs.CommitInfo, error) {
	ctx := pachClient.Ctx()
	if commit == nil {
		return nil, fmt.Errorf("cannot inspect nil commit")
	}
	if err := d.checkIsAuthorized(pachClient, commit.Repo, auth.Scope_READER); err != nil {
		return nil, err
	}

	// Check if the commitID is a branch name
	var commitInfo *pfs.CommitInfo
	if _, err := col.NewSTM(ctx, d.etcdClient, func(stm col.STM) error {
		var err error
		commitInfo, err = d.resolveCommit(stm, commit)
		return err
	}); err != nil {
		return nil, err
	}

	commits := d.commits(commit.Repo.Name).ReadOnly(ctx)
	if blockState == pfs.CommitState_READY {
		// Wait for each provenant commit to be finished
		for _, p := range commitInfo.Provenance {
			d.inspectCommit(pachClient, p.Commit, pfs.CommitState_FINISHED)
		}
	}
	if blockState == pfs.CommitState_FINISHED {
		// Watch the CommitInfo until the commit has been finished
		if err := func() error {
			commitInfoWatcher, err := commits.WatchOne(commit.ID)
			if err != nil {
				return err
			}
			defer commitInfoWatcher.Close()
			for {
				var commitID string
				_commitInfo := new(pfs.CommitInfo)
				event := <-commitInfoWatcher.Watch()
				switch event.Type {
				case watch.EventError:
					return event.Err
				case watch.EventPut:
					if err := event.Unmarshal(&commitID, _commitInfo); err != nil {
						return fmt.Errorf("Unmarshal: %v", err)
					}
				case watch.EventDelete:
					return pfsserver.ErrCommitDeleted{commit}
				}
				if _commitInfo.Finished != nil {
					commitInfo = _commitInfo
					break
				}
			}
			return nil
		}(); err != nil {
			return nil, err
		}
	}
	return commitInfo, nil
}
func (d *driver) scratchCommitPrefix(commit *pfs.Commit) string {
	// TODO(msteffen) this doesn't currenty (2018-2-4) use d.scratchPrefix(),
	// but probably should? If this is changed, filepathFromEtcdPath will also
	// need to change.
	return path.Join(commit.Repo.Name, commit.ID)
}
func (d *driver) scratchFilePrefix(file *pfs.File) (string, error) {
	return path.Join(d.scratchCommitPrefix(file.Commit), file.Path), nil
}
func (d *driver) getTreeForFile(pachClient *client.APIClient, file *pfs.File) (hashtree.HashTree, error) {
	if file.Commit == nil {
		t, err := hashtree.NewDBHashTree(d.storageRoot)
		if err != nil {
			return nil, err
		}
		return t, nil
	}
	commitInfo, err := d.inspectCommit(pachClient, file.Commit, pfs.CommitState_STARTED)
	if err != nil {
		return nil, err
	}
	if commitInfo.Finished != nil {
		tree, err := d.getTreeForCommit(pachClient, file.Commit)
		if err != nil {
			return nil, err
		}
		return tree, nil
	}
	parentTree, err := d.getTreeForCommit(pachClient, commitInfo.ParentCommit)
	if err != nil {
		return nil, err
	}
	return d.getTreeForOpenCommit(pachClient, file, parentTree)
}
func provenantOnInput(provenance []*pfs.CommitProvenance) bool {
	provenanceCount := len(provenance)
	for _, p := range provenance {
		// in particular, we want to exclude provenance on the spec repo (used e.g. for spouts)
		if p.Commit.Repo.Name == ppsconsts.SpecRepo {
			provenanceCount--
			break
		}
	}
	return provenanceCount > 0
}
func nodeToFileInfo(ci *pfs.CommitInfo, path string, node *hashtree.NodeProto, full bool) *pfs.FileInfo {
	fileInfo := &pfs.FileInfo{
		File: &pfs.File{
			Commit: ci.Commit,
			Path:   path,
		},
		SizeBytes: uint64(node.SubtreeSize),
		Hash:      node.Hash,
		Committed: ci.Finished,
	}
	if node.FileNode != nil {
		fileInfo.FileType = pfs.FileType_FILE
		if full {
			fileInfo.Objects = node.FileNode.Objects
			fileInfo.BlockRefs = node.FileNode.BlockRefs
		}
	} else if node.DirNode != nil {
		fileInfo.FileType = pfs.FileType_DIR
		if full {
			fileInfo.Children = node.DirNode.Children
		}
	}
	return fileInfo
}
func (d *driver) fileHistory(pachClient *client.APIClient, file *pfs.File, history int64, f func(*pfs.FileInfo) error) error {
	var fi *pfs.FileInfo
	for {
		_fi, err := d.inspectFile(pachClient, file)
		if err != nil {
			if _, ok := err.(pfsserver.ErrFileNotFound); ok {
				return f(fi)
			}
			return err
		}
		if fi != nil && bytes.Compare(fi.Hash, _fi.Hash) != 0 {
			if err := f(fi); err != nil {
				return err
			}
			if history > 0 {
				history--
				if history == 0 {
					return nil
				}
			}
		}
		fi = _fi
		ci, err := d.inspectCommit(pachClient, file.Commit, pfs.CommitState_STARTED)
		if err != nil {
			return err
		}
		if ci.ParentCommit == nil {
			return f(fi)
		}
		file.Commit = ci.ParentCommit
	}
}
func (d *driver) upsertPutFileRecords(pachClient *client.APIClient, file *pfs.File, newRecords *pfs.PutFileRecords) error {
	prefix, err := d.scratchFilePrefix(file)
	if err != nil {
		return err
	}

	ctx := pachClient.Ctx()
	_, err = col.NewSTM(ctx, d.etcdClient, func(stm col.STM) error {
		commitsCol := d.openCommits.ReadOnly(ctx)
		var commit pfs.Commit
		err := commitsCol.Get(file.Commit.ID, &commit)
		if err != nil {
			return err
		}
		// Dumb check to make sure the unmarshalled value exists (and matches the current ID)
		// to denote that the current commit is indeed open
		if commit.ID != file.Commit.ID {
			return fmt.Errorf("commit %v is not open", file.Commit.ID)
		}
		recordsCol := d.putFileRecords.ReadWrite(stm)
		var existingRecords pfs.PutFileRecords
		return recordsCol.Upsert(prefix, &existingRecords, func() error {
			if newRecords.Tombstone {
				existingRecords.Tombstone = true
				existingRecords.Records = nil
			}
			existingRecords.Split = newRecords.Split
			existingRecords.Records = append(existingRecords.Records, newRecords.Records...)
			existingRecords.Header = newRecords.Header
			existingRecords.Footer = newRecords.Footer
			return nil
		})
	})
	if err != nil {
		return err
	}

	return err
}
func (r *PGDumpReader) ReadRow() ([]byte, error) {
	if len(r.Header) == 0 {
		err := r.readHeader()
		if err != nil {
			return nil, err
		}
	}
	endLine := "\\.\n" // Trailing '\.' denotes the end of the row inserts
	row, err := r.rd.ReadBytes('\n')
	if err != nil && err != io.EOF {
		return nil, fmt.Errorf("error reading pgdump row: %v", err)
	}
	// corner case: some pgdump files separate lines with \r\n (even on linux),
	// so clean this case up so all handling below is unified
	if len(row) >= 2 && row[len(row)-2] == '\r' {
		row[len(row)-2] = '\n'
		row = row[:len(row)-1]
	}
	if string(row) == endLine {
		r.Footer = append(r.Footer, row...)
		err = r.readFooter()
		row = nil // The endline is part of the footer
	}
	if err == io.EOF && len(r.Footer) == 0 {
		return nil, fmt.Errorf("invalid pgdump - missing footer")
	}
	return row, err
}
func NewReporter(clusterID string, kubeClient *kube.Clientset) *Reporter {
	reporter := &Reporter{
		segmentClient: newPersistentClient(),
		clusterID:     clusterID,
		kubeClient:    kubeClient,
	}
	go reporter.reportClusterMetrics()
	return reporter
}
func ReportUserAction(ctx context.Context, r *Reporter, action string) func(time.Time, error) {
	if r == nil {
		// This happens when stubbing out metrics for testing, e.g. src/server/pfs/server/server_test.go
		return func(time.Time, error) {}
	}
	// If we report nil, segment sees it, but mixpanel omits the field
	r.reportUserAction(ctx, fmt.Sprintf("%vStarted", action), 1)
	return func(start time.Time, err error) {
		if err == nil {
			r.reportUserAction(ctx, fmt.Sprintf("%vFinished", action), time.Since(start).Seconds())
		} else {
			r.reportUserAction(ctx, fmt.Sprintf("%vErrored", action), err.Error())
		}
	}
}
func FinishReportAndFlushUserAction(action string, err error, start time.Time) func() {
	var wait func()
	if err != nil {
		wait = reportAndFlushUserAction(fmt.Sprintf("%vErrored", action), err)
	} else {
		wait = reportAndFlushUserAction(fmt.Sprintf("%vFinished", action), time.Since(start).Seconds())
	}
	return wait
}
func (r *Reader) Read(data []byte) (int, error) {
	var totalRead int
	for len(data) > 0 {
		n, err := r.r.Read(data)
		data = data[n:]
		totalRead += n
		if err != nil {
			// If all DataRefs have been read, then io.EOF.
			if len(r.dataRefs) == 0 {
				return totalRead, io.EOF
			}
			// Get next chunk if necessary.
			if r.curr == nil || r.curr.Chunk.Hash != r.dataRefs[0].Chunk.Hash {
				if err := r.readChunk(r.dataRefs[0].Chunk); err != nil {
					return totalRead, err
				}
			}
			r.curr = r.dataRefs[0]
			r.dataRefs = r.dataRefs[1:]
			r.r = bytes.NewReader(r.buf.Bytes()[r.curr.OffsetBytes : r.curr.OffsetBytes+r.curr.SizeBytes])
		}
	}
	return totalRead, nil

}
func ActivateCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	var expires string
	activate := &cobra.Command{
		Use: "{{alias}} <activation-code>",
		Short: "Activate the enterprise features of Pachyderm with an activation " +
			"code",
		Long: "Activate the enterprise features of Pachyderm with an activation " +
			"code",
		Run: cmdutil.RunFixedArgs(1, func(args []string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %s", err.Error())
			}
			defer c.Close()
			req := &enterprise.ActivateRequest{}
			req.ActivationCode = args[0]
			if expires != "" {
				t, err := parseISO8601(expires)
				if err != nil {
					return fmt.Errorf("could not parse the timestamp \"%s\": %s", expires, err.Error())
				}
				req.Expires, err = types.TimestampProto(t)
				if err != nil {
					return fmt.Errorf("error converting expiration time \"%s\"; %s", t.String(), err.Error())
				}
			}
			resp, err := c.Enterprise.Activate(c.Ctx(), req)
			if err != nil {
				return err
			}
			ts, err := types.TimestampFromProto(resp.Info.Expires)
			if err != nil {
				return fmt.Errorf("Activation request succeeded, but could not "+
					"convert token expiration time to a timestamp: %s", err.Error())
			}
			fmt.Printf("Activation succeeded. Your Pachyderm Enterprise token "+
				"expires %s\n", ts.String())
			return nil
		}),
	}
	activate.PersistentFlags().StringVar(&expires, "expires", "", "A timestamp "+
		"indicating when the token provided above should expire (formatted as an "+
		"RFC 3339/ISO 8601 datetime). This is only applied if it's earlier than "+
		"the signed expiration time encoded in 'activation-code', and therefore "+
		"is only useful for testing.")

	return cmdutil.CreateAlias(activate, "enterprise activate")
}
func GetStateCmd(noMetrics, noPortForwarding *bool) *cobra.Command {
	getState := &cobra.Command{
		Short: "Check whether the Pachyderm cluster has enterprise features " +
			"activated",
		Long: "Check whether the Pachyderm cluster has enterprise features " +
			"activated",
		Run: cmdutil.Run(func(args []string) error {
			c, err := client.NewOnUserMachine(!*noMetrics, !*noPortForwarding, "user")
			if err != nil {
				return fmt.Errorf("could not connect: %s", err.Error())
			}
			defer c.Close()
			resp, err := c.Enterprise.GetState(c.Ctx(), &enterprise.GetStateRequest{})
			if err != nil {
				return err
			}
			if resp.State == enterprise.State_NONE {
				fmt.Println("No Pachyderm Enterprise token was found")
				return nil
			}
			ts, err := types.TimestampFromProto(resp.Info.Expires)
			if err != nil {
				return fmt.Errorf("Activation request succeeded, but could not "+
					"convert token expiration time to a timestamp: %s", err.Error())
			}
			fmt.Printf("Pachyderm Enterprise token state: %s\nExpiration: %s\n",
				resp.State.String(), ts.String())
			return nil
		}),
	}
	return cmdutil.CreateAlias(getState, "enterprise get-state")
}
func Cmds(noMetrics, noPortForwarding *bool) []*cobra.Command {
	var commands []*cobra.Command

	enterprise := &cobra.Command{
		Short: "Enterprise commands enable Pachyderm Enterprise features",
		Long:  "Enterprise commands enable Pachyderm Enterprise features",
	}
	commands = append(commands, cmdutil.CreateAlias(enterprise, "enterprise"))

	commands = append(commands, ActivateCmd(noMetrics, noPortForwarding))
	commands = append(commands, GetStateCmd(noMetrics, noPortForwarding))

	return commands
}
func NewConfiguration(config interface{}) *Configuration {
	configuration := &Configuration{}
	switch config.(type) {
	case *GlobalConfiguration:
		configuration.GlobalConfiguration = config.(*GlobalConfiguration)
		return configuration
	case *PachdFullConfiguration:
		configuration.GlobalConfiguration = &config.(*PachdFullConfiguration).GlobalConfiguration
		configuration.PachdSpecificConfiguration = &config.(*PachdFullConfiguration).PachdSpecificConfiguration
		return configuration
	case *WorkerFullConfiguration:
		configuration.GlobalConfiguration = &config.(*WorkerFullConfiguration).GlobalConfiguration
		configuration.WorkerSpecificConfiguration = &config.(*WorkerFullConfiguration).WorkerSpecificConfiguration
		return configuration
	default:
		return nil
	}
}
func Repos(etcdClient *etcd.Client, etcdPrefix string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, reposPrefix),
		nil,
		&pfs.RepoInfo{},
		nil,
		nil,
	)
}
func PutFileRecords(etcdClient *etcd.Client, etcdPrefix string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, putFileRecordsPrefix),
		nil,
		&pfs.PutFileRecords{},
		nil,
		nil,
	)
}
func Commits(etcdClient *etcd.Client, etcdPrefix string, repo string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, commitsPrefix, repo),
		[]*col.Index{ProvenanceIndex},
		&pfs.CommitInfo{},
		nil,
		nil,
	)
}
func Branches(etcdClient *etcd.Client, etcdPrefix string, repo string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, branchesPrefix, repo),
		nil,
		&pfs.BranchInfo{},
		func(key string) error {
			if uuid.IsUUIDWithoutDashes(key) {
				return fmt.Errorf("branch name cannot be a UUID V4")
			}
			return nil
		},
		nil,
	)
}
func OpenCommits(etcdClient *etcd.Client, etcdPrefix string) col.Collection {
	return col.NewCollection(
		etcdClient,
		path.Join(etcdPrefix, openCommitsPrefix),
		nil,
		&pfs.Commit{},
		nil,
		nil,
	)
}
func NewDAG(nodes map[string][]string) *DAG {
	result := &DAG{
		parents:  make(map[string][]string),
		children: make(map[string][]string),
		leaves:   make(map[string]bool),
	}
	for id, parents := range nodes {
		result.NewNode(id, parents)
	}
	return result
}
func (d *DAG) NewNode(id string, parents []string) {
	d.parents[id] = parents
	for _, parentID := range parents {
		d.children[parentID] = append(d.children[parentID], id)
		d.leaves[parentID] = false
	}
	if _, ok := d.leaves[id]; !ok {
		d.leaves[id] = true
	}
}
func (d *DAG) Sorted() []string {
	seen := make(map[string]bool)
	var result []string
	for id := range d.parents {
		result = append(result, dfs(id, d.parents, seen)...)
	}
	return result
}
func (d *DAG) Leaves() []string {
	var result []string
	for id, isLeaf := range d.leaves {
		// isLeaf might be false, explicit mark nodes as non leaves
		if isLeaf {
			result = append(result, id)
		}
	}
	return result
}
func (d *DAG) Ancestors(id string, from []string) []string {
	seen := make(map[string]bool)
	for _, fromID := range from {
		seen[fromID] = true
	}
	return dfs(id, d.parents, seen)
}
func (d *DAG) Descendants(id string, to []string) []string {
	seen := make(map[string]bool)
	for _, toID := range to {
		seen[toID] = true
	}
	return bfs(id, d.children, seen)
}
func (d *DAG) Ghosts() []string {
	var result []string
	for id := range d.children {
		if _, ok := d.parents[id]; !ok {
			result = append(result, id)
		}
	}
	return result
}
func NewPortForwarder(namespace string) (*PortForwarder, error) {
	if namespace == "" {
		namespace = "default"
	}

	rules := clientcmd.NewDefaultClientConfigLoadingRules()
	overrides := &clientcmd.ConfigOverrides{}
	kubeConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(rules, overrides)
	config, err := kubeConfig.ClientConfig()
	if err != nil {
		return nil, err
	}

	client, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, err
	}

	core := client.CoreV1()

	return &PortForwarder{
		core:          core,
		client:        core.RESTClient(),
		config:        config,
		namespace:     namespace,
		logger:        log.StandardLogger().Writer(),
		stopChansLock: &sync.Mutex{},
		stopChans:     []chan struct{}{},
		shutdown:      false,
	}, nil
}
func (f *PortForwarder) Run(appName string, localPort, remotePort uint16) error {
	podNameSelector := map[string]string{
		"suite": "pachyderm",
		"app":   appName,
	}

	podList, err := f.core.Pods(f.namespace).List(metav1.ListOptions{
		LabelSelector: metav1.FormatLabelSelector(metav1.SetAsLabelSelector(podNameSelector)),
		TypeMeta: metav1.TypeMeta{
			Kind:       "ListOptions",
			APIVersion: "v1",
		},
	})
	if err != nil {
		return err
	}
	if len(podList.Items) == 0 {
		return fmt.Errorf("No pods found for app %s", appName)
	}

	// Choose a random pod
	podName := podList.Items[rand.Intn(len(podList.Items))].Name

	url := f.client.Post().
		Resource("pods").
		Namespace(f.namespace).
		Name(podName).
		SubResource("portforward").
		URL()

	transport, upgrader, err := spdy.RoundTripperFor(f.config)
	if err != nil {
		return err
	}

	dialer := spdy.NewDialer(upgrader, &http.Client{Transport: transport}, "POST", url)
	ports := []string{fmt.Sprintf("%d:%d", localPort, remotePort)}
	readyChan := make(chan struct{}, 1)
	stopChan := make(chan struct{}, 1)

	// Ensure that the port forwarder isn't already shutdown, and append the
	// shutdown channel so this forwarder can be closed
	f.stopChansLock.Lock()
	if f.shutdown {
		f.stopChansLock.Unlock()
		return fmt.Errorf("port forwarder is shutdown")
	}
	f.stopChans = append(f.stopChans, stopChan)
	f.stopChansLock.Unlock()

	fw, err := portforward.New(dialer, ports, stopChan, readyChan, ioutil.Discard, f.logger)
	if err != nil {
		return err
	}

	errChan := make(chan error, 1)
	go func() { errChan <- fw.ForwardPorts() }()

	select {
	case err = <-errChan:
		return fmt.Errorf("port forwarding failed: %v", err)
	case <-fw.Ready:
		return nil
	}
}
func (f *PortForwarder) RunForDaemon(localPort, remotePort uint16) error {
	if localPort == 0 {
		localPort = pachdLocalPort
	}
	if remotePort == 0 {
		remotePort = pachdRemotePort
	}
	return f.Run("pachd", localPort, remotePort)
}
func (f *PortForwarder) RunForSAMLACS(localPort uint16) error {
	if localPort == 0 {
		localPort = samlAcsLocalPort
	}
	return f.Run("pachd", localPort, 654)
}
func (f *PortForwarder) RunForDashUI(localPort uint16) error {
	if localPort == 0 {
		localPort = dashUILocalPort
	}
	return f.Run("dash", localPort, 8080)
}
func (f *PortForwarder) RunForDashWebSocket(localPort uint16) error {
	if localPort == 0 {
		localPort = dashWebSocketLocalPort
	}
	return f.Run("dash", localPort, 8081)
}
func (f *PortForwarder) RunForPFS(localPort uint16) error {
	if localPort == 0 {
		localPort = pfsLocalPort
	}
	return f.Run("pachd", localPort, 30652)
}
func (f *PortForwarder) RunForS3Gateway(localPort uint16) error {
	if localPort == 0 {
		localPort = s3gatewayLocalPort
	}
	return f.Run("pachd", localPort, 600)
}
func (f *PortForwarder) Lock() error {
	pidfile.SetPidfilePath(path.Join(os.Getenv("HOME"), ".pachyderm/port-forward.pid"))
	return pidfile.Write()
}
func (f *PortForwarder) Close() {
	defer f.logger.Close()

	f.stopChansLock.Lock()
	defer f.stopChansLock.Unlock()

	if f.shutdown {
		panic("port forwarder already shutdown")
	}

	f.shutdown = true

	for _, stopChan := range f.stopChans {
		close(stopChan)
	}
}
func (e *Event) Unmarshal(key *string, val proto.Message) error {
	if err := CheckType(e.Template, val); err != nil {
		return err
	}
	*key = string(e.Key)
	return proto.Unmarshal(e.Value, val)
}
func (e *Event) UnmarshalPrev(key *string, val proto.Message) error {
	if err := CheckType(e.Template, val); err != nil {
		return err
	}
	*key = string(e.PrevKey)
	return proto.Unmarshal(e.PrevValue, val)
}
func MakeWatcher(eventCh chan *Event, done chan struct{}) Watcher {
	return &watcher{
		eventCh: eventCh,
		done:    done,
	}
}
func CheckType(template proto.Message, val interface{}) error {
	if template != nil {
		valType, templateType := reflect.TypeOf(val), reflect.TypeOf(template)
		if valType != templateType {
			return fmt.Errorf("invalid type, got: %s, expected: %s", valType, templateType)
		}
	}
	return nil
}
func NewPool(kubeClient *kube.Clientset, namespace string, serviceName string, port int, queueSize int64, opts ...grpc.DialOption) (*Pool, error) {
	endpointsInterface := kubeClient.CoreV1().Endpoints(namespace)

	watch, err := endpointsInterface.Watch(metav1.ListOptions{
		LabelSelector: metav1.FormatLabelSelector(metav1.SetAsLabelSelector(
			map[string]string{"app": serviceName},
		)),
		Watch: true,
	})
	if err != nil {
		return nil, err
	}

	pool := &Pool{
		port:           port,
		endpointsWatch: watch,
		opts:           opts,
		done:           make(chan struct{}),
		queueSize:      queueSize,
	}
	pool.connsCond = sync.NewCond(&pool.connsLock)
	go pool.watchEndpoints()
	return pool, nil
}
func (p *Pool) Do(ctx context.Context, f func(cc *grpc.ClientConn) error) error {
	var conn *connCount
	if err := func() error {
		p.connsLock.Lock()
		defer p.connsLock.Unlock()
		for {
			for addr, mapConn := range p.conns {
				if mapConn.cc == nil {
					cc, err := grpc.DialContext(ctx, addr, p.opts...)
					if err != nil {
						return fmt.Errorf("failed to connect to %s: %+v", addr, err)
					}
					mapConn.cc = cc
					conn = mapConn
					// We break because this conn has a count of 0 which we know
					// we're not beating
					break
				} else {
					mapConnCount := atomic.LoadInt64(&mapConn.count)
					if mapConnCount < p.queueSize && (conn == nil || mapConnCount < atomic.LoadInt64(&conn.count)) {
						conn = mapConn
					}
				}
			}
			if conn == nil {
				p.connsCond.Wait()
			} else {
				atomic.AddInt64(&conn.count, 1)
				break
			}
		}
		return nil
	}(); err != nil {
		return err
	}
	defer p.connsCond.Broadcast()
	defer atomic.AddInt64(&conn.count, -1)
	return f(conn.cc)
}
func (p *Pool) Close() error {
	close(p.done)
	var retErr error
	for _, conn := range p.conns {
		if conn.cc != nil {
			if err := conn.cc.Close(); err != nil {
				retErr = err
			}
		}
	}
	return retErr
}
func buildImage(client *docker.Client, repo string, contextDir string, dockerfile string, destTag string) error {
	destImage := fmt.Sprintf("%s:%s", repo, destTag)

	fmt.Printf("Building %s, this may take a while.\n", destImage)

	err := client.BuildImage(docker.BuildImageOptions{
		Name:         destImage,
		ContextDir:   contextDir,
		Dockerfile:   dockerfile,
		OutputStream: os.Stdout,
	})

	if err != nil {
		return fmt.Errorf("could not build docker image: %s", err)
	}

	return nil
}
func pushImage(client *docker.Client, authConfig docker.AuthConfiguration, repo string, sourceTag string, destTag string) (string, error) {
	sourceImage := fmt.Sprintf("%s:%s", repo, sourceTag)
	destImage := fmt.Sprintf("%s:%s", repo, destTag)

	fmt.Printf("Tagging/pushing %s, this may take a while.\n", destImage)

	if err := client.TagImage(sourceImage, docker.TagImageOptions{
		Repo:    repo,
		Tag:     destTag,
		Context: context.Background(),
	}); err != nil {
		err = fmt.Errorf("could not tag docker image: %s", err)
		return "", err
	}

	if err := client.PushImage(
		docker.PushImageOptions{
			Name: repo,
			Tag:  destTag,
		},
		authConfig,
	); err != nil {
		err = fmt.Errorf("could not push docker image: %s", err)
		return "", err
	}

	return destImage, nil
}
func newMinioClient(endpoint, bucket, id, secret string, secure bool) (*minioClient, error) {
	mclient, err := minio.New(endpoint, id, secret, secure)
	if err != nil {
		return nil, err
	}
	return &minioClient{
		bucket: bucket,
		Client: mclient,
	}, nil
}
func newMinioClientV2(endpoint, bucket, id, secret string, secure bool) (*minioClient, error) {
	mclient, err := minio.NewV2(endpoint, id, secret, secure)
	if err != nil {
		return nil, err
	}
	return &minioClient{
		bucket: bucket,
		Client: mclient,
	}, nil
}
func newMinioWriter(ctx context.Context, client *minioClient, name string) *minioWriter {
	reader, writer := io.Pipe()
	w := &minioWriter{
		ctx:     ctx,
		errChan: make(chan error),
		pipe:    writer,
	}
	go func() {
		_, err := client.PutObject(client.bucket, name, reader, "application/octet-stream")
		if err != nil {
			reader.CloseWithError(err)
		}
		w.errChan <- err
	}()
	return w
}
func (w *minioWriter) Close() error {
	span, _ := tracing.AddSpanToAnyExisting(w.ctx, "minioWriter.Close")
	defer tracing.FinishAnySpan(span)
	if err := w.pipe.Close(); err != nil {
		return err
	}
	return <-w.errChan
}
func PipelineRepo(pipeline *ppsclient.Pipeline) *pfs.Repo {
	return &pfs.Repo{Name: pipeline.Name}
}
func PipelineRcName(name string, version uint64) string {
	// k8s won't allow RC names that contain upper-case letters
	// or underscores
	// TODO: deal with name collision
	name = strings.Replace(name, "_", "-", -1)
	return fmt.Sprintf("pipeline-%s-v%d", strings.ToLower(name), version)
}
func GetRequestsResourceListFromPipeline(pipelineInfo *pps.PipelineInfo) (*v1.ResourceList, error) {
	return getResourceListFromSpec(pipelineInfo.ResourceRequests, pipelineInfo.CacheSize)
}
func GetLimitsResourceListFromPipeline(pipelineInfo *pps.PipelineInfo) (*v1.ResourceList, error) {
	return getResourceListFromSpec(pipelineInfo.ResourceLimits, pipelineInfo.CacheSize)
}
func getNumNodes(kubeClient *kube.Clientset) (int, error) {
	nodeList, err := kubeClient.CoreV1().Nodes().List(metav1.ListOptions{})
	if err != nil {
		return 0, fmt.Errorf("unable to retrieve node list from k8s to determine parallelism: %v", err)
	}
	if len(nodeList.Items) == 0 {
		return 0, fmt.Errorf("pachyderm.pps.jobserver: no k8s nodes found")
	}
	return len(nodeList.Items), nil
}
func GetExpectedNumWorkers(kubeClient *kube.Clientset, spec *ppsclient.ParallelismSpec) (int, error) {
	if spec == nil || (spec.Constant == 0 && spec.Coefficient == 0) {
		return 1, nil
	} else if spec.Constant > 0 && spec.Coefficient == 0 {
		return int(spec.Constant), nil
	} else if spec.Constant == 0 && spec.Coefficient > 0 {
		// Start ('coefficient' * 'nodes') workers. Determine number of workers
		numNodes, err := getNumNodes(kubeClient)
		if err != nil {
			return 0, err
		}
		result := math.Floor(spec.Coefficient * float64(numNodes))
		return int(math.Max(result, 1)), nil
	}
	return 0, fmt.Errorf("Unable to interpret ParallelismSpec %+v", spec)
}
func GetExpectedNumHashtrees(spec *ppsclient.HashtreeSpec) (int64, error) {
	if spec == nil || spec.Constant == 0 {
		return 1, nil
	} else if spec.Constant > 0 {
		return int64(spec.Constant), nil
	}
	return 0, fmt.Errorf("unable to interpret HashtreeSpec %+v", spec)
}
func FailPipeline(ctx context.Context, etcdClient *etcd.Client, pipelinesCollection col.Collection, pipelineName string, reason string) error {
	_, err := col.NewSTM(ctx, etcdClient, func(stm col.STM) error {
		pipelines := pipelinesCollection.ReadWrite(stm)
		pipelinePtr := new(pps.EtcdPipelineInfo)
		if err := pipelines.Get(pipelineName, pipelinePtr); err != nil {
			return err
		}
		pipelinePtr.State = pps.PipelineState_PIPELINE_FAILURE
		pipelinePtr.Reason = reason
		pipelines.Put(pipelineName, pipelinePtr)
		return nil
	})
	return err
}
func JobInput(pipelineInfo *pps.PipelineInfo, outputCommitInfo *pfs.CommitInfo) *pps.Input {
	// branchToCommit maps strings of the form "<repo>/<branch>" to PFS commits
	branchToCommit := make(map[string]*pfs.Commit)
	key := path.Join
	for _, prov := range outputCommitInfo.Provenance {
		branchToCommit[key(prov.Commit.Repo.Name, prov.Branch.Name)] = prov.Commit
	}
	jobInput := proto.Clone(pipelineInfo.Input).(*pps.Input)
	pps.VisitInput(jobInput, func(input *pps.Input) {
		if input.Pfs != nil {
			if commit, ok := branchToCommit[key(input.Pfs.Repo, input.Pfs.Branch)]; ok {
				input.Pfs.Commit = commit.ID
			}
		}
		if input.Cron != nil {
			if commit, ok := branchToCommit[key(input.Cron.Repo, "master")]; ok {
				input.Cron.Commit = commit.ID
			}
		}
		if input.Git != nil {
			if commit, ok := branchToCommit[key(input.Git.Name, input.Git.Branch)]; ok {
				input.Git.Commit = commit.ID
			}
		}
	})
	return jobInput
}
func PipelineReqFromInfo(pipelineInfo *ppsclient.PipelineInfo) *ppsclient.CreatePipelineRequest {
	return &ppsclient.CreatePipelineRequest{
		Pipeline:           pipelineInfo.Pipeline,
		Transform:          pipelineInfo.Transform,
		ParallelismSpec:    pipelineInfo.ParallelismSpec,
		HashtreeSpec:       pipelineInfo.HashtreeSpec,
		Egress:             pipelineInfo.Egress,
		OutputBranch:       pipelineInfo.OutputBranch,
		ScaleDownThreshold: pipelineInfo.ScaleDownThreshold,
		ResourceRequests:   pipelineInfo.ResourceRequests,
		ResourceLimits:     pipelineInfo.ResourceLimits,
		Input:              pipelineInfo.Input,
		Description:        pipelineInfo.Description,
		CacheSize:          pipelineInfo.CacheSize,
		EnableStats:        pipelineInfo.EnableStats,
		Batch:              pipelineInfo.Batch,
		MaxQueueSize:       pipelineInfo.MaxQueueSize,
		Service:            pipelineInfo.Service,
		ChunkSpec:          pipelineInfo.ChunkSpec,
		DatumTimeout:       pipelineInfo.DatumTimeout,
		JobTimeout:         pipelineInfo.JobTimeout,
		Salt:               pipelineInfo.Salt,
	}
}
func NewPipelineManifestReader(path string) (result *PipelineManifestReader, retErr error) {
	result = &PipelineManifestReader{}
	var pipelineReader io.Reader
	if path == "-" {
		pipelineReader = io.TeeReader(os.Stdin, &result.buf)
		fmt.Print("Reading from stdin.\n")
	} else if url, err := url.Parse(path); err == nil && url.Scheme != "" {
		resp, err := http.Get(url.String())
		if err != nil {
			return nil, err
		}
		defer func() {
			if err := resp.Body.Close(); err != nil && retErr == nil {
				retErr = err
			}
		}()
		rawBytes, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			return nil, err
		}
		pipelineReader = io.TeeReader(strings.NewReader(string(rawBytes)), &result.buf)
	} else {
		rawBytes, err := ioutil.ReadFile(path)
		if err != nil {
			return nil, err
		}

		pipelineReader = io.TeeReader(strings.NewReader(string(rawBytes)), &result.buf)
	}
	result.decoder = json.NewDecoder(pipelineReader)
	return result, nil
}
func (r *PipelineManifestReader) NextCreatePipelineRequest() (*ppsclient.CreatePipelineRequest, error) {
	var result ppsclient.CreatePipelineRequest
	if err := jsonpb.UnmarshalNext(r.decoder, &result); err != nil {
		if err == io.EOF {
			return nil, err
		}
		return nil, fmt.Errorf("malformed pipeline spec: %s", err)
	}
	return &result, nil
}
func DescribeSyntaxError(originalErr error, parsedBuffer bytes.Buffer) error {

	sErr, ok := originalErr.(*json.SyntaxError)
	if !ok {
		return originalErr
	}

	buffer := make([]byte, sErr.Offset)
	parsedBuffer.Read(buffer)

	lineOffset := strings.LastIndex(string(buffer[:len(buffer)-1]), "\n")
	if lineOffset == -1 {
		lineOffset = 0
	}

	lines := strings.Split(string(buffer[:len(buffer)-1]), "\n")
	lineNumber := len(lines)

	descriptiveErrorString := fmt.Sprintf("Syntax Error on line %v:\n%v\n%v^\n%v\n",
		lineNumber,
		string(buffer[lineOffset:]),
		strings.Repeat(" ", int(sErr.Offset)-2-lineOffset),
		originalErr,
	)

	return errors.New(descriptiveErrorString)
}
func UpdateJobState(pipelines col.ReadWriteCollection, jobs col.ReadWriteCollection, jobPtr *pps.EtcdJobInfo, state pps.JobState, reason string) error {
	// Update pipeline
	pipelinePtr := &pps.EtcdPipelineInfo{}
	if err := pipelines.Get(jobPtr.Pipeline.Name, pipelinePtr); err != nil {
		return err
	}
	if pipelinePtr.JobCounts == nil {
		pipelinePtr.JobCounts = make(map[int32]int32)
	}
	if pipelinePtr.JobCounts[int32(jobPtr.State)] != 0 {
		pipelinePtr.JobCounts[int32(jobPtr.State)]--
	}
	pipelinePtr.JobCounts[int32(state)]++
	pipelinePtr.LastJobState = state
	if err := pipelines.Put(jobPtr.Pipeline.Name, pipelinePtr); err != nil {
		return err
	}

	// Update job info
	var err error
	if state == pps.JobState_JOB_STARTING {
		jobPtr.Started, err = types.TimestampProto(time.Now())
	} else if IsTerminal(state) {
		jobPtr.Finished, err = types.TimestampProto(time.Now())
	}
	if err != nil {
		return err
	}
	jobPtr.State = state
	jobPtr.Reason = reason
	return jobs.Put(jobPtr.Job.ID, jobPtr)
}
func New() string {
	var result string
	backoff.RetryNotify(func() error {
		uuid, err := uuid.NewV4()
		if err != nil {
			return err
		}
		result = uuid.String()
		return nil
	}, backoff.NewInfiniteBackOff(), func(err error, d time.Duration) error {
		fmt.Printf("error from uuid.NewV4: %v", err)
		return nil
	})
	return result
}
func (h *HTTPError) Code() int {
	if h == nil {
		return http.StatusOK
	}
	return h.code
}
func NewHTTPError(code int, formatStr string, args ...interface{}) *HTTPError {
	return &HTTPError{
		code: code,
		err:  fmt.Sprintf(formatStr, args...),
	}
}
func NewStorage(objC obj.Client, prefix string) *Storage {
	return &Storage{
		objC:   objC,
		prefix: prefix,
	}
}
func (s *Storage) DeleteAll(ctx context.Context) error {
	return s.objC.Walk(ctx, s.prefix, func(hash string) error {
		return s.objC.Delete(ctx, hash)
	})
}
func Chunk(data []byte, chunkSize int) [][]byte {
	var result [][]byte
	for i := 0; i < len(data); i += chunkSize {
		end := i + chunkSize
		if end > len(data) {
			end = len(data)
		}
		result = append(result, data[i:end])
	}
	return result
}
func ChunkReader(r io.Reader, f func([]byte) error) (int, error) {
	var total int
	buf := GetBuffer()
	defer PutBuffer(buf)
	for {
		n, err := r.Read(buf)
		if n == 0 && err != nil {
			if err == io.EOF {
				return total, nil
			}
			return total, err
		}
		if err := f(buf[:n]); err != nil {
			return total, err
		}
		total += n
	}
}
func NewStreamingBytesReader(streamingBytesClient StreamingBytesClient, cancel context.CancelFunc) io.ReadCloser {
	return &streamingBytesReader{streamingBytesClient: streamingBytesClient, cancel: cancel}
}
func WriteToStreamingBytesServer(reader io.Reader, streamingBytesServer StreamingBytesServer) error {
	buf := GetBuffer()
	defer PutBuffer(buf)
	_, err := io.CopyBuffer(NewStreamingBytesWriter(streamingBytesServer), ReaderWrapper{reader}, buf)
	return err
}
func WriteFromStreamingBytesClient(streamingBytesClient StreamingBytesClient, writer io.Writer) error {
	for bytesValue, err := streamingBytesClient.Recv(); err != io.EOF; bytesValue, err = streamingBytesClient.Recv() {
		if err != nil {
			return err
		}
		if _, err = writer.Write(bytesValue.Value); err != nil {
			return err
		}
	}
	return nil
}
func NewSidecarAPIServer(
	env *serviceenv.ServiceEnv,
	etcdPrefix string,
	iamRole string,
	reporter *metrics.Reporter,
	workerGrpcPort uint16,
	pprofPort uint16,
	httpPort uint16,
	peerPort uint16,
) (ppsclient.APIServer, error) {
	apiServer := &apiServer{
		Logger:         log.NewLogger("pps.API"),
		env:            env,
		etcdPrefix:     etcdPrefix,
		iamRole:        iamRole,
		reporter:       reporter,
		workerUsesRoot: true,
		pipelines:      ppsdb.Pipelines(env.GetEtcdClient(), etcdPrefix),
		jobs:           ppsdb.Jobs(env.GetEtcdClient(), etcdPrefix),
		workerGrpcPort: workerGrpcPort,
		pprofPort:      pprofPort,
		httpPort:       httpPort,
		peerPort:       peerPort,
	}
	return apiServer, nil
}
func NewEnterpriseServer(env *serviceenv.ServiceEnv, etcdPrefix string) (ec.APIServer, error) {
	s := &apiServer{
		pachLogger: log.NewLogger("enterprise.API"),
		env:        env,
		enterpriseToken: col.NewCollection(
			env.GetEtcdClient(),
			etcdPrefix, // only one collection--no extra prefix needed
			nil,
			&ec.EnterpriseRecord{},
			nil,
			nil,
		),
	}
	s.enterpriseExpiration.Store(time.Time{})
	go s.watchEnterpriseToken(etcdPrefix)
	return s, nil
}
func validateActivationCode(code string) (expiration time.Time, err error) {
	// Parse the public key.  If these steps fail, something is seriously
	// wrong and we should crash the service by panicking.
	block, _ := pem.Decode([]byte(publicKey))
	if block == nil {
		return time.Time{}, fmt.Errorf("failed to pem decode public key")
	}
	pub, err := x509.ParsePKIXPublicKey(block.Bytes)
	if err != nil {
		return time.Time{}, fmt.Errorf("failed to parse DER encoded public key: %s", err.Error())
	}
	rsaPub, ok := pub.(*rsa.PublicKey)
	if !ok {
		return time.Time{}, fmt.Errorf("public key isn't an RSA key")
	}

	// Decode the base64-encoded activation code
	decodedActivationCode, err := base64.StdEncoding.DecodeString(code)
	if err != nil {
		return time.Time{}, fmt.Errorf("activation code is not base64 encoded")
	}
	activationCode := &activationCode{}
	if err := json.Unmarshal(decodedActivationCode, &activationCode); err != nil {
		return time.Time{}, fmt.Errorf("activation code is not valid JSON")
	}

	// Decode the signature
	decodedSignature, err := base64.StdEncoding.DecodeString(activationCode.Signature)
	if err != nil {
		return time.Time{}, fmt.Errorf("signature is not base64 encoded")
	}

	// Compute the sha256 checksum of the token
	hashedToken := sha256.Sum256([]byte(activationCode.Token))

	// Verify that the signature is valid
	if err := rsa.VerifyPKCS1v15(rsaPub, crypto.SHA256, hashedToken[:], decodedSignature); err != nil {
		return time.Time{}, fmt.Errorf("invalid signature in activation code")
	}

	// Unmarshal the token
	token := token{}
	if err := json.Unmarshal([]byte(activationCode.Token), &token); err != nil {
		return time.Time{}, fmt.Errorf("token is not valid JSON")
	}

	// Parse the expiration. Note that this string is generated by Date.toJSON()
	// running in node, so Go's definition of RFC 3339 timestamps (which is
	// incomplete) must be compatible with the strings that node generates. So far
	// it seems to work.
	expiration, err = time.Parse(time.RFC3339, token.Expiry)
	if err != nil {
		return time.Time{}, fmt.Errorf("expiration is not valid ISO 8601 string")
	}
	// Check that the activation code has not expired
	if time.Now().After(expiration) {
		return time.Time{}, fmt.Errorf("the activation code has expired")
	}
	return expiration, nil
}
func (a *apiServer) Activate(ctx context.Context, req *ec.ActivateRequest) (resp *ec.ActivateResponse, retErr error) {
	a.LogReq(req)
	defer func(start time.Time) { a.pachLogger.Log(req, resp, retErr, time.Since(start)) }(time.Now())

	// Validate the activation code
	expiration, err := validateActivationCode(req.ActivationCode)
	if err != nil {
		return nil, fmt.Errorf("error validating activation code: %s", err.Error())
	}
	// Allow request to override expiration in the activation code, for testing
	if req.Expires != nil {
		customExpiration, err := types.TimestampFromProto(req.Expires)
		if err == nil && expiration.After(customExpiration) {
			expiration = customExpiration
		}
	}
	expirationProto, err := types.TimestampProto(expiration)
	if err != nil {
		return nil, fmt.Errorf("could not convert expiration time \"%s\" to proto: %s", expiration.String(), err.Error())
	}
	if _, err := col.NewSTM(ctx, a.env.GetEtcdClient(), func(stm col.STM) error {
		e := a.enterpriseToken.ReadWrite(stm)
		// blind write
		return e.Put(enterpriseTokenKey, &ec.EnterpriseRecord{
			ActivationCode: req.ActivationCode,
			Expires:        expirationProto,
		})
	}); err != nil {
		return nil, err
	}

	// Wait until watcher observes the write
	if err := backoff.Retry(func() error {
		if t := a.enterpriseExpiration.Load().(time.Time); t.IsZero() {
			return fmt.Errorf("enterprise not activated")
		}
		return nil
	}, backoff.RetryEvery(time.Second)); err != nil {
		return nil, err
	}
	time.Sleep(time.Second) // give other pachd nodes time to observe the write

	return &ec.ActivateResponse{
		Info: &ec.TokenInfo{
			Expires: expirationProto,
		},
	}, nil
}
func (a *apiServer) Deactivate(ctx context.Context, req *ec.DeactivateRequest) (resp *ec.DeactivateResponse, retErr error) {
	a.LogReq(req)
	defer func(start time.Time) { a.pachLogger.Log(req, resp, retErr, time.Since(start)) }(time.Now())

	pachClient := a.env.GetPachClient(ctx)
	if err := pachClient.DeleteAll(); err != nil {
		return nil, fmt.Errorf("could not delete all pachyderm data: %v", err)
	}

	if _, err := col.NewSTM(ctx, a.env.GetEtcdClient(), func(stm col.STM) error {
		// blind delete
		return a.enterpriseToken.ReadWrite(stm).Delete(enterpriseTokenKey)
	}); err != nil {
		return nil, err
	}

	// Wait until watcher observes the write
	if err := backoff.Retry(func() error {
		if t := a.enterpriseExpiration.Load().(time.Time); !t.IsZero() {
			return fmt.Errorf("enterprise still activated")
		}
		return nil
	}, backoff.RetryEvery(time.Second)); err != nil {
		return nil, err
	}
	time.Sleep(time.Second) // give other pachd nodes time to observe the write

	return &ec.DeactivateResponse{}, nil
}
func lookExtensions(path, dir string) (string, error) {
	if filepath.Base(path) == path {
		path = filepath.Join(".", path)
	}
	if dir == "" {
		return exec.LookPath(path)
	}
	if filepath.VolumeName(path) != "" {
		return exec.LookPath(path)
	}
	if len(path) > 1 && os.IsPathSeparator(path[0]) {
		return exec.LookPath(path)
	}
	dirandpath := filepath.Join(dir, path)
	// We assume that LookPath will only add file extension.
	lp, err := exec.LookPath(dirandpath)
	if err != nil {
		return "", err
	}
	ext := strings.TrimPrefix(lp, dirandpath)
	return path + ext, nil
}
func (c *Cmd) Start() error {
	if c.lookPathErr != nil {
		c.closeDescriptors(c.closeAfterStart)
		c.closeDescriptors(c.closeAfterWait)
		return c.lookPathErr
	}
	if runtime.GOOS == "windows" {
		lp, err := lookExtensions(c.Path, c.Dir)
		if err != nil {
			c.closeDescriptors(c.closeAfterStart)
			c.closeDescriptors(c.closeAfterWait)
			return err
		}
		c.Path = lp
	}
	if c.Process != nil {
		return errors.New("exec: already started")
	}
	if c.ctx != nil {
		select {
		case <-c.ctx.Done():
			c.closeDescriptors(c.closeAfterStart)
			c.closeDescriptors(c.closeAfterWait)
			return c.ctx.Err()
		default:
		}
	}

	type F func(*Cmd) (*os.File, error)
	for _, setupFd := range []F{(*Cmd).stdin, (*Cmd).stdout, (*Cmd).stderr} {
		fd, err := setupFd(c)
		if err != nil {
			c.closeDescriptors(c.closeAfterStart)
			c.closeDescriptors(c.closeAfterWait)
			return err
		}
		c.childFiles = append(c.childFiles, fd)
	}
	c.childFiles = append(c.childFiles, c.ExtraFiles...)

	var err error
	c.Process, err = os.StartProcess(c.Path, c.argv(), &os.ProcAttr{
		Dir:   c.Dir,
		Files: c.childFiles,
		Env:   dedupEnv(c.envv()),
		Sys:   c.SysProcAttr,
	})
	if err != nil {
		c.closeDescriptors(c.closeAfterStart)
		c.closeDescriptors(c.closeAfterWait)
		return err
	}

	c.closeDescriptors(c.closeAfterStart)

	c.errch = make(chan error, len(c.goroutine))
	for _, fn := range c.goroutine {
		go func(fn func() error) {
			c.errch <- fn()
		}(fn)
	}

	if c.ctx != nil {
		c.waitDone = make(chan struct{})
		go func() {
			select {
			case <-c.ctx.Done():
				c.Process.Kill()
			case <-c.waitDone:
			}
		}()
	}

	return nil
}
func (c *Cmd) CombinedOutput() ([]byte, error) {
	if c.Stdout != nil {
		return nil, errors.New("exec: Stdout already set")
	}
	if c.Stderr != nil {
		return nil, errors.New("exec: Stderr already set")
	}
	var b bytes.Buffer
	c.Stdout = &b
	c.Stderr = &b
	err := c.Run()
	return b.Bytes(), err
}
func (c *Cmd) StdinPipe() (io.WriteCloser, error) {
	if c.Stdin != nil {
		return nil, errors.New("exec: Stdin already set")
	}
	if c.Process != nil {
		return nil, errors.New("exec: StdinPipe after process started")
	}
	pr, pw, err := os.Pipe()
	if err != nil {
		return nil, err
	}
	c.Stdin = pr
	c.closeAfterStart = append(c.closeAfterStart, pr)
	wc := &closeOnce{File: pw}
	c.closeAfterWait = append(c.closeAfterWait, closerFunc(wc.safeClose))
	return wc, nil
}
func (c *Cmd) StdoutPipe() (io.ReadCloser, error) {
	if c.Stdout != nil {
		return nil, errors.New("exec: Stdout already set")
	}
	if c.Process != nil {
		return nil, errors.New("exec: StdoutPipe after process started")
	}
	pr, pw, err := os.Pipe()
	if err != nil {
		return nil, err
	}
	c.Stdout = pw
	c.closeAfterStart = append(c.closeAfterStart, pw)
	c.closeAfterWait = append(c.closeAfterWait, pr)
	return pr, nil
}
func dedupEnvCase(caseInsensitive bool, env []string) []string {
	out := make([]string, 0, len(env))
	saw := map[string]int{} // key => index into out
	for _, kv := range env {
		eq := strings.Index(kv, "=")
		if eq < 0 {
			out = append(out, kv)
			continue
		}
		k := kv[:eq]
		if caseInsensitive {
			k = strings.ToLower(k)
		}
		if dupIdx, isDup := saw[k]; isDup {
			out[dupIdx] = kv
			continue
		}
		saw[k] = len(out)
		out = append(out, kv)
	}
	return out
}
func InputName(input *Input) string {
	switch {
	case input == nil:
		return ""
	case input.Pfs != nil:
		return input.Pfs.Name
	case input.Cross != nil:
		if len(input.Cross) > 0 {
			return InputName(input.Cross[0])
		}
	case input.Union != nil:
		if len(input.Union) > 0 {
			return InputName(input.Union[0])
		}
	}
	return ""
}
func SortInput(input *Input) {
	VisitInput(input, func(input *Input) {
		SortInputs := func(inputs []*Input) {
			sort.SliceStable(inputs, func(i, j int) bool { return InputName(inputs[i]) < InputName(inputs[j]) })
		}
		switch {
		case input.Cross != nil:
			SortInputs(input.Cross)
		case input.Union != nil:
			SortInputs(input.Union)
		}
	})
}
func InputBranches(input *Input) []*pfs.Branch {
	var result []*pfs.Branch
	VisitInput(input, func(input *Input) {
		if input.Pfs != nil {
			result = append(result, &pfs.Branch{
				Repo: &pfs.Repo{Name: input.Pfs.Repo},
				Name: input.Pfs.Branch,
			})
		}
		if input.Cron != nil {
			result = append(result, &pfs.Branch{
				Repo: &pfs.Repo{Name: input.Cron.Repo},
				Name: "master",
			})
		}
		if input.Git != nil {
			result = append(result, &pfs.Branch{
				Repo: &pfs.Repo{Name: input.Git.Name},
				Name: input.Git.Branch,
			})
		}
	})
	return result
}
func ValidateGitCloneURL(url string) error {
	exampleURL := "https://github.com/org/foo.git"
	if url == "" {
		return fmt.Errorf("clone URL is missing (example clone URL %v)", exampleURL)
	}
	// Use the git client's validator to make sure its a valid URL
	o := &git.CloneOptions{
		URL: url,
	}
	if err := o.Validate(); err != nil {
		return err
	}

	// Make sure its the type that we want. Of the following we
	// only accept the 'clone' type of url:
	//     git_url: "git://github.com/sjezewski/testgithook.git",
	//     ssh_url: "git@github.com:sjezewski/testgithook.git",
	//     clone_url: "https://github.com/sjezewski/testgithook.git",
	//     svn_url: "https://github.com/sjezewski/testgithook",
	invalidErr := fmt.Errorf("clone URL is missing .git suffix (example clone URL %v)", exampleURL)

	if !strings.HasSuffix(url, ".git") {
		// svn_url case
		return invalidErr
	}
	if !strings.HasPrefix(url, "https://") {
		// git_url or ssh_url cases
		return invalidErr
	}

	return nil
}
func containsEmpty(vals []string) bool {
	for _, val := range vals {
		if val == "" {
			return true
		}
	}
	return false
}
func NewAPIServer(address string, storageRoot string, clusterInfo *admin.ClusterInfo) APIServer {
	return &apiServer{
		Logger:      log.NewLogger("admin.API"),
		address:     address,
		storageRoot: storageRoot,
		clusterInfo: clusterInfo,
	}
}
func Ago(timestamp *types.Timestamp) string {
	t, _ := types.TimestampFromProto(timestamp)
	if t.Equal(time.Time{}) {
		return ""
	}
	return fmt.Sprintf("%s ago", units.HumanDuration(time.Since(t)))
}
func TimeDifference(from *types.Timestamp, to *types.Timestamp) string {
	tFrom, _ := types.TimestampFromProto(from)
	tTo, _ := types.TimestampFromProto(to)
	return units.HumanDuration(tTo.Sub(tFrom))
}
func Duration(d *types.Duration) string {
	duration, _ := types.DurationFromProto(d)
	return units.HumanDuration(duration)
}
func (c APIClient) InspectCluster() (*admin.ClusterInfo, error) {
	clusterInfo, err := c.AdminAPIClient.InspectCluster(c.Ctx(), &types.Empty{})
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	return clusterInfo, nil
}
func (c APIClient) Extract(objects bool, f func(op *admin.Op) error) error {
	extractClient, err := c.AdminAPIClient.Extract(c.Ctx(), &admin.ExtractRequest{NoObjects: !objects})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	for {
		op, err := extractClient.Recv()
		if err == io.EOF {
			break
		}
		if err != nil {
			return grpcutil.ScrubGRPC(err)
		}
		if err := f(op); err != nil {
			return err
		}
	}
	return nil
}
func (c APIClient) ExtractAll(objects bool) ([]*admin.Op, error) {
	var result []*admin.Op
	if err := c.Extract(objects, func(op *admin.Op) error {
		result = append(result, op)
		return nil
	}); err != nil {
		return nil, err
	}
	return result, nil
}
func (c APIClient) ExtractWriter(objects bool, w io.Writer) error {
	writer := pbutil.NewWriter(w)
	return c.Extract(objects, func(op *admin.Op) error {
		_, err := writer.Write(op)
		return err
	})
}
func (c APIClient) ExtractURL(url string) error {
	extractClient, err := c.AdminAPIClient.Extract(c.Ctx(), &admin.ExtractRequest{URL: url})
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	resp, err := extractClient.Recv()
	if err == nil {
		return fmt.Errorf("unexpected response from extract: %v", resp)
	}
	if err != io.EOF {
		return err
	}
	return nil
}
func (c APIClient) ExtractPipeline(pipelineName string) (*pps.CreatePipelineRequest, error) {
	op, err := c.AdminAPIClient.ExtractPipeline(c.Ctx(), &admin.ExtractPipelineRequest{Pipeline: NewPipeline(pipelineName)})
	if err != nil {
		return nil, grpcutil.ScrubGRPC(err)
	}
	if op.Op1_9 == nil || op.Op1_9.Pipeline == nil {
		return nil, fmt.Errorf("malformed response is missing pipeline")
	}
	return op.Op1_9.Pipeline, nil
}
func (c APIClient) Restore(ops []*admin.Op) (retErr error) {
	restoreClient, err := c.AdminAPIClient.Restore(c.Ctx())
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if _, err := restoreClient.CloseAndRecv(); err != nil && retErr == nil {
			retErr = grpcutil.ScrubGRPC(err)
		}
	}()
	for _, op := range ops {
		if err := restoreClient.Send(&admin.RestoreRequest{Op: op}); err != nil {
			return grpcutil.ScrubGRPC(err)
		}
	}
	return nil
}
func (c APIClient) RestoreReader(r io.Reader) (retErr error) {
	restoreClient, err := c.AdminAPIClient.Restore(c.Ctx())
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if _, err := restoreClient.CloseAndRecv(); err != nil && retErr == nil {
			retErr = grpcutil.ScrubGRPC(err)
		}
	}()
	reader := pbutil.NewReader(r)
	op := &admin.Op{}
	for {
		if err := reader.Read(op); err != nil {
			if err == io.EOF {
				break
			}
			return err
		}
		if err := restoreClient.Send(&admin.RestoreRequest{Op: op}); err != nil {
			return grpcutil.ScrubGRPC(err)
		}
	}
	return nil
}
func (c APIClient) RestoreFrom(objects bool, otherC *APIClient) (retErr error) {
	restoreClient, err := c.AdminAPIClient.Restore(c.Ctx())
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if _, err := restoreClient.CloseAndRecv(); err != nil && retErr == nil {
			retErr = grpcutil.ScrubGRPC(err)
		}
	}()
	return otherC.Extract(objects, func(op *admin.Op) error {
		return restoreClient.Send(&admin.RestoreRequest{Op: op})
	})
}
func (c APIClient) RestoreURL(url string) (retErr error) {
	restoreClient, err := c.AdminAPIClient.Restore(c.Ctx())
	if err != nil {
		return grpcutil.ScrubGRPC(err)
	}
	defer func() {
		if _, err := restoreClient.CloseAndRecv(); err != nil && retErr == nil {
			retErr = grpcutil.ScrubGRPC(err)
		}
	}()
	return grpcutil.ScrubGRPC(restoreClient.Send(&admin.RestoreRequest{URL: url}))
}
func IgnoreTypes(typs ...interface{}) cmp.Option {
	tf := newTypeFilter(typs...)
	return cmp.FilterPath(tf.filter, cmp.Ignore())
}
func (s *textList) AppendEllipsis(ds diffStats) {
	hasStats := ds != diffStats{}
	if len(*s) == 0 || !(*s)[len(*s)-1].Value.Equal(textEllipsis) {
		if hasStats {
			*s = append(*s, textRecord{Value: textEllipsis, Comment: ds})
		} else {
			*s = append(*s, textRecord{Value: textEllipsis})
		}
		return
	}
	if hasStats {
		(*s)[len(*s)-1].Comment = (*s)[len(*s)-1].Comment.(diffStats).Append(ds)
	}
}
func IsType(t reflect.Type, ft funcType) bool {
	if t == nil || t.Kind() != reflect.Func || t.IsVariadic() {
		return false
	}
	ni, no := t.NumIn(), t.NumOut()
	switch ft {
	case tbFunc: // func(T) bool
		if ni == 1 && no == 1 && t.Out(0) == boolType {
			return true
		}
	case ttbFunc: // func(T, T) bool
		if ni == 2 && no == 1 && t.In(0) == t.In(1) && t.Out(0) == boolType {
			return true
		}
	case trbFunc: // func(T, R) bool
		if ni == 2 && no == 1 && t.Out(0) == boolType {
			return true
		}
	case tibFunc: // func(T, I) bool
		if ni == 2 && no == 1 && t.In(0).AssignableTo(t.In(1)) && t.Out(0) == boolType {
			return true
		}
	case trFunc: // func(T) R
		if ni == 1 && no == 1 {
			return true
		}
	}
	return false
}
func NameOf(v reflect.Value) string {
	fnc := runtime.FuncForPC(v.Pointer())
	if fnc == nil {
		return "<unknown>"
	}
	fullName := fnc.Name() // e.g., "long/path/name/mypkg.(*MyType).(long/path/name/mypkg.myMethod)-fm"

	// Method closures have a "-fm" suffix.
	fullName = strings.TrimSuffix(fullName, "-fm")

	var name string
	for len(fullName) > 0 {
		inParen := strings.HasSuffix(fullName, ")")
		fullName = strings.TrimSuffix(fullName, ")")

		s := lastIdentRx.FindString(fullName)
		if s == "" {
			break
		}
		name = s + "." + name
		fullName = strings.TrimSuffix(fullName, s)

		if i := strings.LastIndexByte(fullName, '('); inParen && i >= 0 {
			fullName = fullName[:i]
		}
		fullName = strings.TrimSuffix(fullName, ".")
	}
	return strings.TrimSuffix(name, ".")
}
func PointerOf(v reflect.Value) Pointer {
	// The proper representation of a pointer is unsafe.Pointer,
	// which is necessary if the GC ever uses a moving collector.
	return Pointer{unsafe.Pointer(v.Pointer()), v.Type()}
}
func (es EditScript) String() string {
	b := make([]byte, len(es))
	for i, e := range es {
		switch e {
		case Identity:
			b[i] = '.'
		case UniqueX:
			b[i] = 'X'
		case UniqueY:
			b[i] = 'Y'
		case Modified:
			b[i] = 'M'
		default:
			panic("invalid edit-type")
		}
	}
	return string(b)
}
func (es EditScript) stats() (s struct{ NI, NX, NY, NM int }) {
	for _, e := range es {
		switch e {
		case Identity:
			s.NI++
		case UniqueX:
			s.NX++
		case UniqueY:
			s.NY++
		case Modified:
			s.NM++
		default:
			panic("invalid edit-type")
		}
	}
	return
}
func (p *path) connect(dst point, f EqualFunc) {
	if p.dir > 0 {
		// Connect in forward direction.
		for dst.X > p.X && dst.Y > p.Y {
			switch r := f(p.X, p.Y); {
			case r.Equal():
				p.append(Identity)
			case r.Similar():
				p.append(Modified)
			case dst.X-p.X >= dst.Y-p.Y:
				p.append(UniqueX)
			default:
				p.append(UniqueY)
			}
		}
		for dst.X > p.X {
			p.append(UniqueX)
		}
		for dst.Y > p.Y {
			p.append(UniqueY)
		}
	} else {
		// Connect in reverse direction.
		for p.X > dst.X && p.Y > dst.Y {
			switch r := f(p.X-1, p.Y-1); {
			case r.Equal():
				p.append(Identity)
			case r.Similar():
				p.append(Modified)
			case p.Y-dst.Y >= p.X-dst.X:
				p.append(UniqueY)
			default:
				p.append(UniqueX)
			}
		}
		for p.X > dst.X {
			p.append(UniqueX)
		}
		for p.Y > dst.Y {
			p.append(UniqueY)
		}
	}
}
func EquateNaNs() cmp.Option {
	return cmp.Options{
		cmp.FilterValues(areNaNsF64s, cmp.Comparer(equateAlways)),
		cmp.FilterValues(areNaNsF32s, cmp.Comparer(equateAlways)),
	}
}
func (pa Path) Index(i int) PathStep {
	if i < 0 {
		i = len(pa) + i
	}
	if i < 0 || i >= len(pa) {
		return pathStep{}
	}
	return pa[i]
}
func (si SliceIndex) Key() int {
	if si.xkey != si.ykey {
		return -1
	}
	return si.xkey
}
func (r *defaultReporter) String() string {
	assert(r.root != nil && r.curr == nil)
	if r.root.NumDiff == 0 {
		return ""
	}
	return formatOptions{}.FormatDiff(r.root).String()
}
func (opts formatOptions) FormatType(t reflect.Type, s textNode) textNode {
	// Check whether to emit the type or not.
	switch opts.TypeMode {
	case autoType:
		switch t.Kind() {
		case reflect.Struct, reflect.Slice, reflect.Array, reflect.Map:
			if s.Equal(textNil) {
				return s
			}
		default:
			return s
		}
	case elideType:
		return s
	}

	// Determine the type label, applying special handling for unnamed types.
	typeName := t.String()
	if t.Name() == "" {
		// According to Go grammar, certain type literals contain symbols that
		// do not strongly bind to the next lexicographical token (e.g., *T).
		switch t.Kind() {
		case reflect.Chan, reflect.Func, reflect.Ptr:
			typeName = "(" + typeName + ")"
		}
		typeName = strings.Replace(typeName, "struct {", "struct{", -1)
		typeName = strings.Replace(typeName, "interface {", "interface{", -1)
	}

	// Avoid wrap the value in parenthesis if unnecessary.
	if s, ok := s.(textWrap); ok {
		hasParens := strings.HasPrefix(s.Prefix, "(") && strings.HasSuffix(s.Suffix, ")")
		hasBraces := strings.HasPrefix(s.Prefix, "{") && strings.HasSuffix(s.Suffix, "}")
		if hasParens || hasBraces {
			return textWrap{typeName, s, ""}
		}
	}
	return textWrap{typeName + "(", s, ")"}
}
func formatMapKey(v reflect.Value) string {
	var opts formatOptions
	opts.TypeMode = elideType
	opts.AvoidStringer = true
	opts.ShallowPointers = true
	s := opts.FormatValue(v, visitedPointers{}).String()
	return strings.TrimSpace(s)
}
func formatString(s string) string {
	// Use quoted string if it the same length as a raw string literal.
	// Otherwise, attempt to use the raw string form.
	qs := strconv.Quote(s)
	if len(qs) == 1+len(s)+1 {
		return qs
	}

	// Disallow newlines to ensure output is a single line.
	// Only allow printable runes for readability purposes.
	rawInvalid := func(r rune) bool {
		return r == '`' || r == '\n' || !(unicode.IsPrint(r) || r == '\t')
	}
	if strings.IndexFunc(s, rawInvalid) < 0 {
		return "`" + s + "`"
	}
	return qs
}
func formatHex(u uint64) string {
	var f string
	switch {
	case u <= 0xff:
		f = "0x%02x"
	case u <= 0xffff:
		f = "0x%04x"
	case u <= 0xffffff:
		f = "0x%06x"
	case u <= 0xffffffff:
		f = "0x%08x"
	case u <= 0xffffffffff:
		f = "0x%010x"
	case u <= 0xffffffffffff:
		f = "0x%012x"
	case u <= 0xffffffffffffff:
		f = "0x%014x"
	case u <= 0xffffffffffffffff:
		f = "0x%016x"
	}
	return fmt.Sprintf(f, u)
}
func formatPointer(v reflect.Value) string {
	p := v.Pointer()
	if flags.Deterministic {
		p = 0xdeadf00f // Only used for stable testing purposes
	}
	return fmt.Sprintf("⟪0x%x⟫", p)
}
func (m visitedPointers) Visit(v reflect.Value) bool {
	p := value.PointerOf(v)
	_, visited := m[p]
	m[p] = struct{}{}
	return visited
}
func retrieveUnexportedField(v reflect.Value, f reflect.StructField) reflect.Value {
	return reflect.NewAt(f.Type, unsafe.Pointer(v.UnsafeAddr()+f.Offset)).Elem()
}
func (ft *fieldTree) insert(cname []string) {
	if ft.sub == nil {
		ft.sub = make(map[string]fieldTree)
	}
	if len(cname) == 0 {
		ft.ok = true
		return
	}
	sub := ft.sub[cname[0]]
	sub.insert(cname[1:])
	ft.sub[cname[0]] = sub
}
func (ft fieldTree) matchPrefix(p cmp.Path) bool {
	for _, ps := range p {
		switch ps := ps.(type) {
		case cmp.StructField:
			ft = ft.sub[ps.Name()]
			if ft.ok {
				return true
			}
			if len(ft.sub) == 0 {
				return false
			}
		case cmp.Indirect:
		default:
			return false
		}
	}
	return false
}
func canonicalName(t reflect.Type, sel string) ([]string, error) {
	var name string
	sel = strings.TrimPrefix(sel, ".")
	if sel == "" {
		return nil, fmt.Errorf("name must not be empty")
	}
	if i := strings.IndexByte(sel, '.'); i < 0 {
		name, sel = sel, ""
	} else {
		name, sel = sel[:i], sel[i:]
	}

	// Type must be a struct or pointer to struct.
	if t.Kind() == reflect.Ptr {
		t = t.Elem()
	}
	if t.Kind() != reflect.Struct {
		return nil, fmt.Errorf("%v must be a struct", t)
	}

	// Find the canonical name for this current field name.
	// If the field exists in an embedded struct, then it will be expanded.
	if !isExported(name) {
		// Disallow unexported fields:
		//	* To discourage people from actually touching unexported fields
		//	* FieldByName is buggy (https://golang.org/issue/4876)
		return []string{name}, fmt.Errorf("name must be exported")
	}
	sf, ok := t.FieldByName(name)
	if !ok {
		return []string{name}, fmt.Errorf("does not exist")
	}
	var ss []string
	for i := range sf.Index {
		ss = append(ss, t.FieldByIndex(sf.Index[:i+1]).Name)
	}
	if sel == "" {
		return ss, nil
	}
	ssPost, err := canonicalName(sf.Type, sel)
	return append(ss, ssPost...), err
}
func FilterPath(f func(Path) bool, opt Option) Option {
	if f == nil {
		panic("invalid path filter function")
	}
	if opt := normalizeOption(opt); opt != nil {
		return &pathFilter{fnc: f, opt: opt}
	}
	return nil
}
func normalizeOption(src Option) Option {
	switch opts := flattenOptions(nil, Options{src}); len(opts) {
	case 0:
		return nil
	case 1:
		return opts[0]
	default:
		return opts
	}
}
func flattenOptions(dst, src Options) Options {
	for _, opt := range src {
		switch opt := opt.(type) {
		case nil:
			continue
		case Options:
			dst = flattenOptions(dst, opt)
		case coreOption:
			dst = append(dst, opt)
		default:
			panic(fmt.Sprintf("invalid option type: %T", opt))
		}
	}
	return dst
}
func (opts formatOptions) CanFormatDiffSlice(v *valueNode) bool {
	switch {
	case opts.DiffMode != diffUnknown:
		return false // Must be formatting in diff mode
	case v.NumDiff == 0:
		return false // No differences detected
	case v.NumIgnored+v.NumCompared+v.NumTransformed > 0:
		// TODO: Handle the case where someone uses bytes.Equal on a large slice.
		return false // Some custom option was used to determined equality
	case !v.ValueX.IsValid() || !v.ValueY.IsValid():
		return false // Both values must be valid
	}

	switch t := v.Type; t.Kind() {
	case reflect.String:
	case reflect.Array, reflect.Slice:
		// Only slices of primitive types have specialized handling.
		switch t.Elem().Kind() {
		case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64,
			reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr,
			reflect.Bool, reflect.Float32, reflect.Float64, reflect.Complex64, reflect.Complex128:
		default:
			return false
		}

		// If a sufficient number of elements already differ,
		// use specialized formatting even if length requirement is not met.
		if v.NumDiff > v.NumSame {
			return true
		}
	default:
		return false
	}

	// Use specialized string diffing for longer slices or strings.
	const minLength = 64
	return v.ValueX.Len() >= minLength && v.ValueY.Len() >= minLength
}
func formatASCII(s string) string {
	b := bytes.Repeat([]byte{'.'}, len(s))
	for i := 0; i < len(s); i++ {
		if ' ' <= s[i] && s[i] <= '~' {
			b[i] = s[i]
		}
	}
	return string(b)
}
func coalesceAdjacentEdits(name string, es diff.EditScript) (groups []diffStats) {
	var prevCase int // Arbitrary index into which case last occurred
	lastStats := func(i int) *diffStats {
		if prevCase != i {
			groups = append(groups, diffStats{Name: name})
			prevCase = i
		}
		return &groups[len(groups)-1]
	}
	for _, e := range es {
		switch e {
		case diff.Identity:
			lastStats(1).NumIdentical++
		case diff.UniqueX:
			lastStats(2).NumRemoved++
		case diff.UniqueY:
			lastStats(2).NumInserted++
		case diff.Modified:
			lastStats(2).NumModified++
		}
	}
	return groups
}
func SortKeys(vs []reflect.Value) []reflect.Value {
	if len(vs) == 0 {
		return vs
	}

	// Sort the map keys.
	sort.Slice(vs, func(i, j int) bool { return isLess(vs[i], vs[j]) })

	// Deduplicate keys (fails for NaNs).
	vs2 := vs[:1]
	for _, v := range vs[1:] {
		if isLess(vs2[len(vs2)-1], v) {
			vs2 = append(vs2, v)
		}
	}
	return vs2
}
func (opts formatOptions) FormatDiff(v *valueNode) textNode {
	// Check whether we have specialized formatting for this node.
	// This is not necessary, but helpful for producing more readable outputs.
	if opts.CanFormatDiffSlice(v) {
		return opts.FormatDiffSlice(v)
	}

	// For leaf nodes, format the value based on the reflect.Values alone.
	if v.MaxDepth == 0 {
		switch opts.DiffMode {
		case diffUnknown, diffIdentical:
			// Format Equal.
			if v.NumDiff == 0 {
				outx := opts.FormatValue(v.ValueX, visitedPointers{})
				outy := opts.FormatValue(v.ValueY, visitedPointers{})
				if v.NumIgnored > 0 && v.NumSame == 0 {
					return textEllipsis
				} else if outx.Len() < outy.Len() {
					return outx
				} else {
					return outy
				}
			}

			// Format unequal.
			assert(opts.DiffMode == diffUnknown)
			var list textList
			outx := opts.WithTypeMode(elideType).FormatValue(v.ValueX, visitedPointers{})
			outy := opts.WithTypeMode(elideType).FormatValue(v.ValueY, visitedPointers{})
			if outx != nil {
				list = append(list, textRecord{Diff: '-', Value: outx})
			}
			if outy != nil {
				list = append(list, textRecord{Diff: '+', Value: outy})
			}
			return opts.WithTypeMode(emitType).FormatType(v.Type, list)
		case diffRemoved:
			return opts.FormatValue(v.ValueX, visitedPointers{})
		case diffInserted:
			return opts.FormatValue(v.ValueY, visitedPointers{})
		default:
			panic("invalid diff mode")
		}
	}

	// Descend into the child value node.
	if v.TransformerName != "" {
		out := opts.WithTypeMode(emitType).FormatDiff(v.Value)
		out = textWrap{"Inverse(" + v.TransformerName + ", ", out, ")"}
		return opts.FormatType(v.Type, out)
	} else {
		switch k := v.Type.Kind(); k {
		case reflect.Struct, reflect.Array, reflect.Slice, reflect.Map:
			return opts.FormatType(v.Type, opts.formatDiffList(v.Records, k))
		case reflect.Ptr:
			return textWrap{"&", opts.FormatDiff(v.Value), ""}
		case reflect.Interface:
			return opts.WithTypeMode(emitType).FormatDiff(v.Value)
		default:
			panic(fmt.Sprintf("%v cannot have children", k))
		}
	}
}
func coalesceAdjacentRecords(name string, recs []reportRecord) (groups []diffStats) {
	var prevCase int // Arbitrary index into which case last occurred
	lastStats := func(i int) *diffStats {
		if prevCase != i {
			groups = append(groups, diffStats{Name: name})
			prevCase = i
		}
		return &groups[len(groups)-1]
	}
	for _, r := range recs {
		switch rv := r.Value; {
		case rv.NumIgnored > 0 && rv.NumSame+rv.NumDiff == 0:
			lastStats(1).NumIgnored++
		case rv.NumDiff == 0:
			lastStats(1).NumIdentical++
		case rv.NumDiff > 0 && !rv.ValueY.IsValid():
			lastStats(2).NumRemoved++
		case rv.NumDiff > 0 && !rv.ValueX.IsValid():
			lastStats(2).NumInserted++
		default:
			lastStats(2).NumModified++
		}
	}
	return groups
}
func Diff(x, y interface{}, opts ...Option) string {
	r := new(defaultReporter)
	eq := Equal(x, y, Options(opts), Reporter(r))
	d := r.String()
	if (d == "") != eq {
		panic("inconsistent difference and equality results")
	}
	return d
}
func (s *state) statelessCompare(step PathStep) diff.Result {
	// We do not save and restore the curPath because all of the compareX
	// methods should properly push and pop from the path.
	// It is an implementation bug if the contents of curPath differs from
	// when calling this function to when returning from it.

	oldResult, oldReporters := s.result, s.reporters
	s.result = diff.Result{} // Reset result
	s.reporters = nil        // Remove reporters to avoid spurious printouts
	s.compareAny(step)
	res := s.result
	s.result, s.reporters = oldResult, oldReporters
	return res
}
func sanitizeValue(v reflect.Value, t reflect.Type) reflect.Value {
	// TODO(dsnet): Workaround for reflect bug (https://golang.org/issue/22143).
	if !flags.AtLeastGo110 {
		if v.Kind() == reflect.Interface && v.IsNil() && v.Type() != t {
			return reflect.New(t).Elem()
		}
	}
	return v
}
func (rc *recChecker) Check(p Path) {
	const minLen = 1 << 16
	if rc.next == 0 {
		rc.next = minLen
	}
	if len(p) < rc.next {
		return
	}
	rc.next <<= 1

	// Check whether the same transformer has appeared at least twice.
	var ss []string
	m := map[Option]int{}
	for _, ps := range p {
		if t, ok := ps.(Transform); ok {
			t := t.Option()
			if m[t] == 1 { // Transformer was used exactly once before
				tf := t.(*transformer).fnc.Type()
				ss = append(ss, fmt.Sprintf("%v: %v => %v", t, tf.In(0), tf.Out(0)))
			}
			m[t]++
		}
	}
	if len(ss) > 0 {
		const warning = "recursive set of Transformers detected"
		const help = "consider using cmpopts.AcyclicTransformer"
		set := strings.Join(ss, "\n\t")
		panic(fmt.Sprintf("%s:\n\t%s\n%s", warning, set, help))
	}
}
func makeAddressable(v reflect.Value) reflect.Value {
	if v.CanAddr() {
		return v
	}
	vc := reflect.New(v.Type()).Elem()
	vc.Set(v)
	return vc
}
func (lf Field) Marshal(visitor Encoder) {
	switch lf.fieldType {
	case stringType:
		visitor.EmitString(lf.key, lf.stringVal)
	case boolType:
		visitor.EmitBool(lf.key, lf.numericVal != 0)
	case intType:
		visitor.EmitInt(lf.key, int(lf.numericVal))
	case int32Type:
		visitor.EmitInt32(lf.key, int32(lf.numericVal))
	case int64Type:
		visitor.EmitInt64(lf.key, int64(lf.numericVal))
	case uint32Type:
		visitor.EmitUint32(lf.key, uint32(lf.numericVal))
	case uint64Type:
		visitor.EmitUint64(lf.key, uint64(lf.numericVal))
	case float32Type:
		visitor.EmitFloat32(lf.key, math.Float32frombits(uint32(lf.numericVal)))
	case float64Type:
		visitor.EmitFloat64(lf.key, math.Float64frombits(uint64(lf.numericVal)))
	case errorType:
		if err, ok := lf.interfaceVal.(error); ok {
			visitor.EmitString(lf.key, err.Error())
		} else {
			visitor.EmitString(lf.key, "<nil>")
		}
	case objectType:
		visitor.EmitObject(lf.key, lf.interfaceVal)
	case lazyLoggerType:
		visitor.EmitLazyLogger(lf.interfaceVal.(LazyLogger))
	case noopType:
		// intentionally left blank
	}
}
func (lf Field) String() string {
	return fmt.Sprint(lf.key, ":", lf.Value())
}
func (t Tag) Set(s Span) {
	s.SetTag(t.Key, t.Value)
}
func (t *TextMapPropagator) Inject(spanContext MockSpanContext, carrier interface{}) error {
	writer, ok := carrier.(opentracing.TextMapWriter)
	if !ok {
		return opentracing.ErrInvalidCarrier
	}
	// Ids:
	writer.Set(mockTextMapIdsPrefix+"traceid", strconv.Itoa(spanContext.TraceID))
	writer.Set(mockTextMapIdsPrefix+"spanid", strconv.Itoa(spanContext.SpanID))
	writer.Set(mockTextMapIdsPrefix+"sampled", fmt.Sprint(spanContext.Sampled))
	// Baggage:
	for baggageKey, baggageVal := range spanContext.Baggage {
		safeVal := baggageVal
		if t.HTTPHeaders {
			safeVal = url.QueryEscape(baggageVal)
		}
		writer.Set(mockTextMapBaggagePrefix+baggageKey, safeVal)
	}
	return nil
}
func (t *TextMapPropagator) Extract(carrier interface{}) (MockSpanContext, error) {
	reader, ok := carrier.(opentracing.TextMapReader)
	if !ok {
		return emptyContext, opentracing.ErrInvalidCarrier
	}
	rval := MockSpanContext{0, 0, true, nil}
	err := reader.ForeachKey(func(key, val string) error {
		lowerKey := strings.ToLower(key)
		switch {
		case lowerKey == mockTextMapIdsPrefix+"traceid":
			// Ids:
			i, err := strconv.Atoi(val)
			if err != nil {
				return err
			}
			rval.TraceID = i
		case lowerKey == mockTextMapIdsPrefix+"spanid":
			// Ids:
			i, err := strconv.Atoi(val)
			if err != nil {
				return err
			}
			rval.SpanID = i
		case lowerKey == mockTextMapIdsPrefix+"sampled":
			b, err := strconv.ParseBool(val)
			if err != nil {
				return err
			}
			rval.Sampled = b
		case strings.HasPrefix(lowerKey, mockTextMapBaggagePrefix):
			// Baggage:
			if rval.Baggage == nil {
				rval.Baggage = make(map[string]string)
			}
			safeVal := val
			if t.HTTPHeaders {
				// unescape errors are ignored, nothing can be done
				if rawVal, err := url.QueryUnescape(val); err == nil {
					safeVal = rawVal
				}
			}
			rval.Baggage[lowerKey[len(mockTextMapBaggagePrefix):]] = safeVal
		}
		return nil
	})
	if rval.TraceID == 0 || rval.SpanID == 0 {
		return emptyContext, opentracing.ErrSpanContextNotFound
	}
	if err != nil {
		return emptyContext, err
	}
	return rval, nil
}
func (ld *LogData) ToLogRecord() LogRecord {
	var literalTimestamp time.Time
	if ld.Timestamp.IsZero() {
		literalTimestamp = time.Now()
	} else {
		literalTimestamp = ld.Timestamp
	}
	rval := LogRecord{
		Timestamp: literalTimestamp,
	}
	if ld.Payload == nil {
		rval.Fields = []log.Field{
			log.String("event", ld.Event),
		}
	} else {
		rval.Fields = []log.Field{
			log.String("event", ld.Event),
			log.Object("payload", ld.Payload),
		}
	}
	return rval
}
func New() *MockTracer {
	t := &MockTracer{
		finishedSpans: []*MockSpan{},
		injectors:     make(map[interface{}]Injector),
		extractors:    make(map[interface{}]Extractor),
	}

	// register default injectors/extractors
	textPropagator := new(TextMapPropagator)
	t.RegisterInjector(opentracing.TextMap, textPropagator)
	t.RegisterExtractor(opentracing.TextMap, textPropagator)

	httpPropagator := &TextMapPropagator{HTTPHeaders: true}
	t.RegisterInjector(opentracing.HTTPHeaders, httpPropagator)
	t.RegisterExtractor(opentracing.HTTPHeaders, httpPropagator)

	return t
}
func (t *MockTracer) StartSpan(operationName string, opts ...opentracing.StartSpanOption) opentracing.Span {
	sso := opentracing.StartSpanOptions{}
	for _, o := range opts {
		o.Apply(&sso)
	}
	return newMockSpan(t, operationName, sso)
}
func (t *MockTracer) RegisterInjector(format interface{}, injector Injector) {
	t.injectors[format] = injector
}
func (t *MockTracer) RegisterExtractor(format interface{}, extractor Extractor) {
	t.extractors[format] = extractor
}
func (t *MockTracer) Inject(sm opentracing.SpanContext, format interface{}, carrier interface{}) error {
	spanContext, ok := sm.(MockSpanContext)
	if !ok {
		return opentracing.ErrInvalidCarrier
	}
	injector, ok := t.injectors[format]
	if !ok {
		return opentracing.ErrUnsupportedFormat
	}
	return injector.Inject(spanContext, carrier)
}
func (t *MockTracer) Extract(format interface{}, carrier interface{}) (opentracing.SpanContext, error) {
	extractor, ok := t.extractors[format]
	if !ok {
		return nil, opentracing.ErrUnsupportedFormat
	}
	return extractor.Extract(carrier)
}
func ContextWithSpan(ctx context.Context, span Span) context.Context {
	return context.WithValue(ctx, activeSpanKey, span)
}
func (tag uint32TagName) Set(span opentracing.Span, value uint32) {
	span.SetTag(string(tag), value)
}
func (tag uint16TagName) Set(span opentracing.Span, value uint16) {
	span.SetTag(string(tag), value)
}
func (tag boolTagName) Set(span opentracing.Span, value bool) {
	span.SetTag(string(tag), value)
}
func (tag ipv4Tag) SetString(span opentracing.Span, value string) {
	span.SetTag(string(tag), value)
}
func (m *MockKeyValue) EmitString(key, value string) {
	m.Key = key
	m.ValueKind = reflect.TypeOf(value).Kind()
	m.ValueString = fmt.Sprint(value)
}
func (m *MockKeyValue) EmitLazyLogger(value log.LazyLogger) {
	var meta MockKeyValue
	value(&meta)
	m.Key = meta.Key
	m.ValueKind = meta.ValueKind
	m.ValueString = meta.ValueString
}
func RunAPIChecks(
	t *testing.T,
	newTracer func() (tracer opentracing.Tracer, closer func()),
	opts ...APICheckOption,
) {
	s := &APICheckSuite{newTracer: newTracer}
	for _, opt := range opts {
		opt(s)
	}
	suite.Run(t, s)
}
func CheckBaggageValues(val bool) APICheckOption {
	return func(s *APICheckSuite) {
		s.opts.CheckBaggageValues = val
	}
}
func CheckExtract(val bool) APICheckOption {
	return func(s *APICheckSuite) {
		s.opts.CheckExtract = val
	}
}
func CheckInject(val bool) APICheckOption {
	return func(s *APICheckSuite) {
		s.opts.CheckInject = val
	}
}
func CheckEverything() APICheckOption {
	return func(s *APICheckSuite) {
		s.opts.CheckBaggageValues = true
		s.opts.CheckExtract = true
		s.opts.CheckInject = true
	}
}
func UseProbe(probe APICheckProbe) APICheckOption {
	return func(s *APICheckSuite) {
		s.opts.Probe = probe
	}
}
func (c MockSpanContext) WithBaggageItem(key, value string) MockSpanContext {
	var newBaggage map[string]string
	if c.Baggage == nil {
		newBaggage = map[string]string{key: value}
	} else {
		newBaggage = make(map[string]string, len(c.Baggage)+1)
		for k, v := range c.Baggage {
			newBaggage[k] = v
		}
		newBaggage[key] = value
	}
	// Use positional parameters so the compiler will help catch new fields.
	return MockSpanContext{c.TraceID, c.SpanID, c.Sampled, newBaggage}
}
func (s *MockSpan) Tags() map[string]interface{} {
	s.RLock()
	defer s.RUnlock()
	tags := make(map[string]interface{})
	for k, v := range s.tags {
		tags[k] = v
	}
	return tags
}
func (s *MockSpan) Tag(k string) interface{} {
	s.RLock()
	defer s.RUnlock()
	return s.tags[k]
}
func (s *MockSpan) Logs() []MockLogRecord {
	s.RLock()
	defer s.RUnlock()
	logs := make([]MockLogRecord, len(s.logs))
	copy(logs, s.logs)
	return logs
}
func (s *MockSpan) Context() opentracing.SpanContext {
	s.Lock()
	defer s.Unlock()
	return s.SpanContext
}
func (s *MockSpan) SetTag(key string, value interface{}) opentracing.Span {
	s.Lock()
	defer s.Unlock()
	if key == string(ext.SamplingPriority) {
		if v, ok := value.(uint16); ok {
			s.SpanContext.Sampled = v > 0
			return s
		}
		if v, ok := value.(int); ok {
			s.SpanContext.Sampled = v > 0
			return s
		}
	}
	s.tags[key] = value
	return s
}
func (s *MockSpan) SetBaggageItem(key, val string) opentracing.Span {
	s.Lock()
	defer s.Unlock()
	s.SpanContext = s.SpanContext.WithBaggageItem(key, val)
	return s
}
func (s *MockSpan) BaggageItem(key string) string {
	s.RLock()
	defer s.RUnlock()
	return s.SpanContext.Baggage[key]
}
func (s *MockSpan) Finish() {
	s.Lock()
	s.FinishTime = time.Now()
	s.Unlock()
	s.tracer.recordSpan(s)
}
func (s *MockSpan) FinishWithOptions(opts opentracing.FinishOptions) {
	s.Lock()
	s.FinishTime = opts.FinishTime
	s.Unlock()

	// Handle any late-bound LogRecords.
	for _, lr := range opts.LogRecords {
		s.logFieldsWithTimestamp(lr.Timestamp, lr.Fields...)
	}
	// Handle (deprecated) BulkLogData.
	for _, ld := range opts.BulkLogData {
		if ld.Payload != nil {
			s.logFieldsWithTimestamp(
				ld.Timestamp,
				log.String("event", ld.Event),
				log.Object("payload", ld.Payload))
		} else {
			s.logFieldsWithTimestamp(
				ld.Timestamp,
				log.String("event", ld.Event))
		}
	}

	s.tracer.recordSpan(s)
}
func (s *MockSpan) String() string {
	return fmt.Sprintf(
		"traceId=%d, spanId=%d, parentId=%d, sampled=%t, name=%s",
		s.SpanContext.TraceID, s.SpanContext.SpanID, s.ParentID,
		s.SpanContext.Sampled, s.OperationName)
}
func (s *MockSpan) LogFields(fields ...log.Field) {
	s.logFieldsWithTimestamp(time.Now(), fields...)
}
func (s *MockSpan) logFieldsWithTimestamp(ts time.Time, fields ...log.Field) {
	lr := MockLogRecord{
		Timestamp: ts,
		Fields:    make([]MockKeyValue, len(fields)),
	}
	for i, f := range fields {
		outField := &(lr.Fields[i])
		f.Marshal(outField)
	}

	s.Lock()
	defer s.Unlock()
	s.logs = append(s.logs, lr)
}
func (s *MockSpan) LogKV(keyValues ...interface{}) {
	if len(keyValues)%2 != 0 {
		s.LogFields(log.Error(fmt.Errorf("Non-even keyValues len: %v", len(keyValues))))
		return
	}
	fields, err := log.InterleavedKVToFields(keyValues...)
	if err != nil {
		s.LogFields(log.Error(err), log.String("function", "LogKV"))
		return
	}
	s.LogFields(fields...)
}
func (s *MockSpan) LogEvent(event string) {
	s.LogFields(log.String("event", event))
}
func (s *MockSpan) LogEventWithPayload(event string, payload interface{}) {
	s.LogFields(log.String("event", event), log.Object("payload", payload))
}
func (s *MockSpan) SetOperationName(operationName string) opentracing.Span {
	s.Lock()
	defer s.Unlock()
	s.OperationName = operationName
	return s
}
func registriesDirPath(sys *types.SystemContext) string {
	if sys != nil {
		if sys.RegistriesDirPath != "" {
			return sys.RegistriesDirPath
		}
		if sys.RootForImplicitAbsolutePaths != "" {
			return filepath.Join(sys.RootForImplicitAbsolutePaths, systemRegistriesDirPath)
		}
	}
	return systemRegistriesDirPath
}
func loadAndMergeConfig(dirPath string) (*registryConfiguration, error) {
	mergedConfig := registryConfiguration{Docker: map[string]registryNamespace{}}
	dockerDefaultMergedFrom := ""
	nsMergedFrom := map[string]string{}

	dir, err := os.Open(dirPath)
	if err != nil {
		if os.IsNotExist(err) {
			return &mergedConfig, nil
		}
		return nil, err
	}
	configNames, err := dir.Readdirnames(0)
	if err != nil {
		return nil, err
	}
	for _, configName := range configNames {
		if !strings.HasSuffix(configName, ".yaml") {
			continue
		}
		configPath := filepath.Join(dirPath, configName)
		configBytes, err := ioutil.ReadFile(configPath)
		if err != nil {
			return nil, err
		}

		var config registryConfiguration
		err = yaml.Unmarshal(configBytes, &config)
		if err != nil {
			return nil, errors.Wrapf(err, "Error parsing %s", configPath)
		}

		if config.DefaultDocker != nil {
			if mergedConfig.DefaultDocker != nil {
				return nil, errors.Errorf(`Error parsing signature storage configuration: "default-docker" defined both in "%s" and "%s"`,
					dockerDefaultMergedFrom, configPath)
			}
			mergedConfig.DefaultDocker = config.DefaultDocker
			dockerDefaultMergedFrom = configPath
		}

		for nsName, nsConfig := range config.Docker { // includes config.Docker == nil
			if _, ok := mergedConfig.Docker[nsName]; ok {
				return nil, errors.Errorf(`Error parsing signature storage configuration: "docker" namespace "%s" defined both in "%s" and "%s"`,
					nsName, nsMergedFrom[nsName], configPath)
			}
			mergedConfig.Docker[nsName] = nsConfig
			nsMergedFrom[nsName] = configPath
		}
	}

	return &mergedConfig, nil
}
func ParseReference(ref string) (types.ImageReference, error) {
	r, err := reference.ParseNormalizedNamed(ref)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to parse image reference %q", ref)
	}
	tagged, ok := r.(reference.NamedTagged)
	if !ok {
		return nil, errors.Errorf("invalid image reference %s, expected format: 'hostname/namespace/stream:tag'", ref)
	}
	return NewReference(tagged)
}
func NewReference(dockerRef reference.NamedTagged) (types.ImageReference, error) {
	r := strings.SplitN(reference.Path(dockerRef), "/", 3)
	if len(r) != 2 {
		return nil, errors.Errorf("invalid image reference: %s, expected format: 'hostname/namespace/stream:tag'",
			reference.FamiliarString(dockerRef))
	}
	return openshiftReference{
		namespace:       r[0],
		stream:          r[1],
		dockerReference: dockerRef,
	}, nil
}
func CheckAuth(ctx context.Context, sys *types.SystemContext, username, password, registry string) error {
	client, err := newDockerClient(sys, registry, registry)
	if err != nil {
		return errors.Wrapf(err, "error creating new docker client")
	}
	client.username = username
	client.password = password

	resp, err := client.makeRequest(ctx, "GET", "/v2/", nil, nil, v2Auth, nil)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	switch resp.StatusCode {
	case http.StatusOK:
		return nil
	case http.StatusUnauthorized:
		return ErrUnauthorizedForCredentials
	default:
		return errors.Errorf("error occured with status code %d (%s)", resp.StatusCode, http.StatusText(resp.StatusCode))
	}
}
func (c *dockerClient) doHTTP(req *http.Request) (*http.Response, error) {
	tr := tlsclientconfig.NewTransport()
	tr.TLSClientConfig = c.tlsClientConfig
	httpClient := &http.Client{Transport: tr}
	return httpClient.Do(req)
}
func (c *dockerClient) detectPropertiesHelper(ctx context.Context) error {
	if c.scheme != "" {
		return nil
	}

	// We overwrite the TLS clients `InsecureSkipVerify` only if explicitly
	// specified by the system context
	if c.sys != nil && c.sys.DockerInsecureSkipTLSVerify != types.OptionalBoolUndefined {
		c.tlsClientConfig.InsecureSkipVerify = c.sys.DockerInsecureSkipTLSVerify == types.OptionalBoolTrue
	}

	ping := func(scheme string) error {
		url := fmt.Sprintf(resolvedPingV2URL, scheme, c.registry)
		resp, err := c.makeRequestToResolvedURL(ctx, "GET", url, nil, nil, -1, noAuth, nil)
		if err != nil {
			logrus.Debugf("Ping %s err %s (%#v)", url, err.Error(), err)
			return err
		}
		defer resp.Body.Close()
		logrus.Debugf("Ping %s status %d", url, resp.StatusCode)
		if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusUnauthorized {
			return errors.Errorf("error pinging registry %s, response code %d (%s)", c.registry, resp.StatusCode, http.StatusText(resp.StatusCode))
		}
		c.challenges = parseAuthHeader(resp.Header)
		c.scheme = scheme
		c.supportsSignatures = resp.Header.Get("X-Registry-Supports-Signatures") == "1"
		return nil
	}
	err := ping("https")
	if err != nil && c.tlsClientConfig.InsecureSkipVerify {
		err = ping("http")
	}
	if err != nil {
		err = errors.Wrap(err, "pinging docker registry returned")
		if c.sys != nil && c.sys.DockerDisableV1Ping {
			return err
		}
		// best effort to understand if we're talking to a V1 registry
		pingV1 := func(scheme string) bool {
			url := fmt.Sprintf(resolvedPingV1URL, scheme, c.registry)
			resp, err := c.makeRequestToResolvedURL(ctx, "GET", url, nil, nil, -1, noAuth, nil)
			if err != nil {
				logrus.Debugf("Ping %s err %s (%#v)", url, err.Error(), err)
				return false
			}
			defer resp.Body.Close()
			logrus.Debugf("Ping %s status %d", url, resp.StatusCode)
			if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusUnauthorized {
				return false
			}
			return true
		}
		isV1 := pingV1("https")
		if !isV1 && c.tlsClientConfig.InsecureSkipVerify {
			isV1 = pingV1("http")
		}
		if isV1 {
			err = ErrV1NotSupported
		}
	}
	return err
}
func (c *dockerClient) detectProperties(ctx context.Context) error {
	c.detectPropertiesOnce.Do(func() { c.detectPropertiesError = c.detectPropertiesHelper(ctx) })
	return c.detectPropertiesError
}
func (c *dockerClient) getExtensionsSignatures(ctx context.Context, ref dockerReference, manifestDigest digest.Digest) (*extensionSignatureList, error) {
	path := fmt.Sprintf(extensionsSignaturePath, reference.Path(ref.ref), manifestDigest)
	res, err := c.makeRequest(ctx, "GET", path, nil, nil, v2Auth, nil)
	if err != nil {
		return nil, err
	}
	defer res.Body.Close()
	if res.StatusCode != http.StatusOK {
		return nil, errors.Wrapf(client.HandleErrorResponse(res), "Error downloading signatures for %s in %s", manifestDigest, ref.ref.Name())
	}
	body, err := ioutil.ReadAll(res.Body)
	if err != nil {
		return nil, err
	}

	var parsedBody extensionSignatureList
	if err := json.Unmarshal(body, &parsedBody); err != nil {
		return nil, errors.Wrapf(err, "Error decoding signature list")
	}
	return &parsedBody, nil
}
func NewTransport() *http.Transport {
	direct := &net.Dialer{
		Timeout:   30 * time.Second,
		KeepAlive: 30 * time.Second,
		DualStack: true,
	}
	tr := &http.Transport{
		Proxy:               http.ProxyFromEnvironment,
		Dial:                direct.Dial,
		TLSHandshakeTimeout: 10 * time.Second,
		// TODO(dmcgowan): Call close idle connections when complete and use keep alive
		DisableKeepAlives: true,
	}
	proxyDialer, err := sockets.DialerFromEnvironment(direct)
	if err == nil {
		tr.Dial = proxyDialer.Dial
	}
	return tr
}
func readRegistryConf(sys *types.SystemContext) ([]byte, error) {
	return ioutil.ReadFile(RegistriesConfPath(sys))
}
func GetRegistries(sys *types.SystemContext) ([]string, error) {
	config, err := loadRegistryConf(sys)
	if err != nil {
		return nil, err
	}
	return config.Registries.Search.Registries, nil
}
func GetInsecureRegistries(sys *types.SystemContext) ([]string, error) {
	config, err := loadRegistryConf(sys)
	if err != nil {
		return nil, err
	}
	return config.Registries.Insecure.Registries, nil
}
func RegistriesConfPath(ctx *types.SystemContext) string {
	path := systemRegistriesConfPath
	if ctx != nil {
		if ctx.SystemRegistriesConfPath != "" {
			path = ctx.SystemRegistriesConfPath
		} else if ctx.RootForImplicitAbsolutePaths != "" {
			path = filepath.Join(ctx.RootForImplicitAbsolutePaths, systemRegistriesConfPath)
		}
	}
	return path
}
func NewOptionalBool(b bool) OptionalBool {
	o := OptionalBoolFalse
	if b == true {
		o = OptionalBoolTrue
	}
	return o
}
func (pc *PolicyContext) changeState(expected, new policyContextState) error {
	if pc.state != expected {
		return errors.Errorf(`"Invalid PolicyContext state, expected "%s", found "%s"`, expected, pc.state)
	}
	pc.state = new
	return nil
}
func (pc *PolicyContext) Destroy() error {
	if err := pc.changeState(pcReady, pcDestroying); err != nil {
		return err
	}
	// FIXME: destroy
	return pc.changeState(pcDestroying, pcDestroyed)
}
func policyIdentityLogName(ref types.ImageReference) string {
	return ref.Transport().Name() + ":" + ref.PolicyConfigurationIdentity()
}
func (pc *PolicyContext) requirementsForImageRef(ref types.ImageReference) PolicyRequirements {
	// Do we have a PolicyTransportScopes for this transport?
	transportName := ref.Transport().Name()
	if transportScopes, ok := pc.Policy.Transports[transportName]; ok {
		// Look for a full match.
		identity := ref.PolicyConfigurationIdentity()
		if req, ok := transportScopes[identity]; ok {
			logrus.Debugf(` Using transport "%s" policy section %s`, transportName, identity)
			return req
		}

		// Look for a match of the possible parent namespaces.
		for _, name := range ref.PolicyConfigurationNamespaces() {
			if req, ok := transportScopes[name]; ok {
				logrus.Debugf(` Using transport "%s" specific policy section %s`, transportName, name)
				return req
			}
		}

		// Look for a default match for the transport.
		if req, ok := transportScopes[""]; ok {
			logrus.Debugf(` Using transport "%s" policy section ""`, transportName)
			return req
		}
	}

	logrus.Debugf(" Using default policy section")
	return pc.Policy.Default
}
func ParseImageName(imgName string) (types.ImageReference, error) {
	parts := strings.SplitN(imgName, ":", 2)
	if len(parts) != 2 {
		return nil, errors.Errorf(`Invalid image name "%s", expected colon-separated transport:reference`, imgName)
	}
	transport := transports.Get(parts[0])
	if transport == nil {
		return nil, errors.Errorf(`Invalid image name "%s", unknown transport "%s"`, imgName, parts[0])
	}
	return transport.ParseReference(parts[1])
}
func BlobInfoFromOCI1Descriptor(desc imgspecv1.Descriptor) types.BlobInfo {
	return types.BlobInfo{
		Digest:      desc.Digest,
		Size:        desc.Size,
		URLs:        desc.URLs,
		Annotations: desc.Annotations,
		MediaType:   desc.MediaType,
	}
}
func OCI1FromManifest(manifest []byte) (*OCI1, error) {
	oci1 := OCI1{}
	if err := json.Unmarshal(manifest, &oci1); err != nil {
		return nil, err
	}
	return &oci1, nil
}
func OCI1FromComponents(config imgspecv1.Descriptor, layers []imgspecv1.Descriptor) *OCI1 {
	return &OCI1{
		imgspecv1.Manifest{
			Versioned: specs.Versioned{SchemaVersion: 2},
			Config:    config,
			Layers:    layers,
		},
	}
}
func newReference(ref reference.Named) (dockerReference, error) {
	if reference.IsNameOnly(ref) {
		return dockerReference{}, errors.Errorf("Docker reference %s has neither a tag nor a digest", reference.FamiliarString(ref))
	}
	// A github.com/distribution/reference value can have a tag and a digest at the same time!
	// The docker/distribution API does not really support that (we can’t ask for an image with a specific
	// tag and digest), so fail.  This MAY be accepted in the future.
	// (Even if it were supported, the semantics of policy namespaces are unclear - should we drop
	// the tag or the digest first?)
	_, isTagged := ref.(reference.NamedTagged)
	_, isDigested := ref.(reference.Canonical)
	if isTagged && isDigested {
		return dockerReference{}, errors.Errorf("Docker references with both a tag and digest are currently not supported")
	}

	return dockerReference{
		ref: ref,
	}, nil
}
func (ref dockerReference) tagOrDigest() (string, error) {
	if ref, ok := ref.ref.(reference.Canonical); ok {
		return ref.Digest().String(), nil
	}
	if ref, ok := ref.ref.(reference.NamedTagged); ok {
		return ref.Tag(), nil
	}
	// This should not happen, NewReference above refuses reference.IsNameOnly values.
	return "", errors.Errorf("Internal inconsistency: Reference %s unexpectedly has neither a digest nor a tag", reference.FamiliarString(ref.ref))
}
func (ic *imageCopier) updateEmbeddedDockerReference() error {
	if ic.c.dest.IgnoresEmbeddedDockerReference() {
		return nil // Destination would prefer us not to update the embedded reference.
	}
	destRef := ic.c.dest.Reference().DockerReference()
	if destRef == nil {
		return nil // Destination does not care about Docker references
	}
	if !ic.src.EmbeddedDockerReferenceConflicts(destRef) {
		return nil // No reference embedded in the manifest, or it matches destRef already.
	}

	if !ic.canModifyManifest {
		return errors.Errorf("Copying a schema1 image with an embedded Docker reference to %s (Docker reference %s) would invalidate existing signatures. Explicitly enable signature removal to proceed anyway",
			transports.ImageName(ic.c.dest.Reference()), destRef.String())
	}
	ic.manifestUpdates.EmbeddedDockerReference = destRef
	return nil
}
func isTTY(w io.Writer) bool {
	if f, ok := w.(*os.File); ok {
		return terminal.IsTerminal(int(f.Fd()))
	}
	return false
}
func (ic *imageCopier) copyUpdatedConfigAndManifest(ctx context.Context) ([]byte, error) {
	pendingImage := ic.src
	if !reflect.DeepEqual(*ic.manifestUpdates, types.ManifestUpdateOptions{InformationOnly: ic.manifestUpdates.InformationOnly}) {
		if !ic.canModifyManifest {
			return nil, errors.Errorf("Internal error: copy needs an updated manifest but that was known to be forbidden")
		}
		if !ic.diffIDsAreNeeded && ic.src.UpdatedImageNeedsLayerDiffIDs(*ic.manifestUpdates) {
			// We have set ic.diffIDsAreNeeded based on the preferred MIME type returned by determineManifestConversion.
			// So, this can only happen if we are trying to upload using one of the other MIME type candidates.
			// Because UpdatedImageNeedsLayerDiffIDs is true only when converting from s1 to s2, this case should only arise
			// when ic.c.dest.SupportedManifestMIMETypes() includes both s1 and s2, the upload using s1 failed, and we are now trying s2.
			// Supposedly s2-only registries do not exist or are extremely rare, so failing with this error message is good enough for now.
			// If handling such registries turns out to be necessary, we could compute ic.diffIDsAreNeeded based on the full list of manifest MIME type candidates.
			return nil, errors.Errorf("Can not convert image to %s, preparing DiffIDs for this case is not supported", ic.manifestUpdates.ManifestMIMEType)
		}
		pi, err := ic.src.UpdatedImage(ctx, *ic.manifestUpdates)
		if err != nil {
			return nil, errors.Wrap(err, "Error creating an updated image manifest")
		}
		pendingImage = pi
	}
	manifest, _, err := pendingImage.Manifest(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "Error reading manifest")
	}

	if err := ic.c.copyConfig(ctx, pendingImage); err != nil {
		return nil, err
	}

	ic.c.Printf("Writing manifest to image destination\n")
	if err := ic.c.dest.PutManifest(ctx, manifest); err != nil {
		return nil, errors.Wrap(err, "Error writing manifest")
	}
	return manifest, nil
}
func (c *copier) createProgressBar(pool *mpb.Progress, info types.BlobInfo, kind string, onComplete string) *mpb.Bar {
	// shortDigestLen is the length of the digest used for blobs.
	const shortDigestLen = 12

	prefix := fmt.Sprintf("Copying %s %s", kind, info.Digest.Encoded())
	// Truncate the prefix (chopping of some part of the digest) to make all progress bars aligned in a column.
	maxPrefixLen := len("Copying blob ") + shortDigestLen
	if len(prefix) > maxPrefixLen {
		prefix = prefix[:maxPrefixLen]
	}

	bar := pool.AddBar(info.Size,
		mpb.BarClearOnComplete(),
		mpb.PrependDecorators(
			decor.Name(prefix),
		),
		mpb.AppendDecorators(
			decor.OnComplete(decor.CountersKibiByte("%.1f / %.1f"), " "+onComplete),
		),
	)
	if c.progressOutput == ioutil.Discard {
		c.Printf("Copying %s %s\n", kind, info.Digest)
	}
	return bar
}
func (c *copier) copyConfig(ctx context.Context, src types.Image) error {
	srcInfo := src.ConfigInfo()
	if srcInfo.Digest != "" {
		configBlob, err := src.ConfigBlob(ctx)
		if err != nil {
			return errors.Wrapf(err, "Error reading config blob %s", srcInfo.Digest)
		}

		destInfo, err := func() (types.BlobInfo, error) { // A scope for defer
			progressPool, progressCleanup := c.newProgressPool(ctx)
			defer progressCleanup()
			bar := c.createProgressBar(progressPool, srcInfo, "config", "done")
			destInfo, err := c.copyBlobFromStream(ctx, bytes.NewReader(configBlob), srcInfo, nil, false, true, bar)
			if err != nil {
				return types.BlobInfo{}, err
			}
			bar.SetTotal(int64(len(configBlob)), true)
			return destInfo, nil
		}()
		if err != nil {
			return nil
		}
		if destInfo.Digest != srcInfo.Digest {
			return errors.Errorf("Internal error: copying uncompressed config blob %s changed digest to %s", srcInfo.Digest, destInfo.Digest)
		}
	}
	return nil
}
func diffIDComputationGoroutine(dest chan<- diffIDResult, layerStream io.ReadCloser, decompressor compression.DecompressorFunc) {
	result := diffIDResult{
		digest: "",
		err:    errors.New("Internal error: unexpected panic in diffIDComputationGoroutine"),
	}
	defer func() { dest <- result }()
	defer layerStream.Close() // We do not care to bother the other end of the pipe with other failures; we send them to dest instead.

	result.digest, result.err = computeDiffID(layerStream, decompressor)
}
func computeDiffID(stream io.Reader, decompressor compression.DecompressorFunc) (digest.Digest, error) {
	if decompressor != nil {
		s, err := decompressor(stream)
		if err != nil {
			return "", err
		}
		defer s.Close()
		stream = s
	}

	return digest.Canonical.FromReader(stream)
}
func compressGoroutine(dest *io.PipeWriter, src io.Reader) {
	err := errors.New("Internal error: unexpected panic in compressGoroutine")
	defer func() { // Note that this is not the same as {defer dest.CloseWithError(err)}; we need err to be evaluated lazily.
		dest.CloseWithError(err) // CloseWithError(nil) is equivalent to Close()
	}()

	zipper := pgzip.NewWriter(dest)
	defer zipper.Close()

	_, err = io.Copy(zipper, src) // Sets err to nil, i.e. causes dest.Close()
}
func newDockerClient(sys *types.SystemContext) (*dockerclient.Client, error) {
	host := dockerclient.DefaultDockerHost
	if sys != nil && sys.DockerDaemonHost != "" {
		host = sys.DockerDaemonHost
	}

	// Sadly, unix:// sockets don't work transparently with dockerclient.NewClient.
	// They work fine with a nil httpClient; with a non-nil httpClient, the transport’s
	// TLSClientConfig must be nil (or the client will try using HTTPS over the PF_UNIX socket
	// regardless of the values in the *tls.Config), and we would have to call sockets.ConfigureTransport.
	//
	// We don't really want to configure anything for unix:// sockets, so just pass a nil *http.Client.
	//
	// Similarly, if we want to communicate over plain HTTP on a TCP socket, we also need to set
	// TLSClientConfig to nil. This can be achieved by using the form `http://`
	url, err := dockerclient.ParseHostURL(host)
	if err != nil {
		return nil, err
	}
	var httpClient *http.Client
	if url.Scheme != "unix" {
		if url.Scheme == "http" {
			httpClient = httpConfig()
		} else {
			hc, err := tlsConfig(sys)
			if err != nil {
				return nil, err
			}
			httpClient = hc
		}
	}

	return dockerclient.NewClient(host, defaultAPIVersion, httpClient, nil)
}
func defaultPolicyPath(sys *types.SystemContext) string {
	if sys != nil {
		if sys.SignaturePolicyPath != "" {
			return sys.SignaturePolicyPath
		}
		if sys.RootForImplicitAbsolutePaths != "" {
			return filepath.Join(sys.RootForImplicitAbsolutePaths, systemDefaultPolicyPath)
		}
	}
	return systemDefaultPolicyPath
}
func NewPolicyFromFile(fileName string) (*Policy, error) {
	contents, err := ioutil.ReadFile(fileName)
	if err != nil {
		return nil, err
	}
	policy, err := NewPolicyFromBytes(contents)
	if err != nil {
		return nil, errors.Wrapf(err, "invalid policy in %q", fileName)
	}
	return policy, nil
}
func NewPolicyFromBytes(data []byte) (*Policy, error) {
	p := Policy{}
	if err := json.Unmarshal(data, &p); err != nil {
		return nil, InvalidPolicyFormatError(err.Error())
	}
	return &p, nil
}
func newPolicyRequirementFromJSON(data []byte) (PolicyRequirement, error) {
	var typeField prCommon
	if err := json.Unmarshal(data, &typeField); err != nil {
		return nil, err
	}
	var res PolicyRequirement
	switch typeField.Type {
	case prTypeInsecureAcceptAnything:
		res = &prInsecureAcceptAnything{}
	case prTypeReject:
		res = &prReject{}
	case prTypeSignedBy:
		res = &prSignedBy{}
	case prTypeSignedBaseLayer:
		res = &prSignedBaseLayer{}
	default:
		return nil, InvalidPolicyFormatError(fmt.Sprintf("Unknown policy requirement type \"%s\"", typeField.Type))
	}
	if err := json.Unmarshal(data, &res); err != nil {
		return nil, err
	}
	return res, nil
}
func newPRSignedBy(keyType sbKeyType, keyPath string, keyData []byte, signedIdentity PolicyReferenceMatch) (*prSignedBy, error) {
	if !keyType.IsValid() {
		return nil, InvalidPolicyFormatError(fmt.Sprintf("invalid keyType \"%s\"", keyType))
	}
	if len(keyPath) > 0 && len(keyData) > 0 {
		return nil, InvalidPolicyFormatError("keyType and keyData cannot be used simultaneously")
	}
	if signedIdentity == nil {
		return nil, InvalidPolicyFormatError("signedIdentity not specified")
	}
	return &prSignedBy{
		prCommon:       prCommon{Type: prTypeSignedBy},
		KeyType:        keyType,
		KeyPath:        keyPath,
		KeyData:        keyData,
		SignedIdentity: signedIdentity,
	}, nil
}
func newPRSignedByKeyPath(keyType sbKeyType, keyPath string, signedIdentity PolicyReferenceMatch) (*prSignedBy, error) {
	return newPRSignedBy(keyType, keyPath, nil, signedIdentity)
}
func NewPRSignedByKeyPath(keyType sbKeyType, keyPath string, signedIdentity PolicyReferenceMatch) (PolicyRequirement, error) {
	return newPRSignedByKeyPath(keyType, keyPath, signedIdentity)
}
func newPRSignedByKeyData(keyType sbKeyType, keyData []byte, signedIdentity PolicyReferenceMatch) (*prSignedBy, error) {
	return newPRSignedBy(keyType, "", keyData, signedIdentity)
}
func NewPRSignedByKeyData(keyType sbKeyType, keyData []byte, signedIdentity PolicyReferenceMatch) (PolicyRequirement, error) {
	return newPRSignedByKeyData(keyType, keyData, signedIdentity)
}
func (kt sbKeyType) IsValid() bool {
	switch kt {
	case SBKeyTypeGPGKeys, SBKeyTypeSignedByGPGKeys,
		SBKeyTypeX509Certificates, SBKeyTypeSignedByX509CAs:
		return true
	default:
		return false
	}
}
func newPRSignedBaseLayer(baseLayerIdentity PolicyReferenceMatch) (*prSignedBaseLayer, error) {
	if baseLayerIdentity == nil {
		return nil, InvalidPolicyFormatError("baseLayerIdentity not specified")
	}
	return &prSignedBaseLayer{
		prCommon:          prCommon{Type: prTypeSignedBaseLayer},
		BaseLayerIdentity: baseLayerIdentity,
	}, nil
}
func newPolicyReferenceMatchFromJSON(data []byte) (PolicyReferenceMatch, error) {
	var typeField prmCommon
	if err := json.Unmarshal(data, &typeField); err != nil {
		return nil, err
	}
	var res PolicyReferenceMatch
	switch typeField.Type {
	case prmTypeMatchExact:
		res = &prmMatchExact{}
	case prmTypeMatchRepoDigestOrExact:
		res = &prmMatchRepoDigestOrExact{}
	case prmTypeMatchRepository:
		res = &prmMatchRepository{}
	case prmTypeExactReference:
		res = &prmExactReference{}
	case prmTypeExactRepository:
		res = &prmExactRepository{}
	default:
		return nil, InvalidPolicyFormatError(fmt.Sprintf("Unknown policy reference match type \"%s\"", typeField.Type))
	}
	if err := json.Unmarshal(data, &res); err != nil {
		return nil, err
	}
	return res, nil
}
func newPRMExactReference(dockerReference string) (*prmExactReference, error) {
	ref, err := reference.ParseNormalizedNamed(dockerReference)
	if err != nil {
		return nil, InvalidPolicyFormatError(fmt.Sprintf("Invalid format of dockerReference %s: %s", dockerReference, err.Error()))
	}
	if reference.IsNameOnly(ref) {
		return nil, InvalidPolicyFormatError(fmt.Sprintf("dockerReference %s contains neither a tag nor digest", dockerReference))
	}
	return &prmExactReference{
		prmCommon:       prmCommon{Type: prmTypeExactReference},
		DockerReference: dockerReference,
	}, nil
}
func newPRMExactRepository(dockerRepository string) (*prmExactRepository, error) {
	if _, err := reference.ParseNormalizedNamed(dockerRepository); err != nil {
		return nil, InvalidPolicyFormatError(fmt.Sprintf("Invalid format of dockerRepository %s: %s", dockerRepository, err.Error()))
	}
	return &prmExactRepository{
		prmCommon:        prmCommon{Type: prmTypeExactRepository},
		DockerRepository: dockerRepository,
	}, nil
}
func newImageSource(imageRef storageReference) (*storageImageSource, error) {
	// First, locate the image.
	img, err := imageRef.resolveImage()
	if err != nil {
		return nil, err
	}

	// Build the reader object.
	image := &storageImageSource{
		imageRef:       imageRef,
		image:          img,
		layerPosition:  make(map[digest.Digest]int),
		SignatureSizes: []int{},
	}
	if img.Metadata != "" {
		if err := json.Unmarshal([]byte(img.Metadata), image); err != nil {
			return nil, errors.Wrap(err, "error decoding metadata for source image")
		}
	}
	return image, nil
}
func (s *storageImageSource) getBlobAndLayerID(info types.BlobInfo) (rc io.ReadCloser, n int64, layerID string, err error) {
	var layer storage.Layer
	var diffOptions *storage.DiffOptions
	// We need a valid digest value.
	err = info.Digest.Validate()
	if err != nil {
		return nil, -1, "", err
	}
	// Check if the blob corresponds to a diff that was used to initialize any layers.  Our
	// callers should try to retrieve layers using their uncompressed digests, so no need to
	// check if they're using one of the compressed digests, which we can't reproduce anyway.
	layers, err := s.imageRef.transport.store.LayersByUncompressedDigest(info.Digest)
	// If it's not a layer, then it must be a data item.
	if len(layers) == 0 {
		b, err := s.imageRef.transport.store.ImageBigData(s.image.ID, info.Digest.String())
		if err != nil {
			return nil, -1, "", err
		}
		r := bytes.NewReader(b)
		logrus.Debugf("exporting opaque data as blob %q", info.Digest.String())
		return ioutil.NopCloser(r), int64(r.Len()), "", nil
	}
	// Step through the list of matching layers.  Tests may want to verify that if we have multiple layers
	// which claim to have the same contents, that we actually do have multiple layers, otherwise we could
	// just go ahead and use the first one every time.
	s.getBlobMutex.Lock()
	i := s.layerPosition[info.Digest]
	s.layerPosition[info.Digest] = i + 1
	s.getBlobMutex.Unlock()
	if len(layers) > 0 {
		layer = layers[i%len(layers)]
	}
	// Force the storage layer to not try to match any compression that was used when the layer was first
	// handed to it.
	noCompression := archive.Uncompressed
	diffOptions = &storage.DiffOptions{
		Compression: &noCompression,
	}
	if layer.UncompressedSize < 0 {
		n = -1
	} else {
		n = layer.UncompressedSize
	}
	logrus.Debugf("exporting filesystem layer %q without compression for blob %q", layer.ID, info.Digest)
	rc, err = s.imageRef.transport.store.Diff("", layer.ID, diffOptions)
	if err != nil {
		return nil, -1, "", err
	}
	return rc, n, layer.ID, err
}
func (s *storageImageDestination) computeID(m manifest.Manifest) string {
	// Build the diffID list.  We need the decompressed sums that we've been calculating to
	// fill in the DiffIDs.  It's expected (but not enforced by us) that the number of
	// diffIDs corresponds to the number of non-EmptyLayer entries in the history.
	var diffIDs []digest.Digest
	switch m := m.(type) {
	case *manifest.Schema1:
		// Build a list of the diffIDs we've generated for the non-throwaway FS layers,
		// in reverse of the order in which they were originally listed.
		for i, compat := range m.ExtractedV1Compatibility {
			if compat.ThrowAway {
				continue
			}
			blobSum := m.FSLayers[i].BlobSum
			diffID, ok := s.blobDiffIDs[blobSum]
			if !ok {
				logrus.Infof("error looking up diffID for layer %q", blobSum.String())
				return ""
			}
			diffIDs = append([]digest.Digest{diffID}, diffIDs...)
		}
	case *manifest.Schema2, *manifest.OCI1:
		// We know the ID calculation for these formats doesn't actually use the diffIDs,
		// so we don't need to populate the diffID list.
	default:
		return ""
	}
	id, err := m.ImageID(diffIDs)
	if err != nil {
		return ""
	}
	return id
}
func (s *storageImageDestination) PutManifest(ctx context.Context, manifestBlob []byte) error {
	if s.imageRef.named != nil {
		if digested, ok := s.imageRef.named.(reference.Digested); ok {
			matches, err := manifest.MatchesDigest(manifestBlob, digested.Digest())
			if err != nil {
				return err
			}
			if !matches {
				return fmt.Errorf("Manifest does not match expected digest %s", digested.Digest())
			}
		}
	}

	s.manifest = make([]byte, len(manifestBlob))
	copy(s.manifest, manifestBlob)
	return nil
}
func (s *storageImageDestination) PutSignatures(ctx context.Context, signatures [][]byte) error {
	sizes := []int{}
	sigblob := []byte{}
	for _, sig := range signatures {
		sizes = append(sizes, len(sig))
		newblob := make([]byte, len(sigblob)+len(sig))
		copy(newblob, sigblob)
		copy(newblob[len(sigblob):], sig)
		sigblob = newblob
	}
	s.signatures = sigblob
	s.SignatureSizes = sizes
	return nil
}
func newImage(ctx context.Context, sys *types.SystemContext, s storageReference) (types.ImageCloser, error) {
	src, err := newImageSource(s)
	if err != nil {
		return nil, err
	}
	img, err := image.FromSource(ctx, sys, src)
	if err != nil {
		return nil, err
	}
	size, err := src.getSize()
	if err != nil {
		return nil, err
	}
	return &storageImageCloser{ImageCloser: img, size: size}, nil
}
func newImageSource(ctx context.Context, sys *types.SystemContext, ref ociArchiveReference) (types.ImageSource, error) {
	tempDirRef, err := createUntarTempDir(ref)
	if err != nil {
		return nil, errors.Wrap(err, "error creating temp directory")
	}

	unpackedSrc, err := tempDirRef.ociRefExtracted.NewImageSource(ctx, sys)
	if err != nil {
		if err := tempDirRef.deleteTempDir(); err != nil {
			return nil, errors.Wrapf(err, "error deleting temp directory %q", tempDirRef.tempDirectory)
		}
		return nil, err
	}
	return &ociArchiveImageSource{ref: ref,
		unpackedSrc: unpackedSrc,
		tempDirRef:  tempDirRef}, nil
}
func LoadManifestDescriptor(imgRef types.ImageReference) (imgspecv1.Descriptor, error) {
	ociArchRef, ok := imgRef.(ociArchiveReference)
	if !ok {
		return imgspecv1.Descriptor{}, errors.Errorf("error typecasting, need type ociArchiveReference")
	}
	tempDirRef, err := createUntarTempDir(ociArchRef)
	if err != nil {
		return imgspecv1.Descriptor{}, errors.Wrap(err, "error creating temp directory")
	}
	defer tempDirRef.deleteTempDir()

	descriptor, err := ocilayout.LoadManifestDescriptor(tempDirRef.ociRefExtracted)
	if err != nil {
		return imgspecv1.Descriptor{}, errors.Wrap(err, "error loading index")
	}
	return descriptor, nil
}
func (s *ociArchiveImageSource) Close() error {
	defer s.tempDirRef.deleteTempDir()
	return s.unpackedSrc.Close()
}
func (os *orderedSet) append(s string) {
	if _, ok := os.included[s]; !ok {
		os.list = append(os.list, s)
		os.included[s] = struct{}{}
	}
}
func isMultiImage(ctx context.Context, img types.UnparsedImage) (bool, error) {
	_, mt, err := img.Manifest(ctx)
	if err != nil {
		return false, err
	}
	return manifest.MIMETypeIsMultiImage(mt), nil
}
func lockPath(path string) {
	pl := func() *pathLock { // A scope for defer
		pathLocksMutex.Lock()
		defer pathLocksMutex.Unlock()
		pl, ok := pathLocks[path]
		if ok {
			pl.refCount++
		} else {
			pl = &pathLock{refCount: 1, mutex: sync.Mutex{}}
			pathLocks[path] = pl
		}
		return pl
	}()
	pl.mutex.Lock()
}
func unlockPath(path string) {
	pathLocksMutex.Lock()
	defer pathLocksMutex.Unlock()
	pl, ok := pathLocks[path]
	if !ok {
		// Should this return an error instead? BlobInfoCache ultimately ignores errors…
		panic(fmt.Sprintf("Internal error: unlocking nonexistent lock for path %s", path))
	}
	pl.mutex.Unlock()
	pl.refCount--
	if pl.refCount == 0 {
		delete(pathLocks, path)
	}
}
func (bdc *cache) view(fn func(tx *bolt.Tx) error) (retErr error) {
	// bolt.Open(bdc.path, 0600, &bolt.Options{ReadOnly: true}) will, if the file does not exist,
	// nevertheless create it, but with an O_RDONLY file descriptor, try to initialize it, and fail — while holding
	// a read lock, blocking any future writes.
	// Hence this preliminary check, which is RACY: Another process could remove the file
	// between the Lstat call and opening the database.
	if _, err := os.Lstat(bdc.path); err != nil && os.IsNotExist(err) {
		return err
	}

	lockPath(bdc.path)
	defer unlockPath(bdc.path)
	db, err := bolt.Open(bdc.path, 0600, &bolt.Options{ReadOnly: true})
	if err != nil {
		return err
	}
	defer func() {
		if err := db.Close(); retErr == nil && err != nil {
			retErr = err
		}
	}()

	return db.View(fn)
}
func (bdc *cache) update(fn func(tx *bolt.Tx) error) (retErr error) {
	lockPath(bdc.path)
	defer unlockPath(bdc.path)
	db, err := bolt.Open(bdc.path, 0600, nil)
	if err != nil {
		return err
	}
	defer func() {
		if err := db.Close(); retErr == nil && err != nil {
			retErr = err
		}
	}()

	return db.Update(fn)
}
func (bdc *cache) uncompressedDigest(tx *bolt.Tx, anyDigest digest.Digest) digest.Digest {
	if b := tx.Bucket(uncompressedDigestBucket); b != nil {
		if uncompressedBytes := b.Get([]byte(anyDigest.String())); uncompressedBytes != nil {
			d, err := digest.Parse(string(uncompressedBytes))
			if err == nil {
				return d
			}
			// FIXME? Log err (but throttle the log volume on repeated accesses)?
		}
	}
	// Presence in digestsByUncompressedBucket implies that anyDigest must already refer to an uncompressed digest.
	// This way we don't have to waste storage space with trivial (uncompressed, uncompressed) mappings
	// when we already record a (compressed, uncompressed) pair.
	if b := tx.Bucket(digestByUncompressedBucket); b != nil {
		if b = b.Bucket([]byte(anyDigest.String())); b != nil {
			c := b.Cursor()
			if k, _ := c.First(); k != nil { // The bucket is non-empty
				return anyDigest
			}
		}
	}
	return ""
}
func (bdc *cache) appendReplacementCandidates(candidates []prioritize.CandidateWithTime, scopeBucket *bolt.Bucket, digest digest.Digest) []prioritize.CandidateWithTime {
	b := scopeBucket.Bucket([]byte(digest.String()))
	if b == nil {
		return candidates
	}
	_ = b.ForEach(func(k, v []byte) error {
		t := time.Time{}
		if err := t.UnmarshalBinary(v); err != nil {
			return err
		}
		candidates = append(candidates, prioritize.CandidateWithTime{
			Candidate: types.BICReplacementCandidate{
				Digest:   digest,
				Location: types.BICLocationReference{Opaque: string(k)},
			},
			LastSeen: t,
		})
		return nil
	}) // FIXME? Log error (but throttle the log volume on repeated accesses)?
	return candidates
}
func indexExists(ref ociReference) bool {
	_, err := os.Stat(ref.indexPath())
	if err == nil {
		return true
	}
	if os.IsNotExist(err) {
		return false
	}
	return true
}
func (c *copier) createSignature(manifest []byte, keyIdentity string) ([]byte, error) {
	mech, err := signature.NewGPGSigningMechanism()
	if err != nil {
		return nil, errors.Wrap(err, "Error initializing GPG")
	}
	defer mech.Close()
	if err := mech.SupportsSigning(); err != nil {
		return nil, errors.Wrap(err, "Signing not supported")
	}

	dockerReference := c.dest.Reference().DockerReference()
	if dockerReference == nil {
		return nil, errors.Errorf("Cannot determine canonical Docker reference for destination %s", transports.ImageName(c.dest.Reference()))
	}

	c.Printf("Signing manifest\n")
	newSig, err := signature.SignDockerManifest(manifest, dockerReference.String(), mech, keyIdentity)
	if err != nil {
		return nil, errors.Wrap(err, "Error creating signature")
	}
	return newSig, nil
}
func ParseReference(reference string) (types.ImageReference, error) {
	dir, image := internal.SplitPathAndImage(reference)
	return NewReference(dir, image)
}
func NewReference(dir, image string) (types.ImageReference, error) {
	resolved, err := explicitfilepath.ResolvePathToFullyExplicit(dir)
	if err != nil {
		return nil, err
	}

	if err := internal.ValidateOCIPath(dir); err != nil {
		return nil, err
	}

	if err = internal.ValidateImageName(image); err != nil {
		return nil, err
	}

	return ociReference{dir: dir, resolvedDir: resolved, image: image}, nil
}
func (ref ociReference) getIndex() (*imgspecv1.Index, error) {
	indexJSON, err := os.Open(ref.indexPath())
	if err != nil {
		return nil, err
	}
	defer indexJSON.Close()

	index := &imgspecv1.Index{}
	if err := json.NewDecoder(indexJSON).Decode(index); err != nil {
		return nil, err
	}
	return index, nil
}
func LoadManifestDescriptor(imgRef types.ImageReference) (imgspecv1.Descriptor, error) {
	ociRef, ok := imgRef.(ociReference)
	if !ok {
		return imgspecv1.Descriptor{}, errors.Errorf("error typecasting, need type ociRef")
	}
	return ociRef.getManifestDescriptor()
}
func (ref ociReference) blobPath(digest digest.Digest, sharedBlobDir string) (string, error) {
	if err := digest.Validate(); err != nil {
		return "", errors.Wrapf(err, "unexpected digest reference %s", digest)
	}
	blobDir := filepath.Join(ref.dir, "blobs")
	if sharedBlobDir != "" {
		blobDir = sharedBlobDir
	}
	return filepath.Join(blobDir, digest.Algorithm().String(), digest.Hex()), nil
}
func SignDockerManifest(m []byte, dockerReference string, mech SigningMechanism, keyIdentity string) ([]byte, error) {
	manifestDigest, err := manifest.Digest(m)
	if err != nil {
		return nil, err
	}
	sig := newUntrustedSignature(manifestDigest, dockerReference)
	return sig.sign(mech, keyIdentity)
}
func VerifyDockerManifestSignature(unverifiedSignature, unverifiedManifest []byte,
	expectedDockerReference string, mech SigningMechanism, expectedKeyIdentity string) (*Signature, error) {
	expectedRef, err := reference.ParseNormalizedNamed(expectedDockerReference)
	if err != nil {
		return nil, err
	}
	sig, err := verifyAndExtractSignature(mech, unverifiedSignature, signatureAcceptanceRules{
		validateKeyIdentity: func(keyIdentity string) error {
			if keyIdentity != expectedKeyIdentity {
				return InvalidSignatureError{msg: fmt.Sprintf("Signature by %s does not match expected fingerprint %s", keyIdentity, expectedKeyIdentity)}
			}
			return nil
		},
		validateSignedDockerReference: func(signedDockerReference string) error {
			signedRef, err := reference.ParseNormalizedNamed(signedDockerReference)
			if err != nil {
				return InvalidSignatureError{msg: fmt.Sprintf("Invalid docker reference %s in signature", signedDockerReference)}
			}
			if signedRef.String() != expectedRef.String() {
				return InvalidSignatureError{msg: fmt.Sprintf("Docker reference %s does not match %s",
					signedDockerReference, expectedDockerReference)}
			}
			return nil
		},
		validateSignedDockerManifestDigest: func(signedDockerManifestDigest digest.Digest) error {
			matches, err := manifest.MatchesDigest(unverifiedManifest, signedDockerManifestDigest)
			if err != nil {
				return err
			}
			if !matches {
				return InvalidSignatureError{msg: fmt.Sprintf("Signature for docker digest %q does not match", signedDockerManifestDigest)}
			}
			return nil
		},
	})
	if err != nil {
		return nil, err
	}
	return sig, nil
}
func newOpenshiftClient(ref openshiftReference) (*openshiftClient, error) {
	// We have already done this parsing in ParseReference, but thrown away
	// httpClient. So, parse again.
	// (We could also rework/split restClientFor to "get base URL" to be done
	// in ParseReference, and "get httpClient" to be done here.  But until/unless
	// we support non-default clusters, this is good enough.)

	// Overall, this is modelled on openshift/origin/pkg/cmd/util/clientcmd.New().ClientConfig() and openshift/origin/pkg/client.
	cmdConfig := defaultClientConfig()
	logrus.Debugf("cmdConfig: %#v", cmdConfig)
	restConfig, err := cmdConfig.ClientConfig()
	if err != nil {
		return nil, err
	}
	// REMOVED: SetOpenShiftDefaults (values are not overridable in config files, so hard-coded these defaults.)
	logrus.Debugf("restConfig: %#v", restConfig)
	baseURL, httpClient, err := restClientFor(restConfig)
	if err != nil {
		return nil, err
	}
	logrus.Debugf("URL: %#v", *baseURL)

	if httpClient == nil {
		httpClient = http.DefaultClient
	}

	return &openshiftClient{
		ref:         ref,
		baseURL:     baseURL,
		httpClient:  httpClient,
		bearerToken: restConfig.BearerToken,
		username:    restConfig.Username,
		password:    restConfig.Password,
	}, nil
}
func (c *openshiftClient) doRequest(ctx context.Context, method, path string, requestBody []byte) ([]byte, error) {
	url := *c.baseURL
	url.Path = path
	var requestBodyReader io.Reader
	if requestBody != nil {
		logrus.Debugf("Will send body: %s", requestBody)
		requestBodyReader = bytes.NewReader(requestBody)
	}
	req, err := http.NewRequest(method, url.String(), requestBodyReader)
	if err != nil {
		return nil, err
	}
	req = req.WithContext(ctx)

	if len(c.bearerToken) != 0 {
		req.Header.Set("Authorization", "Bearer "+c.bearerToken)
	} else if len(c.username) != 0 {
		req.SetBasicAuth(c.username, c.password)
	}
	req.Header.Set("Accept", "application/json, */*")
	req.Header.Set("User-Agent", fmt.Sprintf("skopeo/%s", version.Version))
	if requestBody != nil {
		req.Header.Set("Content-Type", "application/json")
	}

	logrus.Debugf("%s %s", method, url.String())
	res, err := c.httpClient.Do(req)
	if err != nil {
		return nil, err
	}
	defer res.Body.Close()
	body, err := ioutil.ReadAll(res.Body)
	if err != nil {
		return nil, err
	}
	logrus.Debugf("Got body: %s", body)
	// FIXME: Just throwing this useful information away only to try to guess later...
	logrus.Debugf("Got content-type: %s", res.Header.Get("Content-Type"))

	var status status
	statusValid := false
	if err := json.Unmarshal(body, &status); err == nil && len(status.Status) > 0 {
		statusValid = true
	}

	switch {
	case res.StatusCode == http.StatusSwitchingProtocols: // FIXME?! No idea why this weird case exists in k8s.io/kubernetes/pkg/client/restclient.
		if statusValid && status.Status != "Success" {
			return nil, errors.New(status.Message)
		}
	case res.StatusCode >= http.StatusOK && res.StatusCode <= http.StatusPartialContent:
		// OK.
	default:
		if statusValid {
			return nil, errors.New(status.Message)
		}
		return nil, errors.Errorf("HTTP error: status code: %d (%s), body: %s", res.StatusCode, http.StatusText(res.StatusCode), string(body))
	}

	return body, nil
}
func (c *openshiftClient) getImage(ctx context.Context, imageStreamImageName string) (*image, error) {
	// FIXME: validate components per validation.IsValidPathSegmentName?
	path := fmt.Sprintf("/oapi/v1/namespaces/%s/imagestreamimages/%s@%s", c.ref.namespace, c.ref.stream, imageStreamImageName)
	body, err := c.doRequest(ctx, "GET", path, nil)
	if err != nil {
		return nil, err
	}
	// Note: This does absolutely no kind/version checking or conversions.
	var isi imageStreamImage
	if err := json.Unmarshal(body, &isi); err != nil {
		return nil, err
	}
	return &isi.Image, nil
}
func (c *openshiftClient) convertDockerImageReference(ref string) (string, error) {
	parts := strings.SplitN(ref, "/", 2)
	if len(parts) != 2 {
		return "", errors.Errorf("Invalid format of docker reference %s: missing '/'", ref)
	}
	return reference.Domain(c.ref.dockerReference) + "/" + parts[1], nil
}
func (s *openshiftImageSource) ensureImageIsResolved(ctx context.Context) error {
	if s.docker != nil {
		return nil
	}

	// FIXME: validate components per validation.IsValidPathSegmentName?
	path := fmt.Sprintf("/oapi/v1/namespaces/%s/imagestreams/%s", s.client.ref.namespace, s.client.ref.stream)
	body, err := s.client.doRequest(ctx, "GET", path, nil)
	if err != nil {
		return err
	}
	// Note: This does absolutely no kind/version checking or conversions.
	var is imageStream
	if err := json.Unmarshal(body, &is); err != nil {
		return err
	}
	var te *tagEvent
	for _, tag := range is.Status.Tags {
		if tag.Tag != s.client.ref.dockerReference.Tag() {
			continue
		}
		if len(tag.Items) > 0 {
			te = &tag.Items[0]
			break
		}
	}
	if te == nil {
		return errors.Errorf("No matching tag found")
	}
	logrus.Debugf("tag event %#v", te)
	dockerRefString, err := s.client.convertDockerImageReference(te.DockerImageReference)
	if err != nil {
		return err
	}
	logrus.Debugf("Resolved reference %#v", dockerRefString)
	dockerRef, err := docker.ParseReference("//" + dockerRefString)
	if err != nil {
		return err
	}
	d, err := dockerRef.NewImageSource(ctx, s.sys)
	if err != nil {
		return err
	}
	s.docker = d
	s.imageStreamImageName = te.Image
	return nil
}
func newImageDestination(ctx context.Context, sys *types.SystemContext, ref openshiftReference) (types.ImageDestination, error) {
	client, err := newOpenshiftClient(ref)
	if err != nil {
		return nil, err
	}

	// FIXME: Should this always use a digest, not a tag? Uploading to Docker by tag requires the tag _inside_ the manifest to match,
	// i.e. a single signed image cannot be available under multiple tags.  But with types.ImageDestination, we don't know
	// the manifest digest at this point.
	dockerRefString := fmt.Sprintf("//%s/%s/%s:%s", reference.Domain(client.ref.dockerReference), client.ref.namespace, client.ref.stream, client.ref.dockerReference.Tag())
	dockerRef, err := docker.ParseReference(dockerRefString)
	if err != nil {
		return nil, err
	}
	docker, err := dockerRef.NewImageDestination(ctx, sys)
	if err != nil {
		return nil, err
	}

	return &openshiftImageDestination{
		client: client,
		docker: docker,
	}, nil
}
func newUntrustedSignature(dockerManifestDigest digest.Digest, dockerReference string) untrustedSignature {
	// Use intermediate variables for these values so that we can take their addresses.
	// Golang guarantees that they will have a new address on every execution.
	creatorID := "atomic " + version.Version
	timestamp := time.Now().Unix()
	return untrustedSignature{
		UntrustedDockerManifestDigest: dockerManifestDigest,
		UntrustedDockerReference:      dockerReference,
		UntrustedCreatorID:            &creatorID,
		UntrustedTimestamp:            &timestamp,
	}
}
func (s untrustedSignature) MarshalJSON() ([]byte, error) {
	if s.UntrustedDockerManifestDigest == "" || s.UntrustedDockerReference == "" {
		return nil, errors.New("Unexpected empty signature content")
	}
	critical := map[string]interface{}{
		"type":     signatureType,
		"image":    map[string]string{"docker-manifest-digest": s.UntrustedDockerManifestDigest.String()},
		"identity": map[string]string{"docker-reference": s.UntrustedDockerReference},
	}
	optional := map[string]interface{}{}
	if s.UntrustedCreatorID != nil {
		optional["creator"] = *s.UntrustedCreatorID
	}
	if s.UntrustedTimestamp != nil {
		optional["timestamp"] = *s.UntrustedTimestamp
	}
	signature := map[string]interface{}{
		"critical": critical,
		"optional": optional,
	}
	return json.Marshal(signature)
}
func (s *untrustedSignature) UnmarshalJSON(data []byte) error {
	err := s.strictUnmarshalJSON(data)
	if err != nil {
		if _, ok := err.(jsonFormatError); ok {
			err = InvalidSignatureError{msg: err.Error()}
		}
	}
	return err
}
func verifyAndExtractSignature(mech SigningMechanism, unverifiedSignature []byte, rules signatureAcceptanceRules) (*Signature, error) {
	signed, keyIdentity, err := mech.Verify(unverifiedSignature)
	if err != nil {
		return nil, err
	}
	if err := rules.validateKeyIdentity(keyIdentity); err != nil {
		return nil, err
	}

	var unmatchedSignature untrustedSignature
	if err := json.Unmarshal(signed, &unmatchedSignature); err != nil {
		return nil, InvalidSignatureError{msg: err.Error()}
	}
	if err := rules.validateSignedDockerManifestDigest(unmatchedSignature.UntrustedDockerManifestDigest); err != nil {
		return nil, err
	}
	if err := rules.validateSignedDockerReference(unmatchedSignature.UntrustedDockerReference); err != nil {
		return nil, err
	}
	// signatureAcceptanceRules have accepted this value.
	return &Signature{
		DockerManifestDigest: unmatchedSignature.UntrustedDockerManifestDigest,
		DockerReference:      unmatchedSignature.UntrustedDockerReference,
	}, nil
}
func (e *Endpoint) RewriteReference(ref reference.Named, prefix string) (reference.Named, error) {
	if ref == nil {
		return nil, fmt.Errorf("provided reference is nil")
	}
	if prefix == "" {
		return ref, nil
	}
	refString := ref.String()
	if refMatchesPrefix(refString, prefix) {
		newNamedRef := strings.Replace(refString, prefix, e.Location, 1)
		newParsedRef, err := reference.ParseNamed(newNamedRef)
		if newParsedRef != nil {
			logrus.Debugf("reference rewritten from '%v' to '%v'", refString, newParsedRef.String())
		}
		if err != nil {
			return nil, errors.Wrapf(err, "error rewriting reference")
		}
		return newParsedRef, nil
	}

	return nil, fmt.Errorf("invalid prefix '%v' for reference '%v'", prefix, refString)
}
func getV1Registries(config *tomlConfig) ([]Registry, error) {
	regMap := make(map[string]*Registry)
	// We must preserve the order of config.V1Registries.Search.Registries at least.  The order of the
	// other registries is not really important, but make it deterministic (the same for the same config file)
	// to minimize behavior inconsistency and not contribute to difficult-to-reproduce situations.
	registryOrder := []string{}

	getRegistry := func(location string) (*Registry, error) { // Note: _pointer_ to a long-lived object
		var err error
		location, err = parseLocation(location)
		if err != nil {
			return nil, err
		}
		reg, exists := regMap[location]
		if !exists {
			reg = &Registry{
				Endpoint: Endpoint{Location: location},
				Mirrors:  []Endpoint{},
				Prefix:   location,
			}
			regMap[location] = reg
			registryOrder = append(registryOrder, location)
		}
		return reg, nil
	}

	// Note: config.V1Registries.Search needs to be processed first to ensure registryOrder is populated in the right order
	// if one of the search registries is also in one of the other lists.
	for _, search := range config.V1TOMLConfig.Search.Registries {
		reg, err := getRegistry(search)
		if err != nil {
			return nil, err
		}
		reg.Search = true
	}
	for _, blocked := range config.V1TOMLConfig.Block.Registries {
		reg, err := getRegistry(blocked)
		if err != nil {
			return nil, err
		}
		reg.Blocked = true
	}
	for _, insecure := range config.V1TOMLConfig.Insecure.Registries {
		reg, err := getRegistry(insecure)
		if err != nil {
			return nil, err
		}
		reg.Insecure = true
	}

	registries := []Registry{}
	for _, location := range registryOrder {
		reg := regMap[location]
		registries = append(registries, *reg)
	}
	return registries, nil
}
func getConfigPath(ctx *types.SystemContext) string {
	confPath := systemRegistriesConfPath
	if ctx != nil {
		if ctx.SystemRegistriesConfPath != "" {
			confPath = ctx.SystemRegistriesConfPath
		} else if ctx.RootForImplicitAbsolutePaths != "" {
			confPath = filepath.Join(ctx.RootForImplicitAbsolutePaths, systemRegistriesConfPath)
		}
	}
	return confPath
}
func GetRegistries(ctx *types.SystemContext) ([]Registry, error) {
	configPath := getConfigPath(ctx)

	configMutex.Lock()
	defer configMutex.Unlock()
	// if the config has already been loaded, return the cached registries
	if registries, inCache := configCache[configPath]; inCache {
		return registries, nil
	}

	// load the config
	config, err := loadRegistryConf(configPath)
	if err != nil {
		// Return an empty []Registry if we use the default config,
		// which implies that the config path of the SystemContext
		// isn't set.  Note: if ctx.SystemRegistriesConfPath points to
		// the default config, we will still return an error.
		if os.IsNotExist(err) && (ctx == nil || ctx.SystemRegistriesConfPath == "") {
			return []Registry{}, nil
		}
		return nil, err
	}

	registries := config.Registries

	// backwards compatibility for v1 configs
	v1Registries, err := getV1Registries(config)
	if err != nil {
		return nil, err
	}
	if len(v1Registries) > 0 {
		if len(registries) > 0 {
			return nil, &InvalidRegistries{s: "mixing sysregistry v1/v2 is not supported"}
		}
		registries = v1Registries
	}

	registries, err = postProcessRegistries(registries)
	if err != nil {
		return nil, err
	}

	// populate the cache
	configCache[configPath] = registries

	return registries, err
}
func readRegistryConf(configPath string) ([]byte, error) {
	configBytes, err := ioutil.ReadFile(configPath)
	return configBytes, err
}
func (i *sourcedImage) Manifest(ctx context.Context) ([]byte, string, error) {
	return i.manifestBlob, i.manifestMIMEType, nil
}
func (r *tarballReference) ConfigUpdate(config imgspecv1.Image, annotations map[string]string) error {
	r.config = config
	if r.annotations == nil {
		r.annotations = make(map[string]string)
	}
	for k, v := range annotations {
		r.annotations[k] = v
	}
	return nil
}
func parseImageAndDockerReference(image types.UnparsedImage, s2 string) (reference.Named, reference.Named, error) {
	r1 := image.Reference().DockerReference()
	if r1 == nil {
		return nil, nil, PolicyRequirementError(fmt.Sprintf("Docker reference match attempted on image %s with no known Docker reference identity",
			transports.ImageName(image.Reference())))
	}
	r2, err := reference.ParseNormalizedNamed(s2)
	if err != nil {
		return nil, nil, err
	}
	return r1, r2, nil
}
func parseDockerReferences(s1, s2 string) (reference.Named, reference.Named, error) {
	r1, err := reference.ParseNormalizedNamed(s1)
	if err != nil {
		return nil, nil, err
	}
	r2, err := reference.ParseNormalizedNamed(s2)
	if err != nil {
		return nil, nil, err
	}
	return r1, r2, nil
}
func ListNames() []string {
	kt.mu.Lock()
	defer kt.mu.Unlock()
	deprecated := map[string]bool{
		"atomic": true,
	}
	var names []string
	for _, transport := range kt.transports {
		if !deprecated[transport.Name()] {
			names = append(names, transport.Name())
		}
	}
	sort.Strings(names)
	return names
}
func NewReference(image string, repo string) (types.ImageReference, error) {
	// image is not _really_ in a containers/image/docker/reference format;
	// as far as the libOSTree ociimage/* namespace is concerned, it is more or
	// less an arbitrary string with an implied tag.
	// Parse the image using reference.ParseNormalizedNamed so that we can
	// check whether the images has a tag specified and we can add ":latest" if needed
	ostreeImage, err := reference.ParseNormalizedNamed(image)
	if err != nil {
		return nil, err
	}

	if reference.IsNameOnly(ostreeImage) {
		image = image + ":latest"
	}

	resolved, err := explicitfilepath.ResolvePathToFullyExplicit(repo)
	if err != nil {
		// With os.IsNotExist(err), the parent directory of repo is also not existent;
		// that should ordinarily not happen, but it would be a bit weird to reject
		// references which do not specify a repo just because the implicit defaultOSTreeRepo
		// does not exist.
		if os.IsNotExist(err) && repo == defaultOSTreeRepo {
			resolved = repo
		} else {
			return nil, err
		}
	}
	// This is necessary to prevent directory paths returned by PolicyConfigurationNamespaces
	// from being ambiguous with values of PolicyConfigurationIdentity.
	if strings.Contains(resolved, ":") {
		return nil, errors.Errorf("Invalid OSTree reference %s@%s: path %s contains a colon", image, repo, resolved)
	}

	return ostreeReference{
		image:      image,
		branchName: encodeOStreeRef(image),
		repo:       resolved,
	}, nil
}
func (ref ostreeReference) signaturePath(index int) string {
	return filepath.Join("manifest", fmt.Sprintf("signature-%d", index+1))
}
func ValidateImageName(image string) error {
	if len(image) == 0 {
		return nil
	}

	var err error
	if !refRegexp.MatchString(image) {
		err = errors.Errorf("Invalid image %s", image)
	}
	return err
}
func SplitPathAndImage(reference string) (string, string) {
	if runtime.GOOS == "windows" {
		return splitPathAndImageWindows(reference)
	}
	return splitPathAndImageNonWindows(reference)
}
func ValidateOCIPath(path string) error {
	if runtime.GOOS == "windows" {
		// On Windows we must allow for a ':' as part of the path
		if strings.Count(path, ":") > 1 {
			return errors.Errorf("Invalid OCI reference: path %s contains more than one colon", path)
		}
	} else {
		if strings.Contains(path, ":") {
			return errors.Errorf("Invalid OCI reference: path %s contains a colon", path)
		}
	}
	return nil
}
func ValidateScope(scope string) error {
	var err error
	if runtime.GOOS == "windows" {
		err = validateScopeWindows(scope)
	} else {
		err = validateScopeNonWindows(scope)
	}
	if err != nil {
		return err
	}

	cleaned := filepath.Clean(scope)
	if cleaned != scope {
		return errors.Errorf(`Invalid scope %s: Uses non-canonical path format, perhaps try with path %s`, scope, cleaned)
	}

	return nil
}
func BlobInfoFromSchema2Descriptor(desc Schema2Descriptor) types.BlobInfo {
	return types.BlobInfo{
		Digest:    desc.Digest,
		Size:      desc.Size,
		URLs:      desc.URLs,
		MediaType: desc.MediaType,
	}
}
func Schema2FromManifest(manifest []byte) (*Schema2, error) {
	s2 := Schema2{}
	if err := json.Unmarshal(manifest, &s2); err != nil {
		return nil, err
	}
	return &s2, nil
}
func Schema2FromComponents(config Schema2Descriptor, layers []Schema2Descriptor) *Schema2 {
	return &Schema2{
		SchemaVersion:     2,
		MediaType:         DockerV2Schema2MediaType,
		ConfigDescriptor:  config,
		LayersDescriptors: layers,
	}
}
func SetAuthentication(sys *types.SystemContext, registry, username, password string) error {
	return modifyJSON(sys, func(auths *dockerConfigFile) (bool, error) {
		if ch, exists := auths.CredHelpers[registry]; exists {
			return false, setAuthToCredHelper(ch, registry, username, password)
		}

		creds := base64.StdEncoding.EncodeToString([]byte(username + ":" + password))
		newCreds := dockerAuthConfig{Auth: creds}
		auths.AuthConfigs[registry] = newCreds
		return true, nil
	})
}
func RemoveAuthentication(sys *types.SystemContext, registry string) error {
	return modifyJSON(sys, func(auths *dockerConfigFile) (bool, error) {
		// First try cred helpers.
		if ch, exists := auths.CredHelpers[registry]; exists {
			return false, deleteAuthFromCredHelper(ch, registry)
		}

		if _, ok := auths.AuthConfigs[registry]; ok {
			delete(auths.AuthConfigs, registry)
		} else if _, ok := auths.AuthConfigs[normalizeRegistry(registry)]; ok {
			delete(auths.AuthConfigs, normalizeRegistry(registry))
		} else {
			return false, ErrNotLoggedIn
		}
		return true, nil
	})
}
func RemoveAllAuthentication(sys *types.SystemContext) error {
	return modifyJSON(sys, func(auths *dockerConfigFile) (bool, error) {
		auths.CredHelpers = make(map[string]string)
		auths.AuthConfigs = make(map[string]dockerAuthConfig)
		return true, nil
	})
}
func readJSONFile(path string, legacyFormat bool) (dockerConfigFile, error) {
	var auths dockerConfigFile

	raw, err := ioutil.ReadFile(path)
	if err != nil {
		if os.IsNotExist(err) {
			auths.AuthConfigs = map[string]dockerAuthConfig{}
			return auths, nil
		}
		return dockerConfigFile{}, err
	}

	if legacyFormat {
		if err = json.Unmarshal(raw, &auths.AuthConfigs); err != nil {
			return dockerConfigFile{}, errors.Wrapf(err, "error unmarshaling JSON at %q", path)
		}
		return auths, nil
	}

	if err = json.Unmarshal(raw, &auths); err != nil {
		return dockerConfigFile{}, errors.Wrapf(err, "error unmarshaling JSON at %q", path)
	}

	return auths, nil
}
func modifyJSON(sys *types.SystemContext, editor func(auths *dockerConfigFile) (bool, error)) error {
	path, err := getPathToAuth(sys)
	if err != nil {
		return err
	}

	dir := filepath.Dir(path)
	if _, err := os.Stat(dir); os.IsNotExist(err) {
		if err = os.MkdirAll(dir, 0700); err != nil {
			return errors.Wrapf(err, "error creating directory %q", dir)
		}
	}

	auths, err := readJSONFile(path, false)
	if err != nil {
		return errors.Wrapf(err, "error reading JSON file %q", path)
	}

	updated, err := editor(&auths)
	if err != nil {
		return errors.Wrapf(err, "error updating %q", path)
	}
	if updated {
		newData, err := json.MarshalIndent(auths, "", "\t")
		if err != nil {
			return errors.Wrapf(err, "error marshaling JSON %q", path)
		}

		if err = ioutil.WriteFile(path, newData, 0755); err != nil {
			return errors.Wrapf(err, "error writing to file %q", path)
		}
	}

	return nil
}
func findAuthentication(registry, path string, legacyFormat bool) (string, string, error) {
	auths, err := readJSONFile(path, legacyFormat)
	if err != nil {
		return "", "", errors.Wrapf(err, "error reading JSON file %q", path)
	}

	// First try cred helpers. They should always be normalized.
	if ch, exists := auths.CredHelpers[registry]; exists {
		return getAuthFromCredHelper(ch, registry)
	}

	// I'm feeling lucky
	if val, exists := auths.AuthConfigs[registry]; exists {
		return decodeDockerAuth(val.Auth)
	}

	// bad luck; let's normalize the entries first
	registry = normalizeRegistry(registry)
	normalizedAuths := map[string]dockerAuthConfig{}
	for k, v := range auths.AuthConfigs {
		normalizedAuths[normalizeRegistry(k)] = v
	}
	if val, exists := normalizedAuths[registry]; exists {
		return decodeDockerAuth(val.Auth)
	}
	return "", "", nil
}
func NewDestination(dest io.Writer, ref reference.NamedTagged) *Destination {
	repoTags := []reference.NamedTagged{}
	if ref != nil {
		repoTags = append(repoTags, ref)
	}
	return &Destination{
		writer:   dest,
		tar:      tar.NewWriter(dest),
		repoTags: repoTags,
		blobs:    make(map[digest.Digest]types.BlobInfo),
	}
}
func (d *Destination) AddRepoTags(tags []reference.NamedTagged) {
	d.repoTags = append(d.repoTags, tags...)
}
func (d *Destination) writeLegacyLayerMetadata(layerDescriptors []manifest.Schema2Descriptor) (layerPaths []string, lastLayerID string, err error) {
	var chainID digest.Digest
	lastLayerID = ""
	for i, l := range layerDescriptors {
		// This chainID value matches the computation in docker/docker/layer.CreateChainID …
		if chainID == "" {
			chainID = l.Digest
		} else {
			chainID = digest.Canonical.FromString(chainID.String() + " " + l.Digest.String())
		}
		// … but note that this image ID does not match docker/docker/image/v1.CreateID. At least recent
		// versions allocate new IDs on load, as long as the IDs we use are unique / cannot loop.
		//
		// Overall, the goal of computing a digest dependent on the full history is to avoid reusing an image ID
		// (and possibly creating a loop in the "parent" links) if a layer with the same DiffID appears two or more
		// times in layersDescriptors.  The ChainID values are sufficient for this, the v1.CreateID computation
		// which also mixes in the full image configuration seems unnecessary, at least as long as we are storing
		// only a single image per tarball, i.e. all DiffID prefixes are unique (can’t differ only with
		// configuration).
		layerID := chainID.Hex()

		physicalLayerPath := l.Digest.Hex() + ".tar"
		// The layer itself has been stored into physicalLayerPath in PutManifest.
		// So, use that path for layerPaths used in the non-legacy manifest
		layerPaths = append(layerPaths, physicalLayerPath)
		// ... and create a symlink for the legacy format;
		if err := d.sendSymlink(filepath.Join(layerID, legacyLayerFileName), filepath.Join("..", physicalLayerPath)); err != nil {
			return nil, "", errors.Wrap(err, "Error creating layer symbolic link")
		}

		b := []byte("1.0")
		if err := d.sendBytes(filepath.Join(layerID, legacyVersionFileName), b); err != nil {
			return nil, "", errors.Wrap(err, "Error writing VERSION file")
		}

		// The legacy format requires a config file per layer
		layerConfig := make(map[string]interface{})
		layerConfig["id"] = layerID

		// The root layer doesn't have any parent
		if lastLayerID != "" {
			layerConfig["parent"] = lastLayerID
		}
		// The root layer configuration file is generated by using subpart of the image configuration
		if i == len(layerDescriptors)-1 {
			var config map[string]*json.RawMessage
			err := json.Unmarshal(d.config, &config)
			if err != nil {
				return nil, "", errors.Wrap(err, "Error unmarshaling config")
			}
			for _, attr := range [7]string{"architecture", "config", "container", "container_config", "created", "docker_version", "os"} {
				layerConfig[attr] = config[attr]
			}
		}
		b, err := json.Marshal(layerConfig)
		if err != nil {
			return nil, "", errors.Wrap(err, "Error marshaling layer config")
		}
		if err := d.sendBytes(filepath.Join(layerID, legacyConfigFileName), b); err != nil {
			return nil, "", errors.Wrap(err, "Error writing config json file")
		}

		lastLayerID = layerID
	}
	return layerPaths, lastLayerID, nil
}
func (d *Destination) sendSymlink(path string, target string) error {
	hdr, err := tar.FileInfoHeader(&tarFI{path: path, size: 0, isSymlink: true}, target)
	if err != nil {
		return nil
	}
	logrus.Debugf("Sending as tar link %s -> %s", path, target)
	return d.tar.WriteHeader(hdr)
}
func (d *Destination) sendBytes(path string, b []byte) error {
	return d.sendFile(path, int64(len(b)), bytes.NewReader(b))
}
func (d *Destination) sendFile(path string, expectedSize int64, stream io.Reader) error {
	hdr, err := tar.FileInfoHeader(&tarFI{path: path, size: expectedSize}, "")
	if err != nil {
		return nil
	}
	logrus.Debugf("Sending as tar file %s", path)
	if err := d.tar.WriteHeader(hdr); err != nil {
		return err
	}
	// TODO: This can take quite some time, and should ideally be cancellable using a context.Context.
	size, err := io.Copy(d.tar, stream)
	if err != nil {
		return err
	}
	if size != expectedSize {
		return errors.Errorf("Size mismatch when copying %s, expected %d, got %d", path, expectedSize, size)
	}
	return nil
}
func (d *Destination) Commit(ctx context.Context) error {
	return d.tar.Close()
}
func imageMatchesRepo(image *storage.Image, ref reference.Named) bool {
	repo := ref.Name()
	for _, name := range image.Names {
		if named, err := reference.ParseNormalizedNamed(name); err == nil {
			if named.Name() == repo {
				return true
			}
		}
	}
	return false
}
func (s *storageReference) resolveImage() (*storage.Image, error) {
	var loadedImage *storage.Image
	if s.id == "" && s.named != nil {
		// Look for an image that has the expanded reference name as an explicit Name value.
		image, err := s.transport.store.Image(s.named.String())
		if image != nil && err == nil {
			loadedImage = image
			s.id = image.ID
		}
	}
	if s.id == "" && s.named != nil {
		if digested, ok := s.named.(reference.Digested); ok {
			// Look for an image with the specified digest that has the same name,
			// though possibly with a different tag or digest, as a Name value, so
			// that the canonical reference can be implicitly resolved to the image.
			images, err := s.transport.store.ImagesByDigest(digested.Digest())
			if err == nil && len(images) > 0 {
				for _, image := range images {
					if imageMatchesRepo(image, s.named) {
						loadedImage = image
						s.id = image.ID
						break
					}
				}
			}
		}
	}
	if s.id == "" {
		logrus.Debugf("reference %q does not resolve to an image ID", s.StringWithinTransport())
		return nil, errors.Wrapf(ErrNoSuchImage, "reference %q does not resolve to an image ID", s.StringWithinTransport())
	}
	if loadedImage == nil {
		img, err := s.transport.store.Image(s.id)
		if err != nil {
			return nil, errors.Wrapf(err, "error reading image %q", s.id)
		}
		loadedImage = img
	}
	if s.named != nil {
		if !imageMatchesRepo(loadedImage, s.named) {
			logrus.Errorf("no image matching reference %q found", s.StringWithinTransport())
			return nil, ErrNoSuchImage
		}
	}
	// Default to having the image digest that we hand back match the most recently
	// added manifest...
	if digest, ok := loadedImage.BigDataDigests[storage.ImageDigestBigDataKey]; ok {
		loadedImage.Digest = digest
	}
	// ... unless the named reference says otherwise, and it matches one of the digests
	// in the image.  For those cases, set the Digest field to that value, for the
	// sake of older consumers that don't know there's a whole list in there now.
	if s.named != nil {
		if digested, ok := s.named.(reference.Digested); ok {
			for _, digest := range loadedImage.Digests {
				if digest == digested.Digest() {
					loadedImage.Digest = digest
					break
				}
			}
		}
	}
	return loadedImage, nil
}
func (s storageReference) Transport() types.ImageTransport {
	return &storageTransport{
		store:         s.transport.store,
		defaultUIDMap: s.transport.defaultUIDMap,
		defaultGIDMap: s.transport.defaultGIDMap,
	}
}
func (s storageReference) StringWithinTransport() string {
	optionsList := ""
	options := s.transport.store.GraphOptions()
	if len(options) > 0 {
		optionsList = ":" + strings.Join(options, ",")
	}
	res := "[" + s.transport.store.GraphDriverName() + "@" + s.transport.store.GraphRoot() + "+" + s.transport.store.RunRoot() + optionsList + "]"
	if s.named != nil {
		res = res + s.named.String()
	}
	if s.id != "" {
		res = res + "@" + s.id
	}
	return res
}
func (s storageReference) PolicyConfigurationNamespaces() []string {
	storeSpec := "[" + s.transport.store.GraphDriverName() + "@" + s.transport.store.GraphRoot() + "]"
	driverlessStoreSpec := "[" + s.transport.store.GraphRoot() + "]"
	namespaces := []string{}
	if s.named != nil {
		if s.id != "" {
			// The reference without the ID is also a valid namespace.
			namespaces = append(namespaces, storeSpec+s.named.String())
		}
		tagged, isTagged := s.named.(reference.Tagged)
		_, isDigested := s.named.(reference.Digested)
		if isTagged && isDigested { // s.named is "name:tag@digest"; add a "name:tag" parent namespace.
			namespaces = append(namespaces, storeSpec+s.named.Name()+":"+tagged.Tag())
		}
		components := strings.Split(s.named.Name(), "/")
		for len(components) > 0 {
			namespaces = append(namespaces, storeSpec+strings.Join(components, "/"))
			components = components[:len(components)-1]
		}
	}
	namespaces = append(namespaces, storeSpec)
	namespaces = append(namespaces, driverlessStoreSpec)
	return namespaces
}
func GzipDecompressor(r io.Reader) (io.ReadCloser, error) {
	return pgzip.NewReader(r)
}
func Bzip2Decompressor(r io.Reader) (io.ReadCloser, error) {
	return ioutil.NopCloser(bzip2.NewReader(r)), nil
}
func XzDecompressor(r io.Reader) (io.ReadCloser, error) {
	r, err := xz.NewReader(r)
	if err != nil {
		return nil, err
	}
	return ioutil.NopCloser(r), nil
}
func DetectCompression(input io.Reader) (DecompressorFunc, io.Reader, error) {
	buffer := [8]byte{}

	n, err := io.ReadAtLeast(input, buffer[:], len(buffer))
	if err != nil && err != io.EOF && err != io.ErrUnexpectedEOF {
		// This is a “real” error. We could just ignore it this time, process the data we have, and hope that the source will report the same error again.
		// Instead, fail immediately with the original error cause instead of a possibly secondary/misleading error returned later.
		return nil, nil, err
	}

	var decompressor DecompressorFunc
	for name, algo := range compressionAlgos {
		if bytes.HasPrefix(buffer[:n], algo.prefix) {
			logrus.Debugf("Detected compression format %s", name)
			decompressor = algo.decompressor
			break
		}
	}
	if decompressor == nil {
		logrus.Debugf("No compression detected")
	}

	return decompressor, io.MultiReader(bytes.NewReader(buffer[:n]), input), nil
}
func newImageDestination(sys *types.SystemContext, ref dockerReference) (types.ImageDestination, error) {
	c, err := newDockerClientFromRef(sys, ref, true, "pull,push")
	if err != nil {
		return nil, err
	}
	return &dockerImageDestination{
		ref: ref,
		c:   c,
	}, nil
}
func (d *dockerImageDestination) mountBlob(ctx context.Context, srcRepo reference.Named, srcDigest digest.Digest, extraScope *authScope) error {
	u := url.URL{
		Path: fmt.Sprintf(blobUploadPath, reference.Path(d.ref.ref)),
		RawQuery: url.Values{
			"mount": {srcDigest.String()},
			"from":  {reference.Path(srcRepo)},
		}.Encode(),
	}
	mountPath := u.String()
	logrus.Debugf("Trying to mount %s", mountPath)
	res, err := d.c.makeRequest(ctx, "POST", mountPath, nil, nil, v2Auth, extraScope)
	if err != nil {
		return err
	}
	defer res.Body.Close()
	switch res.StatusCode {
	case http.StatusCreated:
		logrus.Debugf("... mount OK")
		return nil
	case http.StatusAccepted:
		// Oops, the mount was ignored - either the registry does not support that yet, or the blob does not exist; the registry has started an ordinary upload process.
		// Abort, and let the ultimate caller do an upload when its ready, instead.
		// NOTE: This does not really work in docker/distribution servers, which incorrectly require the "delete" action in the token's scope, and is thus entirely untested.
		uploadLocation, err := res.Location()
		if err != nil {
			return errors.Wrap(err, "Error determining upload URL after a mount attempt")
		}
		logrus.Debugf("... started an upload instead of mounting, trying to cancel at %s", uploadLocation.String())
		res2, err := d.c.makeRequestToResolvedURL(ctx, "DELETE", uploadLocation.String(), nil, nil, -1, v2Auth, extraScope)
		if err != nil {
			logrus.Debugf("Error trying to cancel an inadvertent upload: %s", err)
		} else {
			defer res2.Body.Close()
			if res2.StatusCode != http.StatusNoContent {
				logrus.Debugf("Error trying to cancel an inadvertent upload, status %s", http.StatusText(res.StatusCode))
			}
		}
		// Anyway, if canceling the upload fails, ignore it and return the more important error:
		return fmt.Errorf("Mounting %s from %s to %s started an upload instead", srcDigest, srcRepo.Name(), d.ref.ref.Name())
	default:
		logrus.Debugf("Error mounting, response %#v", *res)
		return errors.Wrapf(client.HandleErrorResponse(res), "Error mounting %s from %s to %s", srcDigest, srcRepo.Name(), d.ref.ref.Name())
	}
}
func bicTransportScope(ref dockerReference) types.BICTransportScope {
	// Blobs can be reused across the whole registry.
	return types.BICTransportScope{Opaque: reference.Domain(ref.ref)}
}
func newBICLocationReference(ref dockerReference) types.BICLocationReference {
	// Blobs are scoped to repositories (the tag/digest are not necessary to reuse a blob).
	return types.BICLocationReference{Opaque: ref.ref.Name()}
}
func parseBICLocationReference(lr types.BICLocationReference) (reference.Named, error) {
	return reference.ParseNormalizedNamed(lr.Opaque)
}
func NewSourceFromStream(inputStream io.Reader) (*Source, error) {
	// FIXME: use SystemContext here.
	// Save inputStream to a temporary file
	tarCopyFile, err := ioutil.TempFile(tmpdir.TemporaryDirectoryForBigFiles(), "docker-tar")
	if err != nil {
		return nil, errors.Wrap(err, "error creating temporary file")
	}
	defer tarCopyFile.Close()

	succeeded := false
	defer func() {
		if !succeeded {
			os.Remove(tarCopyFile.Name())
		}
	}()

	// In order to be compatible with docker-load, we need to support
	// auto-decompression (it's also a nice quality-of-life thing to avoid
	// giving users really confusing "invalid tar header" errors).
	uncompressedStream, _, err := compression.AutoDecompress(inputStream)
	if err != nil {
		return nil, errors.Wrap(err, "Error auto-decompressing input")
	}
	defer uncompressedStream.Close()

	// Copy the plain archive to the temporary file.
	//
	// TODO: This can take quite some time, and should ideally be cancellable
	//       using a context.Context.
	if _, err := io.Copy(tarCopyFile, uncompressedStream); err != nil {
		return nil, errors.Wrapf(err, "error copying contents to temporary file %q", tarCopyFile.Name())
	}
	succeeded = true

	return &Source{
		tarPath:              tarCopyFile.Name(),
		removeTarPathOnClose: true,
	}, nil
}
func (s *Source) readTarComponent(path string) ([]byte, error) {
	file, err := s.openTarComponent(path)
	if err != nil {
		return nil, errors.Wrapf(err, "Error loading tar component %s", path)
	}
	defer file.Close()
	bytes, err := ioutil.ReadAll(file)
	if err != nil {
		return nil, err
	}
	return bytes, nil
}
func (s *Source) ensureCachedDataIsPresent() error {
	s.cacheDataLock.Do(func() {
		// Read and parse manifest.json
		tarManifest, err := s.loadTarManifest()
		if err != nil {
			s.cacheDataResult = err
			return
		}

		// Check to make sure length is 1
		if len(tarManifest) != 1 {
			s.cacheDataResult = errors.Errorf("Unexpected tar manifest.json: expected 1 item, got %d", len(tarManifest))
			return
		}

		// Read and parse config.
		configBytes, err := s.readTarComponent(tarManifest[0].Config)
		if err != nil {
			s.cacheDataResult = err
			return
		}
		var parsedConfig manifest.Schema2Image // There's a lot of info there, but we only really care about layer DiffIDs.
		if err := json.Unmarshal(configBytes, &parsedConfig); err != nil {
			s.cacheDataResult = errors.Wrapf(err, "Error decoding tar config %s", tarManifest[0].Config)
			return
		}

		knownLayers, err := s.prepareLayerData(&tarManifest[0], &parsedConfig)
		if err != nil {
			s.cacheDataResult = err
			return
		}

		// Success; commit.
		s.tarManifest = &tarManifest[0]
		s.configBytes = configBytes
		s.configDigest = digest.FromBytes(configBytes)
		s.orderedDiffIDList = parsedConfig.RootFS.DiffIDs
		s.knownLayers = knownLayers
	})
	return s.cacheDataResult
}
func (s *Source) loadTarManifest() ([]ManifestItem, error) {
	// FIXME? Do we need to deal with the legacy format?
	bytes, err := s.readTarComponent(manifestFileName)
	if err != nil {
		return nil, err
	}
	var items []ManifestItem
	if err := json.Unmarshal(bytes, &items); err != nil {
		return nil, errors.Wrap(err, "Error decoding tar manifest.json")
	}
	return items, nil
}
func (s *Source) Close() error {
	if s.removeTarPathOnClose {
		return os.Remove(s.tarPath)
	}
	return nil
}
func newImageDestination(ctx context.Context, sys *types.SystemContext, ref daemonReference) (types.ImageDestination, error) {
	if ref.ref == nil {
		return nil, errors.Errorf("Invalid destination docker-daemon:%s: a destination must be a name:tag", ref.StringWithinTransport())
	}
	namedTaggedRef, ok := ref.ref.(reference.NamedTagged)
	if !ok {
		return nil, errors.Errorf("Invalid destination docker-daemon:%s: a destination must be a name:tag", ref.StringWithinTransport())
	}

	var mustMatchRuntimeOS = true
	if sys != nil && sys.DockerDaemonHost != client.DefaultDockerHost {
		mustMatchRuntimeOS = false
	}

	c, err := newDockerClient(sys)
	if err != nil {
		return nil, errors.Wrap(err, "Error initializing docker engine client")
	}

	reader, writer := io.Pipe()
	// Commit() may never be called, so we may never read from this channel; so, make this buffered to allow imageLoadGoroutine to write status and terminate even if we never read it.
	statusChannel := make(chan error, 1)

	goroutineContext, goroutineCancel := context.WithCancel(ctx)
	go imageLoadGoroutine(goroutineContext, c, reader, statusChannel)

	return &daemonImageDestination{
		ref:                ref,
		mustMatchRuntimeOS: mustMatchRuntimeOS,
		Destination:        tarfile.NewDestination(writer, namedTaggedRef),
		goroutineCancel:    goroutineCancel,
		statusChannel:      statusChannel,
		writer:             writer,
		committed:          false,
	}, nil
}
func imageLoadGoroutine(ctx context.Context, c *client.Client, reader *io.PipeReader, statusChannel chan<- error) {
	err := errors.New("Internal error: unexpected panic in imageLoadGoroutine")
	defer func() {
		logrus.Debugf("docker-daemon: sending done, status %v", err)
		statusChannel <- err
	}()
	defer func() {
		if err == nil {
			reader.Close()
		} else {
			reader.CloseWithError(err)
		}
	}()

	resp, err := c.ImageLoad(ctx, reader, true)
	if err != nil {
		err = errors.Wrap(err, "Error saving image to docker engine")
		return
	}
	defer resp.Body.Close()
}
func NewReference(file, image string) (types.ImageReference, error) {
	resolved, err := explicitfilepath.ResolvePathToFullyExplicit(file)
	if err != nil {
		return nil, err
	}

	if err := internal.ValidateOCIPath(file); err != nil {
		return nil, err
	}

	if err := internal.ValidateImageName(image); err != nil {
		return nil, err
	}

	return ociArchiveReference{file: file, resolvedFile: resolved, image: image}, nil
}
func createOCIRef(image string) (tempDirOCIRef, error) {
	dir, err := ioutil.TempDir(tmpdir.TemporaryDirectoryForBigFiles(), "oci")
	if err != nil {
		return tempDirOCIRef{}, errors.Wrapf(err, "error creating temp directory")
	}
	ociRef, err := ocilayout.NewReference(dir, image)
	if err != nil {
		return tempDirOCIRef{}, err
	}

	tempDirRef := tempDirOCIRef{tempDirectory: dir, ociRefExtracted: ociRef}
	return tempDirRef, nil
}
func createUntarTempDir(ref ociArchiveReference) (tempDirOCIRef, error) {
	tempDirRef, err := createOCIRef(ref.image)
	if err != nil {
		return tempDirOCIRef{}, errors.Wrap(err, "error creating oci reference")
	}
	src := ref.resolvedFile
	dst := tempDirRef.tempDirectory
	// TODO: This can take quite some time, and should ideally be cancellable using a context.Context.
	if err := archive.UntarPath(src, dst); err != nil {
		if err := tempDirRef.deleteTempDir(); err != nil {
			return tempDirOCIRef{}, errors.Wrapf(err, "error deleting temp directory %q", tempDirRef.tempDirectory)
		}
		return tempDirOCIRef{}, errors.Wrapf(err, "error untarring file %q", tempDirRef.tempDirectory)
	}
	return tempDirRef, nil
}
func destructivelyPrioritizeReplacementCandidatesWithMax(cs []CandidateWithTime, primaryDigest, uncompressedDigest digest.Digest, maxCandidates int) []types.BICReplacementCandidate {
	// We don't need to use sort.Stable() because nanosecond timestamps are (presumably?) unique, so no two elements should
	// compare equal.
	sort.Sort(&candidateSortState{
		cs:                 cs,
		primaryDigest:      primaryDigest,
		uncompressedDigest: uncompressedDigest,
	})

	resLength := len(cs)
	if resLength > maxCandidates {
		resLength = maxCandidates
	}
	res := make([]types.BICReplacementCandidate, resLength)
	for i := range res {
		res[i] = cs[i].Candidate
	}
	return res
}
func newImageDestination(ref ostreeReference, tmpDirPath string) (types.ImageDestination, error) {
	tmpDirPath = filepath.Join(tmpDirPath, ref.branchName)
	if err := ensureDirectoryExists(tmpDirPath); err != nil {
		return nil, err
	}
	return &ostreeImageDestination{ref, "", manifestSchema{}, tmpDirPath, map[string]*blobToImport{}, "", 0, nil}, nil
}
func resolveExistingPathToFullyExplicit(path string) (string, error) {
	resolved, err := filepath.Abs(path)
	if err != nil {
		return "", err // Coverage: This can fail only if os.Getwd() fails.
	}
	resolved, err = filepath.EvalSymlinks(resolved)
	if err != nil {
		return "", err
	}
	return filepath.Clean(resolved), nil
}
func newImageDestination(ref dirReference, compress bool) (types.ImageDestination, error) {
	d := &dirImageDestination{ref: ref, compress: compress}

	// If directory exists check if it is empty
	// if not empty, check whether the contents match that of a container image directory and overwrite the contents
	// if the contents don't match throw an error
	dirExists, err := pathExists(d.ref.resolvedPath)
	if err != nil {
		return nil, errors.Wrapf(err, "error checking for path %q", d.ref.resolvedPath)
	}
	if dirExists {
		isEmpty, err := isDirEmpty(d.ref.resolvedPath)
		if err != nil {
			return nil, err
		}

		if !isEmpty {
			versionExists, err := pathExists(d.ref.versionPath())
			if err != nil {
				return nil, errors.Wrapf(err, "error checking if path exists %q", d.ref.versionPath())
			}
			if versionExists {
				contents, err := ioutil.ReadFile(d.ref.versionPath())
				if err != nil {
					return nil, err
				}
				// check if contents of version file is what we expect it to be
				if string(contents) != version {
					return nil, ErrNotContainerImageDir
				}
			} else {
				return nil, ErrNotContainerImageDir
			}
			// delete directory contents so that only one image is in the directory at a time
			if err = removeDirContents(d.ref.resolvedPath); err != nil {
				return nil, errors.Wrapf(err, "error erasing contents in %q", d.ref.resolvedPath)
			}
			logrus.Debugf("overwriting existing container image directory %q", d.ref.resolvedPath)
		}
	} else {
		// create directory if it doesn't exist
		if err := os.MkdirAll(d.ref.resolvedPath, 0755); err != nil {
			return nil, errors.Wrapf(err, "unable to create directory %q", d.ref.resolvedPath)
		}
	}
	// create version file
	err = ioutil.WriteFile(d.ref.versionPath(), []byte(version), 0644)
	if err != nil {
		return nil, errors.Wrapf(err, "error creating version file %q", d.ref.versionPath())
	}
	return d, nil
}
func isDirEmpty(path string) (bool, error) {
	files, err := ioutil.ReadDir(path)
	if err != nil {
		return false, err
	}
	return len(files) == 0, nil
}
func removeDirContents(path string) error {
	files, err := ioutil.ReadDir(path)
	if err != nil {
		return err
	}

	for _, file := range files {
		if err := os.RemoveAll(filepath.Join(path, file.Name())); err != nil {
			return err
		}
	}
	return nil
}
func GetRepositoryTags(ctx context.Context, sys *types.SystemContext, ref types.ImageReference) ([]string, error) {
	dr, ok := ref.(dockerReference)
	if !ok {
		return nil, errors.Errorf("ref must be a dockerReference")
	}

	path := fmt.Sprintf(tagsPath, reference.Path(dr.ref))
	client, err := newDockerClientFromRef(sys, dr, false, "pull")
	if err != nil {
		return nil, errors.Wrap(err, "failed to create client")
	}

	tags := make([]string, 0)

	for {
		res, err := client.makeRequest(ctx, "GET", path, nil, nil, v2Auth, nil)
		if err != nil {
			return nil, err
		}
		defer res.Body.Close()
		if res.StatusCode != http.StatusOK {
			// print url also
			return nil, errors.Errorf("Invalid status code returned when fetching tags list %d (%s)", res.StatusCode, http.StatusText(res.StatusCode))
		}

		var tagsHolder struct {
			Tags []string
		}
		if err = json.NewDecoder(res.Body).Decode(&tagsHolder); err != nil {
			return nil, err
		}
		tags = append(tags, tagsHolder.Tags...)

		link := res.Header.Get("Link")
		if link == "" {
			break
		}

		linkURLStr := strings.Trim(strings.Split(link, ";")[0], "<>")
		linkURL, err := url.Parse(linkURLStr)
		if err != nil {
			return tags, err
		}

		// can be relative or absolute, but we only want the path (and I
		// guess we're in trouble if it forwards to a new place...)
		path = linkURL.Path
		if linkURL.RawQuery != "" {
			path += "?"
			path += linkURL.RawQuery
		}
	}
	return tags, nil
}
func DefaultCache(sys *types.SystemContext) types.BlobInfoCache {
	dir, err := blobInfoCacheDir(sys, getRootlessUID())
	if err != nil {
		logrus.Debugf("Error determining a location for %s, using a memory-only cache", blobInfoCacheFilename)
		return memory.New()
	}
	path := filepath.Join(dir, blobInfoCacheFilename)
	if err := os.MkdirAll(dir, 0700); err != nil {
		logrus.Debugf("Error creating parent directories for %s, using a memory-only cache: %v", blobInfoCacheFilename, err)
		return memory.New()
	}

	logrus.Debugf("Using blob info cache at %s", path)
	return boltdb.New(path)
}
func (mem *cache) uncompressedDigestLocked(anyDigest digest.Digest) digest.Digest {
	if d, ok := mem.uncompressedDigests[anyDigest]; ok {
		return d
	}
	// Presence in digestsByUncompressed implies that anyDigest must already refer to an uncompressed digest.
	// This way we don't have to waste storage space with trivial (uncompressed, uncompressed) mappings
	// when we already record a (compressed, uncompressed) pair.
	if m, ok := mem.digestsByUncompressed[anyDigest]; ok && len(m) > 0 {
		return anyDigest
	}
	return ""
}
func (d *ociArchiveImageDestination) Close() error {
	defer d.tempDirRef.deleteTempDir()
	return d.unpackedDest.Close()
}
func (d *ociArchiveImageDestination) PutManifest(ctx context.Context, m []byte) error {
	return d.unpackedDest.PutManifest(ctx, m)
}
func (d *ociArchiveImageDestination) Commit(ctx context.Context) error {
	if err := d.unpackedDest.Commit(ctx); err != nil {
		return errors.Wrapf(err, "error storing image %q", d.ref.image)
	}

	// path of directory to tar up
	src := d.tempDirRef.tempDirectory
	// path to save tarred up file
	dst := d.ref.resolvedFile
	return tarDirectory(src, dst)
}
func tarDirectory(src, dst string) error {
	// input is a stream of bytes from the archive of the directory at path
	input, err := archive.Tar(src, archive.Uncompressed)
	if err != nil {
		return errors.Wrapf(err, "error retrieving stream of bytes from %q", src)
	}

	// creates the tar file
	outFile, err := os.Create(dst)
	if err != nil {
		return errors.Wrapf(err, "error creating tar file %q", dst)
	}
	defer outFile.Close()

	// copies the contents of the directory to the tar file
	// TODO: This can take quite some time, and should ideally be cancellable using a context.Context.
	_, err = io.Copy(outFile, input)

	return err
}
func (s storageTransport) ParseStoreReference(store storage.Store, ref string) (*storageReference, error) {
	if ref == "" {
		return nil, errors.Wrapf(ErrInvalidReference, "%q is an empty reference", ref)
	}
	if ref[0] == '[' {
		// Ignore the store specifier.
		closeIndex := strings.IndexRune(ref, ']')
		if closeIndex < 1 {
			return nil, errors.Wrapf(ErrInvalidReference, "store specifier in %q did not end", ref)
		}
		ref = ref[closeIndex+1:]
	}

	// The reference may end with an image ID.  Image IDs and digests use the same "@" separator;
	// here we only peel away an image ID, and leave digests alone.
	split := strings.LastIndex(ref, "@")
	id := ""
	if split != -1 {
		possibleID := ref[split+1:]
		if possibleID == "" {
			return nil, errors.Wrapf(ErrInvalidReference, "empty trailing digest or ID in %q", ref)
		}
		// If it looks like a digest, leave it alone for now.
		if _, err := digest.Parse(possibleID); err != nil {
			// Otherwise…
			if idSum, err := digest.Parse("sha256:" + possibleID); err == nil && idSum.Validate() == nil {
				id = possibleID // … it is a full ID
			} else if img, err := store.Image(possibleID); err == nil && img != nil && len(possibleID) >= minimumTruncatedIDLength && strings.HasPrefix(img.ID, possibleID) {
				// … it is a truncated version of the ID of an image that's present in local storage,
				// so we might as well use the expanded value.
				id = img.ID
			} else {
				return nil, errors.Wrapf(ErrInvalidReference, "%q does not look like an image ID or digest", possibleID)
			}
			// We have recognized an image ID; peel it off.
			ref = ref[:split]
		}
	}

	// If we only have one @-delimited portion, then _maybe_ it's a truncated image ID.  Only check on that if it's
	// at least of what we guess is a reasonable minimum length, because we don't want a really short value
	// like "a" matching an image by ID prefix when the input was actually meant to specify an image name.
	if id == "" && len(ref) >= minimumTruncatedIDLength && !strings.ContainsAny(ref, "@:") {
		if img, err := store.Image(ref); err == nil && img != nil && strings.HasPrefix(img.ID, ref) {
			// It's a truncated version of the ID of an image that's present in local storage;
			// we need to expand it.
			id = img.ID
			ref = ""
		}
	}

	var named reference.Named
	// Unless we have an un-named "ID" or "@ID" reference (where ID might only have been a prefix), which has been
	// completely parsed above, the initial portion should be a name, possibly with a tag and/or a digest..
	if ref != "" {
		var err error
		named, err = reference.ParseNormalizedNamed(ref)
		if err != nil {
			return nil, errors.Wrapf(err, "error parsing named reference %q", ref)
		}
		named = reference.TagNameOnly(named)
	}

	result, err := newReference(storageTransport{store: store, defaultUIDMap: s.defaultUIDMap, defaultGIDMap: s.defaultGIDMap}, named, id)
	if err != nil {
		return nil, err
	}
	logrus.Debugf("parsed reference into %q", result.StringWithinTransport())
	return result, nil
}
func chooseDigestFromManifestList(sys *types.SystemContext, blob []byte) (digest.Digest, error) {
	wantedArch := runtime.GOARCH
	if sys != nil && sys.ArchitectureChoice != "" {
		wantedArch = sys.ArchitectureChoice
	}
	wantedOS := runtime.GOOS
	if sys != nil && sys.OSChoice != "" {
		wantedOS = sys.OSChoice
	}

	list := manifestList{}
	if err := json.Unmarshal(blob, &list); err != nil {
		return "", err
	}
	for _, d := range list.Manifests {
		if d.Platform.Architecture == wantedArch && d.Platform.OS == wantedOS {
			return d.Digest, nil
		}
	}
	return "", fmt.Errorf("no image found in manifest list for architecture %s, OS %s", wantedArch, wantedOS)
}
func ChooseManifestInstanceFromManifestList(ctx context.Context, sys *types.SystemContext, src types.UnparsedImage) (digest.Digest, error) {
	// For now this only handles manifest.DockerV2ListMediaType; we can generalize it later,
	// probably along with manifest list editing.
	blob, mt, err := src.Manifest(ctx)
	if err != nil {
		return "", err
	}
	if mt != manifest.DockerV2ListMediaType {
		return "", fmt.Errorf("Internal error: Trying to select an image from a non-manifest-list manifest type %s", mt)
	}
	return chooseDigestFromManifestList(sys, blob)
}
func manifestSchema1FromComponents(ref reference.Named, fsLayers []manifest.Schema1FSLayers, history []manifest.Schema1History, architecture string) (genericManifest, error) {
	m, err := manifest.Schema1FromComponents(ref, fsLayers, history, architecture)
	if err != nil {
		return nil, err
	}
	return &manifestSchema1{m: m}, nil
}
func (s *dockerImageSource) manifestDigest(ctx context.Context, instanceDigest *digest.Digest) (digest.Digest, error) {
	if instanceDigest != nil {
		return *instanceDigest, nil
	}
	if digested, ok := s.ref.ref.(reference.Digested); ok {
		d := digested.Digest()
		if d.Algorithm() == digest.Canonical {
			return d, nil
		}
	}
	if err := s.ensureManifestIsLoaded(ctx); err != nil {
		return "", err
	}
	return manifest.Digest(s.cachedManifest)
}
func deleteImage(ctx context.Context, sys *types.SystemContext, ref dockerReference) error {
	// docker/distribution does not document what action should be used for deleting images.
	//
	// Current docker/distribution requires "pull" for reading the manifest and "delete" for deleting it.
	// quay.io requires "push" (an explicit "pull" is unnecessary), does not grant any token (fails parsing the request) if "delete" is included.
	// OpenShift ignores the action string (both the password and the token is an OpenShift API token identifying a user).
	//
	// We have to hard-code a single string, luckily both docker/distribution and quay.io support "*" to mean "everything".
	c, err := newDockerClientFromRef(sys, ref, true, "*")
	if err != nil {
		return err
	}

	// When retrieving the digest from a registry >= 2.3 use the following header:
	//   "Accept": "application/vnd.docker.distribution.manifest.v2+json"
	headers := make(map[string][]string)
	headers["Accept"] = []string{manifest.DockerV2Schema2MediaType}

	refTail, err := ref.tagOrDigest()
	if err != nil {
		return err
	}
	getPath := fmt.Sprintf(manifestPath, reference.Path(ref.ref), refTail)
	get, err := c.makeRequest(ctx, "GET", getPath, headers, nil, v2Auth, nil)
	if err != nil {
		return err
	}
	defer get.Body.Close()
	manifestBody, err := ioutil.ReadAll(get.Body)
	if err != nil {
		return err
	}
	switch get.StatusCode {
	case http.StatusOK:
	case http.StatusNotFound:
		return errors.Errorf("Unable to delete %v. Image may not exist or is not stored with a v2 Schema in a v2 registry", ref.ref)
	default:
		return errors.Errorf("Failed to delete %v: %s (%v)", ref.ref, manifestBody, get.Status)
	}

	digest := get.Header.Get("Docker-Content-Digest")
	deletePath := fmt.Sprintf(manifestPath, reference.Path(ref.ref), digest)

	// When retrieving the digest from a registry >= 2.3 use the following header:
	//   "Accept": "application/vnd.docker.distribution.manifest.v2+json"
	delete, err := c.makeRequest(ctx, "DELETE", deletePath, headers, nil, v2Auth, nil)
	if err != nil {
		return err
	}
	defer delete.Body.Close()

	body, err := ioutil.ReadAll(delete.Body)
	if err != nil {
		return err
	}
	if delete.StatusCode != http.StatusAccepted {
		return errors.Errorf("Failed to delete %v: %s (%v)", deletePath, string(body), delete.Status)
	}

	if c.signatureBase != nil {
		manifestDigest, err := manifest.Digest(manifestBody)
		if err != nil {
			return err
		}

		for i := 0; ; i++ {
			url := signatureStorageURL(c.signatureBase, manifestDigest, i)
			if url == nil {
				return errors.Errorf("Internal error: signatureStorageURL with non-nil base returned nil")
			}
			missing, err := c.deleteOneSignature(url)
			if err != nil {
				return err
			}
			if missing {
				break
			}
		}
	}

	return nil
}
func Schema1FromComponents(ref reference.Named, fsLayers []Schema1FSLayers, history []Schema1History, architecture string) (*Schema1, error) {
	var name, tag string
	if ref != nil { // Well, what to do if it _is_ nil? Most consumers actually don't use these fields nowadays, so we might as well try not supplying them.
		name = reference.Path(ref)
		if tagged, ok := ref.(reference.NamedTagged); ok {
			tag = tagged.Tag()
		}
	}
	s1 := Schema1{
		Name:          name,
		Tag:           tag,
		Architecture:  architecture,
		FSLayers:      fsLayers,
		History:       history,
		SchemaVersion: 1,
	}
	if err := s1.initialize(); err != nil {
		return nil, err
	}
	return &s1, nil
}
func (m *Schema1) initialize() error {
	if len(m.FSLayers) != len(m.History) {
		return errors.New("length of history not equal to number of layers")
	}
	if len(m.FSLayers) == 0 {
		return errors.New("no FSLayers in manifest")
	}
	m.ExtractedV1Compatibility = make([]Schema1V1Compatibility, len(m.History))
	for i, h := range m.History {
		if err := json.Unmarshal([]byte(h.V1Compatibility), &m.ExtractedV1Compatibility[i]); err != nil {
			return errors.Wrapf(err, "Error parsing v2s1 history entry %d", i)
		}
	}
	return nil
}
func (m *Schema1) ToSchema2Config(diffIDs []digest.Digest) ([]byte, error) {
	// Convert the schema 1 compat info into a schema 2 config, constructing some of the fields
	// that aren't directly comparable using info from the manifest.
	if len(m.History) == 0 {
		return nil, errors.New("image has no layers")
	}
	s1 := Schema2V1Image{}
	config := []byte(m.History[0].V1Compatibility)
	err := json.Unmarshal(config, &s1)
	if err != nil {
		return nil, errors.Wrapf(err, "error decoding configuration")
	}
	// Images created with versions prior to 1.8.3 require us to re-encode the encoded object,
	// adding some fields that aren't "omitempty".
	if s1.DockerVersion != "" && versions.LessThan(s1.DockerVersion, "1.8.3") {
		config, err = json.Marshal(&s1)
		if err != nil {
			return nil, errors.Wrapf(err, "error re-encoding compat image config %#v", s1)
		}
	}
	// Build the history.
	convertedHistory := []Schema2History{}
	for _, compat := range m.ExtractedV1Compatibility {
		hitem := Schema2History{
			Created:    compat.Created,
			CreatedBy:  strings.Join(compat.ContainerConfig.Cmd, " "),
			Author:     compat.Author,
			Comment:    compat.Comment,
			EmptyLayer: compat.ThrowAway,
		}
		convertedHistory = append([]Schema2History{hitem}, convertedHistory...)
	}
	// Build the rootfs information.  We need the decompressed sums that we've been
	// calculating to fill in the DiffIDs.  It's expected (but not enforced by us)
	// that the number of diffIDs corresponds to the number of non-EmptyLayer
	// entries in the history.
	rootFS := &Schema2RootFS{
		Type:    "layers",
		DiffIDs: diffIDs,
	}
	// And now for some raw manipulation.
	raw := make(map[string]*json.RawMessage)
	err = json.Unmarshal(config, &raw)
	if err != nil {
		return nil, errors.Wrapf(err, "error re-decoding compat image config %#v", s1)
	}
	// Drop some fields.
	delete(raw, "id")
	delete(raw, "parent")
	delete(raw, "parent_id")
	delete(raw, "layer_id")
	delete(raw, "throwaway")
	delete(raw, "Size")
	// Add the history and rootfs information.
	rootfs, err := json.Marshal(rootFS)
	if err != nil {
		return nil, errors.Errorf("error encoding rootfs information %#v: %v", rootFS, err)
	}
	rawRootfs := json.RawMessage(rootfs)
	raw["rootfs"] = &rawRootfs
	history, err := json.Marshal(convertedHistory)
	if err != nil {
		return nil, errors.Errorf("error encoding history information %#v: %v", convertedHistory, err)
	}
	rawHistory := json.RawMessage(history)
	raw["history"] = &rawHistory
	// Encode the result.
	config, err = json.Marshal(raw)
	if err != nil {
		return nil, errors.Errorf("error re-encoding compat image config %#v: %v", s1, err)
	}
	return config, nil
}
func Digest(manifest []byte) (digest.Digest, error) {
	if GuessMIMEType(manifest) == DockerV2Schema1SignedMediaType {
		sig, err := libtrust.ParsePrettySignature(manifest, "signatures")
		if err != nil {
			return "", err
		}
		manifest, err = sig.Payload()
		if err != nil {
			// Coverage: This should never happen, libtrust's Payload() can fail only if joseBase64UrlDecode() fails, on a string
			// that libtrust itself has josebase64UrlEncode()d
			return "", err
		}
	}

	return digest.FromBytes(manifest), nil
}
func MatchesDigest(manifest []byte, expectedDigest digest.Digest) (bool, error) {
	// This should eventually support various digest types.
	actualDigest, err := Digest(manifest)
	if err != nil {
		return false, err
	}
	return expectedDigest == actualDigest, nil
}
func NormalizedMIMEType(input string) string {
	switch input {
	// "application/json" is a valid v2s1 value per https://github.com/docker/distribution/blob/master/docs/spec/manifest-v2-1.md .
	// This works for now, when nothing else seems to return "application/json"; if that were not true, the mapping/detection might
	// need to happen within the ImageSource.
	case "application/json":
		return DockerV2Schema1SignedMediaType
	case DockerV2Schema1MediaType, DockerV2Schema1SignedMediaType,
		imgspecv1.MediaTypeImageManifest,
		DockerV2Schema2MediaType,
		DockerV2ListMediaType:
		return input
	default:
		// If it's not a recognized manifest media type, or we have failed determining the type, we'll try one last time
		// to deserialize using v2s1 as per https://github.com/docker/distribution/blob/master/manifests.go#L108
		// and https://github.com/docker/distribution/blob/master/manifest/schema1/manifest.go#L50
		//
		// Crane registries can also return "text/plain", or pretty much anything else depending on a file extension “recognized” in the tag.
		// This makes no real sense, but it happens
		// because requests for manifests are
		// redirected to a content distribution
		// network which is configured that way. See https://bugzilla.redhat.com/show_bug.cgi?id=1389442
		return DockerV2Schema1SignedMediaType
	}
}
func FromBlob(manblob []byte, mt string) (Manifest, error) {
	switch NormalizedMIMEType(mt) {
	case DockerV2Schema1MediaType, DockerV2Schema1SignedMediaType:
		return Schema1FromManifest(manblob)
	case imgspecv1.MediaTypeImageManifest:
		return OCI1FromManifest(manblob)
	case DockerV2Schema2MediaType:
		return Schema2FromManifest(manblob)
	case DockerV2ListMediaType:
		return nil, fmt.Errorf("Treating manifest lists as individual manifests is not implemented")
	default: // Note that this may not be reachable, NormalizedMIMEType has a default for unknown values.
		return nil, fmt.Errorf("Unimplemented manifest MIME type %s", mt)
	}
}
func NewReference(path string) (types.ImageReference, error) {
	resolved, err := explicitfilepath.ResolvePathToFullyExplicit(path)
	if err != nil {
		return nil, err
	}
	return dirReference{path: path, resolvedPath: resolved}, nil
}
func (ref dirReference) layerPath(digest digest.Digest) string {
	// FIXME: Should we keep the digest identification?
	return filepath.Join(ref.path, digest.Hex())
}
func (ref dirReference) signaturePath(index int) string {
	return filepath.Join(ref.path, fmt.Sprintf("signature-%d", index+1))
}
func New(n int, ctor func() Worker) *Pool {
	p := &Pool{
		ctor:    ctor,
		reqChan: make(chan workRequest),
	}
	p.SetSize(n)

	return p
}
func NewFunc(n int, f func(interface{}) interface{}) *Pool {
	return New(n, func() Worker {
		return &closureWorker{
			processor: f,
		}
	})
}
func (p *Pool) ProcessTimed(
	payload interface{},
	timeout time.Duration,
) (interface{}, error) {
	atomic.AddInt64(&p.queuedJobs, 1)
	defer atomic.AddInt64(&p.queuedJobs, -1)

	tout := time.NewTimer(timeout)

	var request workRequest
	var open bool

	select {
	case request, open = <-p.reqChan:
		if !open {
			return nil, ErrPoolNotRunning
		}
	case <-tout.C:
		return nil, ErrJobTimedOut
	}

	select {
	case request.jobChan <- payload:
	case <-tout.C:
		request.interruptFunc()
		return nil, ErrJobTimedOut
	}

	select {
	case payload, open = <-request.retChan:
		if !open {
			return nil, ErrWorkerClosed
		}
	case <-tout.C:
		request.interruptFunc()
		return nil, ErrJobTimedOut
	}

	tout.Stop()
	return payload, nil
}
func (p *Pool) SetSize(n int) {
	p.workerMut.Lock()
	defer p.workerMut.Unlock()

	lWorkers := len(p.workers)
	if lWorkers == n {
		return
	}

	// Add extra workers if N > len(workers)
	for i := lWorkers; i < n; i++ {
		p.workers = append(p.workers, newWorkerWrapper(p.reqChan, p.ctor()))
	}

	// Asynchronously stop all workers > N
	for i := n; i < lWorkers; i++ {
		p.workers[i].stop()
	}

	// Synchronously wait for all workers > N to stop
	for i := n; i < lWorkers; i++ {
		p.workers[i].join()
	}

	// Remove stopped workers from slice
	p.workers = p.workers[:n]
}
func (p *Pool) GetSize() int {
	p.workerMut.Lock()
	defer p.workerMut.Unlock()

	return len(p.workers)
}
func (r *Rect) TL() Point {
	return Point{int(r.x), int(r.y)}
}
func (r *Rect) BR() Point {
	return Point{int(r.x) + int(r.width), int(r.y) + int(r.height)}
}
func (box *Box2D) CVBox() C.CvBox2D {
	var cvBox C.CvBox2D
	cvBox.angle = C.float(box.angle)
	cvBox.center.x = C.float(box.center.X)
	cvBox.center.y = C.float(box.center.Y)
	cvBox.size.width = C.float(box.size.Width)
	cvBox.size.height = C.float(box.size.Height)
	return cvBox
}
func (box *Box2D) Points() []Point2D32f {
	var pts [4]C.CvPoint2D32f
	C.cvBoxPoints(
		box.CVBox(),
		(*C.CvPoint2D32f)(unsafe.Pointer(&pts[0])),
	)
	outPts := make([]Point2D32f, 4)
	for i, p := range pts {
		outPts[i].X = float32(p.x)
		outPts[i].Y = float32(p.y)
	}
	return outPts
}
func FOURCC(c1, c2, c3, c4 int8) uint32 {
	rv := C.GoOpenCV_FOURCC_(C.int(c1), C.int(c2), C.int(c3), C.int(c4))
	return uint32(rv)
}
func Merge(imgBlue, imgGreen, imgRed, imgAlpha, dst *IplImage) {
	C.cvMerge(
		unsafe.Pointer(imgBlue),
		unsafe.Pointer(imgGreen),
		unsafe.Pointer(imgRed),
		unsafe.Pointer(imgAlpha),
		unsafe.Pointer(dst),
	)
}
func Split(src, imgBlue, imgGreen, imgRed, imgAlpha *IplImage) {
	C.cvSplit(
		unsafe.Pointer(src),
		unsafe.Pointer(imgBlue),
		unsafe.Pointer(imgGreen),
		unsafe.Pointer(imgRed),
		unsafe.Pointer(imgAlpha),
	)
}
func AddWeighted(src1 *IplImage, alpha float64, src2 *IplImage, beta float64, gamma float64, dst *IplImage) {
	C.cvAddWeighted(
		unsafe.Pointer(src1),
		C.double(alpha),
		unsafe.Pointer(src2),
		C.double(beta),
		C.double(gamma),
		unsafe.Pointer(dst),
	)
}
func And(src1, src2, dst *IplImage) {
	AndWithMask(src1, src2, dst, nil)
}
func AndWithMask(src1, src2, dst, mask *IplImage) {
	C.cvAnd(
		unsafe.Pointer(src1),
		unsafe.Pointer(src2),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func AndScalar(src *IplImage, value Scalar, dst *IplImage) {
	AndScalarWithMask(src, value, dst, nil)
}
func AndScalarWithMask(src *IplImage, value Scalar, dst, mask *IplImage) {
	C.cvAndS(
		unsafe.Pointer(src),
		(C.CvScalar)(value),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func Or(src1, src2, dst *IplImage) {
	OrWithMask(src1, src2, dst, nil)
}
func OrWithMask(src1, src2, dst, mask *IplImage) {
	C.cvOr(
		unsafe.Pointer(src1),
		unsafe.Pointer(src2),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func OrScalar(src *IplImage, value Scalar, dst *IplImage) {
	OrScalarWithMask(src, value, dst, nil)
}
func OrScalarWithMask(src *IplImage, value Scalar, dst, mask *IplImage) {
	C.cvOrS(
		unsafe.Pointer(src),
		(C.CvScalar)(value),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func AddWithMask(src1, src2, dst, mask *IplImage) {
	C.cvAdd(
		unsafe.Pointer(src1),
		unsafe.Pointer(src2),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func AddScalar(src *IplImage, value Scalar, dst *IplImage) {
	AddScalarWithMask(src, value, dst, nil)
}
func AddScalarWithMask(src *IplImage, value Scalar, dst, mask *IplImage) {
	C.cvAddS(
		unsafe.Pointer(src),
		(C.CvScalar)(value),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func Subtract(src1, src2, dst *IplImage) {
	SubtractWithMask(src1, src2, dst, nil)
}
func SubtractWithMask(src1, src2, dst, mask *IplImage) {
	C.cvSub(
		unsafe.Pointer(src1),
		unsafe.Pointer(src2),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func SubScalar(src *IplImage, value Scalar, dst *IplImage) {
	SubScalarWithMask(src, value, dst, nil)
}
func SubScalarWithMask(src *IplImage, value Scalar, dst, mask *IplImage) {
	C.cvSubS(
		unsafe.Pointer(src),
		(C.CvScalar)(value),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func SubScalarRev(value Scalar, src, dst *IplImage) {
	SubScalarWithMaskRev(value, src, dst, nil)
}
func SubScalarWithMaskRev(value Scalar, src, dst, mask *IplImage) {
	C.cvSubRS(
		unsafe.Pointer(src),
		(C.CvScalar)(value),
		unsafe.Pointer(dst),
		unsafe.Pointer(mask),
	)
}
func AbsDiff(src1, src2, dst *IplImage) {
	C.cvAbsDiff(
		unsafe.Pointer(src1),
		unsafe.Pointer(src2),
		unsafe.Pointer(dst),
	)
}
func AbsDiffScalar(src *IplImage, value Scalar, dst *IplImage) {
	C.cvAbsDiffS(
		unsafe.Pointer(src),
		unsafe.Pointer(dst),
		(C.CvScalar)(value),
	)
}
func MeanStdDevWithMask(src, mask *IplImage) (Scalar, Scalar) {
	var mean, stdDev Scalar
	C.cvAvgSdv(
		unsafe.Pointer(src),
		(*C.CvScalar)(&mean),
		(*C.CvScalar)(&stdDev),
		unsafe.Pointer(mask),
	)

	return mean, stdDev
}
func CreateSeq(seq_flags, elem_size int) *Seq {
	return (*Seq)(C.cvCreateSeq(
		C.int(seq_flags),
		C.size_t(unsafe.Sizeof(Seq{})),
		C.size_t(elem_size),
		C.cvCreateMemStorage(C.int(0)),
	))
}
func (seq *Seq) Push(element unsafe.Pointer) unsafe.Pointer {
	return unsafe.Pointer(C.cvSeqPush((*C.struct_CvSeq)(seq), element))
}
func (seq *Seq) Pop(element unsafe.Pointer) {
	C.cvSeqPop((*C.struct_CvSeq)(seq), element)
}
func (seq *Seq) PushFront(element unsafe.Pointer) unsafe.Pointer {
	return unsafe.Pointer((C.cvSeqPushFront((*C.struct_CvSeq)(seq), element)))
}
func (seq *Seq) PopFront(element unsafe.Pointer) {
	C.cvSeqPopFront((*C.struct_CvSeq)(seq), element)
}
func (seq *Seq) GetElemAt(index int) unsafe.Pointer {
	return (unsafe.Pointer)(C.cvGetSeqElem(
		(*C.struct_CvSeq)(seq),
		C.int(index),
	))
}
func (seq *Seq) RemoveAt(index int) {
	C.cvSeqRemove((*C.struct_CvSeq)(seq), C.int(index))
}
func Delay(delay time.Duration) Option {
	return func(c *Config) {
		c.delay = delay
	}
}
func BackOffDelay(n uint, config *Config) time.Duration {
	return config.delay * (1 << (n - 1))
}
func (e Error) Error() string {
	logWithNumber := make([]string, lenWithoutNil(e))
	for i, l := range e {
		if l != nil {
			logWithNumber[i] = fmt.Sprintf("#%d: %s", i+1, l.Error())
		}
	}

	return fmt.Sprintf("All attempts fail:\n%s", strings.Join(logWithNumber, "\n"))
}
func (r *RequestBuilder) Arguments(args ...string) *RequestBuilder {
	r.args = append(r.args, args...)
	return r
}
func (r *RequestBuilder) BodyString(body string) *RequestBuilder {
	return r.Body(strings.NewReader(body))
}
func (r *RequestBuilder) BodyBytes(body []byte) *RequestBuilder {
	return r.Body(bytes.NewReader(body))
}
func (r *RequestBuilder) Body(body io.Reader) *RequestBuilder {
	r.body = body
	return r
}
func (r *RequestBuilder) Option(key string, value interface{}) *RequestBuilder {
	var s string
	switch v := value.(type) {
	case bool:
		s = strconv.FormatBool(v)
	case string:
		s = v
	case []byte:
		s = string(v)
	default:
		// slow case.
		s = fmt.Sprint(value)
	}
	if r.opts == nil {
		r.opts = make(map[string]string, 1)
	}
	r.opts[key] = s
	return r
}
func (r *RequestBuilder) Header(name, value string) *RequestBuilder {
	if r.headers == nil {
		r.headers = make(map[string]string, 1)
	}
	r.headers[name] = value
	return r
}
func (r *RequestBuilder) Send(ctx context.Context) (*Response, error) {
	req := NewRequest(ctx, r.shell.url, r.command, r.args...)
	req.Opts = r.opts
	req.Headers = r.headers
	req.Body = r.body
	return req.Send(&r.shell.httpcli)
}
func (r *RequestBuilder) Exec(ctx context.Context, res interface{}) error {
	httpRes, err := r.Send(ctx)
	if err != nil {
		return err
	}

	if res == nil {
		lateErr := httpRes.Close()
		if httpRes.Error != nil {
			return httpRes.Error
		}
		return lateErr
	}

	return httpRes.Decode(res)
}
func (s *PubSubSubscription) Next() (*Message, error) {
	if s.resp.Error != nil {
		return nil, s.resp.Error
	}

	d := json.NewDecoder(s.resp.Output)

	var r struct {
		From     []byte   `json:"from,omitempty"`
		Data     []byte   `json:"data,omitempty"`
		Seqno    []byte   `json:"seqno,omitempty"`
		TopicIDs []string `json:"topicIDs,omitempty"`
	}

	err := d.Decode(&r)
	if err != nil {
		return nil, err
	}

	from, err := peer.IDFromBytes(r.From)
	if err != nil {
		return nil, err
	}
	return &Message{
		From:     from,
		Data:     r.Data,
		Seqno:    r.Seqno,
		TopicIDs: r.TopicIDs,
	}, nil
}
func (s *PubSubSubscription) Cancel() error {
	if s.resp.Output == nil {
		return nil
	}

	return s.resp.Output.Close()
}
func (s *Shell) FileList(path string) (*UnixLsObject, error) {
	var out lsOutput
	if err := s.Request("file/ls", path).Exec(context.Background(), &out); err != nil {
		return nil, err
	}

	for _, object := range out.Objects {
		return object, nil
	}

	return nil, fmt.Errorf("no object in results")
}
func (s *Shell) Cat(path string) (io.ReadCloser, error) {
	resp, err := s.Request("cat", path).Send(context.Background())
	if err != nil {
		return nil, err
	}
	if resp.Error != nil {
		return nil, resp.Error
	}

	return resp.Output, nil
}
func (s *Shell) List(path string) ([]*LsLink, error) {
	var out struct{ Objects []LsObject }
	err := s.Request("ls", path).Exec(context.Background(), &out)
	if err != nil {
		return nil, err
	}
	if len(out.Objects) != 1 {
		return nil, errors.New("bad response from server")
	}
	return out.Objects[0].Links, nil
}
func (s *Shell) Pin(path string) error {
	return s.Request("pin/add", path).
		Option("recursive", true).
		Exec(context.Background(), nil)
}
func (s *Shell) Pins() (map[string]PinInfo, error) {
	var raw struct{ Keys map[string]PinInfo }
	return raw.Keys, s.Request("pin/ls").Exec(context.Background(), &raw)
}
func (s *Shell) Version() (string, string, error) {
	ver := struct {
		Version string
		Commit  string
	}{}

	if err := s.Request("version").Exec(context.Background(), &ver); err != nil {
		return "", "", err
	}
	return ver.Version, ver.Commit, nil
}
func (s *Shell) SwarmPeers(ctx context.Context) (*SwarmConnInfos, error) {
	v := &SwarmConnInfos{}
	err := s.Request("swarm/peers").Exec(ctx, &v)
	return v, err
}
func (s *Shell) SwarmConnect(ctx context.Context, addr ...string) error {
	var conn *swarmConnection
	err := s.Request("swarm/connect").
		Arguments(addr...).
		Exec(ctx, &conn)
	return err
}
func DagPutOptions(opts ...DagPutOption) (*DagPutSettings, error) {
	options := &DagPutSettings{
		InputEnc: "json",
		Kind:     "cbor",
		Pin:      "false",
		Hash:     "sha2-256",
	}

	for _, opt := range opts {
		err := opt(options)
		if err != nil {
			return nil, err
		}
	}
	return options, nil
}
func (dagOpts) Pin(pin string) DagPutOption {
	return func(opts *DagPutSettings) error {
		opts.Pin = pin
		return nil
	}
}
func (dagOpts) Kind(kind string) DagPutOption {
	return func(opts *DagPutSettings) error {
		opts.Kind = kind
		return nil
	}
}
func (dagOpts) Hash(hash string) DagPutOption {
	return func(opts *DagPutSettings) error {
		opts.Hash = hash
		return nil
	}
}
func (s *Shell) AddDir(dir string) (string, error) {
	stat, err := os.Lstat(dir)
	if err != nil {
		return "", err
	}

	sf, err := files.NewSerialFile(dir, false, stat)
	if err != nil {
		return "", err
	}
	slf := files.NewSliceDirectory([]files.DirEntry{files.FileEntry(filepath.Base(dir), sf)})
	reader := files.NewMultiFileReader(slf, true)

	resp, err := s.Request("add").
		Option("recursive", true).
		Body(reader).
		Send(context.Background())
	if err != nil {
		return "", nil
	}

	defer resp.Close()

	if resp.Error != nil {
		return "", resp.Error
	}

	dec := json.NewDecoder(resp.Output)
	var final string
	for {
		var out object
		err = dec.Decode(&out)
		if err != nil {
			if err == io.EOF {
				break
			}
			return "", err
		}
		final = out.Hash
	}

	if final == "" {
		return "", errors.New("no results received")
	}

	return final, nil
}
func (s *Shell) Publish(node string, value string) error {
	var pubResp PublishResponse
	req := s.Request("name/publish")
	if node != "" {
		req.Arguments(node)
	}
	req.Arguments(value)

	return req.Exec(context.Background(), &pubResp)
}
func (s *Shell) PublishWithDetails(contentHash, key string, lifetime, ttl time.Duration, resolve bool) (*PublishResponse, error) {
	var pubResp PublishResponse
	req := s.Request("name/publish", contentHash).Option("resolve", resolve)
	if key != "" {
		req.Option("key", key)
	}
	if lifetime != 0 {
		req.Option("lifetime", lifetime)
	}
	if ttl.Seconds() > 0 {
		req.Option("ttl", ttl)
	}
	err := req.Exec(context.Background(), &pubResp)
	if err != nil {
		return nil, err
	}
	return &pubResp, nil
}
func (pv PlanValue) ResolveValue(bindVars map[string]*querypb.BindVariable) (Value, error) {
	switch {
	case pv.Key != "":
		bv, err := pv.lookupValue(bindVars)
		if err != nil {
			return NULL, err
		}
		return MakeTrusted(bv.Type, bv.Value), nil
	case !pv.Value.IsNull():
		return pv.Value, nil
	case pv.ListKey != "" || pv.Values != nil:
		// This code is unreachable because the parser does not allow
		// multi-value constructs where a single value is expected.
		return NULL, errors.New("a list was supplied where a single value was expected")
	}
	return NULL, nil
}
func (pq *ParsedQuery) GenerateQuery(bindVariables map[string]*querypb.BindVariable, extras map[string]Encodable) ([]byte, error) {
	if len(pq.bindLocations) == 0 {
		return []byte(pq.Query), nil
	}
	buf := bytes.NewBuffer(make([]byte, 0, len(pq.Query)))
	current := 0
	for _, loc := range pq.bindLocations {
		buf.WriteString(pq.Query[current:loc.offset])
		name := pq.Query[loc.offset : loc.offset+loc.length]
		if encodable, ok := extras[name[1:]]; ok {
			encodable.EncodeSQL(buf)
		} else {
			supplied, _, err := FetchBindVar(name, bindVariables)
			if err != nil {
				return nil, err
			}
			EncodeValue(buf, supplied)
		}
		current = loc.offset + loc.length
	}
	buf.WriteString(pq.Query[current:])
	return buf.Bytes(), nil
}
func EncodeValue(buf *bytes.Buffer, value *querypb.BindVariable) {
	if value.Type != querypb.Type_TUPLE {
		// Since we already check for TUPLE, we don't expect an error.
		v, _ := sqltypes.BindVariableToValue(value)
		v.EncodeSQL(buf)
		return
	}

	// It's a TUPLE.
	buf.WriteByte('(')
	for i, bv := range value.Values {
		if i != 0 {
			buf.WriteString(", ")
		}
		sqltypes.ProtoToValue(bv).EncodeSQL(buf)
	}
	buf.WriteByte(')')
}
func (tkn *Tokenizer) Lex(lval *yySymType) int {
	typ, val := tkn.Scan()
	for typ == COMMENT {
		if tkn.AllowComments {
			break
		}
		typ, val = tkn.Scan()
	}
	lval.bytes = val
	tkn.lastToken = val
	return typ
}
func (tkn *Tokenizer) skipStatement() {
	ch := tkn.lastChar
	for ch != ';' && ch != eofChar {
		tkn.next()
		ch = tkn.lastChar
	}
}
func (tkn *Tokenizer) reset() {
	tkn.ParseTree = nil
	tkn.partialDDL = nil
	tkn.specialComment = nil
	tkn.posVarIndex = 0
	tkn.nesting = 0
	tkn.ForceEOF = false
}
func Preview(sql string) int {
	trimmed := StripLeadingComments(sql)

	firstWord := trimmed
	if end := strings.IndexFunc(trimmed, unicode.IsSpace); end != -1 {
		firstWord = trimmed[:end]
	}
	firstWord = strings.TrimLeftFunc(firstWord, func(r rune) bool { return !unicode.IsLetter(r) })
	// Comparison is done in order of priority.
	loweredFirstWord := strings.ToLower(firstWord)
	switch loweredFirstWord {
	case "select":
		return StmtSelect
	case "stream":
		return StmtStream
	case "insert":
		return StmtInsert
	case "replace":
		return StmtReplace
	case "update":
		return StmtUpdate
	case "delete":
		return StmtDelete
	}
	// For the following statements it is not sufficient to rely
	// on loweredFirstWord. This is because they are not statements
	// in the grammar and we are relying on Preview to parse them.
	// For instance, we don't want: "BEGIN JUNK" to be parsed
	// as StmtBegin.
	trimmedNoComments, _ := SplitMarginComments(trimmed)
	switch strings.ToLower(trimmedNoComments) {
	case "begin", "start transaction":
		return StmtBegin
	case "commit":
		return StmtCommit
	case "rollback":
		return StmtRollback
	}
	switch loweredFirstWord {
	case "create", "alter", "rename", "drop", "truncate":
		return StmtDDL
	case "set":
		return StmtSet
	case "show":
		return StmtShow
	case "use":
		return StmtUse
	case "analyze", "describe", "desc", "explain", "repair", "optimize":
		return StmtOther
	}
	if strings.Index(trimmed, "/*!") == 0 {
		return StmtComment
	}
	return StmtUnknown
}
func NewPlanValue(node Expr) (sqltypes.PlanValue, error) {
	switch node := node.(type) {
	case *SQLVal:
		switch node.Type {
		case ValArg:
			return sqltypes.PlanValue{Key: string(node.Val[1:])}, nil
		case IntVal:
			n, err := sqltypes.NewIntegral(string(node.Val))
			if err != nil {
				return sqltypes.PlanValue{}, fmt.Errorf("%v", err)
			}
			return sqltypes.PlanValue{Value: n}, nil
		case StrVal:
			return sqltypes.PlanValue{Value: sqltypes.MakeTrusted(sqltypes.VarBinary, node.Val)}, nil
		case HexVal:
			v, err := node.HexDecode()
			if err != nil {
				return sqltypes.PlanValue{}, fmt.Errorf("%v", err)
			}
			return sqltypes.PlanValue{Value: sqltypes.MakeTrusted(sqltypes.VarBinary, v)}, nil
		}
	case ListArg:
		return sqltypes.PlanValue{ListKey: string(node[2:])}, nil
	case ValTuple:
		pv := sqltypes.PlanValue{
			Values: make([]sqltypes.PlanValue, 0, len(node)),
		}
		for _, val := range node {
			innerpv, err := NewPlanValue(val)
			if err != nil {
				return sqltypes.PlanValue{}, err
			}
			if innerpv.ListKey != "" || innerpv.Values != nil {
				return sqltypes.PlanValue{}, errors.New("unsupported: nested lists")
			}
			pv.Values = append(pv.Values, innerpv)
		}
		return pv, nil
	case *NullVal:
		return sqltypes.PlanValue{}, nil
	}
	return sqltypes.PlanValue{}, fmt.Errorf("expression is too complex '%v'", String(node))
}
func StringIn(str string, values ...string) bool {
	for _, val := range values {
		if str == val {
			return true
		}
	}
	return false
}
func NewTrackedBuffer(nodeFormatter NodeFormatter) *TrackedBuffer {
	return &TrackedBuffer{
		Buffer:        new(bytes.Buffer),
		nodeFormatter: nodeFormatter,
	}
}
func NewStringArena(size int) *StringArena {
	sa := &StringArena{buf: make([]byte, 0, size)}
	pbytes := (*reflect.SliceHeader)(unsafe.Pointer(&sa.buf))
	pstring := (*reflect.StringHeader)(unsafe.Pointer(&sa.str))
	pstring.Data = pbytes.Data
	pstring.Len = pbytes.Cap
	return sa
}
func (sa *StringArena) NewString(b []byte) string {
	if len(b) == 0 {
		return ""
	}
	if len(sa.buf)+len(b) > cap(sa.buf) {
		return string(b)
	}
	start := len(sa.buf)
	sa.buf = append(sa.buf, b...)
	return sa.str[start : start+len(b)]
}
func (sa *StringArena) SpaceLeft() int {
	return cap(sa.buf) - len(sa.buf)
}
func ParseStrictDDL(sql string) (Statement, error) {
	tokenizer := NewStringTokenizer(sql)
	if yyParse(tokenizer) != 0 {
		return nil, tokenizer.LastError
	}
	return tokenizer.ParseTree, nil
}
func ParseNext(tokenizer *Tokenizer) (Statement, error) {
	if tokenizer.lastChar == ';' {
		tokenizer.next()
		tokenizer.skipBlank()
	}
	if tokenizer.lastChar == eofChar {
		return nil, io.EOF
	}

	tokenizer.reset()
	tokenizer.multi = true
	if yyParse(tokenizer) != 0 {
		if tokenizer.partialDDL != nil {
			tokenizer.ParseTree = tokenizer.partialDDL
			return tokenizer.ParseTree, nil
		}
		return nil, tokenizer.LastError
	}
	return tokenizer.ParseTree, nil
}
func Append(buf *bytes.Buffer, node SQLNode) {
	tbuf := &TrackedBuffer{
		Buffer: buf,
	}
	node.Format(tbuf)
}
func ExprFromValue(value sqltypes.Value) (Expr, error) {
	// The type checks here follow the rules defined in sqltypes/types.go.
	switch {
	case value.Type() == sqltypes.Null:
		return &NullVal{}, nil
	case value.IsIntegral():
		return NewIntVal(value.ToBytes()), nil
	case value.IsFloat() || value.Type() == sqltypes.Decimal:
		return NewFloatVal(value.ToBytes()), nil
	case value.IsQuoted():
		return NewStrVal(value.ToBytes()), nil
	default:
		// We cannot support sqltypes.Expression, or any other invalid type.
		return nil, fmt.Errorf("cannot convert value %v to AST", value)
	}
}
func Backtick(in string) string {
	var buf bytes.Buffer
	buf.WriteByte('`')
	for _, c := range in {
		buf.WriteRune(c)
		if c == '`' {
			buf.WriteByte('`')
		}
	}
	buf.WriteByte('`')
	return buf.String()
}
func NewValue(typ querypb.Type, val []byte) (v Value, err error) {
	switch {
	case IsSigned(typ):
		if _, err := strconv.ParseInt(string(val), 0, 64); err != nil {
			return NULL, err
		}
		return MakeTrusted(typ, val), nil
	case IsUnsigned(typ):
		if _, err := strconv.ParseUint(string(val), 0, 64); err != nil {
			return NULL, err
		}
		return MakeTrusted(typ, val), nil
	case IsFloat(typ) || typ == Decimal:
		if _, err := strconv.ParseFloat(string(val), 64); err != nil {
			return NULL, err
		}
		return MakeTrusted(typ, val), nil
	case IsQuoted(typ) || typ == Null:
		return MakeTrusted(typ, val), nil
	}
	// All other types are unsafe or invalid.
	return NULL, fmt.Errorf("invalid type specified for MakeValue: %v", typ)
}
func (v Value) String() string {
	if v.typ == Null {
		return "NULL"
	}
	if v.IsQuoted() {
		return fmt.Sprintf("%v(%q)", v.typ, v.val)
	}
	return fmt.Sprintf("%v(%s)", v.typ, v.val)
}
func (v Value) EncodeSQL(b BinWriter) {
	switch {
	case v.typ == Null:
		b.Write(nullstr)
	case v.IsQuoted():
		encodeBytesSQL(v.val, b)
	default:
		b.Write(v.val)
	}
}
func (iv InsertValues) EncodeSQL(buf *bytes.Buffer) {
	for i, rows := range iv {
		if i != 0 {
			buf.WriteString(", ")
		}
		buf.WriteByte('(')
		for j, bv := range rows {
			if j != 0 {
				buf.WriteString(", ")
			}
			bv.EncodeSQL(buf)
		}
		buf.WriteByte(')')
	}
}
func (tpl *TupleEqualityList) EncodeSQL(buf *bytes.Buffer) {
	if len(tpl.Columns) == 1 {
		tpl.encodeAsIn(buf)
		return
	}
	tpl.encodeAsEquality(buf)
}
func (nz *normalizer) WalkStatement(node SQLNode) (bool, error) {
	switch node := node.(type) {
	case *Select:
		_ = Walk(nz.WalkSelect, node)
		// Don't continue
		return false, nil
	case *SQLVal:
		nz.convertSQLVal(node)
	case *ComparisonExpr:
		nz.convertComparison(node)
	}
	return true, nil
}
func (nz *normalizer) WalkSelect(node SQLNode) (bool, error) {
	switch node := node.(type) {
	case *SQLVal:
		nz.convertSQLValDedup(node)
	case *ComparisonExpr:
		nz.convertComparison(node)
	}
	return true, nil
}
func BindVariablesEqual(x, y map[string]*querypb.BindVariable) bool {
	return reflect.DeepEqual(&querypb.BoundQuery{BindVariables: x}, &querypb.BoundQuery{BindVariables: y})
}
func New(options ...Options) *JWTMiddleware {

	var opts Options
	if len(options) == 0 {
		opts = Options{}
	} else {
		opts = options[0]
	}

	if opts.UserProperty == "" {
		opts.UserProperty = "user"
	}

	if opts.ErrorHandler == nil {
		opts.ErrorHandler = OnError
	}

	if opts.Extractor == nil {
		opts.Extractor = FromAuthHeader
	}

	return &JWTMiddleware{
		Options: opts,
	}
}
func (m *JWTMiddleware) HandlerWithNext(w http.ResponseWriter, r *http.Request, next http.HandlerFunc) {
	err := m.CheckJWT(w, r)

	// If there was an error, do not call next.
	if err == nil && next != nil {
		next(w, r)
	}
}
func FromAuthHeader(r *http.Request) (string, error) {
	authHeader := r.Header.Get("Authorization")
	if authHeader == "" {
		return "", nil // No error, just no token
	}

	// TODO: Make this a bit more robust, parsing-wise
	authHeaderParts := strings.Split(authHeader, " ")
	if len(authHeaderParts) != 2 || strings.ToLower(authHeaderParts[0]) != "bearer" {
		return "", errors.New("Authorization header format must be Bearer {token}")
	}

	return authHeaderParts[1], nil
}
func FromParameter(param string) TokenExtractor {
	return func(r *http.Request) (string, error) {
		return r.URL.Query().Get(param), nil
	}
}
func FromFirst(extractors ...TokenExtractor) TokenExtractor {
	return func(r *http.Request) (string, error) {
		for _, ex := range extractors {
			token, err := ex(r)
			if err != nil {
				return "", err
			}
			if token != "" {
				return token, nil
			}
		}
		return "", nil
	}
}
func (p *PubSub) getHelloPacket() *RPC {
	var rpc RPC
	for t := range p.myTopics {
		as := &pb.RPC_SubOpts{
			Topicid:   proto.String(t),
			Subscribe: proto.Bool(true),
		}
		rpc.Subscriptions = append(rpc.Subscriptions, as)
	}
	return &rpc
}
func NewFloodsubWithProtocols(ctx context.Context, h host.Host, ps []protocol.ID, opts ...Option) (*PubSub, error) {
	rt := &FloodSubRouter{
		protocols: ps,
	}
	return NewPubSub(ctx, h, rt, opts...)
}
func NewFloodSub(ctx context.Context, h host.Host, opts ...Option) (*PubSub, error) {
	return NewFloodsubWithProtocols(ctx, h, []protocol.ID{FloodSubID}, opts...)
}
func NewLRUBlacklist(cap int) (Blacklist, error) {
	c, err := lru.New(cap)
	if err != nil {
		return nil, err
	}

	b := &LRUBlacklist{lru: c}
	return b, nil
}
func NewRandomSub(ctx context.Context, h host.Host, opts ...Option) (*PubSub, error) {
	rt := &RandomSubRouter{
		peers: make(map[peer.ID]protocol.ID),
	}
	return NewPubSub(ctx, h, rt, opts...)
}
func NewGossipSub(ctx context.Context, h host.Host, opts ...Option) (*PubSub, error) {
	rt := &GossipSubRouter{
		peers:   make(map[peer.ID]protocol.ID),
		mesh:    make(map[string]map[peer.ID]struct{}),
		fanout:  make(map[string]map[peer.ID]struct{}),
		lastpub: make(map[string]int64),
		gossip:  make(map[peer.ID][]*pb.ControlIHave),
		control: make(map[peer.ID]*pb.ControlMessage),
		mcache:  NewMessageCache(GossipSubHistoryGossip, GossipSubHistoryLength),
	}
	return NewPubSub(ctx, h, rt, opts...)
}
func NewPubSub(ctx context.Context, h host.Host, rt PubSubRouter, opts ...Option) (*PubSub, error) {
	ps := &PubSub{
		host:             h,
		ctx:              ctx,
		rt:               rt,
		signID:           h.ID(),
		signKey:          h.Peerstore().PrivKey(h.ID()),
		signStrict:       true,
		incoming:         make(chan *RPC, 32),
		publish:          make(chan *Message),
		newPeers:         make(chan peer.ID),
		newPeerStream:    make(chan inet.Stream),
		newPeerError:     make(chan peer.ID),
		peerDead:         make(chan peer.ID),
		cancelCh:         make(chan *Subscription),
		getPeers:         make(chan *listPeerReq),
		addSub:           make(chan *addSubReq),
		getTopics:        make(chan *topicReq),
		sendMsg:          make(chan *sendReq, 32),
		addVal:           make(chan *addValReq),
		rmVal:            make(chan *rmValReq),
		validateThrottle: make(chan struct{}, defaultValidateThrottle),
		eval:             make(chan func()),
		myTopics:         make(map[string]map[*Subscription]struct{}),
		topics:           make(map[string]map[peer.ID]struct{}),
		peers:            make(map[peer.ID]chan *RPC),
		topicVals:        make(map[string]*topicVal),
		blacklist:        NewMapBlacklist(),
		blacklistPeer:    make(chan peer.ID),
		seenMessages:     timecache.NewTimeCache(TimeCacheDuration),
		counter:          uint64(time.Now().UnixNano()),
	}

	for _, opt := range opts {
		err := opt(ps)
		if err != nil {
			return nil, err
		}
	}

	if ps.signStrict && ps.signKey == nil {
		return nil, fmt.Errorf("strict signature verification enabled but message signing is disabled")
	}

	rt.Attach(ps)

	for _, id := range rt.Protocols() {
		h.SetStreamHandler(id, ps.handleNewStream)
	}
	h.Network().Notify((*PubSubNotif)(ps))

	go ps.processLoop(ctx)

	return ps, nil
}
func WithValidateThrottle(n int) Option {
	return func(ps *PubSub) error {
		ps.validateThrottle = make(chan struct{}, n)
		return nil
	}
}
func WithBlacklist(b Blacklist) Option {
	return func(p *PubSub) error {
		p.blacklist = b
		return nil
	}
}
func (p *PubSub) handleRemoveSubscription(sub *Subscription) {
	subs := p.myTopics[sub.topic]

	if subs == nil {
		return
	}

	sub.err = fmt.Errorf("subscription cancelled by calling sub.Cancel()")
	close(sub.ch)
	delete(subs, sub)

	if len(subs) == 0 {
		delete(p.myTopics, sub.topic)
		p.announce(sub.topic, false)
		p.rt.Leave(sub.topic)
	}
}
func (p *PubSub) handleAddSubscription(req *addSubReq) {
	sub := req.sub
	subs := p.myTopics[sub.topic]

	// announce we want this topic
	if len(subs) == 0 {
		p.announce(sub.topic, true)
		p.rt.Join(sub.topic)
	}

	// make new if not there
	if subs == nil {
		p.myTopics[sub.topic] = make(map[*Subscription]struct{})
		subs = p.myTopics[sub.topic]
	}

	sub.ch = make(chan *Message, 32)
	sub.cancelCh = p.cancelCh

	p.myTopics[sub.topic][sub] = struct{}{}

	req.resp <- sub
}
func (p *PubSub) announce(topic string, sub bool) {
	subopt := &pb.RPC_SubOpts{
		Topicid:   &topic,
		Subscribe: &sub,
	}

	out := rpcWithSubs(subopt)
	for pid, peer := range p.peers {
		select {
		case peer <- out:
		default:
			log.Infof("Can't send announce message to peer %s: queue full; scheduling retry", pid)
			go p.announceRetry(pid, topic, sub)
		}
	}
}
func (p *PubSub) notifySubs(msg *pb.Message) {
	for _, topic := range msg.GetTopicIDs() {
		subs := p.myTopics[topic]
		for f := range subs {
			select {
			case f.ch <- &Message{msg}:
			default:
				log.Infof("Can't deliver message to subscription for topic %s; subscriber too slow", topic)
			}
		}
	}
}
func (p *PubSub) seenMessage(id string) bool {
	return p.seenMessages.Has(id)
}
func (p *PubSub) subscribedToMsg(msg *pb.Message) bool {
	if len(p.myTopics) == 0 {
		return false
	}

	for _, t := range msg.GetTopicIDs() {
		if _, ok := p.myTopics[t]; ok {
			return true
		}
	}
	return false
}
func msgID(pmsg *pb.Message) string {
	return string(pmsg.GetFrom()) + string(pmsg.GetSeqno())
}
func (p *PubSub) pushMsg(vals []*topicVal, src peer.ID, msg *Message) {
	// reject messages from blacklisted peers
	if p.blacklist.Contains(src) {
		log.Warningf("dropping message from blacklisted peer %s", src)
		return
	}

	// even if they are forwarded by good peers
	if p.blacklist.Contains(msg.GetFrom()) {
		log.Warningf("dropping message from blacklisted source %s", src)
		return
	}

	// reject unsigned messages when strict before we even process the id
	if p.signStrict && msg.Signature == nil {
		log.Debugf("dropping unsigned message from %s", src)
		return
	}

	// have we already seen and validated this message?
	id := msgID(msg.Message)
	if p.seenMessage(id) {
		return
	}

	if len(vals) > 0 || msg.Signature != nil {
		// validation is asynchronous and globally throttled with the throttleValidate semaphore.
		// the purpose of the global throttle is to bound the goncurrency possible from incoming
		// network traffic; each validator also has an individual throttle to preclude
		// slow (or faulty) validators from starving other topics; see validate below.
		select {
		case p.validateThrottle <- struct{}{}:
			go func() {
				p.validate(vals, src, msg)
				<-p.validateThrottle
			}()
		default:
			log.Warningf("message validation throttled; dropping message from %s", src)
		}
		return
	}

	p.publishMessage(src, msg.Message)
}
func (p *PubSub) validate(vals []*topicVal, src peer.ID, msg *Message) {
	if msg.Signature != nil {
		if !p.validateSignature(msg) {
			log.Warningf("message signature validation failed; dropping message from %s", src)
			return
		}
	}

	if len(vals) > 0 {
		if !p.validateTopic(vals, src, msg) {
			log.Warningf("message validation failed; dropping message from %s", src)
			return
		}
	}

	// all validators were successful, send the message
	p.sendMsg <- &sendReq{
		from: src,
		msg:  msg,
	}
}
func (p *PubSub) validateSingleTopic(val *topicVal, src peer.ID, msg *Message) bool {
	select {
	case val.validateThrottle <- struct{}{}:
		ctx, cancel := context.WithCancel(p.ctx)
		defer cancel()

		res := val.validateMsg(ctx, src, msg)
		<-val.validateThrottle

		return res

	default:
		log.Debugf("validation throttled for topic %s", val.topic)
		return false
	}
}
func (p *PubSub) getValidators(msg *Message) []*topicVal {
	var vals []*topicVal

	for _, topic := range msg.GetTopicIDs() {
		val, ok := p.topicVals[topic]
		if !ok {
			continue
		}

		vals = append(vals, val)
	}

	return vals
}
func (p *PubSub) Subscribe(topic string, opts ...SubOpt) (*Subscription, error) {
	td := pb.TopicDescriptor{Name: &topic}

	return p.SubscribeByTopicDescriptor(&td, opts...)
}
func (p *PubSub) SubscribeByTopicDescriptor(td *pb.TopicDescriptor, opts ...SubOpt) (*Subscription, error) {
	if td.GetAuth().GetMode() != pb.TopicDescriptor_AuthOpts_NONE {
		return nil, fmt.Errorf("auth mode not yet supported")
	}

	if td.GetEnc().GetMode() != pb.TopicDescriptor_EncOpts_NONE {
		return nil, fmt.Errorf("encryption mode not yet supported")
	}

	sub := &Subscription{
		topic: td.GetName(),
	}

	for _, opt := range opts {
		err := opt(sub)
		if err != nil {
			return nil, err
		}
	}

	out := make(chan *Subscription, 1)
	p.addSub <- &addSubReq{
		sub:  sub,
		resp: out,
	}

	return <-out, nil
}
func (p *PubSub) GetTopics() []string {
	out := make(chan []string, 1)
	p.getTopics <- &topicReq{resp: out}
	return <-out
}
func (p *PubSub) Publish(topic string, data []byte) error {
	seqno := p.nextSeqno()
	m := &pb.Message{
		Data:     data,
		TopicIDs: []string{topic},
		From:     []byte(p.host.ID()),
		Seqno:    seqno,
	}
	if p.signKey != nil {
		m.From = []byte(p.signID)
		err := signMessage(p.signID, p.signKey, m)
		if err != nil {
			return err
		}
	}
	p.publish <- &Message{m}
	return nil
}
func (p *PubSub) ListPeers(topic string) []peer.ID {
	out := make(chan []peer.ID)
	p.getPeers <- &listPeerReq{
		resp:  out,
		topic: topic,
	}
	return <-out
}
func WithValidatorTimeout(timeout time.Duration) ValidatorOpt {
	return func(addVal *addValReq) error {
		addVal.timeout = timeout
		return nil
	}
}
func WithValidatorConcurrency(n int) ValidatorOpt {
	return func(addVal *addValReq) error {
		addVal.throttle = n
		return nil
	}
}
func (p *PubSub) RegisterTopicValidator(topic string, val Validator, opts ...ValidatorOpt) error {
	addVal := &addValReq{
		topic:    topic,
		validate: val,
		resp:     make(chan error, 1),
	}

	for _, opt := range opts {
		err := opt(addVal)
		if err != nil {
			return err
		}
	}

	p.addVal <- addVal
	return <-addVal.resp
}
func (p *PubSub) UnregisterTopicValidator(topic string) error {
	rmVal := &rmValReq{
		topic: topic,
		resp:  make(chan error, 1),
	}

	p.rmVal <- rmVal
	return <-rmVal.resp
}
func DefaultMetricPrefix(name string, tags map[string]string) string {
	return MetricWithPrefix("tchannel.", name, tags)
}
func MetricWithPrefix(prefix, name string, tags map[string]string) string {
	buf := bufPool.Get().(*bytes.Buffer)
	buf.Reset()

	if prefix != "" {
		buf.WriteString(prefix)
	}
	buf.WriteString(name)

	addKeys := make([]string, 0, 5)
	switch {
	case strings.HasPrefix(name, "outbound"):
		addKeys = append(addKeys, "service", "target-service", "target-endpoint")
		if strings.HasPrefix(name, "outbound.calls.retries") {
			addKeys = append(addKeys, "retry-count")
		}
	case strings.HasPrefix(name, "inbound"):
		addKeys = append(addKeys, "calling-service", "service", "endpoint")
	}

	for _, k := range addKeys {
		buf.WriteByte('.')
		v, ok := tags[k]
		if ok {
			writeClean(buf, v)
		} else {
			buf.WriteString("no-")
			buf.WriteString(k)
		}
	}

	m := buf.String()
	bufPool.Put(buf)
	return m
}
func NewClient(ch *tchannel.Channel, targetService string, opts *ClientOptions) *Client {
	client := &Client{
		ch:            ch,
		targetService: targetService,
	}
	if opts != nil && opts.HostPort != "" {
		client.hostPort = opts.HostPort
	}
	return client
}
func (c *Client) Call(ctx Context, method string, arg, resp interface{}) error {
	var (
		headers = ctx.Headers()

		respHeaders map[string]string
		respErr     ErrApplication
		errAt       string
		isOK        bool
	)

	err := c.ch.RunWithRetry(ctx, func(ctx context.Context, rs *tchannel.RequestState) error {
		respHeaders, respErr, isOK = nil, nil, false
		errAt = "connect"

		call, err := c.startCall(ctx, method, &tchannel.CallOptions{
			Format:       tchannel.JSON,
			RequestState: rs,
		})
		if err != nil {
			return err
		}

		isOK, errAt, err = makeCall(call, headers, arg, &respHeaders, resp, &respErr)
		return err
	})
	if err != nil {
		// TODO: Don't lose the error type here.
		return fmt.Errorf("%s: %v", errAt, err)
	}
	if !isOK {
		return respErr
	}

	return nil
}
func CallPeer(ctx Context, peer *tchannel.Peer, serviceName, method string, arg, resp interface{}) error {
	call, err := peer.BeginCall(ctx, serviceName, method, &tchannel.CallOptions{Format: tchannel.JSON})
	if err != nil {
		return err
	}

	return wrapCall(ctx, call, method, arg, resp)
}
func CallSC(ctx Context, sc *tchannel.SubChannel, method string, arg, resp interface{}) error {
	call, err := sc.BeginCall(ctx, method, &tchannel.CallOptions{Format: tchannel.JSON})
	if err != nil {
		return err
	}

	return wrapCall(ctx, call, method, arg, resp)
}
func ReadResponse(call tchannel.ArgReadable) (*http.Response, error) {
	var arg2 []byte
	if err := tchannel.NewArgReader(call.Arg2Reader()).Read(&arg2); err != nil {
		return nil, err
	}

	rb := typed.NewReadBuffer(arg2)
	statusCode := rb.ReadUint16()
	message := readVarintString(rb)

	response := &http.Response{
		StatusCode: int(statusCode),
		Status:     fmt.Sprintf("%v %v", statusCode, message),
		Proto:      "HTTP/1.1",
		ProtoMajor: 1,
		ProtoMinor: 1,
		Header:     make(http.Header),
	}
	readHeaders(rb, response.Header)
	if err := rb.Err(); err != nil {
		return nil, err
	}

	arg3Reader, err := call.Arg3Reader()
	if err != nil {
		return nil, err
	}

	response.Body = arg3Reader
	return response, nil
}
func (w *tchanResponseWriter) writeHeaders() {
	// TODO(prashant): Allow creating write buffers that let you grow the buffer underneath.
	wb := typed.NewWriteBufferWithSize(10000)
	wb.WriteUint16(uint16(w.statusCode))
	writeVarintString(wb, http.StatusText(w.statusCode))
	writeHeaders(wb, w.headers)

	arg2Writer, err := w.response.Arg2Writer()
	if err != nil {
		w.err = err
		return
	}
	if _, w.err = wb.FlushTo(arg2Writer); w.err != nil {
		return
	}
	if w.err = arg2Writer.Close(); w.err != nil {
		return
	}

	w.arg3Writer, w.err = w.response.Arg3Writer()
}
func ResponseWriter(response tchannel.ArgWritable) (http.ResponseWriter, func() error) {
	responseWriter := newTChanResponseWriter(response)
	return responseWriter, responseWriter.finish
}
func ReadHeaders(r io.Reader) (map[string]string, error) {
	reader := typed.NewReader(r)
	m, err := readHeaders(reader)
	reader.Release()

	return m, err
}
func NewTCPRawRelay(dests []string) (Relay, error) {
	return newTCPRelay(dests, func(_ bool, src, dst net.Conn) {
		io.Copy(src, dst)
	})
}
func NewClient(ch *tchannel.Channel, config Configuration, opts *ClientOptions) (*Client, error) {
	client := &Client{tchan: ch, quit: make(chan struct{})}
	if opts != nil {
		client.opts = *opts
	}
	if client.opts.Timeout == 0 {
		client.opts.Timeout = 3 * time.Second
	}
	if client.opts.TimeoutPerAttempt == 0 {
		client.opts.TimeoutPerAttempt = time.Second
	}
	if client.opts.Handler == nil {
		client.opts.Handler = nullHandler{}
	}
	if client.opts.TimeSleep == nil {
		client.opts.TimeSleep = time.Sleep
	}

	if err := parseConfig(&config); err != nil {
		return nil, err
	}

	// Add the given initial nodes as peers.
	for _, node := range config.InitialNodes {
		addPeer(ch, node)
	}

	client.jsonClient = tjson.NewClient(ch, hyperbahnServiceName, nil)
	thriftClient := tthrift.NewClient(ch, hyperbahnServiceName, nil)
	client.hyperbahnClient = htypes.NewTChanHyperbahnClient(thriftClient)

	return client, nil
}
func (c *Client) Advertise(otherServices ...tchannel.Registrar) error {
	c.getServiceNames(otherServices)

	if err := c.initialAdvertise(); err != nil {
		return err
	}

	c.opts.Handler.On(Advertised)
	go c.advertiseLoop()
	return nil
}
func (h *handler) Handle(tctx context.Context, call *tchannel.InboundCall) error {
	var headers map[string]string
	if err := tchannel.NewArgReader(call.Arg2Reader()).ReadJSON(&headers); err != nil {
		return fmt.Errorf("arg2 read failed: %v", err)
	}
	tctx = tchannel.ExtractInboundSpan(tctx, call, headers, h.tracer())
	ctx := WithHeaders(tctx, headers)

	var arg3 reflect.Value
	var callArg reflect.Value
	if h.isArgMap {
		arg3 = reflect.New(h.argType)
		// New returns a pointer, but the method accepts the map directly.
		callArg = arg3.Elem()
	} else {
		arg3 = reflect.New(h.argType.Elem())
		callArg = arg3
	}
	if err := tchannel.NewArgReader(call.Arg3Reader()).ReadJSON(arg3.Interface()); err != nil {
		return fmt.Errorf("arg3 read failed: %v", err)
	}

	args := []reflect.Value{reflect.ValueOf(ctx), callArg}
	results := h.handler.Call(args)

	res := results[0].Interface()
	err := results[1].Interface()
	// If an error was returned, we create an error arg3 to respond with.
	if err != nil {
		// TODO(prashantv): More consistent error handling between json/raw/thrift..
		if serr, ok := err.(tchannel.SystemError); ok {
			return call.Response().SendSystemError(serr)
		}

		call.Response().SetApplicationError()
		// TODO(prashant): Allow client to customize the error in more ways.
		res = struct {
			Type    string `json:"type"`
			Message string `json:"message"`
		}{
			Type:    "error",
			Message: err.(error).Error(),
		}
	}

	if err := tchannel.NewArgWriter(call.Response().Arg2Writer()).WriteJSON(ctx.ResponseHeaders()); err != nil {
		return err
	}

	return tchannel.NewArgWriter(call.Response().Arg3Writer()).WriteJSON(res)
}
func (s *Server) Start() error {
	if s.HostPort == "" {
		s.HostPort = ":" + common.DefaultServerPort
	}
	channelOpts := &tchannel.ChannelOptions{
		Tracer: s.Tracer,
	}
	ch, err := tchannel.NewChannel(common.DefaultServiceName, channelOpts)
	if err != nil {
		return err
	}

	if err := ch.ListenAndServe(s.HostPort); err != nil {
		return err
	}
	s.HostPort = ch.PeerInfo().HostPort // override in case it was ":0"
	log.Printf("Started tchannel server at %s\n", s.HostPort)
	s.Ch = ch
	return nil
}
func (s *Server) Port() string {
	hostPortSplit := strings.Split(s.HostPort, ":")
	port := hostPortSplit[len(hostPortSplit)-1]
	return port
}
func (l *PeerList) SetStrategy(sc ScoreCalculator) {
	l.Lock()
	defer l.Unlock()

	l.scoreCalculator = sc
	for _, ps := range l.peersByHostPort {
		newScore := l.scoreCalculator.GetScore(ps.Peer)
		l.updatePeer(ps, newScore)
	}
}
func (l *PeerList) Add(hostPort string) *Peer {
	if ps, ok := l.exists(hostPort); ok {
		return ps.Peer
	}
	l.Lock()
	defer l.Unlock()

	if p, ok := l.peersByHostPort[hostPort]; ok {
		return p.Peer
	}

	p := l.parent.Add(hostPort)
	p.addSC()
	ps := newPeerScore(p, l.scoreCalculator.GetScore(p))

	l.peersByHostPort[hostPort] = ps
	l.peerHeap.addPeer(ps)
	return p
}
func (l *PeerList) GetNew(prevSelected map[string]struct{}) (*Peer, error) {
	l.Lock()
	defer l.Unlock()
	if l.peerHeap.Len() == 0 {
		return nil, ErrNoPeers
	}

	// Select a peer, avoiding previously selected peers. If all peers have been previously
	// selected, then it's OK to repick them.
	peer := l.choosePeer(prevSelected, true /* avoidHost */)
	if peer == nil {
		peer = l.choosePeer(prevSelected, false /* avoidHost */)
	}
	if peer == nil {
		return nil, ErrNoNewPeers
	}
	return peer, nil
}
func (l *PeerList) Get(prevSelected map[string]struct{}) (*Peer, error) {
	peer, err := l.GetNew(prevSelected)
	if err == ErrNoNewPeers {
		l.Lock()
		peer = l.choosePeer(nil, false /* avoidHost */)
		l.Unlock()
	} else if err != nil {
		return nil, err
	}
	if peer == nil {
		return nil, ErrNoPeers
	}
	return peer, nil
}
func (l *PeerList) Remove(hostPort string) error {
	l.Lock()
	defer l.Unlock()

	p, ok := l.peersByHostPort[hostPort]
	if !ok {
		return ErrPeerNotFound
	}

	p.delSC()
	delete(l.peersByHostPort, hostPort)
	l.peerHeap.removePeer(p)

	return nil
}
func (l *PeerList) Copy() map[string]*Peer {
	l.RLock()
	defer l.RUnlock()

	listCopy := make(map[string]*Peer)
	for k, v := range l.peersByHostPort {
		listCopy[k] = v.Peer
	}
	return listCopy
}
func (l *PeerList) Len() int {
	l.RLock()
	defer l.RUnlock()
	return l.peerHeap.Len()
}
func (l *PeerList) exists(hostPort string) (*peerScore, bool) {
	l.RLock()
	ps, ok := l.peersByHostPort[hostPort]
	l.RUnlock()

	return ps, ok
}
func (l *PeerList) getPeerScore(hostPort string) (*peerScore, uint64, bool) {
	ps, ok := l.peersByHostPort[hostPort]
	if !ok {
		return nil, 0, false
	}
	return ps, ps.score, ok
}
func (l *PeerList) onPeerChange(p *Peer) {
	l.RLock()
	ps, psScore, ok := l.getPeerScore(p.hostPort)
	sc := l.scoreCalculator
	l.RUnlock()
	if !ok {
		return
	}

	newScore := sc.GetScore(ps.Peer)
	if newScore == psScore {
		return
	}

	l.Lock()
	l.updatePeer(ps, newScore)
	l.Unlock()
}
func (l *PeerList) updatePeer(ps *peerScore, newScore uint64) {
	if ps.score == newScore {
		return
	}

	ps.score = newScore
	l.peerHeap.updatePeer(ps)
}
func (p *Peer) getConn(i int) *Connection {
	inboundLen := len(p.inboundConnections)
	if i < inboundLen {
		return p.inboundConnections[i]
	}

	return p.outboundConnections[i-inboundLen]
}
func (p *Peer) GetConnection(ctx context.Context) (*Connection, error) {
	if activeConn, ok := p.getActiveConn(); ok {
		return activeConn, nil
	}

	// Lock here to restrict new connection creation attempts to one goroutine
	p.newConnLock.Lock()
	defer p.newConnLock.Unlock()

	// Check active connections again in case someone else got ahead of us.
	if activeConn, ok := p.getActiveConn(); ok {
		return activeConn, nil
	}

	// No active connections, make a new outgoing connection.
	return p.Connect(ctx)
}
func (p *Peer) getConnectionRelay(timeout time.Duration) (*Connection, error) {
	if conn, ok := p.getActiveConn(); ok {
		return conn, nil
	}

	// Lock here to restrict new connection creation attempts to one goroutine
	p.newConnLock.Lock()
	defer p.newConnLock.Unlock()

	// Check active connections again in case someone else got ahead of us.
	if activeConn, ok := p.getActiveConn(); ok {
		return activeConn, nil
	}

	// When the relay creates outbound connections, we don't want those services
	// to ever connect back to us and send us traffic. We hide the host:port
	// so that service instances on remote machines don't try to connect back
	// and don't try to send Hyperbahn traffic on this connection.
	ctx, cancel := NewContextBuilder(timeout).HideListeningOnOutbound().Build()
	defer cancel()

	return p.Connect(ctx)
}
func (p *Peer) canRemove() bool {
	p.RLock()
	count := len(p.inboundConnections) + len(p.outboundConnections) + int(p.scCount)
	p.RUnlock()
	return count == 0
}
func (p *Peer) addConnection(c *Connection, direction connectionDirection) error {
	conns := p.connectionsFor(direction)

	if c.readState() != connectionActive {
		return ErrInvalidConnectionState
	}

	p.Lock()
	*conns = append(*conns, c)
	p.Unlock()

	// Inform third parties that a peer gained a connection.
	p.onStatusChanged(p)

	return nil
}
func (p *Peer) removeConnection(connsPtr *[]*Connection, changed *Connection) bool {
	conns := *connsPtr
	for i, c := range conns {
		if c == changed {
			// Remove the connection by moving the last item forward, and slicing the list.
			last := len(conns) - 1
			conns[i], conns[last] = conns[last], nil
			*connsPtr = conns[:last]
			return true
		}
	}

	return false
}
func (p *Peer) connectionCloseStateChange(changed *Connection) {
	if changed.IsActive() {
		return
	}

	p.Lock()
	found := p.removeConnection(&p.inboundConnections, changed)
	if !found {
		found = p.removeConnection(&p.outboundConnections, changed)
	}
	p.Unlock()

	if found {
		p.onClosedConnRemoved(p)
		// Inform third parties that a peer lost a connection.
		p.onStatusChanged(p)
	}
}
func (p *Peer) Connect(ctx context.Context) (*Connection, error) {
	return p.channel.Connect(ctx, p.hostPort)
}
func (p *Peer) BeginCall(ctx context.Context, serviceName, methodName string, callOptions *CallOptions) (*OutboundCall, error) {
	if callOptions == nil {
		callOptions = defaultCallOptions
	}
	callOptions.RequestState.AddSelectedPeer(p.HostPort())

	if err := validateCall(ctx, serviceName, methodName, callOptions); err != nil {
		return nil, err
	}

	conn, err := p.GetConnection(ctx)
	if err != nil {
		return nil, err
	}

	call, err := conn.beginCall(ctx, serviceName, methodName, callOptions)
	if err != nil {
		return nil, err
	}

	return call, err
}
func (p *Peer) NumConnections() (inbound int, outbound int) {
	p.RLock()
	inbound = len(p.inboundConnections)
	outbound = len(p.outboundConnections)
	p.RUnlock()
	return inbound, outbound
}
func (p *Peer) NumPendingOutbound() int {
	count := 0
	p.RLock()
	for _, c := range p.outboundConnections {
		count += c.outbound.count()
	}

	for _, c := range p.inboundConnections {
		count += c.outbound.count()
	}
	p.RUnlock()
	return count
}
func isEphemeralHostPort(hostPort string) bool {
	return hostPort == "" || hostPort == ephemeralHostPort || strings.HasSuffix(hostPort, ":0")
}
func (h *kvHandler) Get(ctx thrift.Context, key string) (string, error) {
	if err := isValidKey(key); err != nil {
		return "", err
	}

	h.RLock()
	defer h.RUnlock()

	if val, ok := h.vals[key]; ok {
		return val, nil
	}

	return "", &keyvalue.KeyNotFound{Key: key}
}
func (h *kvHandler) Set(ctx thrift.Context, key, value string) error {
	if err := isValidKey(key); err != nil {
		return err
	}

	h.Lock()
	defer h.Unlock()

	h.vals[key] = value
	// Example of how to use response headers. Normally, these values should be passed via result structs.
	ctx.SetResponseHeaders(map[string]string{"count": fmt.Sprint(len(h.vals))})
	return nil
}
func (h *kvHandler) ClearAll(ctx thrift.Context) error {
	if !isAdmin(ctx) {
		return &keyvalue.NotAuthorized{}
	}

	h.Lock()
	defer h.Unlock()

	h.vals = make(map[string]string)
	return nil
}
func NewChannel(serviceName string, opts *ChannelOptions) (*Channel, error) {
	if serviceName == "" {
		return nil, ErrNoServiceName
	}

	if opts == nil {
		opts = &ChannelOptions{}
	}

	processName := opts.ProcessName
	if processName == "" {
		processName = fmt.Sprintf("%s[%d]", filepath.Base(os.Args[0]), os.Getpid())
	}

	logger := opts.Logger
	if logger == nil {
		logger = NullLogger
	}

	statsReporter := opts.StatsReporter
	if statsReporter == nil {
		statsReporter = NullStatsReporter
	}

	timeNow := opts.TimeNow
	if timeNow == nil {
		timeNow = time.Now
	}

	timeTicker := opts.TimeTicker
	if timeTicker == nil {
		timeTicker = time.NewTicker
	}

	chID := _nextChID.Inc()
	logger = logger.WithFields(
		LogField{"serviceName", serviceName},
		LogField{"process", processName},
		LogField{"chID", chID},
	)

	if err := opts.validateIdleCheck(); err != nil {
		return nil, err
	}

	ch := &Channel{
		channelConnectionCommon: channelConnectionCommon{
			log:           logger,
			relayLocal:    toStringSet(opts.RelayLocalHandlers),
			statsReporter: statsReporter,
			subChannels:   &subChannelMap{},
			timeNow:       timeNow,
			timeTicker:    timeTicker,
			tracer:        opts.Tracer,
		},
		chID:              chID,
		connectionOptions: opts.DefaultConnectionOptions.withDefaults(),
		relayHost:         opts.RelayHost,
		relayMaxTimeout:   validateRelayMaxTimeout(opts.RelayMaxTimeout, logger),
		relayTimerVerify:  opts.RelayTimerVerification,
		closed:            make(chan struct{}),
	}
	ch.peers = newRootPeerList(ch, opts.OnPeerStatusChanged).newChild()

	if opts.Handler != nil {
		ch.handler = opts.Handler
	} else {
		ch.handler = channelHandler{ch}
	}

	ch.mutable.peerInfo = LocalPeerInfo{
		PeerInfo: PeerInfo{
			ProcessName: processName,
			HostPort:    ephemeralHostPort,
			IsEphemeral: true,
			Version: PeerVersion{
				Language:        "go",
				LanguageVersion: strings.TrimPrefix(runtime.Version(), "go"),
				TChannelVersion: VersionInfo,
			},
		},
		ServiceName: serviceName,
	}
	ch.mutable.state = ChannelClient
	ch.mutable.conns = make(map[uint32]*Connection)
	ch.createCommonStats()

	// Register internal unless the root handler has been overridden, since
	// Register will panic.
	if opts.Handler == nil {
		ch.registerInternal()
	}

	registerNewChannel(ch)

	if opts.RelayHost != nil {
		opts.RelayHost.SetChannel(ch)
	}

	// Start the idle connection timer.
	ch.mutable.idleSweep = startIdleSweep(ch, opts)

	return ch, nil
}
func (ch *Channel) Serve(l net.Listener) error {
	mutable := &ch.mutable
	mutable.Lock()
	defer mutable.Unlock()

	if mutable.l != nil {
		return errAlreadyListening
	}
	mutable.l = tnet.Wrap(l)

	if mutable.state != ChannelClient {
		return errInvalidStateForOp
	}
	mutable.state = ChannelListening

	mutable.peerInfo.HostPort = l.Addr().String()
	mutable.peerInfo.IsEphemeral = false
	ch.log = ch.log.WithFields(LogField{"hostPort", mutable.peerInfo.HostPort})
	ch.log.Info("Channel is listening.")
	go ch.serve()
	return nil
}
func (ch *Channel) ListenAndServe(hostPort string) error {
	mutable := &ch.mutable
	mutable.RLock()

	if mutable.l != nil {
		mutable.RUnlock()
		return errAlreadyListening
	}

	l, err := net.Listen("tcp", hostPort)
	if err != nil {
		mutable.RUnlock()
		return err
	}

	mutable.RUnlock()
	return ch.Serve(l)
}
func (ch *Channel) Register(h Handler, methodName string) {
	if _, ok := ch.handler.(channelHandler); !ok {
		panic("can't register handler when channel configured with alternate root handler")
	}
	ch.GetSubChannel(ch.PeerInfo().ServiceName).Register(h, methodName)
}
func (ch *Channel) PeerInfo() LocalPeerInfo {
	ch.mutable.RLock()
	peerInfo := ch.mutable.peerInfo
	ch.mutable.RUnlock()

	return peerInfo
}
func (ch *Channel) GetSubChannel(serviceName string, opts ...SubChannelOption) *SubChannel {
	sub, added := ch.subChannels.getOrAdd(serviceName, ch)
	if added {
		for _, opt := range opts {
			opt(sub)
		}
	}
	return sub
}
func (ch *Channel) serve() {
	acceptBackoff := 0 * time.Millisecond

	for {
		netConn, err := ch.mutable.l.Accept()
		if err != nil {
			// Backoff from new accepts if this is a temporary error
			if ne, ok := err.(net.Error); ok && ne.Temporary() {
				if acceptBackoff == 0 {
					acceptBackoff = 5 * time.Millisecond
				} else {
					acceptBackoff *= 2
				}
				if max := 1 * time.Second; acceptBackoff > max {
					acceptBackoff = max
				}
				ch.log.WithFields(
					ErrField(err),
					LogField{"backoff", acceptBackoff},
				).Warn("Accept error, will wait and retry.")
				time.Sleep(acceptBackoff)
				continue
			} else {
				// Only log an error if this didn't happen due to a Close.
				if ch.State() >= ChannelStartClose {
					return
				}
				ch.log.WithFields(ErrField(err)).Fatal("Unrecoverable accept error, closing server.")
				return
			}
		}

		acceptBackoff = 0

		// Perform the connection handshake in a background goroutine.
		go func() {
			// Register the connection in the peer once the channel is set up.
			events := connectionEvents{
				OnActive:           ch.inboundConnectionActive,
				OnCloseStateChange: ch.connectionCloseStateChange,
				OnExchangeUpdated:  ch.exchangeUpdated,
			}
			if _, err := ch.inboundHandshake(context.Background(), netConn, events); err != nil {
				netConn.Close()
			}
		}()
	}
}
func (ch *Channel) Ping(ctx context.Context, hostPort string) error {
	peer := ch.RootPeers().GetOrAdd(hostPort)
	conn, err := peer.GetConnection(ctx)
	if err != nil {
		return err
	}

	return conn.ping(ctx)
}
func (ch *Channel) StatsTags() map[string]string {
	m := make(map[string]string)
	for k, v := range ch.commonStatsTags {
		m[k] = v
	}
	return m
}
func (ch *Channel) Connect(ctx context.Context, hostPort string) (*Connection, error) {
	switch state := ch.State(); state {
	case ChannelClient, ChannelListening:
		break
	default:
		ch.log.Debugf("Connect rejecting new connection as state is %v", state)
		return nil, errInvalidStateForOp
	}

	// The context timeout applies to the whole call, but users may want a lower
	// connect timeout (e.g. for streams).
	if params := getTChannelParams(ctx); params != nil && params.connectTimeout > 0 {
		var cancel context.CancelFunc
		ctx, cancel = context.WithTimeout(ctx, params.connectTimeout)
		defer cancel()
	}

	events := connectionEvents{
		OnActive:           ch.outboundConnectionActive,
		OnCloseStateChange: ch.connectionCloseStateChange,
		OnExchangeUpdated:  ch.exchangeUpdated,
	}

	if err := ctx.Err(); err != nil {
		return nil, GetContextError(err)
	}

	timeout := getTimeout(ctx)
	tcpConn, err := dialContext(ctx, hostPort)
	if err != nil {
		if ne, ok := err.(net.Error); ok && ne.Timeout() {
			ch.log.WithFields(
				LogField{"remoteHostPort", hostPort},
				LogField{"timeout", timeout},
			).Info("Outbound net.Dial timed out.")
			err = ErrTimeout
		} else if ctx.Err() == context.Canceled {
			ch.log.WithFields(
				LogField{"remoteHostPort", hostPort},
			).Info("Outbound net.Dial was cancelled.")
			err = GetContextError(ErrRequestCancelled)
		} else {
			ch.log.WithFields(
				ErrField(err),
				LogField{"remoteHostPort", hostPort},
			).Info("Outbound net.Dial failed.")
		}
		return nil, err
	}

	conn, err := ch.outboundHandshake(ctx, tcpConn, hostPort, events)
	if conn != nil {
		// It's possible that the connection we just created responds with a host:port
		// that is not what we tried to connect to. E.g., we may have connected to
		// 127.0.0.1:1234, but the returned host:port may be 10.0.0.1:1234.
		// In this case, the connection won't be added to 127.0.0.1:1234 peer
		// and so future calls to that peer may end up creating new connections. To
		// avoid this issue, and to avoid clients being aware of any TCP relays, we
		// add the connection to the intended peer.
		if hostPort != conn.remotePeerInfo.HostPort {
			conn.log.Debugf("Outbound connection host:port mismatch, adding to peer %v", conn.remotePeerInfo.HostPort)
			ch.addConnectionToPeer(hostPort, conn, outbound)
		}
	}

	return conn, err
}
func (ch *Channel) exchangeUpdated(c *Connection) {
	if c.remotePeerInfo.HostPort == "" {
		// Hostport is unknown until we get init resp.
		return
	}

	p, ok := ch.RootPeers().Get(c.remotePeerInfo.HostPort)
	if !ok {
		return
	}

	ch.updatePeer(p)
}
func (ch *Channel) updatePeer(p *Peer) {
	ch.peers.onPeerChange(p)
	ch.subChannels.updatePeer(p)
	p.callOnUpdateComplete()
}
func (ch *Channel) addConnection(c *Connection, direction connectionDirection) bool {
	ch.mutable.Lock()
	defer ch.mutable.Unlock()

	if c.readState() != connectionActive {
		return false
	}

	switch state := ch.mutable.state; state {
	case ChannelClient, ChannelListening:
		break
	default:
		return false
	}

	ch.mutable.conns[c.connID] = c
	return true
}
func (ch *Channel) removeClosedConn(c *Connection) {
	if c.readState() != connectionClosed {
		return
	}

	ch.mutable.Lock()
	delete(ch.mutable.conns, c.connID)
	ch.mutable.Unlock()
}
func (ch *Channel) connectionCloseStateChange(c *Connection) {
	ch.removeClosedConn(c)
	if peer, ok := ch.RootPeers().Get(c.remotePeerInfo.HostPort); ok {
		peer.connectionCloseStateChange(c)
		ch.updatePeer(peer)
	}
	if c.outboundHP != "" && c.outboundHP != c.remotePeerInfo.HostPort {
		// Outbound connections may be in multiple peers.
		if peer, ok := ch.RootPeers().Get(c.outboundHP); ok {
			peer.connectionCloseStateChange(c)
			ch.updatePeer(peer)
		}
	}

	chState := ch.State()
	if chState != ChannelStartClose && chState != ChannelInboundClosed {
		return
	}

	ch.mutable.RLock()
	minState := ch.getMinConnectionState()
	ch.mutable.RUnlock()

	var updateTo ChannelState
	if minState >= connectionClosed {
		updateTo = ChannelClosed
	} else if minState >= connectionInboundClosed && chState == ChannelStartClose {
		updateTo = ChannelInboundClosed
	}

	var updatedToState ChannelState
	if updateTo > 0 {
		ch.mutable.Lock()
		// Recheck the state as it's possible another goroutine changed the state
		// from what we expected, and so we might make a stale change.
		if ch.mutable.state == chState {
			ch.mutable.state = updateTo
			updatedToState = updateTo
		}
		ch.mutable.Unlock()
		chState = updateTo
	}

	c.log.Debugf("ConnectionCloseStateChange channel state = %v connection minState = %v",
		chState, minState)

	if updatedToState == ChannelClosed {
		ch.onClosed()
	}
}
func (ch *Channel) State() ChannelState {
	ch.mutable.RLock()
	state := ch.mutable.state
	ch.mutable.RUnlock()

	return state
}
func NewReader(reader io.Reader) *Reader {
	r := readerPool.Get().(*Reader)
	r.reader = reader
	r.err = nil
	return r
}
func (r *Reader) ReadUint16() uint16 {
	if r.err != nil {
		return 0
	}

	buf := r.buf[:2]

	var readN int
	readN, r.err = io.ReadFull(r.reader, buf)
	if readN < 2 {
		return 0
	}
	return binary.BigEndian.Uint16(buf)
}
func (r *Reader) ReadString(n int) string {
	if r.err != nil {
		return ""
	}

	var buf []byte
	if n <= maxPoolStringLen {
		buf = r.buf[:n]
	} else {
		buf = make([]byte, n)
	}

	var readN int
	readN, r.err = io.ReadFull(r.reader, buf)
	if readN < n {
		return ""
	}
	s := string(buf)

	return s
}
func (r *Reader) ReadLen16String() string {
	len := r.ReadUint16()
	return r.ReadString(int(len))
}
func (b *Behavior) Register(ch *tchannel.Channel) {
	b.registerThrift(ch)
	b.registerJSON(ch)
}
func (b *Behavior) Run(t crossdock.T) {
	logParams(t)
	sampled, err := strconv.ParseBool(t.Param(sampledParam))
	if err != nil {
		t.Fatalf("Malformed param %s: %s", sampledParam, err)
	}
	baggage := randomBaggage()

	level1 := &Request{
		ServerRole: RoleS1,
	}
	server1 := t.Param(server1NameParam)

	level2 := &Downstream{
		ServiceName: t.Param(server2NameParam),
		ServerRole:  RoleS2,
		HostPort: fmt.Sprintf("%s:%s",
			b.serviceToHost(t.Param(server2NameParam)),
			b.ServerPort,
		),
		Encoding: t.Param(server2EncodingParam),
	}
	level1.Downstream = level2

	level3 := &Downstream{
		ServiceName: t.Param(server3NameParam),
		ServerRole:  RoleS3,
		HostPort: fmt.Sprintf("%s:%s",
			b.serviceToHost(t.Param(server3NameParam)),
			b.ServerPort,
		),
		Encoding: t.Param(server3EncodingParam),
	}
	level2.Downstream = level3

	resp, err := b.startTrace(t, level1, sampled, baggage)
	if err != nil {
		t.Errorf("Failed to startTrace in S1(%s): %s", server1, err.Error())
		return
	}

	log.Printf("Response: span=%+v, downstream=%+v", resp.Span, resp.Downstream)
	traceID := resp.Span.TraceID

	require := crossdock.Require(t)
	require.NotEmpty(traceID, "Trace ID should not be empty in S1(%s)", server1)

	if validateTrace(t, level1.Downstream, resp, server1, 1, traceID, sampled, baggage) {
		t.Successf("trace checks out")
		log.Println("PASS")
	} else {
		log.Println("FAIL")
	}
}
func (tp *relayTimerPool) Get() *relayTimer {
	timer, ok := tp.pool.Get().(*relayTimer)
	if ok {
		timer.released = false
		return timer
	}

	rt := &relayTimer{
		pool: tp,
	}
	// Go timers are started by default. However, we need to separate creating
	// the timer and starting the timer for use in the relay code paths.
	// To make this work without more locks in the relayTimer, we create a Go timer
	// with a huge timeout so it doesn't run, then stop it so we can start it later.
	rt.timer = time.AfterFunc(time.Duration(math.MaxInt64), rt.OnTimer)
	if !rt.timer.Stop() {
		panic("relayTimer requires timers in stopped state, but failed to stop underlying timer")
	}
	return rt
}
func (tp *relayTimerPool) Put(rt *relayTimer) {
	if tp.verify {
		// If we are trying to verify correct pool behavior, then we don't release
		// the timer, and instead ensure no methods are called after being released.
		return
	}
	tp.pool.Put(rt)
}
func (rt *relayTimer) Start(d time.Duration, items *relayItems, id uint32, isOriginator bool) {
	rt.verifyNotReleased()
	if rt.active {
		panic("Tried to start an already-active timer")
	}

	rt.active = true
	rt.items = items
	rt.id = id
	rt.isOriginator = isOriginator

	if wasActive := rt.timer.Reset(d); wasActive {
		panic("relayTimer's underlying timer was Started multiple times without Stop")
	}
}
func (rt *relayTimer) Release() {
	rt.verifyNotReleased()
	if rt.active {
		panic("only stopped or completed timers can be released")
	}
	rt.released = true
	rt.pool.Put(rt)
}
func NewLogger(writer io.Writer, fields ...LogField) Logger {
	return &writerLogger{writer, fields}
}
func NewTCPFrameRelay(dests []string, modifier func(bool, *tchannel.Frame) *tchannel.Frame) (Relay, error) {
	var err error
	r := &tcpFrameRelay{modifier: modifier}

	r.tcpRelay, err = newTCPRelay(dests, r.handleConnFrameRelay)
	if err != nil {
		return nil, err
	}

	return r, nil
}
func (kt knownTags) tallyTags() map[string]string {
	tallyTags := make(map[string]string, 5)

	if kt.dest != "" {
		tallyTags["dest"] = kt.dest
	}
	if kt.source != "" {
		tallyTags["source"] = kt.source
	}
	if kt.procedure != "" {
		tallyTags["procedure"] = kt.procedure
	}
	if kt.retryCount != "" {
		tallyTags["retry-count"] = kt.retryCount
	}

	return tallyTags
}
func Isolated(s *SubChannel) {
	s.Lock()
	s.peers = s.topChannel.peers.newSibling()
	s.peers.SetStrategy(newLeastPendingCalculator())
	s.Unlock()
}
func (c *SubChannel) Isolated() bool {
	c.RLock()
	defer c.RUnlock()
	return c.topChannel.Peers() != c.peers
}
func (c *SubChannel) Register(h Handler, methodName string) {
	handlers, ok := c.handler.(*handlerMap)
	if !ok {
		panic(fmt.Sprintf(
			"handler for SubChannel(%v) was changed to disallow method registration",
			c.ServiceName(),
		))
	}
	handlers.register(h, methodName)
}
func (c *SubChannel) GetHandlers() map[string]Handler {
	handlers, ok := c.handler.(*handlerMap)
	if !ok {
		panic(fmt.Sprintf(
			"handler for SubChannel(%v) was changed to disallow method registration",
			c.ServiceName(),
		))
	}

	handlers.RLock()
	handlersMap := make(map[string]Handler, len(handlers.handlers))
	for k, v := range handlers.handlers {
		handlersMap[k] = v
	}
	handlers.RUnlock()
	return handlersMap
}
func (c *SubChannel) StatsTags() map[string]string {
	tags := c.topChannel.StatsTags()
	tags["subchannel"] = c.serviceName
	return tags
}
func (subChMap *subChannelMap) registerNewSubChannel(serviceName string, ch *Channel) (_ *SubChannel, added bool) {
	subChMap.Lock()
	defer subChMap.Unlock()

	if subChMap.subchannels == nil {
		subChMap.subchannels = make(map[string]*SubChannel)
	}

	if sc, ok := subChMap.subchannels[serviceName]; ok {
		return sc, false
	}

	sc := newSubChannel(serviceName, ch)
	subChMap.subchannels[serviceName] = sc
	return sc, true
}
func (subChMap *subChannelMap) get(serviceName string) (*SubChannel, bool) {
	subChMap.RLock()
	sc, ok := subChMap.subchannels[serviceName]
	subChMap.RUnlock()
	return sc, ok
}
func (subChMap *subChannelMap) getOrAdd(serviceName string, ch *Channel) (_ *SubChannel, added bool) {
	if sc, ok := subChMap.get(serviceName); ok {
		return sc, false
	}

	return subChMap.registerNewSubChannel(serviceName, ch)
}
func (c *Client) Discover(serviceName string) ([]string, error) {
	ctx, cancel := thrift.NewContext(time.Second)
	defer cancel()

	result, err := c.hyperbahnClient.Discover(ctx, &hyperbahn.DiscoveryQuery{ServiceName: serviceName})
	if err != nil {
		return nil, err
	}

	var hostPorts []string
	for _, peer := range result.GetPeers() {
		hostPorts = append(hostPorts, servicePeerToHostPort(peer))
	}

	return hostPorts, nil
}
func (c *Client) Start() error {
	if err := c.listen(); err != nil {
		return err
	}
	go func() {
		http.Serve(c.listener, c.mux)
	}()
	return nil
}
func (c *Client) listen() error {
	c.setDefaultPort(&c.ClientHostPort, ":"+common.DefaultClientPortHTTP)
	c.setDefaultPort(&c.ServerPort, common.DefaultServerPort)

	c.mux = http.NewServeMux() // Using default mux creates problem in unit tests
	c.mux.Handle("/", crossdock.Handler(c.Behaviors, true))

	listener, err := net.Listen("tcp", c.ClientHostPort)
	if err != nil {
		return err
	}
	c.listener = listener
	c.ClientHostPort = listener.Addr().String() // override in case it was ":0"
	return nil
}
func WriteRequest(call tchannel.ArgWritable, req *http.Request) error {
	// TODO(prashant): Allow creating write buffers that let you grow the buffer underneath.
	wb := typed.NewWriteBufferWithSize(10000)
	wb.WriteLen8String(req.Method)
	writeVarintString(wb, req.URL.String())
	writeHeaders(wb, req.Header)

	arg2Writer, err := call.Arg2Writer()
	if err != nil {
		return err
	}
	if _, err := wb.FlushTo(arg2Writer); err != nil {
		return err
	}
	if err := arg2Writer.Close(); err != nil {
		return err
	}

	arg3Writer, err := call.Arg3Writer()
	if err != nil {
		return err
	}

	if req.Body != nil {
		if _, err = io.Copy(arg3Writer, req.Body); err != nil {
			return err
		}
	}
	return arg3Writer.Close()
}
func ReadRequest(call tchannel.ArgReadable) (*http.Request, error) {
	var arg2 []byte
	if err := tchannel.NewArgReader(call.Arg2Reader()).Read(&arg2); err != nil {
		return nil, err
	}
	rb := typed.NewReadBuffer(arg2)
	method := rb.ReadLen8String()
	url := readVarintString(rb)

	r, err := http.NewRequest(method, url, nil)
	if err != nil {
		return nil, err
	}
	readHeaders(rb, r.Header)

	if err := rb.Err(); err != nil {
		return nil, err
	}

	r.Body, err = call.Arg3Reader()
	return r, err
}
func NewReadBufferWithSize(size int) *ReadBuffer {
	return &ReadBuffer{buffer: make([]byte, size), remaining: nil}
}
func (r *ReadBuffer) ReadByte() (byte, error) {
	if r.err != nil {
		return 0, r.err
	}

	if len(r.remaining) < 1 {
		r.err = ErrEOF
		return 0, r.err
	}

	b := r.remaining[0]
	r.remaining = r.remaining[1:]
	return b, nil
}
func (r *ReadBuffer) ReadBytes(n int) []byte {
	if r.err != nil {
		return nil
	}

	if len(r.remaining) < n {
		r.err = ErrEOF
		return nil
	}

	b := r.remaining[0:n]
	r.remaining = r.remaining[n:]
	return b
}
func (r *ReadBuffer) ReadString(n int) string {
	if b := r.ReadBytes(n); b != nil {
		// TODO(mmihic): This creates a copy, which sucks
		return string(b)
	}

	return ""
}
func (r *ReadBuffer) ReadUint16() uint16 {
	if b := r.ReadBytes(2); b != nil {
		return binary.BigEndian.Uint16(b)
	}

	return 0
}
func (r *ReadBuffer) ReadUint32() uint32 {
	if b := r.ReadBytes(4); b != nil {
		return binary.BigEndian.Uint32(b)
	}

	return 0
}
func (r *ReadBuffer) ReadUint64() uint64 {
	if b := r.ReadBytes(8); b != nil {
		return binary.BigEndian.Uint64(b)
	}

	return 0
}
func (r *ReadBuffer) ReadUvarint() uint64 {
	v, _ := binary.ReadUvarint(r)
	return v
}
func (r *ReadBuffer) ReadLen8String() string {
	n := r.ReadSingleByte()
	return r.ReadString(int(n))
}
func (r *ReadBuffer) ReadLen16String() string {
	n := r.ReadUint16()
	return r.ReadString(int(n))
}
func (r *ReadBuffer) FillFrom(ior io.Reader, n int) (int, error) {
	if len(r.buffer) < n {
		return 0, ErrEOF
	}

	r.err = nil
	r.remaining = r.buffer[:n]
	return io.ReadFull(ior, r.remaining)
}
func (r *ReadBuffer) Wrap(b []byte) {
	r.buffer = b
	r.remaining = b
	r.err = nil
}
func (w *WriteBuffer) WriteSingleByte(n byte) {
	if w.err != nil {
		return
	}

	if len(w.remaining) == 0 {
		w.setErr(ErrBufferFull)
		return
	}

	w.remaining[0] = n
	w.remaining = w.remaining[1:]
}
func (w *WriteBuffer) WriteBytes(in []byte) {
	if b := w.reserve(len(in)); b != nil {
		copy(b, in)
	}
}
func (w *WriteBuffer) WriteUint16(n uint16) {
	if b := w.reserve(2); b != nil {
		binary.BigEndian.PutUint16(b, n)
	}
}
func (w *WriteBuffer) WriteUint32(n uint32) {
	if b := w.reserve(4); b != nil {
		binary.BigEndian.PutUint32(b, n)
	}
}
func (w *WriteBuffer) WriteUint64(n uint64) {
	if b := w.reserve(8); b != nil {
		binary.BigEndian.PutUint64(b, n)
	}
}
func (w *WriteBuffer) WriteUvarint(n uint64) {
	// A uvarint could be up to 10 bytes long.
	buf := make([]byte, 10)
	varBytes := binary.PutUvarint(buf, n)
	if b := w.reserve(varBytes); b != nil {
		copy(b, buf[0:varBytes])
	}
}
func (w *WriteBuffer) WriteString(s string) {
	// NB(mmihic): Don't just call WriteBytes; that will make a double copy
	// of the string due to the cast
	if b := w.reserve(len(s)); b != nil {
		copy(b, s)
	}
}
func (w *WriteBuffer) WriteLen8String(s string) {
	if int(byte(len(s))) != len(s) {
		w.setErr(errStringTooLong)
	}

	w.WriteSingleByte(byte(len(s)))
	w.WriteString(s)
}
func (w *WriteBuffer) WriteLen16String(s string) {
	if int(uint16(len(s))) != len(s) {
		w.setErr(errStringTooLong)
	}

	w.WriteUint16(uint16(len(s)))
	w.WriteString(s)
}
func (w *WriteBuffer) DeferByte() ByteRef {
	if len(w.remaining) == 0 {
		w.setErr(ErrBufferFull)
		return ByteRef(nil)
	}

	// Always zero out references, since the caller expects the default to be 0.
	w.remaining[0] = 0
	bufRef := ByteRef(w.remaining[0:])
	w.remaining = w.remaining[1:]
	return bufRef
}
func (w *WriteBuffer) DeferBytes(n int) BytesRef {
	return BytesRef(w.deferred(n))
}
func (w *WriteBuffer) FlushTo(iow io.Writer) (int, error) {
	dirty := w.buffer[0:w.BytesWritten()]
	return iow.Write(dirty)
}
func (w *WriteBuffer) Reset() {
	w.remaining = w.buffer
	w.err = nil
}
func (w *WriteBuffer) Wrap(b []byte) {
	w.buffer = b
	w.remaining = b
}
func (ref Uint16Ref) Update(n uint16) {
	if ref != nil {
		binary.BigEndian.PutUint16(ref, n)
	}
}
func (ref Uint32Ref) Update(n uint32) {
	if ref != nil {
		binary.BigEndian.PutUint32(ref, n)
	}
}
func (ref Uint64Ref) Update(n uint64) {
	if ref != nil {
		binary.BigEndian.PutUint64(ref, n)
	}
}
func (ref BytesRef) Update(b []byte) {
	if ref != nil {
		copy(ref, b)
	}
}
func (ref BytesRef) UpdateString(s string) {
	if ref != nil {
		copy(ref, s)
	}
}
func (r *fragmentingReader) ArgReader(last bool) (ArgReader, error) {
	if err := r.BeginArgument(last); err != nil {
		return nil, err
	}
	return r, nil
}
func (f *writableFragment) finish(hasMoreFragments bool) {
	f.checksumRef.Update(f.checksum.Sum())
	if hasMoreFragments {
		f.flagsRef.Update(hasMoreFragmentsFlag)
	} else {
		f.checksum.Release()
	}
}
func newWritableChunk(checksum Checksum, contents *typed.WriteBuffer) *writableChunk {
	return &writableChunk{
		size:     0,
		sizeRef:  contents.DeferUint16(),
		checksum: checksum,
		contents: contents,
	}
}
func (c *writableChunk) writeAsFits(b []byte) int {
	if len(b) > c.contents.BytesRemaining() {
		b = b[:c.contents.BytesRemaining()]
	}

	c.checksum.Add(b)
	c.contents.WriteBytes(b)

	written := len(b)
	c.size += uint16(written)
	return written
}
func newFragmentingWriter(logger Logger, sender fragmentSender, checksum Checksum) *fragmentingWriter {
	return &fragmentingWriter{
		logger:   logger,
		sender:   sender,
		checksum: checksum,
		state:    fragmentingWriteStart,
	}
}
func (w *fragmentingWriter) ArgWriter(last bool) (ArgWriter, error) {
	if err := w.BeginArgument(last); err != nil {
		return nil, err
	}
	return w, nil
}
func (w *fragmentingWriter) BeginArgument(last bool) error {
	if w.err != nil {
		return w.err
	}

	switch {
	case w.state == fragmentingWriteComplete:
		w.err = errComplete
		return w.err
	case w.state.isWritingArgument():
		w.err = errAlreadyWritingArgument
		return w.err
	}

	// If we don't have a fragment, request one
	if w.curFragment == nil {
		initial := w.state == fragmentingWriteStart
		if w.curFragment, w.err = w.sender.newFragment(initial, w.checksum); w.err != nil {
			return w.err
		}
	}

	// If there's no room in the current fragment, freak out.  This will
	// only happen due to an implementation error in the TChannel stack
	// itself
	if w.curFragment.contents.BytesRemaining() <= chunkHeaderSize {
		panic(fmt.Errorf("attempting to begin an argument in a fragment with only %d bytes available",
			w.curFragment.contents.BytesRemaining()))
	}

	w.curChunk = newWritableChunk(w.checksum, w.curFragment.contents)
	w.state = fragmentingWriteInArgument
	if last {
		w.state = fragmentingWriteInLastArgument
	}
	return nil
}
func (w *fragmentingWriter) Write(b []byte) (int, error) {
	if w.err != nil {
		return 0, w.err
	}

	if !w.state.isWritingArgument() {
		w.err = errNotWritingArgument
		return 0, w.err
	}

	totalWritten := 0
	for {
		bytesWritten := w.curChunk.writeAsFits(b)
		totalWritten += bytesWritten
		if bytesWritten == len(b) {
			// The whole thing fit, we're done
			return totalWritten, nil
		}

		// There was more data than fit into the fragment, so flush the current fragment,
		// start a new fragment and chunk, and continue writing
		if w.err = w.Flush(); w.err != nil {
			return totalWritten, w.err
		}

		b = b[bytesWritten:]
	}
}
func (w *fragmentingWriter) Flush() error {
	w.curChunk.finish()
	w.curFragment.finish(true)
	if w.err = w.sender.flushFragment(w.curFragment); w.err != nil {
		return w.err
	}

	if w.curFragment, w.err = w.sender.newFragment(false, w.checksum); w.err != nil {
		return w.err
	}

	w.curChunk = newWritableChunk(w.checksum, w.curFragment.contents)
	return nil
}
func (w *fragmentingWriter) Close() error {
	last := w.state == fragmentingWriteInLastArgument
	if w.err != nil {
		return w.err
	}

	if !w.state.isWritingArgument() {
		w.err = errNotWritingArgument
		return w.err
	}

	w.curChunk.finish()

	// There are three possibilities here:
	// 1. There are no more arguments
	//      flush with more_fragments=false, mark the stream as complete
	// 2. There are more arguments, but we can't fit more data into this fragment
	//      flush with more_fragments=true, start new fragment, write empty chunk to indicate
	//      the current argument is complete
	// 3. There are more arguments, and we can fit more data into this fragment
	//      update the chunk but leave the current fragment open
	if last {
		// No more arguments - flush this final fragment and mark ourselves complete
		w.state = fragmentingWriteComplete
		w.curFragment.finish(false)
		w.err = w.sender.flushFragment(w.curFragment)
		w.sender.doneSending()
		return w.err
	}

	w.state = fragmentingWriteWaitingForArgument
	if w.curFragment.contents.BytesRemaining() > chunkHeaderSize {
		// There's enough room in this fragment for the next argument's
		// initial chunk, so we're done here
		return nil
	}

	// This fragment is full - flush and prepare for another argument
	w.curFragment.finish(true)
	if w.err = w.sender.flushFragment(w.curFragment); w.err != nil {
		return w.err
	}

	if w.curFragment, w.err = w.sender.newFragment(false, w.checksum); w.err != nil {
		return w.err
	}

	// Write an empty chunk to indicate this argument has ended
	w.curFragment.contents.WriteUint16(0)
	return nil
}
func (c *Connection) handleCallRes(frame *Frame) bool {
	if err := c.outbound.forwardPeerFrame(frame); err != nil {
		return true
	}
	return false
}
func (response *OutboundCallResponse) Arg2Reader() (ArgReader, error) {
	var method []byte
	if err := NewArgReader(response.arg1Reader()).Read(&method); err != nil {
		return nil, err
	}

	return response.arg2Reader()
}
func (c *Connection) handleError(frame *Frame) bool {
	errMsg := errorMessage{
		id: frame.Header.ID,
	}
	rbuf := typed.NewReadBuffer(frame.SizedPayload())
	if err := errMsg.read(rbuf); err != nil {
		c.log.WithFields(
			LogField{"remotePeer", c.remotePeerInfo},
			ErrField(err),
		).Warn("Unable to read error frame.")
		c.connectionError("parsing error frame", err)
		return true
	}

	if errMsg.errCode == ErrCodeProtocol {
		c.log.WithFields(
			LogField{"remotePeer", c.remotePeerInfo},
			LogField{"error", errMsg.message},
		).Warn("Peer reported protocol error.")
		c.connectionError("received protocol error", errMsg.AsSystemError())
		return true
	}

	if err := c.outbound.forwardPeerFrame(frame); err != nil {
		c.log.WithFields(
			LogField{"frameHeader", frame.Header.String()},
			LogField{"id", errMsg.id},
			LogField{"errorMessage", errMsg.message},
			LogField{"errorCode", errMsg.errCode},
			ErrField(err),
		).Info("Failed to forward error frame.")
		return true
	}

	// If the frame was forwarded, then the other side is responsible for releasing the frame.
	return false
}
func (response *OutboundCallResponse) doneReading(unexpected error) {
	now := response.timeNow()

	isSuccess := unexpected == nil && !response.ApplicationError()
	lastAttempt := isSuccess || !response.requestState.HasRetries(unexpected)

	// TODO how should this work with retries?
	if span := response.span; span != nil {
		if unexpected != nil {
			span.LogEventWithPayload("error", unexpected)
		}
		if !isSuccess && lastAttempt {
			ext.Error.Set(span, true)
		}
		span.FinishWithOptions(opentracing.FinishOptions{FinishTime: now})
	}

	latency := now.Sub(response.startedAt)
	response.statsReporter.RecordTimer("outbound.calls.per-attempt.latency", response.commonStatsTags, latency)
	if lastAttempt {
		requestLatency := response.requestState.SinceStart(now, latency)
		response.statsReporter.RecordTimer("outbound.calls.latency", response.commonStatsTags, requestLatency)
	}
	if retryCount := response.requestState.RetryCount(); retryCount > 0 {
		retryTags := cloneTags(response.commonStatsTags)
		retryTags["retry-count"] = fmt.Sprint(retryCount)
		response.statsReporter.IncCounter("outbound.calls.retries", retryTags, 1)
	}

	if unexpected != nil {
		// TODO(prashant): Report the error code type as per metrics doc and enable.
		// response.statsReporter.IncCounter("outbound.calls.system-errors", response.commonStatsTags, 1)
	} else if response.ApplicationError() {
		// TODO(prashant): Figure out how to add "type" to tags, which TChannel does not know about.
		response.statsReporter.IncCounter("outbound.calls.per-attempt.app-errors", response.commonStatsTags, 1)
		if lastAttempt {
			response.statsReporter.IncCounter("outbound.calls.app-errors", response.commonStatsTags, 1)
		}
	} else {
		response.statsReporter.IncCounter("outbound.calls.success", response.commonStatsTags, 1)
	}

	response.mex.shutdown()
}
func (w *reqResWriter) newFragment(initial bool, checksum Checksum) (*writableFragment, error) {
	if err := w.mex.checkError(); err != nil {
		return nil, w.failed(err)
	}

	message := w.messageForFragment(initial)

	// Create the frame
	frame := w.conn.opts.FramePool.Get()
	frame.Header.ID = w.mex.msgID
	frame.Header.messageType = message.messageType()

	// Write the message into the fragment, reserving flags and checksum bytes
	wbuf := typed.NewWriteBuffer(frame.Payload[:])
	fragment := new(writableFragment)
	fragment.frame = frame
	fragment.flagsRef = wbuf.DeferByte()
	if err := message.write(wbuf); err != nil {
		return nil, err
	}
	wbuf.WriteSingleByte(byte(checksum.TypeCode()))
	fragment.checksumRef = wbuf.DeferBytes(checksum.Size())
	fragment.checksum = checksum
	fragment.contents = wbuf
	return fragment, wbuf.Err()
}
func (w *reqResWriter) flushFragment(fragment *writableFragment) error {
	if w.err != nil {
		return w.err
	}

	frame := fragment.frame.(*Frame)
	frame.Header.SetPayloadSize(uint16(fragment.contents.BytesWritten()))

	if err := w.mex.checkError(); err != nil {
		return w.failed(err)
	}
	select {
	case <-w.mex.ctx.Done():
		return w.failed(GetContextError(w.mex.ctx.Err()))
	case <-w.mex.errCh.c:
		return w.failed(w.mex.errCh.err)
	case w.conn.sendCh <- frame:
		return nil
	}
}
func (w *reqResWriter) failed(err error) error {
	w.log.Debugf("writer failed: %v existing err: %v", err, w.err)
	if w.err != nil {
		return w.err
	}

	w.mex.shutdown()
	w.err = err
	return w.err
}
func (r *reqResReader) arg1Reader() (ArgReader, error) {
	return r.argReader(false /* last */, reqResReaderPreArg1, reqResReaderPreArg2)
}
func (r *reqResReader) arg2Reader() (ArgReader, error) {
	return r.argReader(false /* last */, reqResReaderPreArg2, reqResReaderPreArg3)
}
func (r *reqResReader) arg3Reader() (ArgReader, error) {
	return r.argReader(true /* last */, reqResReaderPreArg3, reqResReaderComplete)
}
func (r *reqResReader) argReader(last bool, inState reqResReaderState, outState reqResReaderState) (ArgReader, error) {
	if r.state != inState {
		return nil, r.failed(errReqResReaderStateMismatch{state: r.state, expectedState: inState})
	}

	argReader, err := r.contents.ArgReader(last)
	if err != nil {
		return nil, r.failed(err)
	}

	r.state = outState
	return argReader, nil
}
func (r *reqResReader) recvNextFragment(initial bool) (*readableFragment, error) {
	if r.initialFragment != nil {
		fragment := r.initialFragment
		r.initialFragment = nil
		r.previousFragment = fragment
		return fragment, nil
	}

	// Wait for the appropriate message from the peer
	message := r.messageForFragment(initial)
	frame, err := r.mex.recvPeerFrameOfType(message.messageType())
	if err != nil {
		if err, ok := err.(errorMessage); ok {
			// If we received a serialized error from the other side, then we should go through
			// the normal doneReading path so stats get updated with this error.
			r.err = err.AsSystemError()
			return nil, err
		}

		return nil, r.failed(err)
	}

	// Parse the message and setup the fragment
	fragment, err := parseInboundFragment(r.mex.framePool, frame, message)
	if err != nil {
		return nil, r.failed(err)
	}

	r.previousFragment = fragment
	return fragment, nil
}
func (r *reqResReader) releasePreviousFragment() {
	fragment := r.previousFragment
	r.previousFragment = nil
	if fragment != nil {
		fragment.done()
	}
}
func (r *reqResReader) failed(err error) error {
	r.log.Debugf("reader failed: %v existing err: %v", err, r.err)
	if r.err != nil {
		return r.err
	}

	r.mex.shutdown()
	r.err = err
	return r.err
}
func parseInboundFragment(framePool FramePool, frame *Frame, message message) (*readableFragment, error) {
	rbuf := typed.NewReadBuffer(frame.SizedPayload())
	fragment := new(readableFragment)
	fragment.flags = rbuf.ReadSingleByte()
	if err := message.read(rbuf); err != nil {
		return nil, err
	}

	fragment.checksumType = ChecksumType(rbuf.ReadSingleByte())
	fragment.checksum = rbuf.ReadBytes(fragment.checksumType.ChecksumSize())
	fragment.contents = rbuf
	fragment.onDone = func() {
		framePool.Release(frame)
	}
	return fragment, rbuf.Err()
}
func NewContext(timeout time.Duration) (Context, context.CancelFunc) {
	ctx, cancel := tchannel.NewContext(timeout)
	return Wrap(ctx), cancel
}
func WithHeaders(ctx context.Context, headers map[string]string) Context {
	return tchannel.WrapWithHeaders(ctx, headers)
}
func (c *Connection) healthCheck(connID uint32) {
	defer close(c.healthCheckDone)

	opts := c.opts.HealthChecks

	ticker := c.timeTicker(opts.Interval)
	defer ticker.Stop()

	consecutiveFailures := 0
	for {
		select {
		case <-ticker.C:
		case <-c.healthCheckCtx.Done():
			return
		}

		ctx, cancel := context.WithTimeout(c.healthCheckCtx, opts.Timeout)
		err := c.ping(ctx)
		cancel()
		c.healthCheckHistory.add(err == nil)
		if err == nil {
			if c.log.Enabled(LogLevelDebug) {
				c.log.Debug("Performed successful active health check.")
			}
			consecutiveFailures = 0
			continue
		}

		// If the health check failed because the connection closed or health
		// checks were stopped, we don't need to log or close the connection.
		if GetSystemErrorCode(err) == ErrCodeCancelled || err == ErrInvalidConnectionState {
			c.log.WithFields(ErrField(err)).Debug("Health checker stopped.")
			return
		}

		consecutiveFailures++
		c.log.WithFields(LogFields{
			{"consecutiveFailures", consecutiveFailures},
			ErrField(err),
			{"failuresToClose", opts.FailuresToClose},
		}...).Warn("Failed active health check.")

		if consecutiveFailures >= opts.FailuresToClose {
			c.close(LogFields{
				{"reason", "health check failure"},
				ErrField(err),
			}...)
			return
		}
	}
}
func (cb *ContextBuilder) SetTimeout(timeout time.Duration) *ContextBuilder {
	cb.Timeout = timeout
	return cb
}
func (cb *ContextBuilder) AddHeader(key, value string) *ContextBuilder {
	if cb.Headers == nil {
		cb.Headers = map[string]string{key: value}
	} else {
		cb.Headers[key] = value
	}
	return cb
}
func (cb *ContextBuilder) SetHeaders(headers map[string]string) *ContextBuilder {
	cb.Headers = headers
	cb.replaceParentHeaders = true
	return cb
}
func (cb *ContextBuilder) SetConnectTimeout(d time.Duration) *ContextBuilder {
	cb.ConnectTimeout = d
	return cb
}
func (cb *ContextBuilder) SetRetryOptions(retryOptions *RetryOptions) *ContextBuilder {
	cb.RetryOptions = retryOptions
	return cb
}
func (cb *ContextBuilder) SetTimeoutPerAttempt(timeoutPerAttempt time.Duration) *ContextBuilder {
	if cb.RetryOptions == nil {
		cb.RetryOptions = &RetryOptions{}
	}
	cb.RetryOptions.TimeoutPerAttempt = timeoutPerAttempt
	return cb
}
func (cb *ContextBuilder) SetParentContext(ctx context.Context) *ContextBuilder {
	cb.ParentContext = ctx
	return cb
}
func (cb *ContextBuilder) Build() (ContextWithHeaders, context.CancelFunc) {
	params := &tchannelCtxParams{
		options:                 cb.CallOptions,
		call:                    cb.incomingCall,
		retryOptions:            cb.RetryOptions,
		connectTimeout:          cb.ConnectTimeout,
		hideListeningOnOutbound: cb.hideListeningOnOutbound,
		tracingDisabled:         cb.TracingDisabled,
	}

	parent := cb.ParentContext
	if parent == nil {
		parent = context.Background()
	} else if headerCtx, ok := parent.(headerCtx); ok {
		// Unwrap any headerCtx, since we'll be rewrapping anyway.
		parent = headerCtx.Context
	}

	var (
		ctx    context.Context
		cancel context.CancelFunc
	)
	// All contexts created must have a timeout, but if the parent
	// already has a timeout, and the user has not specified one, then we
	// can use context.WithCancel
	_, parentHasDeadline := parent.Deadline()
	if cb.Timeout == 0 && parentHasDeadline {
		ctx, cancel = context.WithCancel(parent)
	} else {
		ctx, cancel = context.WithTimeout(parent, cb.Timeout)
	}

	ctx = context.WithValue(ctx, contextKeyTChannel, params)
	return WrapWithHeaders(ctx, cb.getHeaders()), cancel
}
func (c *CallOptions) overrideHeaders(headers transportHeaders) {
	if c.Format != "" {
		headers[ArgScheme] = c.Format.String()
	}
	if c.ShardKey != "" {
		headers[ShardKey] = c.ShardKey
	}
	if c.RoutingKey != "" {
		headers[RoutingKey] = c.RoutingKey
	}
	if c.RoutingDelegate != "" {
		headers[RoutingDelegate] = c.RoutingDelegate
	}
	if c.callerName != "" {
		headers[CallerName] = c.callerName
	}
}
func (r ArgReadHelper) Read(bs *[]byte) error {
	return r.read(func() error {
		var err error
		*bs, err = ioutil.ReadAll(r.reader)
		return err
	})
}
func (r ArgReadHelper) ReadJSON(data interface{}) error {
	return r.read(func() error {
		// TChannel allows for 0 length values (not valid JSON), so we use a bufio.Reader
		// to check whether data is of 0 length.
		reader := bufio.NewReader(r.reader)
		if _, err := reader.Peek(1); err == io.EOF {
			// If the data is 0 length, then we don't try to read anything.
			return nil
		} else if err != nil {
			return err
		}

		d := json.NewDecoder(reader)
		return d.Decode(data)
	})
}
func NewArgWriter(writer io.WriteCloser, err error) ArgWriteHelper {
	return ArgWriteHelper{writer, err}
}
func (w ArgWriteHelper) Write(bs []byte) error {
	return w.write(func() error {
		_, err := w.writer.Write(bs)
		return err
	})
}
func (w ArgWriteHelper) WriteJSON(data interface{}) error {
	return w.write(func() error {
		e := json.NewEncoder(w.writer)
		return e.Encode(data)
	})
}
func Register(registrar tchannel.Registrar) {
	handler := func(ctx context.Context, call *tchannel.InboundCall) {
		req, err := thttp.ReadRequest(call)
		if err != nil {
			registrar.Logger().WithFields(
				tchannel.LogField{Key: "err", Value: err.Error()},
			).Warn("Failed to read HTTP request.")
			return
		}

		serveHTTP(req, call.Response())
	}
	registrar.Register(tchannel.HandlerFunc(handler), "_pprof")
}
func (r *relayItems) Count() int {
	r.RLock()
	n := len(r.items) - int(r.tombs)
	r.RUnlock()
	return n
}
func (r *relayItems) Get(id uint32) (relayItem, bool) {
	r.RLock()
	item, ok := r.items[id]
	r.RUnlock()

	return item, ok
}
func (r *relayItems) Add(id uint32, item relayItem) {
	r.Lock()
	r.items[id] = item
	r.Unlock()
}
func (r *relayItems) Entomb(id uint32, deleteAfter time.Duration) (relayItem, bool) {
	r.Lock()
	if r.tombs > _maxRelayTombs {
		r.Unlock()
		r.logger.WithFields(LogField{"id", id}).Warn("Too many tombstones, deleting relay item immediately.")
		return r.Delete(id)
	}
	item, ok := r.items[id]
	if !ok {
		r.Unlock()
		r.logger.WithFields(LogField{"id", id}).Warn("Can't find relay item to entomb.")
		return item, false
	}
	if item.tomb {
		r.Unlock()
		r.logger.WithFields(LogField{"id", id}).Warn("Re-entombing a tombstone.")
		return item, false
	}
	r.tombs++
	item.tomb = true
	r.items[id] = item
	r.Unlock()

	// TODO: We should be clearing these out in batches, rather than creating
	// individual timers for each item.
	time.AfterFunc(deleteAfter, func() { r.Delete(id) })
	return item, true
}
func NewRelayer(ch *Channel, conn *Connection) *Relayer {
	r := &Relayer{
		relayHost:    ch.RelayHost(),
		maxTimeout:   ch.relayMaxTimeout,
		localHandler: ch.relayLocal,
		outbound:     newRelayItems(conn.log.WithFields(LogField{"relayItems", "outbound"})),
		inbound:      newRelayItems(conn.log.WithFields(LogField{"relayItems", "inbound"})),
		peers:        ch.RootPeers(),
		conn:         conn,
		relayConn: &relay.Conn{
			RemoteAddr:        conn.conn.RemoteAddr().String(),
			RemoteProcessName: conn.RemotePeerInfo().ProcessName,
			IsOutbound:        conn.connDirection == outbound,
		},
		logger: conn.log,
	}
	r.timeouts = newRelayTimerPool(r.timeoutRelayItem, ch.relayTimerVerify)
	return r
}
func (r *Relayer) Relay(f *Frame) error {
	if f.messageType() != messageTypeCallReq {
		err := r.handleNonCallReq(f)
		if err == errUnknownID {
			// This ID may be owned by an outgoing call, so check the outbound
			// message exchange, and if it succeeds, then the frame has been
			// handled successfully.
			if err := r.conn.outbound.forwardPeerFrame(f); err == nil {
				return nil
			}
		}
		return err
	}
	return r.handleCallReq(newLazyCallReq(f))
}
func (r *Relayer) Receive(f *Frame, fType frameType) (sent bool, failureReason string) {
	id := f.Header.ID

	// If we receive a response frame, we expect to find that ID in our outbound.
	// If we receive a request frame, we expect to find that ID in our inbound.
	items := r.receiverItems(fType)

	item, ok := items.Get(id)
	if !ok {
		r.logger.WithFields(
			LogField{"id", id},
		).Warn("Received a frame without a RelayItem.")
		return false, _relayErrorNotFound
	}

	finished := finishesCall(f)
	if item.tomb {
		// Call timed out, ignore this frame. (We've already handled stats.)
		// TODO: metrics for late-arriving frames.
		return true, ""
	}

	// call res frames don't include the OK bit, so we can't wait until the last
	// frame of a relayed RPC to determine if the call succeeded.
	if fType == responseFrame {
		// If we've gotten a response frame, we're the originating relayer and
		// should handle stats.
		if succeeded, failMsg := determinesCallSuccess(f); succeeded {
			item.call.Succeeded()
		} else if len(failMsg) > 0 {
			item.call.Failed(failMsg)
		}
	}
	select {
	case r.conn.sendCh <- f:
	default:
		// Buffer is full, so drop this frame and cancel the call.
		r.logger.WithFields(
			LogField{"id", id},
		).Warn("Dropping call due to slow connection.")

		items := r.receiverItems(fType)

		err := _relayErrorDestConnSlow
		// If we're dealing with a response frame, then the client is slow.
		if fType == responseFrame {
			err = _relayErrorSourceConnSlow
		}

		r.failRelayItem(items, id, err)
		return false, err
	}

	if finished {
		r.finishRelayItem(items, id)
	}

	return true, ""
}
func (r *Relayer) handleNonCallReq(f *Frame) error {
	frameType := frameTypeFor(f)
	finished := finishesCall(f)

	// If we read a request frame, we need to use the outbound map to decide
	// the destination. Otherwise, we use the inbound map.
	items := r.outbound
	if frameType == responseFrame {
		items = r.inbound
	}

	item, ok := items.Get(f.Header.ID)
	if !ok {
		return errUnknownID
	}
	if item.tomb {
		// Call timed out, ignore this frame. (We've already handled stats.)
		// TODO: metrics for late-arriving frames.
		return nil
	}

	originalID := f.Header.ID
	f.Header.ID = item.remapID

	sent, failure := item.destination.Receive(f, frameType)
	if !sent {
		r.failRelayItem(items, originalID, failure)
		return nil
	}

	if finished {
		r.finishRelayItem(items, originalID)
	}
	return nil
}
func (r *Relayer) addRelayItem(isOriginator bool, id, remapID uint32, destination *Relayer, ttl time.Duration, span Span, call RelayCall) relayItem {
	item := relayItem{
		call:        call,
		remapID:     remapID,
		destination: destination,
		span:        span,
	}

	items := r.inbound
	if isOriginator {
		items = r.outbound
	}
	item.timeout = r.timeouts.Get()
	items.Add(id, item)
	item.timeout.Start(ttl, items, id, isOriginator)
	return item
}
func (r *Relayer) failRelayItem(items *relayItems, id uint32, failure string) {
	item, ok := items.Get(id)
	if !ok {
		items.logger.WithFields(LogField{"id", id}).Warn("Attempted to fail non-existent relay item.")
		return
	}

	// The call could time-out right as we entomb it, which would cause spurious
	// error logs, so ensure we can stop the timeout.
	if !item.timeout.Stop() {
		return
	}

	// Entomb it so that we don't get unknown exchange errors on further frames
	// for this call.
	item, ok = items.Entomb(id, _relayTombTTL)
	if !ok {
		return
	}
	if item.call != nil {
		// If the client is too slow, then there's no point sending an error frame.
		if failure != _relayErrorSourceConnSlow {
			r.conn.SendSystemError(id, item.span, errFrameNotSent)
		}
		item.call.Failed(failure)
		item.call.End()
	}

	r.decrementPending()
}
func WriteStruct(writer io.Writer, s thrift.TStruct) error {
	wp := getProtocolWriter(writer)
	err := s.Write(wp.protocol)
	thriftProtocolPool.Put(wp)
	return err
}
func ReadStruct(reader io.Reader, s thrift.TStruct) error {
	wp := getProtocolReader(reader)
	err := s.Read(wp.protocol)
	thriftProtocolPool.Put(wp)
	return err
}
func EnsureEmpty(r io.Reader, stage string) error {
	buf := _bufPool.Get().(*[]byte)
	defer _bufPool.Put(buf)

	n, err := r.Read(*buf)
	if n > 0 {
		return fmt.Errorf("found unexpected bytes after %s, found (upto 128 bytes): %x", stage, (*buf)[:n])
	}
	if err == io.EOF {
		return nil
	}
	return err
}
func NewServer(optFns ...Option) Server {
	opts := getOptions(optFns)
	if opts.external {
		return newExternalServer(opts)
	}

	ch, err := tchannel.NewChannel(opts.svcName, &tchannel.ChannelOptions{
		Logger: tchannel.NewLevelLogger(tchannel.NewLogger(os.Stderr), tchannel.LogLevelWarn),
	})
	if err != nil {
		panic("failed to create channel: " + err.Error())
	}
	if err := ch.ListenAndServe("127.0.0.1:0"); err != nil {
		panic("failed to listen on port 0: " + err.Error())
	}

	s := &internalServer{
		ch:   ch,
		opts: opts,
	}

	tServer := thrift.NewServer(ch)
	tServer.Register(gen.NewTChanSecondServiceServer(handler{calls: &s.thriftCalls}))
	ch.Register(raw.Wrap(rawHandler{calls: &s.rawCalls}), "echo")

	if len(opts.advertiseHosts) > 0 {
		if err := s.Advertise(opts.advertiseHosts); err != nil {
			panic("failed to advertise: " + err.Error())
		}
	}

	return s
}
func (s *internalServer) Advertise(hyperbahnHosts []string) error {
	config := hyperbahn.Configuration{InitialNodes: hyperbahnHosts}
	hc, err := hyperbahn.NewClient(s.ch, config, nil)
	if err != nil {
		panic("failed to setup Hyperbahn client: " + err.Error())
	}
	return hc.Advertise()
}
func (c *Connection) handleCallReqContinue(frame *Frame) bool {
	if err := c.inbound.forwardPeerFrame(frame); err != nil {
		// If forward fails, it's due to a timeout. We can free this frame.
		return true
	}
	return false
}
func (c *Connection) dispatchInbound(_ uint32, _ uint32, call *InboundCall, frame *Frame) {
	if call.log.Enabled(LogLevelDebug) {
		call.log.Debugf("Received incoming call for %s from %s", call.ServiceName(), c.remotePeerInfo)
	}

	if err := call.readMethod(); err != nil {
		call.log.WithFields(
			LogField{"remotePeer", c.remotePeerInfo},
			ErrField(err),
		).Error("Couldn't read method.")
		c.opts.FramePool.Release(frame)
		return
	}

	call.commonStatsTags["endpoint"] = call.methodString
	call.statsReporter.IncCounter("inbound.calls.recvd", call.commonStatsTags, 1)
	if span := call.response.span; span != nil {
		span.SetOperationName(call.methodString)
	}

	// TODO(prashant): This is an expensive way to check for cancellation. Use a heap for timeouts.
	go func() {
		select {
		case <-call.mex.ctx.Done():
			// checking if message exchange timedout or was cancelled
			// only two possible errors at this step:
			// context.DeadlineExceeded
			// context.Canceled
			if call.mex.ctx.Err() != nil {
				call.mex.inboundExpired()
			}
		case <-call.mex.errCh.c:
			if c.log.Enabled(LogLevelDebug) {
				call.log.Debugf("Wait for timeout/cancellation interrupted by error: %v", call.mex.errCh.err)
			}
			// when an exchange errors out, mark the exchange as expired
			// and call cancel so the server handler's context is canceled
			// TODO: move the cancel to the parent context at connnection level
			call.response.cancel()
			call.mex.inboundExpired()
		}
	}()

	c.handler.Handle(call.mex.ctx, call)
}
func (call *InboundCall) CallOptions() *CallOptions {
	return &CallOptions{
		callerName:      call.CallerName(),
		Format:          call.Format(),
		ShardKey:        call.ShardKey(),
		RoutingDelegate: call.RoutingDelegate(),
		RoutingKey:      call.RoutingKey(),
	}
}
func (call *InboundCall) Response() *InboundCallResponse {
	if call.err != nil {
		// While reading Thrift, we cannot distinguish between malformed Thrift and other errors,
		// and so we may try to respond with a bad request. We should ensure that the response
		// is marked as failed if the request has failed so that we don't try to shutdown the exchange
		// a second time.
		call.response.err = call.err
	}
	return call.response
}
func (response *InboundCallResponse) SendSystemError(err error) error {
	if response.err != nil {
		return response.err
	}
	// Fail all future attempts to read fragments
	response.state = reqResWriterComplete
	response.systemError = true
	response.doneSending()
	response.call.releasePreviousFragment()

	span := CurrentSpan(response.mex.ctx)

	return response.conn.SendSystemError(response.mex.msgID, *span, err)
}
func (response *InboundCallResponse) SetApplicationError() error {
	if response.state > reqResWriterPreArg2 {
		return response.failed(errReqResWriterStateMismatch{
			state:         response.state,
			expectedState: reqResWriterPreArg2,
		})
	}
	response.applicationError = true
	return nil
}
func (response *InboundCallResponse) Arg2Writer() (ArgWriter, error) {
	if err := NewArgWriter(response.arg1Writer()).Write(nil); err != nil {
		return nil, err
	}
	return response.arg2Writer()
}
func (response *InboundCallResponse) doneSending() {
	// TODO(prashant): Move this to when the message is actually being sent.
	now := response.timeNow()

	if span := response.span; span != nil {
		if response.applicationError || response.systemError {
			ext.Error.Set(span, true)
		}
		span.FinishWithOptions(opentracing.FinishOptions{FinishTime: now})
	}

	latency := now.Sub(response.calledAt)
	response.statsReporter.RecordTimer("inbound.calls.latency", response.commonStatsTags, latency)

	if response.systemError {
		// TODO(prashant): Report the error code type as per metrics doc and enable.
		// response.statsReporter.IncCounter("inbound.calls.system-errors", response.commonStatsTags, 1)
	} else if response.applicationError {
		response.statsReporter.IncCounter("inbound.calls.app-errors", response.commonStatsTags, 1)
	} else {
		response.statsReporter.IncCounter("inbound.calls.success", response.commonStatsTags, 1)
	}

	// Cancel the context since the response is complete.
	response.cancel()

	// The message exchange is still open if there are no errors, call shutdown.
	if response.err == nil {
		response.mex.shutdown()
	}
}
func newState(v *parser.Thrift, all map[string]parseState) *State {
	typedefs := make(map[string]*parser.Type)
	for k, v := range v.Typedefs {
		typedefs[k] = v.Type
	}

	// Enums are typedefs to an int64.
	i64Type := &parser.Type{Name: "i64"}
	for k := range v.Enums {
		typedefs[k] = i64Type
	}

	return &State{typedefs, nil, all}
}
func (s *State) rootType(thriftType *parser.Type) *parser.Type {
	if state, newType, include := s.checkInclude(thriftType); include != nil {
		return state.rootType(newType)
	}

	if v, ok := s.typedefs[thriftType.Name]; ok {
		return s.rootType(v)
	}
	return thriftType
}
func (s *State) checkInclude(thriftType *parser.Type) (*State, *parser.Type, *Include) {
	parts := strings.SplitN(thriftType.Name, ".", 2)
	if len(parts) < 2 {
		return nil, nil, nil
	}

	newType := *thriftType
	newType.Name = parts[1]

	include := s.includes[parts[0]]
	state := s.all[include.file]
	return state.global, &newType, include
}
func (s *State) isResultPointer(thriftType *parser.Type) bool {
	_, basicGoType := thriftToGo[s.rootType(thriftType).Name]
	return !basicGoType
}
func (s *State) goType(thriftType *parser.Type) string {
	return s.goTypePrefix("", thriftType)
}
func (s *State) goTypePrefix(prefix string, thriftType *parser.Type) string {
	switch thriftType.Name {
	case "binary":
		return "[]byte"
	case "list":
		return "[]" + s.goType(thriftType.ValueType)
	case "set":
		return "map[" + s.goType(thriftType.ValueType) + "]bool"
	case "map":
		return "map[" + s.goType(thriftType.KeyType) + "]" + s.goType(thriftType.ValueType)
	}

	// If the type is imported, then ignore the package.
	if state, newType, include := s.checkInclude(thriftType); include != nil {
		return state.goTypePrefix(include.Package()+".", newType)
	}

	// If the type is a direct Go type, use that.
	if goType, ok := thriftToGo[thriftType.Name]; ok {
		return goType
	}

	goThriftName := goPublicFieldName(thriftType.Name)
	goThriftName = prefix + goThriftName

	// Check if the type has a typedef to the direct Go type.
	rootType := s.rootType(thriftType)
	if _, ok := thriftToGo[rootType.Name]; ok {
		return goThriftName
	}
	if rootType.Name == "list" ||
		rootType.Name == "set" ||
		rootType.Name == "map" {
		return goThriftName
	}

	// If it's a typedef to another struct, then the typedef is defined as a pointer
	// so we do not want the pointer type here.
	if rootType != thriftType {
		return goThriftName
	}

	// If it's not a typedef for a basic type, we use a pointer.
	return "*" + goThriftName
}
func NewContext(timeout time.Duration) (context.Context, context.CancelFunc) {
	return NewContextBuilder(timeout).Build()
}
func newIncomingContext(call IncomingCall, timeout time.Duration) (context.Context, context.CancelFunc) {
	return NewContextBuilder(timeout).
		setIncomingCall(call).
		Build()
}
func CurrentCall(ctx context.Context) IncomingCall {
	if params := getTChannelParams(ctx); params != nil {
		return params.call
	}
	return nil
}
func New(seed int64) *rand.Rand {
	return rand.New(&lockedSource{src: rand.NewSource(seed)})
}
func (h *metaHandler) Health(ctx Context, req *meta.HealthRequest) (*meta.HealthStatus, error) {
	ok, message := h.healthFn(ctx, metaReqToReq(req))
	if message == "" {
		return &meta.HealthStatus{Ok: ok}, nil
	}
	return &meta.HealthStatus{Ok: ok, Message: &message}, nil
}
func (c headerCtx) Headers() map[string]string {
	if h := c.headers(); h != nil {
		return h.reqHeaders
	}
	return nil
}
func (c headerCtx) ResponseHeaders() map[string]string {
	if h := c.headers(); h != nil {
		return h.respHeaders
	}
	return nil
}
func (c headerCtx) SetResponseHeaders(headers map[string]string) {
	if h := c.headers(); h != nil {
		h.respHeaders = headers
		return
	}
	panic("SetResponseHeaders called on ContextWithHeaders not created via WrapWithHeaders")
}
func (c headerCtx) Child() ContextWithHeaders {
	var headersCopy headersContainer
	if h := c.headers(); h != nil {
		headersCopy = *h
	}

	return Wrap(context.WithValue(c.Context, contextKeyHeaders, &headersCopy))
}
func Wrap(ctx context.Context) ContextWithHeaders {
	hctx := headerCtx{Context: ctx}
	if h := hctx.headers(); h != nil {
		return hctx
	}

	// If there is no header container, we should create an empty one.
	return WrapWithHeaders(ctx, nil)
}
func WrapWithHeaders(ctx context.Context, headers map[string]string) ContextWithHeaders {
	h := &headersContainer{
		reqHeaders: headers,
	}
	newCtx := context.WithValue(ctx, contextKeyHeaders, h)
	return headerCtx{Context: newCtx}
}
func WithoutHeaders(ctx context.Context) context.Context {
	return context.WithValue(context.WithValue(ctx, contextKeyTChannel, nil), contextKeyHeaders, nil)
}
func (e *errNotifier) Notify(err error) error {
	// The code should never try to Notify(nil).
	if err == nil {
		panic("cannot Notify with no error")
	}

	// There may be some sort of race where we try to notify the mex twice.
	if !e.notified.CAS(false, true) {
		return fmt.Errorf("cannot broadcast error: %v, already have: %v", err, e.err)
	}

	e.err = err
	close(e.c)
	return nil
}
func (mex *messageExchange) forwardPeerFrame(frame *Frame) error {
	// We want a very specific priority here:
	// 1. Timeouts/cancellation (mex.ctx errors)
	// 2. Whether recvCh has buffer space (non-blocking select over mex.recvCh)
	// 3. Other mex errors (mex.errCh)
	// Which is why we check the context error only (instead of mex.checkError).
	// In the mex.errCh case, we do a non-blocking write to recvCh to prioritize it.
	if err := mex.ctx.Err(); err != nil {
		return GetContextError(err)
	}

	select {
	case mex.recvCh <- frame:
		return nil
	case <-mex.ctx.Done():
		// Note: One slow reader processing a large request could stall the connection.
		// If we see this, we need to increase the recvCh buffer size.
		return GetContextError(mex.ctx.Err())
	case <-mex.errCh.c:
		// Select will randomly choose a case, but we want to prioritize
		// sending a frame over the errCh. Try a non-blocking write.
		select {
		case mex.recvCh <- frame:
			return nil
		default:
		}
		return mex.errCh.err
	}
}
func (mex *messageExchange) recvPeerFrame() (*Frame, error) {
	// We have to check frames/errors in a very specific order here:
	// 1. Timeouts/cancellation (mex.ctx errors)
	// 2. Any pending frames (non-blocking select over mex.recvCh)
	// 3. Other mex errors (mex.errCh)
	// Which is why we check the context error only (instead of mex.checkError)e
	// In the mex.errCh case, we do a non-blocking read from recvCh to prioritize it.
	if err := mex.ctx.Err(); err != nil {
		return nil, GetContextError(err)
	}

	select {
	case frame := <-mex.recvCh:
		if err := mex.checkFrame(frame); err != nil {
			return nil, err
		}
		return frame, nil
	case <-mex.ctx.Done():
		return nil, GetContextError(mex.ctx.Err())
	case <-mex.errCh.c:
		// Select will randomly choose a case, but we want to prioritize
		// receiving a frame over errCh. Try a non-blocking read.
		select {
		case frame := <-mex.recvCh:
			if err := mex.checkFrame(frame); err != nil {
				return nil, err
			}
			return frame, nil
		default:
		}
		return nil, mex.errCh.err
	}
}
func (mex *messageExchange) recvPeerFrameOfType(msgType messageType) (*Frame, error) {
	frame, err := mex.recvPeerFrame()
	if err != nil {
		return nil, err
	}

	switch frame.Header.messageType {
	case msgType:
		return frame, nil

	case messageTypeError:
		// If we read an error frame, we can release it once we deserialize it.
		defer mex.framePool.Release(frame)

		errMsg := errorMessage{
			id: frame.Header.ID,
		}
		var rbuf typed.ReadBuffer
		rbuf.Wrap(frame.SizedPayload())
		if err := errMsg.read(&rbuf); err != nil {
			return nil, err
		}
		return nil, errMsg

	default:
		// TODO(mmihic): Should be treated as a protocol error
		mex.mexset.log.WithFields(
			LogField{"header", frame.Header},
			LogField{"expectedType", msgType},
			LogField{"expectedID", mex.msgID},
		).Warn("Received unexpected frame.")
		return nil, errUnexpectedFrameType
	}
}
func (mex *messageExchange) shutdown() {
	// The reader and writer side can both hit errors and try to shutdown the mex,
	// so we ensure that it's only shut down once.
	if !mex.shutdownAtomic.CAS(false, true) {
		return
	}

	if mex.errChNotified.CAS(false, true) {
		mex.errCh.Notify(errMexShutdown)
	}

	mex.mexset.removeExchange(mex.msgID)
}
func newMessageExchangeSet(log Logger, name string) *messageExchangeSet {
	return &messageExchangeSet{
		name:             name,
		log:              log.WithFields(LogField{"exchange", name}),
		exchanges:        make(map[uint32]*messageExchange),
		expiredExchanges: make(map[uint32]struct{}),
	}
}
func (mexset *messageExchangeSet) addExchange(mex *messageExchange) error {
	if mexset.shutdown {
		return errMexSetShutdown
	}

	if _, ok := mexset.exchanges[mex.msgID]; ok {
		return errDuplicateMex
	}

	mexset.exchanges[mex.msgID] = mex
	return nil
}
func (mexset *messageExchangeSet) newExchange(ctx context.Context, framePool FramePool,
	msgType messageType, msgID uint32, bufferSize int) (*messageExchange, error) {
	if mexset.log.Enabled(LogLevelDebug) {
		mexset.log.Debugf("Creating new %s message exchange for [%v:%d]", mexset.name, msgType, msgID)
	}

	mex := &messageExchange{
		msgType:   msgType,
		msgID:     msgID,
		ctx:       ctx,
		recvCh:    make(chan *Frame, bufferSize),
		errCh:     newErrNotifier(),
		mexset:    mexset,
		framePool: framePool,
	}

	mexset.Lock()
	addErr := mexset.addExchange(mex)
	mexset.Unlock()

	if addErr != nil {
		logger := mexset.log.WithFields(
			LogField{"msgID", mex.msgID},
			LogField{"msgType", mex.msgType},
			LogField{"exchange", mexset.name},
		)
		if addErr == errMexSetShutdown {
			logger.Warn("Attempted to create new mex after mexset shutdown.")
		} else if addErr == errDuplicateMex {
			logger.Warn("Duplicate msg ID for active and new mex.")
		}

		return nil, addErr
	}

	mexset.onAdded()

	// TODO(mmihic): Put into a deadline ordered heap so we can garbage collected expired exchanges
	return mex, nil
}
func (mexset *messageExchangeSet) deleteExchange(msgID uint32) (found, timedOut bool) {
	if _, found := mexset.exchanges[msgID]; found {
		delete(mexset.exchanges, msgID)
		return true, false
	}

	if _, expired := mexset.expiredExchanges[msgID]; expired {
		delete(mexset.expiredExchanges, msgID)
		return false, true
	}

	return false, false
}
func (mexset *messageExchangeSet) removeExchange(msgID uint32) {
	if mexset.log.Enabled(LogLevelDebug) {
		mexset.log.Debugf("Removing %s message exchange %d", mexset.name, msgID)
	}

	mexset.Lock()
	found, expired := mexset.deleteExchange(msgID)
	mexset.Unlock()

	if !found && !expired {
		mexset.log.WithFields(
			LogField{"msgID", msgID},
		).Error("Tried to remove exchange multiple times")
		return
	}

	// If the message exchange was found, then we perform clean up actions.
	// These clean up actions can only be run once per exchange.
	mexset.onRemoved()
}
func (mexset *messageExchangeSet) expireExchange(msgID uint32) {
	mexset.log.Debugf(
		"Removing %s message exchange %d due to timeout, cancellation or blackhole",
		mexset.name,
		msgID,
	)

	mexset.Lock()
	// TODO(aniketp): explore if cancel can be called everytime we expire an exchange
	found, expired := mexset.deleteExchange(msgID)
	if found || expired {
		// Record in expiredExchanges if we deleted the exchange.
		mexset.expiredExchanges[msgID] = struct{}{}
	}
	mexset.Unlock()

	if expired {
		mexset.log.WithFields(LogField{"msgID", msgID}).Info("Exchange expired already")
	}

	mexset.onRemoved()
}
func (mexset *messageExchangeSet) forwardPeerFrame(frame *Frame) error {
	if mexset.log.Enabled(LogLevelDebug) {
		mexset.log.Debugf("forwarding %s %s", mexset.name, frame.Header)
	}

	mexset.RLock()
	mex := mexset.exchanges[frame.Header.ID]
	mexset.RUnlock()

	if mex == nil {
		// This is ok since the exchange might have expired or been cancelled
		mexset.log.WithFields(
			LogField{"frameHeader", frame.Header.String()},
			LogField{"exchange", mexset.name},
		).Info("Received frame for unknown message exchange.")
		return nil
	}

	if err := mex.forwardPeerFrame(frame); err != nil {
		mexset.log.WithFields(
			LogField{"frameHeader", frame.Header.String()},
			LogField{"frameSize", frame.Header.FrameSize()},
			LogField{"exchange", mexset.name},
			ErrField(err),
		).Info("Failed to forward frame.")
		return err
	}

	return nil
}
func (mexset *messageExchangeSet) copyExchanges() (shutdown bool, exchanges map[uint32]*messageExchange) {
	if mexset.shutdown {
		return true, nil
	}

	exchangesCopy := make(map[uint32]*messageExchange, len(mexset.exchanges))
	for k, mex := range mexset.exchanges {
		exchangesCopy[k] = mex
	}

	return false, exchangesCopy
}
func (mexset *messageExchangeSet) stopExchanges(err error) {
	if mexset.log.Enabled(LogLevelDebug) {
		mexset.log.Debugf("stopping %v exchanges due to error: %v", mexset.count(), err)
	}

	mexset.Lock()
	shutdown, exchanges := mexset.copyExchanges()
	mexset.shutdown = true
	mexset.Unlock()

	if shutdown {
		mexset.log.Debugf("mexset has already been shutdown")
		return
	}

	for _, mex := range exchanges {
		// When there's a connection failure, we want to notify blocked callers that the
		// call will fail, but we don't want to shutdown the exchange as only the
		// arg reader/writer should shutdown the exchange. Otherwise, our guarantee
		// on sendChRefs that there's no references to sendCh is violated since
		// readers/writers could still have a reference to sendCh even though
		// we shutdown the exchange and called Done on sendChRefs.
		if mex.errChNotified.CAS(false, true) {
			mex.errCh.Notify(err)
		}
	}
}
func NewFrame(payloadCapacity int) *Frame {
	f := &Frame{}
	f.buffer = make([]byte, payloadCapacity+FrameHeaderSize)
	f.Payload = f.buffer[FrameHeaderSize:]
	f.headerBuffer = f.buffer[:FrameHeaderSize]
	return f
}
func (f *Frame) ReadBody(header []byte, r io.Reader) error {
	// Copy the header into the underlying buffer so we have an assembled frame
	// that can be directly forwarded.
	copy(f.buffer, header)

	// Parse the header into our typed struct.
	if err := f.Header.read(typed.NewReadBuffer(header)); err != nil {
		return err
	}

	switch payloadSize := f.Header.PayloadSize(); {
	case payloadSize > MaxFramePayloadSize:
		return fmt.Errorf("invalid frame size %v", f.Header.size)
	case payloadSize > 0:
		_, err := io.ReadFull(r, f.SizedPayload())
		return err
	default:
		// No payload to read
		return nil
	}
}
func (f *Frame) WriteOut(w io.Writer) error {
	var wbuf typed.WriteBuffer
	wbuf.Wrap(f.headerBuffer)

	if err := f.Header.write(&wbuf); err != nil {
		return err
	}

	fullFrame := f.buffer[:f.Header.FrameSize()]
	if _, err := w.Write(fullFrame); err != nil {
		return err
	}

	return nil
}
func (r RetryOn) CanRetry(err error) bool {
	if r == RetryNever {
		return false
	}
	if r == RetryDefault {
		r = RetryConnectionError
	}

	code := getErrCode(err)

	if code == ErrCodeBusy || code == ErrCodeDeclined {
		return true
	}
	// Never retry bad requests, since it will probably cause another bad request.
	if code == ErrCodeBadRequest {
		return false
	}

	switch r {
	case RetryConnectionError:
		return code == ErrCodeNetwork
	case RetryUnexpected:
		return code == ErrCodeUnexpected
	case RetryIdempotent:
		return true
	}

	return false
}
func (rs *RequestState) HasRetries(err error) bool {
	if rs == nil {
		return false
	}
	rOpts := rs.retryOpts
	return rs.Attempt < rOpts.MaxAttempts && rOpts.RetryOn.CanRetry(err)
}
func (rs *RequestState) SinceStart(now time.Time, fallback time.Duration) time.Duration {
	if rs == nil {
		return fallback
	}
	return now.Sub(rs.Start)
}
func (rs *RequestState) AddSelectedPeer(hostPort string) {
	if rs == nil {
		return
	}

	host := getHost(hostPort)
	if rs.SelectedPeers == nil {
		rs.SelectedPeers = map[string]struct{}{
			hostPort: {},
			host:     {},
		}
	} else {
		rs.SelectedPeers[hostPort] = struct{}{}
		rs.SelectedPeers[host] = struct{}{}
	}
}
func (ch *Channel) RunWithRetry(runCtx context.Context, f RetriableFunc) error {
	var err error

	opts := getRetryOptions(runCtx)
	rs := ch.getRequestState(opts)
	defer requestStatePool.Put(rs)

	for i := 0; i < opts.MaxAttempts; i++ {
		rs.Attempt++

		if opts.TimeoutPerAttempt == 0 {
			err = f(runCtx, rs)
		} else {
			attemptCtx, cancel := context.WithTimeout(runCtx, opts.TimeoutPerAttempt)
			err = f(attemptCtx, rs)
			cancel()
		}

		if err == nil {
			return nil
		}
		if !opts.RetryOn.CanRetry(err) {
			if ch.log.Enabled(LogLevelInfo) {
				ch.log.WithFields(ErrField(err)).Info("Failed after non-retriable error.")
			}
			return err
		}

		ch.log.WithFields(
			ErrField(err),
			LogField{"attempt", rs.Attempt},
			LogField{"maxAttempts", opts.MaxAttempts},
		).Info("Retrying request after retryable error.")
	}

	// Too many retries, return the last error
	return err
}
func (t ChecksumType) ChecksumSize() int {
	switch t {
	case ChecksumTypeNone:
		return 0
	case ChecksumTypeCrc32, ChecksumTypeCrc32C:
		return crc32.Size
	case ChecksumTypeFarmhash:
		return 4
	default:
		return 0
	}
}
func (t ChecksumType) New() Checksum {
	s := t.pool().Get().(Checksum)
	s.Reset()
	return s
}
func parseTemplates(skipTChannel bool, templateFiles []string) ([]*Template, error) {
	var templates []*Template

	if !skipTChannel {
		templates = append(templates, &Template{
			name:     "tchan",
			template: template.Must(parseTemplate(tchannelTmpl)),
		})
	}

	for _, f := range templateFiles {
		t, err := parseTemplateFile(f)
		if err != nil {
			return nil, err
		}

		templates = append(templates, t)
	}

	return templates, nil
}
func NewStringSliceFlag(name string, usage string) *[]string {
	var ss stringSliceFlag
	flag.Var(&ss, name, usage)
	return (*[]string)(&ss)
}
func (t *Template) withStateFuncs(td TemplateData) *template.Template {
	return t.template.Funcs(map[string]interface{}{
		"goType": td.global.goType,
	})
}
func (ch *Channel) IntrospectOthers(opts *IntrospectionOptions) map[string][]ChannelInfo {
	if !opts.IncludeOtherChannels {
		return nil
	}

	channelMap.Lock()
	defer channelMap.Unlock()

	states := make(map[string][]ChannelInfo)
	for svc, channels := range channelMap.existing {
		channelInfos := make([]ChannelInfo, 0, len(channels))
		for _, otherChan := range channels {
			if ch == otherChan {
				continue
			}
			channelInfos = append(channelInfos, otherChan.ReportInfo(opts))
		}
		states[svc] = channelInfos
	}

	return states
}
func (ch *Channel) ReportInfo(opts *IntrospectionOptions) ChannelInfo {
	return ChannelInfo{
		ID:           ch.chID,
		CreatedStack: ch.createdStack,
		LocalPeer:    ch.PeerInfo(),
	}
}
func (l *RootPeerList) IntrospectState(opts *IntrospectionOptions) map[string]PeerRuntimeState {
	return fromPeerList(l, opts)
}
func (subChMap *subChannelMap) IntrospectState(opts *IntrospectionOptions) map[string]SubChannelRuntimeState {
	m := make(map[string]SubChannelRuntimeState)
	subChMap.RLock()
	for k, sc := range subChMap.subchannels {
		state := SubChannelRuntimeState{
			Service:  k,
			Isolated: sc.Isolated(),
		}
		if state.Isolated {
			state.IsolatedPeers = sc.Peers().IntrospectList(opts)
		}
		if hmap, ok := sc.handler.(*handlerMap); ok {
			state.Handler.Type = methodHandler
			methods := make([]string, 0, len(hmap.handlers))
			for k := range hmap.handlers {
				methods = append(methods, k)
			}
			sort.Strings(methods)
			state.Handler.Methods = methods
		} else {
			state.Handler.Type = overrideHandler
		}
		m[k] = state
	}
	subChMap.RUnlock()
	return m
}
func (p *Peer) IntrospectState(opts *IntrospectionOptions) PeerRuntimeState {
	p.RLock()
	defer p.RUnlock()

	return PeerRuntimeState{
		HostPort:            p.hostPort,
		InboundConnections:  getConnectionRuntimeState(p.inboundConnections, opts),
		OutboundConnections: getConnectionRuntimeState(p.outboundConnections, opts),
		ChosenCount:         p.chosenCount.Load(),
		SCCount:             p.scCount,
	}
}
func (c *Connection) IntrospectState(opts *IntrospectionOptions) ConnectionRuntimeState {
	c.stateMut.RLock()
	defer c.stateMut.RUnlock()

	// TODO(prashantv): Add total number of health checks, and health check options.
	state := ConnectionRuntimeState{
		ID:               c.connID,
		ConnectionState:  c.state.String(),
		LocalHostPort:    c.conn.LocalAddr().String(),
		RemoteHostPort:   c.conn.RemoteAddr().String(),
		OutboundHostPort: c.outboundHP,
		RemotePeer:       c.remotePeerInfo,
		InboundExchange:  c.inbound.IntrospectState(opts),
		OutboundExchange: c.outbound.IntrospectState(opts),
		HealthChecks:     c.healthCheckHistory.asBools(),
		LastActivity:     c.lastActivity.Load(),
	}
	if c.relay != nil {
		state.Relayer = c.relay.IntrospectState(opts)
	}
	return state
}
func (r *Relayer) IntrospectState(opts *IntrospectionOptions) RelayerRuntimeState {
	count := r.inbound.Count() + r.outbound.Count()
	return RelayerRuntimeState{
		Count:         count,
		InboundItems:  r.inbound.IntrospectState(opts, "inbound"),
		OutboundItems: r.outbound.IntrospectState(opts, "outbound"),
		MaxTimeout:    r.maxTimeout,
	}
}
func (ri *relayItems) IntrospectState(opts *IntrospectionOptions, name string) RelayItemSetState {
	ri.RLock()
	defer ri.RUnlock()

	setState := RelayItemSetState{
		Name:  name,
		Count: ri.Count(),
	}
	if opts.IncludeExchanges {
		setState.Items = make(map[string]RelayItemState, len(ri.items))
		for k, v := range ri.items {
			if !opts.IncludeTombstones && v.tomb {
				continue
			}
			state := RelayItemState{
				ID:                      k,
				RemapID:                 v.remapID,
				DestinationConnectionID: v.destination.conn.connID,
				Tomb:                    v.tomb,
			}
			setState.Items[strconv.Itoa(int(k))] = state
		}
	}

	return setState
}
func (mexset *messageExchangeSet) IntrospectState(opts *IntrospectionOptions) ExchangeSetRuntimeState {
	mexset.RLock()
	setState := ExchangeSetRuntimeState{
		Name:  mexset.name,
		Count: len(mexset.exchanges),
	}

	if opts.IncludeExchanges {
		setState.Exchanges = make(map[string]ExchangeRuntimeState, len(mexset.exchanges))
		for k, v := range mexset.exchanges {
			state := ExchangeRuntimeState{
				ID:          k,
				MessageType: v.msgType,
			}
			setState.Exchanges[strconv.Itoa(int(k))] = state
		}
	}

	mexset.RUnlock()
	return setState
}
func NewContext(timeout time.Duration) (Context, context.CancelFunc) {
	ctx, cancel := tchannel.NewContext(timeout)
	return tchannel.WrapWithHeaders(ctx, nil), cancel
}
func WriteResponse(response *tchannel.InboundCallResponse, resp *Res) error {
	if resp.SystemErr != nil {
		return response.SendSystemError(resp.SystemErr)
	}
	if resp.IsErr {
		if err := response.SetApplicationError(); err != nil {
			return err
		}
	}
	if err := tchannel.NewArgWriter(response.Arg2Writer()).Write(resp.Arg2); err != nil {
		return err
	}
	return tchannel.NewArgWriter(response.Arg3Writer()).Write(resp.Arg3)
}
func Wrap(handler Handler) tchannel.Handler {
	return tchannel.HandlerFunc(func(ctx context.Context, call *tchannel.InboundCall) {
		args, err := ReadArgs(call)
		if err != nil {
			handler.OnError(ctx, err)
			return
		}

		resp, err := handler.Handle(ctx, args)
		response := call.Response()
		if err != nil {
			resp = &Res{
				SystemErr: err,
			}
		}
		if err := WriteResponse(response, resp); err != nil {
			handler.OnError(ctx, err)
		}
	})
}
func (s *injectableSpan) initFromOpenTracing(span opentracing.Span) error {
	return span.Tracer().Inject(span.Context(), zipkinSpanFormat, s)
}
func (c *Connection) startOutboundSpan(ctx context.Context, serviceName, methodName string, call *OutboundCall, startTime time.Time) opentracing.Span {
	var parent opentracing.SpanContext // ok to be nil
	if s := opentracing.SpanFromContext(ctx); s != nil {
		parent = s.Context()
	}
	span := c.Tracer().StartSpan(
		methodName,
		opentracing.ChildOf(parent),
		opentracing.StartTime(startTime),
	)
	if isTracingDisabled(ctx) {
		ext.SamplingPriority.Set(span, 0)
	}
	ext.SpanKindRPCClient.Set(span)
	ext.PeerService.Set(span, serviceName)
	c.setPeerHostPort(span)
	span.SetTag("as", call.callReq.Headers[ArgScheme])
	var injectable injectableSpan
	if err := injectable.initFromOpenTracing(span); err == nil {
		call.callReq.Tracing = Span(injectable)
	} else {
		call.callReq.Tracing.initRandom()
	}
	return span
}
func intToIP4(ip uint32) net.IP {
	return net.IP{
		byte(ip >> 24 & 0xff),
		byte(ip >> 16 & 0xff),
		byte(ip >> 8 & 0xff),
		byte(ip & 0xff),
	}
}
func servicePeerToHostPort(peer *hyperbahn.ServicePeer) string {
	host := intToIP4(uint32(*peer.IP.Ipv4)).String()
	port := strconv.Itoa(int(peer.Port))
	return net.JoinHostPort(host, port)
}
func NewStatsdReporter(addr, prefix string) (tchannel.StatsReporter, error) {
	client, err := statsd.NewBufferedClient(addr, prefix, time.Second, 0)
	if err != nil {
		return nil, err
	}

	return NewStatsdReporterClient(client), nil
}
func (r *ToS) UnmarshalText(data []byte) error {
	if v, ok := _tosNameToValue[string(data)]; ok {
		*r = v
		return nil
	}

	return fmt.Errorf("invalid ToS %q", string(data))
}
func (ph *peerHeap) Push(x interface{}) {
	n := len(ph.peerScores)
	item := x.(*peerScore)
	item.index = n
	ph.peerScores = append(ph.peerScores, item)
}
func (ph *peerHeap) Pop() interface{} {
	old := *ph
	n := len(old.peerScores)
	item := old.peerScores[n-1]
	item.index = -1 // for safety
	ph.peerScores = old.peerScores[:n-1]
	return item
}
func (ph *peerHeap) updatePeer(peerScore *peerScore) {
	heap.Fix(ph, peerScore.index)
}
func (ph *peerHeap) removePeer(peerScore *peerScore) {
	heap.Remove(ph, peerScore.index)
}
func (ph *peerHeap) pushPeer(peerScore *peerScore) {
	ph.order++
	newOrder := ph.order
	// randRange will affect the deviation of peer's chosenCount
	randRange := ph.Len()/2 + 1
	peerScore.order = newOrder + uint64(ph.rng.Intn(randRange))
	heap.Push(ph, peerScore)
}
func (ph *peerHeap) addPeer(peerScore *peerScore) {
	ph.pushPeer(peerScore)

	// Pick a random element, and swap the order with that peerScore.
	r := ph.rng.Intn(ph.Len())
	ph.swapOrder(peerScore.index, r)
}
func NewClient(ch *tchannel.Channel, serviceName string, opts *ClientOptions) TChanClient {
	client := &client{
		ch:          ch,
		sc:          ch.GetSubChannel(serviceName),
		serviceName: serviceName,
	}
	if opts != nil {
		client.opts = *opts
	}
	return client
}
func (l *RootPeerList) Add(hostPort string) *Peer {
	l.RLock()

	if p, ok := l.peersByHostPort[hostPort]; ok {
		l.RUnlock()
		return p
	}

	l.RUnlock()
	l.Lock()
	defer l.Unlock()

	if p, ok := l.peersByHostPort[hostPort]; ok {
		return p
	}

	var p *Peer
	// To avoid duplicate connections, only the root list should create new
	// peers. All other lists should keep refs to the root list's peers.
	p = newPeer(l.channel, hostPort, l.onPeerStatusChanged, l.onClosedConnRemoved)
	l.peersByHostPort[hostPort] = p
	return p
}
func (l *RootPeerList) Get(hostPort string) (*Peer, bool) {
	l.RLock()
	p, ok := l.peersByHostPort[hostPort]
	l.RUnlock()
	return p, ok
}
func WithTimeout(timeout time.Duration) Option {
	return func(opts *options) {
		opts.timeout = timeout
	}
}
func (s *Service) Methods() []*Method {
	if s.methods != nil {
		return s.methods
	}

	for _, m := range s.Service.Methods {
		s.methods = append(s.methods, &Method{m, s, s.state})
	}
	sort.Sort(byMethodName(s.methods))
	return s.methods
}
func (s *Service) InheritedMethods() []string {
	if s.inheritedMethods != nil {
		return s.inheritedMethods
	}

	for svc := s.ExtendsService; svc != nil; svc = svc.ExtendsService {
		for m := range svc.Service.Methods {
			s.inheritedMethods = append(s.inheritedMethods, m)
		}
	}
	sort.Strings(s.inheritedMethods)

	return s.inheritedMethods
}
func (m *Method) Arguments() []*Field {
	var args []*Field
	for _, f := range m.Method.Arguments {
		args = append(args, &Field{f, m.state})
	}
	return args
}
func (m *Method) ArgList() string {
	args := []string{"ctx " + contextType()}
	for _, arg := range m.Arguments() {
		args = append(args, arg.Declaration())
	}
	return strings.Join(args, ", ")
}
func (m *Method) CallList(reqStruct string) string {
	args := []string{"ctx"}
	for _, arg := range m.Arguments() {
		args = append(args, reqStruct+"."+arg.ArgStructName())
	}
	return strings.Join(args, ", ")
}
func (m *Method) RetType() string {
	if !m.HasReturn() {
		return "error"
	}
	return fmt.Sprintf("(%v, %v)", m.state.goType(m.Method.ReturnType), "error")
}
func (m *Method) WrapResult(respVar string) string {
	if !m.HasReturn() {
		panic("cannot wrap a return when there is no return mode")
	}

	if m.state.isResultPointer(m.ReturnType) {
		return respVar
	}
	return "&" + respVar
}
func (m *Method) ReturnWith(respName string, errName string) string {
	if !m.HasReturn() {
		return errName
	}
	return fmt.Sprintf("%v, %v", respName, errName)
}
func (a *Field) Declaration() string {
	return fmt.Sprintf("%s %s", a.Name(), a.ArgType())
}
func startIdleSweep(ch *Channel, opts *ChannelOptions) *idleSweep {
	is := &idleSweep{
		ch:                ch,
		maxIdleTime:       opts.MaxIdleTime,
		idleCheckInterval: opts.IdleCheckInterval,
	}

	is.start()
	return is
}
func (is *idleSweep) start() {
	if is.started || is.idleCheckInterval <= 0 {
		return
	}

	is.ch.log.WithFields(
		LogField{"idleCheckInterval", is.idleCheckInterval},
		LogField{"maxIdleTime", is.maxIdleTime},
	).Info("Starting idle connections poller.")

	is.started = true
	is.stopCh = make(chan struct{})
	go is.pollerLoop()
}
func (is *idleSweep) Stop() {
	if !is.started {
		return
	}

	is.started = false
	is.ch.log.Info("Stopping idle connections poller.")
	close(is.stopCh)
}
func ResolveWithGoPath(filename string) (string, error) {
	for _, file := range goPathCandidates(filename) {
		if _, err := os.Stat(file); !os.IsNotExist(err) {
			return file, nil
		}
	}

	return "", fmt.Errorf("file not found on GOPATH: %q", filename)
}
func setExtends(state map[string]parseState) error {
	for _, v := range state {
		for _, s := range v.services {
			if s.Extends == "" {
				continue
			}

			var searchServices []*Service
			var searchFor string
			parts := strings.SplitN(s.Extends, ".", 2)
			// If it's not imported, then look at the current file's services.
			if len(parts) < 2 {
				searchServices = v.services
				searchFor = s.Extends
			} else {
				include := v.global.includes[parts[0]]
				s.ExtendsPrefix = include.pkg + "."
				searchServices = state[include.file].services
				searchFor = parts[1]
			}

			foundService := sort.Search(len(searchServices), func(i int) bool {
				return searchServices[i].Name >= searchFor
			})
			if foundService == len(searchServices) {
				return fmt.Errorf("failed to find base service %q for %q", s.Extends, s.Name)
			}
			s.ExtendsService = searchServices[foundService]
		}
	}

	return nil
}
func (hmap *handlerMap) register(h Handler, method string) {
	hmap.Lock()
	defer hmap.Unlock()

	if hmap.handlers == nil {
		hmap.handlers = make(map[string]Handler)
	}

	hmap.handlers[method] = h
}
func NewClient(hosts []string, optFns ...Option) Client {
	opts := getOptions(optFns)
	if opts.external {
		return newExternalClient(hosts, opts)
	}
	if opts.numClients > 1 {
		return newInternalMultiClient(hosts, opts)
	}
	return newClient(hosts, opts)
}
func ListenIP() (net.IP, error) {
	interfaces, err := net.Interfaces()
	if err != nil {
		return nil, err
	}
	return listenIP(interfaces)
}
func (s *listener) Close() error {
	if err := s.Listener.Close(); err != nil {
		return err
	}

	s.cond.L.Lock()
	for s.refs > 0 {
		s.cond.Wait()
	}
	s.cond.L.Unlock()
	return nil
}
func ReadArgsV2(r tchannel.ArgReadable) ([]byte, []byte, error) {
	var arg2, arg3 []byte

	if err := tchannel.NewArgReader(r.Arg2Reader()).Read(&arg2); err != nil {
		return nil, nil, err
	}

	if err := tchannel.NewArgReader(r.Arg3Reader()).Read(&arg3); err != nil {
		return nil, nil, err
	}

	return arg2, arg3, nil
}
func WriteArgs(call *tchannel.OutboundCall, arg2, arg3 []byte) ([]byte, []byte, *tchannel.OutboundCallResponse, error) {
	if err := tchannel.NewArgWriter(call.Arg2Writer()).Write(arg2); err != nil {
		return nil, nil, nil, err
	}

	if err := tchannel.NewArgWriter(call.Arg3Writer()).Write(arg3); err != nil {
		return nil, nil, nil, err
	}

	resp := call.Response()
	var respArg2 []byte
	if err := tchannel.NewArgReader(resp.Arg2Reader()).Read(&respArg2); err != nil {
		return nil, nil, nil, err
	}

	var respArg3 []byte
	if err := tchannel.NewArgReader(resp.Arg3Reader()).Read(&respArg3); err != nil {
		return nil, nil, nil, err
	}

	return respArg2, respArg3, resp, nil
}
func Call(ctx context.Context, ch *tchannel.Channel, hostPort string, serviceName, method string,
	arg2, arg3 []byte) ([]byte, []byte, *tchannel.OutboundCallResponse, error) {

	call, err := ch.BeginCall(ctx, hostPort, serviceName, method, nil)
	if err != nil {
		return nil, nil, nil, err
	}

	return WriteArgs(call, arg2, arg3)
}
func CallSC(ctx context.Context, sc *tchannel.SubChannel, method string, arg2, arg3 []byte) (
	[]byte, []byte, *tchannel.OutboundCallResponse, error) {

	call, err := sc.BeginCall(ctx, method, nil)
	if err != nil {
		return nil, nil, nil, err
	}

	return WriteArgs(call, arg2, arg3)
}
func CallV2(ctx context.Context, sc *tchannel.SubChannel, cArgs CArgs) (*CRes, error) {
	call, err := sc.BeginCall(ctx, cArgs.Method, cArgs.CallOptions)
	if err != nil {
		return nil, err
	}

	arg2, arg3, res, err := WriteArgs(call, cArgs.Arg2, cArgs.Arg3)
	if err != nil {
		return nil, err
	}

	return &CRes{
		Arg2:     arg2,
		Arg3:     arg3,
		AppError: res.ApplicationError(),
	}, nil
}
func NewRealRelay(services map[string][]string) (Relay, error) {
	hosts := &fixedHosts{hosts: services}
	ch, err := tchannel.NewChannel("relay", &tchannel.ChannelOptions{
		RelayHost: relaytest.HostFunc(hosts.Get),
		Logger:    tchannel.NewLevelLogger(tchannel.NewLogger(os.Stderr), tchannel.LogLevelWarn),
	})
	if err != nil {
		return nil, err
	}

	if err := ch.ListenAndServe("127.0.0.1:0"); err != nil {
		return nil, err
	}

	return &realRelay{
		ch:    ch,
		hosts: hosts,
	}, nil
}
func NewServer(registrar tchannel.Registrar) *Server {
	metaHandler := newMetaHandler()
	server := &Server{
		ch:          registrar,
		log:         registrar.Logger(),
		handlers:    make(map[string]handler),
		metaHandler: metaHandler,
		ctxFn:       defaultContextFn,
	}
	server.Register(newTChanMetaServer(metaHandler))
	if ch, ok := registrar.(*tchannel.Channel); ok {
		// Register the meta endpoints on the "tchannel" service name.
		NewServer(ch.GetSubChannel("tchannel"))
	}
	return server
}
func (s *Server) RegisterHealthHandler(f HealthFunc) {
	wrapped := func(ctx Context, r HealthRequest) (bool, string) {
		return f(ctx)
	}
	s.metaHandler.setHandler(wrapped)
}
func (s *Server) Handle(ctx context.Context, call *tchannel.InboundCall) {
	op := call.MethodString()
	service, method, ok := getServiceMethod(op)
	if !ok {
		log.Fatalf("Handle got call for %s which does not match the expected call format", op)
	}

	s.RLock()
	handler, ok := s.handlers[service]
	s.RUnlock()
	if !ok {
		log.Fatalf("Handle got call for service %v which is not registered", service)
	}

	if err := s.handle(ctx, handler, method, call); err != nil {
		s.onError(call, err)
	}
}
func (c SystemErrCode) MetricsKey() string {
	switch c {
	case ErrCodeInvalid:
		// Shouldn't ever need this.
		return "invalid"
	case ErrCodeTimeout:
		return "timeout"
	case ErrCodeCancelled:
		return "cancelled"
	case ErrCodeBusy:
		return "busy"
	case ErrCodeDeclined:
		return "declined"
	case ErrCodeUnexpected:
		return "unexpected-error"
	case ErrCodeBadRequest:
		return "bad-request"
	case ErrCodeNetwork:
		return "network-error"
	case ErrCodeProtocol:
		return "protocol-error"
	default:
		return c.String()
	}
}
func NewSystemError(code SystemErrCode, msg string, args ...interface{}) error {
	return SystemError{code: code, msg: fmt.Sprintf(msg, args...)}
}
func NewWrappedSystemError(code SystemErrCode, wrapped error) error {
	if se, ok := wrapped.(SystemError); ok {
		return se
	}

	return SystemError{code: code, msg: fmt.Sprint(wrapped), wrapped: wrapped}
}
func (se SystemError) Error() string {
	return fmt.Sprintf("tchannel error %v: %s", se.Code(), se.msg)
}
func GetContextError(err error) error {
	if err == context.DeadlineExceeded {
		return ErrTimeout
	}
	if err == context.Canceled {
		return ErrRequestCancelled
	}
	return err
}
func GetSystemErrorCode(err error) SystemErrCode {
	if err == nil {
		return ErrCodeInvalid
	}

	if se, ok := err.(SystemError); ok {
		return se.Code()
	}

	return ErrCodeUnexpected
}
func (c *Connection) ping(ctx context.Context) error {
	req := &pingReq{id: c.NextMessageID()}
	mex, err := c.outbound.newExchange(ctx, c.opts.FramePool, req.messageType(), req.ID(), 1)
	if err != nil {
		return c.connectionError("create ping exchange", err)
	}
	defer c.outbound.removeExchange(req.ID())

	if err := c.sendMessage(req); err != nil {
		return c.connectionError("send ping", err)
	}

	return c.recvMessage(ctx, &pingRes{}, mex)
}
func (c *Connection) handlePingRes(frame *Frame) bool {
	if err := c.outbound.forwardPeerFrame(frame); err != nil {
		c.log.WithFields(LogField{"response", frame.Header}).Warn("Unexpected ping response.")
		return true
	}
	// ping req is waiting for this frame, and will release it.
	return false
}
func (c *Connection) handlePingReq(frame *Frame) {
	if state := c.readState(); state != connectionActive {
		c.protocolError(frame.Header.ID, errConnNotActive{"ping on incoming", state})
		return
	}

	pingRes := &pingRes{id: frame.Header.ID}
	if err := c.sendMessage(pingRes); err != nil {
		c.connectionError("send pong", err)
	}
}
func (c *Connection) SendSystemError(id uint32, span Span, err error) error {
	frame := c.opts.FramePool.Get()

	if err := frame.write(&errorMessage{
		id:      id,
		errCode: GetSystemErrorCode(err),
		tracing: span,
		message: GetSystemErrorMessage(err),
	}); err != nil {

		// This shouldn't happen - it means writing the errorMessage is broken.
		c.log.WithFields(
			LogField{"remotePeer", c.remotePeerInfo},
			LogField{"id", id},
			ErrField(err),
		).Warn("Couldn't create outbound frame.")
		return fmt.Errorf("failed to create outbound error frame: %v", err)
	}

	// When sending errors, we hold the state rlock to ensure that sendCh is not closed
	// as we are sending the frame.
	return c.withStateRLock(func() error {
		// Errors cannot be sent if the connection has been closed.
		if c.state == connectionClosed {
			c.log.WithFields(
				LogField{"remotePeer", c.remotePeerInfo},
				LogField{"id", id},
			).Info("Could not send error frame on closed connection.")
			return fmt.Errorf("failed to send error frame, connection state %v", c.state)
		}

		select {
		case c.sendCh <- frame: // Good to go
			return nil
		default: // If the send buffer is full, log and return an error.
		}
		c.log.WithFields(
			LogField{"remotePeer", c.remotePeerInfo},
			LogField{"id", id},
			ErrField(err),
		).Warn("Couldn't send outbound frame.")
		return fmt.Errorf("failed to send error frame, buffer full")
	})
}
func (c *Connection) connectionError(site string, err error) error {
	var closeLogFields LogFields
	if err == io.EOF {
		closeLogFields = LogFields{{"reason", "network connection EOF"}}
	} else {
		closeLogFields = LogFields{
			{"reason", "connection error"},
			ErrField(err),
		}
	}

	c.stopHealthCheck()
	err = c.logConnectionError(site, err)
	c.close(closeLogFields...)

	// On any connection error, notify the exchanges of this error.
	if c.stoppedExchanges.CAS(false, true) {
		c.outbound.stopExchanges(err)
		c.inbound.stopExchanges(err)
	}

	// checkExchanges will close the connection due to stoppedExchanges.
	c.checkExchanges()
	return err
}
func (c *Connection) withStateLock(f func() error) error {
	c.stateMut.Lock()
	err := f()
	c.stateMut.Unlock()

	return err
}
func (c *Connection) withStateRLock(f func() error) error {
	c.stateMut.RLock()
	err := f()
	c.stateMut.RUnlock()

	return err
}
func (c *Connection) readFrames(_ uint32) {
	headerBuf := make([]byte, FrameHeaderSize)

	handleErr := func(err error) {
		if !c.closeNetworkCalled.Load() {
			c.connectionError("read frames", err)
		} else {
			c.log.Debugf("Ignoring error after connection was closed: %v", err)
		}
	}

	for {
		// Read the header, avoid allocating the frame till we know the size
		// we need to allocate.
		if _, err := io.ReadFull(c.conn, headerBuf); err != nil {
			handleErr(err)
			return
		}

		frame := c.opts.FramePool.Get()
		if err := frame.ReadBody(headerBuf, c.conn); err != nil {
			handleErr(err)
			c.opts.FramePool.Release(frame)
			return
		}

		c.updateLastActivity(frame)

		var releaseFrame bool
		if c.relay == nil {
			releaseFrame = c.handleFrameNoRelay(frame)
		} else {
			releaseFrame = c.handleFrameRelay(frame)
		}
		if releaseFrame {
			c.opts.FramePool.Release(frame)
		}
	}
}
func (c *Connection) writeFrames(_ uint32) {
	for {
		select {
		case f := <-c.sendCh:
			if c.log.Enabled(LogLevelDebug) {
				c.log.Debugf("Writing frame %s", f.Header)
			}

			c.updateLastActivity(f)
			err := f.WriteOut(c.conn)
			c.opts.FramePool.Release(f)
			if err != nil {
				c.connectionError("write frames", err)
				return
			}
		case <-c.stopCh:
			// If there are frames in sendCh, we want to drain them.
			if len(c.sendCh) > 0 {
				continue
			}
			// Close the network once we're no longer writing frames.
			c.closeNetwork()
			return
		}
	}
}
func (c *Connection) hasPendingCalls() bool {
	if c.inbound.count() > 0 || c.outbound.count() > 0 {
		return true
	}
	if !c.relay.canClose() {
		return true
	}

	return false
}
func (c *Connection) checkExchanges() {
	c.callOnExchangeChange()

	moveState := func(fromState, toState connectionState) bool {
		err := c.withStateLock(func() error {
			if c.state != fromState {
				return errors.New("")
			}
			c.state = toState
			return nil
		})
		return err == nil
	}

	curState := c.readState()
	origState := curState

	if curState != connectionClosed && c.stoppedExchanges.Load() {
		if moveState(curState, connectionClosed) {
			curState = connectionClosed
		}
	}

	if curState == connectionStartClose {
		if !c.relay.canClose() {
			return
		}
		if c.inbound.count() == 0 && moveState(connectionStartClose, connectionInboundClosed) {
			curState = connectionInboundClosed
		}
	}

	if curState == connectionInboundClosed {
		// Safety check -- this should never happen since we already did the check
		// when transitioning to connectionInboundClosed.
		if !c.relay.canClose() {
			c.relay.logger.Error("Relay can't close even though state is InboundClosed.")
			return
		}

		if c.outbound.count() == 0 && moveState(connectionInboundClosed, connectionClosed) {
			curState = connectionClosed
		}
	}

	if curState != origState {
		// If the connection is closed, we can notify writeFrames to stop which
		// closes the underlying network connection. We never close sendCh to avoid
		// races causing panics, see 93ef5c112c8b321367ae52d2bd79396e2e874f31
		if curState == connectionClosed {
			close(c.stopCh)
		}

		c.log.WithFields(
			LogField{"newState", curState},
		).Debug("Connection state updated during shutdown.")
		c.callOnCloseStateChange()
	}
}
func (c *Connection) closeNetwork() {
	// NB(mmihic): The sender goroutine will exit once the connection is
	// closed; no need to close the send channel (and closing the send
	// channel would be dangerous since other goroutine might be sending)
	c.log.Debugf("Closing underlying network connection")
	c.stopHealthCheck()
	c.closeNetworkCalled.Store(true)
	if err := c.conn.Close(); err != nil {
		c.log.WithFields(
			LogField{"remotePeer", c.remotePeerInfo},
			ErrField(err),
		).Warn("Couldn't close connection to peer.")
	}
}
func (c *Connection) getLastActivityTime() time.Time {
	return time.Unix(0, c.lastActivity.Load())
}
func Validate(svc *parser.Service) error {
	for _, m := range svc.Methods {
		if err := validateMethod(svc, m); err != nil {
			return err
		}
	}
	return nil
}
func (c *Client) logFailedRegistrationRetry(errLogger tchannel.Logger, consecutiveFailures uint) {
	logFn := errLogger.Info
	if consecutiveFailures > maxAdvertiseFailures {
		logFn = errLogger.Warn
	}

	logFn("Hyperbahn client registration failed, will retry.")
}
func (c *Client) initialAdvertise() error {
	var err error
	for attempt := uint(0); attempt < maxAdvertiseFailures; attempt++ {
		err = c.sendAdvertise()
		if err == nil || err == errEphemeralPeer {
			break
		}

		c.tchan.Logger().WithFields(tchannel.ErrField(err)).Info(
			"Hyperbahn client initial registration failure, will retry")

		// Back off for a while.
		sleepFor := fuzzInterval(advertiseRetryInterval * time.Duration(1<<attempt))
		c.sleep(sleepFor)
	}
	return err
}
func (f lazyCallReq) Service() []byte {
	l := f.Payload[_serviceLenIndex]
	return f.Payload[_serviceNameIndex : _serviceNameIndex+l]
}
func (f lazyCallReq) TTL() time.Duration {
	ttl := binary.BigEndian.Uint32(f.Payload[_ttlIndex : _ttlIndex+_ttlLen])
	return time.Duration(ttl) * time.Millisecond
}
func (f lazyCallReq) SetTTL(d time.Duration) {
	ttl := uint32(d / time.Millisecond)
	binary.BigEndian.PutUint32(f.Payload[_ttlIndex:_ttlIndex+_ttlLen], ttl)
}
func finishesCall(f *Frame) bool {
	switch f.messageType() {
	case messageTypeError:
		return true
	case messageTypeCallRes, messageTypeCallResContinue:
		flags := f.Payload[_flagsIndex]
		return flags&hasMoreFragmentsFlag == 0
	default:
		return false
	}
}
func (ps *PlatformStrings) Flat() []string {
	unique := make(map[string]struct{})
	for _, s := range ps.Generic {
		unique[s] = struct{}{}
	}
	for _, ss := range ps.OS {
		for _, s := range ss {
			unique[s] = struct{}{}
		}
	}
	for _, ss := range ps.Arch {
		for _, s := range ss {
			unique[s] = struct{}{}
		}
	}
	for _, ss := range ps.Platform {
		for _, s := range ss {
			unique[s] = struct{}{}
		}
	}
	flat := make([]string, 0, len(unique))
	for s := range unique {
		flat = append(flat, s)
	}
	sort.Strings(flat)
	return flat
}
func (ps *PlatformStrings) Map(f func(s string) (string, error)) (PlatformStrings, []error) {
	var errors []error
	mapSlice := func(ss []string) ([]string, error) {
		rs := make([]string, 0, len(ss))
		for _, s := range ss {
			if r, err := f(s); err != nil {
				errors = append(errors, err)
			} else if r != "" {
				rs = append(rs, r)
			}
		}
		return rs, nil
	}
	result, _ := ps.MapSlice(mapSlice)
	return result, errors
}
func (ps *PlatformStrings) MapSlice(f func([]string) ([]string, error)) (PlatformStrings, []error) {
	var errors []error

	mapSlice := func(ss []string) []string {
		rs, err := f(ss)
		if err != nil {
			errors = append(errors, err)
			return nil
		}
		return rs
	}

	mapStringMap := func(m map[string][]string) map[string][]string {
		if m == nil {
			return nil
		}
		rm := make(map[string][]string)
		for k, ss := range m {
			ss = mapSlice(ss)
			if len(ss) > 0 {
				rm[k] = ss
			}
		}
		if len(rm) == 0 {
			return nil
		}
		return rm
	}

	mapPlatformMap := func(m map[Platform][]string) map[Platform][]string {
		if m == nil {
			return nil
		}
		rm := make(map[Platform][]string)
		for k, ss := range m {
			ss = mapSlice(ss)
			if len(ss) > 0 {
				rm[k] = ss
			}
		}
		if len(rm) == 0 {
			return nil
		}
		return rm
	}

	result := PlatformStrings{
		Generic:  mapSlice(ps.Generic),
		OS:       mapStringMap(ps.OS),
		Arch:     mapStringMap(ps.Arch),
		Platform: mapPlatformMap(ps.Platform),
	}
	return result, errors
}
func GetProtoConfig(c *config.Config) *ProtoConfig {
	pc := c.Exts[protoName]
	if pc == nil {
		return nil
	}
	return pc.(*ProtoConfig)
}
func MapExprStrings(e bzl.Expr, f func(string) string) bzl.Expr {
	if e == nil {
		return nil
	}
	switch expr := e.(type) {
	case *bzl.StringExpr:
		s := f(expr.Value)
		if s == "" {
			return nil
		}
		ret := *expr
		ret.Value = s
		return &ret

	case *bzl.ListExpr:
		var list []bzl.Expr
		for _, elem := range expr.List {
			elem = MapExprStrings(elem, f)
			if elem != nil {
				list = append(list, elem)
			}
		}
		if len(list) == 0 && len(expr.List) > 0 {
			return nil
		}
		ret := *expr
		ret.List = list
		return &ret

	case *bzl.DictExpr:
		var cases []bzl.Expr
		isEmpty := true
		for _, kv := range expr.List {
			keyval, ok := kv.(*bzl.KeyValueExpr)
			if !ok {
				log.Panicf("unexpected expression in generated imports dict: %#v", kv)
			}
			value := MapExprStrings(keyval.Value, f)
			if value != nil {
				cases = append(cases, &bzl.KeyValueExpr{Key: keyval.Key, Value: value})
				if key, ok := keyval.Key.(*bzl.StringExpr); !ok || key.Value != "//conditions:default" {
					isEmpty = false
				}
			}
		}
		if isEmpty {
			return nil
		}
		ret := *expr
		ret.List = cases
		return &ret

	case *bzl.CallExpr:
		if x, ok := expr.X.(*bzl.Ident); !ok || x.Name != "select" || len(expr.List) != 1 {
			log.Panicf("unexpected call expression in generated imports: %#v", e)
		}
		arg := MapExprStrings(expr.List[0], f)
		if arg == nil {
			return nil
		}
		call := *expr
		call.List[0] = arg
		return &call

	case *bzl.BinaryExpr:
		x := MapExprStrings(expr.X, f)
		y := MapExprStrings(expr.Y, f)
		if x == nil {
			return y
		}
		if y == nil {
			return x
		}
		binop := *expr
		binop.X = x
		binop.Y = y
		return &binop

	default:
		return nil
	}
}
func FlattenExpr(e bzl.Expr) bzl.Expr {
	ps, err := extractPlatformStringsExprs(e)
	if err != nil {
		return e
	}

	ls := makeListSquasher()
	addElem := func(e bzl.Expr) bool {
		s, ok := e.(*bzl.StringExpr)
		if !ok {
			return false
		}
		ls.add(s)
		return true
	}
	addList := func(e bzl.Expr) bool {
		l, ok := e.(*bzl.ListExpr)
		if !ok {
			return false
		}
		for _, elem := range l.List {
			if !addElem(elem) {
				return false
			}
		}
		return true
	}
	addDict := func(d *bzl.DictExpr) bool {
		for _, kv := range d.List {
			if !addList(kv.(*bzl.KeyValueExpr).Value) {
				return false
			}
		}
		return true
	}

	if ps.generic != nil {
		if !addList(ps.generic) {
			return e
		}
	}
	for _, d := range []*bzl.DictExpr{ps.os, ps.arch, ps.platform} {
		if d == nil {
			continue
		}
		if !addDict(d) {
			return e
		}
	}

	return ls.list()
}
func makePlatformStringsExpr(ps platformStringsExprs) bzl.Expr {
	makeSelect := func(dict *bzl.DictExpr) bzl.Expr {
		return &bzl.CallExpr{
			X:    &bzl.Ident{Name: "select"},
			List: []bzl.Expr{dict},
		}
	}
	forceMultiline := func(e bzl.Expr) {
		switch e := e.(type) {
		case *bzl.ListExpr:
			e.ForceMultiLine = true
		case *bzl.CallExpr:
			e.List[0].(*bzl.DictExpr).ForceMultiLine = true
		}
	}

	var parts []bzl.Expr
	if ps.generic != nil {
		parts = append(parts, ps.generic)
	}
	if ps.os != nil {
		parts = append(parts, makeSelect(ps.os))
	}
	if ps.arch != nil {
		parts = append(parts, makeSelect(ps.arch))
	}
	if ps.platform != nil {
		parts = append(parts, makeSelect(ps.platform))
	}

	if len(parts) == 0 {
		return nil
	}
	if len(parts) == 1 {
		return parts[0]
	}
	expr := parts[0]
	forceMultiline(expr)
	for _, part := range parts[1:] {
		forceMultiline(part)
		expr = &bzl.BinaryExpr{
			Op: "+",
			X:  expr,
			Y:  part,
		}
	}
	return expr
}
func (p Platform) String() string {
	switch {
	case p.OS != "" && p.Arch != "":
		return p.OS + "_" + p.Arch
	case p.OS != "":
		return p.OS
	case p.Arch != "":
		return p.Arch
	default:
		return ""
	}
}
func Find(dir string) (string, error) {
	dir, err := filepath.Abs(dir)
	if err != nil {
		return "", err
	}

	for {
		_, err = os.Stat(filepath.Join(dir, workspaceFile))
		if err == nil {
			return dir, nil
		}
		if !os.IsNotExist(err) {
			return "", err
		}
		if strings.HasSuffix(dir, string(os.PathSeparator)) { // stop at root dir
			return "", os.ErrNotExist
		}
		dir = filepath.Dir(dir)
	}
}
func runGazelle(mode mode, dirs []string) error {
	if mode == fastMode && len(dirs) == 0 {
		return nil
	}

	args := []string{os.Getenv("BAZEL_REAL"), "run", *gazelleLabel, "--", "-args"}
	args = append(args, "-index=false")
	if mode == fastMode {
		args = append(args, "-r=false")
		args = append(args, dirs...)
	}

	cmd := exec.Command(args[0], args[1:]...)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	log.Printf("running gazelle: %s\n", strings.Join(cmd.Args, " "))
	return cmd.Run()
}
func restoreBuildFilesInRepo() {
	err := filepath.Walk(".", func(path string, info os.FileInfo, err error) error {
		if err != nil {
			log.Print(err)
			return nil
		}
		restoreBuildFilesInDir(path)
		return nil
	})
	if err != nil {
		log.Print(err)
	}
}
func FixLoads(f *rule.File, knownLoads []rule.LoadInfo) {
	knownFiles := make(map[string]bool)
	knownKinds := make(map[string]string)
	for _, l := range knownLoads {
		knownFiles[l.Name] = true
		for _, k := range l.Symbols {
			knownKinds[k] = l.Name
		}
	}

	// Sync the file. We need File.Loads and File.Rules to contain inserted
	// statements and not deleted statements.
	f.Sync()

	// Scan load statements in the file. Keep track of loads of known files,
	// since these may be changed. Keep track of symbols loaded from unknown
	// files; we will not add loads for these.
	var loads []*rule.Load
	otherLoadedKinds := make(map[string]bool)
	for _, l := range f.Loads {
		if knownFiles[l.Name()] {
			loads = append(loads, l)
			continue
		}
		for _, sym := range l.Symbols() {
			otherLoadedKinds[sym] = true
		}
	}

	// Make a map of all the symbols from known files used in this file.
	usedKinds := make(map[string]map[string]bool)
	for _, r := range f.Rules {
		kind := r.Kind()
		if file, ok := knownKinds[kind]; ok && !otherLoadedKinds[kind] {
			if usedKinds[file] == nil {
				usedKinds[file] = make(map[string]bool)
			}
			usedKinds[file][kind] = true
		}
	}

	// Fix the load statements. The order is important, so we iterate over
	// knownLoads instead of knownFiles.
	for _, known := range knownLoads {
		file := known.Name
		first := true
		for _, l := range loads {
			if l.Name() != file {
				continue
			}
			if first {
				fixLoad(l, file, usedKinds[file], knownKinds)
				first = false
			} else {
				fixLoad(l, file, nil, knownKinds)
			}
			if l.IsEmpty() {
				l.Delete()
			}
		}
		if first {
			load := fixLoad(nil, file, usedKinds[file], knownKinds)
			if load != nil {
				index := newLoadIndex(f, known.After)
				load.Insert(f, index)
			}
		}
	}
}
func fixLoad(load *rule.Load, file string, kinds map[string]bool, knownKinds map[string]string) *rule.Load {
	if load == nil {
		if len(kinds) == 0 {
			return nil
		}
		load = rule.NewLoad(file)
	}

	for k := range kinds {
		load.Add(k)
	}
	for _, k := range load.Symbols() {
		if knownKinds[k] != "" && !kinds[k] {
			load.Remove(k)
		}
	}
	return load
}
func newLoadIndex(f *rule.File, after []string) int {
	if len(after) == 0 {
		return 0
	}
	index := 0
	for _, r := range f.Rules {
		for _, a := range after {
			if r.Kind() == a && r.Index() >= index {
				index = r.Index() + 1
			}
		}
	}
	return index
}
func removeLegacyGoRepository(f *rule.File) {
	for _, l := range f.Loads {
		if l.Name() == "@io_bazel_rules_go//go:def.bzl" {
			l.Remove("go_repository")
			if l.IsEmpty() {
				l.Delete()
			}
		}
	}
}
func (x Version) Compare(y Version) int {
	n := len(x)
	if len(y) < n {
		n = len(y)
	}
	for i := 0; i < n; i++ {
		cmp := x[i] - y[i]
		if cmp != 0 {
			return cmp
		}
	}
	return len(x) - len(y)
}
func ParseVersion(vs string) (Version, error) {
	i := strings.IndexByte(vs, '-')
	if i >= 0 {
		vs = vs[:i]
	}
	cstrs := strings.Split(vs, ".")
	v := make(Version, len(cstrs))
	for i, cstr := range cstrs {
		cn, err := strconv.Atoi(cstr)
		if err != nil {
			return nil, fmt.Errorf("could not parse version string: %q is not an integer", cstr)
		}
		if cn < 0 {
			return nil, fmt.Errorf("could not parse version string: %q is negative", cstr)
		}
		v[i] = cn
	}
	return v, nil
}
func EmptyFile(path, pkg string) *File {
	return &File{
		File: &bzl.File{Path: path, Type: bzl.TypeBuild},
		Path: path,
		Pkg:  pkg,
	}
}
func LoadWorkspaceFile(path, pkg string) (*File, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}
	return LoadWorkspaceData(path, pkg, data)
}
func LoadMacroFile(path, pkg, defName string) (*File, error) {
	data, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}
	return LoadMacroData(path, pkg, defName, data)
}
func EmptyMacroFile(path, pkg, defName string) (*File, error) {
	_, err := os.Create(path)
	if err != nil {
		return nil, err
	}
	return LoadMacroData(path, pkg, defName, nil)
}
func LoadData(path, pkg string, data []byte) (*File, error) {
	ast, err := bzl.ParseBuild(path, data)
	if err != nil {
		return nil, err
	}
	return ScanAST(pkg, ast), nil
}
func LoadWorkspaceData(path, pkg string, data []byte) (*File, error) {
	ast, err := bzl.ParseWorkspace(path, data)
	if err != nil {
		return nil, err
	}
	return ScanAST(pkg, ast), nil
}
func LoadMacroData(path, pkg, defName string, data []byte) (*File, error) {
	ast, err := bzl.ParseBzl(path, data)
	if err != nil {
		return nil, err
	}
	return ScanASTBody(pkg, defName, ast), nil
}
func ScanAST(pkg string, bzlFile *bzl.File) *File {
	return ScanASTBody(pkg, "", bzlFile)
}
func ScanASTBody(pkg, defName string, bzlFile *bzl.File) *File {
	f := &File{
		File: bzlFile,
		Pkg:  pkg,
		Path: bzlFile.Path,
	}
	var defStmt *bzl.DefStmt
	f.Rules, f.Loads, defStmt = scanExprs(defName, bzlFile.Stmt)
	if defStmt != nil {
		f.Rules, _, _ = scanExprs("", defStmt.Body)
		f.function = &function{
			stmt:     defStmt,
			inserted: true,
		}
		if len(defStmt.Body) == 1 {
			if v, ok := defStmt.Body[0].(*bzl.BranchStmt); ok && v.Token == "pass" {
				f.function.hasPass = true
			}
		}
	} else if defName != "" {
		f.function = &function{
			stmt:     &bzl.DefStmt{Name: defName},
			inserted: false,
		}
	}
	f.Directives = ParseDirectives(bzlFile)
	return f
}
func MatchBuildFileName(dir string, names []string, files []os.FileInfo) string {
	for _, name := range names {
		for _, fi := range files {
			if fi.Name() == name && !fi.IsDir() {
				return filepath.Join(dir, name)
			}
		}
	}
	return ""
}
func (f *File) SyncMacroFile(from *File) {
	fromFunc := *from.function.stmt
	_, _, toFunc := scanExprs(from.function.stmt.Name, f.File.Stmt)
	if toFunc != nil {
		*toFunc = fromFunc
	} else {
		f.File.Stmt = append(f.File.Stmt, &fromFunc)
	}
}
func (f *File) MacroName() string {
	if f.function != nil && f.function.stmt != nil {
		return f.function.stmt.Name
	}
	return ""
}
func (f *File) Sync() {
	var loadInserts, loadDeletes, loadStmts []*stmt
	var r, w int
	for r, w = 0, 0; r < len(f.Loads); r++ {
		s := f.Loads[r]
		s.sync()
		if s.deleted {
			loadDeletes = append(loadDeletes, &s.stmt)
			continue
		}
		if s.inserted {
			loadInserts = append(loadInserts, &s.stmt)
			s.inserted = false
		} else {
			loadStmts = append(loadStmts, &s.stmt)
		}
		f.Loads[w] = s
		w++
	}
	f.Loads = f.Loads[:w]
	var ruleInserts, ruleDeletes, ruleStmts []*stmt
	for r, w = 0, 0; r < len(f.Rules); r++ {
		s := f.Rules[r]
		s.sync()
		if s.deleted {
			ruleDeletes = append(ruleDeletes, &s.stmt)
			continue
		}
		if s.inserted {
			ruleInserts = append(ruleInserts, &s.stmt)
			s.inserted = false
		} else {
			ruleStmts = append(ruleStmts, &s.stmt)
		}
		f.Rules[w] = s
		w++
	}
	f.Rules = f.Rules[:w]

	if f.function == nil {
		deletes := append(ruleDeletes, loadDeletes...)
		inserts := append(ruleInserts, loadInserts...)
		stmts := append(ruleStmts, loadStmts...)
		updateStmt(&f.File.Stmt, inserts, deletes, stmts)
	} else {
		updateStmt(&f.File.Stmt, loadInserts, loadDeletes, loadStmts)
		if f.function.hasPass && len(ruleInserts) > 0 {
			f.function.stmt.Body = []bzl.Expr{}
			f.function.hasPass = false
		}
		updateStmt(&f.function.stmt.Body, ruleInserts, ruleDeletes, ruleStmts)
		if len(f.function.stmt.Body) == 0 {
			f.function.stmt.Body = append(f.function.stmt.Body, &bzl.BranchStmt{Token: "pass"})
			f.function.hasPass = true
		}
		if !f.function.inserted {
			f.File.Stmt = append(f.File.Stmt, f.function.stmt)
			f.function.inserted = true
		}
	}
}
func (f *File) Format() []byte {
	f.Sync()
	return bzl.Format(f.File)
}
func (f *File) Save(path string) error {
	f.Sync()
	data := bzl.Format(f.File)
	return ioutil.WriteFile(path, data, 0666)
}
func (f *File) HasDefaultVisibility() bool {
	for _, r := range f.Rules {
		if r.Kind() == "package" && r.Attr("default_visibility") != nil {
			return true
		}
	}
	return false
}
func NewLoad(name string) *Load {
	return &Load{
		stmt: stmt{
			expr: &bzl.LoadStmt{
				Module:       &bzl.StringExpr{Value: name},
				ForceCompact: true,
			},
		},
		name:    name,
		symbols: make(map[string]identPair),
	}
}
func (l *Load) Symbols() []string {
	syms := make([]string, 0, len(l.symbols))
	for sym := range l.symbols {
		syms = append(syms, sym)
	}
	sort.Strings(syms)
	return syms
}
func (l *Load) Has(sym string) bool {
	_, ok := l.symbols[sym]
	return ok
}
func (l *Load) Add(sym string) {
	if _, ok := l.symbols[sym]; !ok {
		i := &bzl.Ident{Name: sym}
		l.symbols[sym] = identPair{to: i, from: i}
		l.updated = true
	}
}
func (l *Load) Remove(sym string) {
	if _, ok := l.symbols[sym]; ok {
		delete(l.symbols, sym)
		l.updated = true
	}
}
func (l *Load) Insert(f *File, index int) {
	l.index = index
	l.inserted = true
	f.Loads = append(f.Loads, l)
}
func NewRule(kind, name string) *Rule {
	nameAttr := &bzl.AssignExpr{
		LHS: &bzl.Ident{Name: "name"},
		RHS: &bzl.StringExpr{Value: name},
		Op:  "=",
	}
	r := &Rule{
		stmt: stmt{
			expr: &bzl.CallExpr{
				X:    &bzl.Ident{Name: kind},
				List: []bzl.Expr{nameAttr},
			},
		},
		kind:    kind,
		attrs:   map[string]*bzl.AssignExpr{"name": nameAttr},
		private: map[string]interface{}{},
	}
	return r
}
func (r *Rule) SetKind(kind string) {
	r.kind = kind
	r.updated = true
}
func (r *Rule) AttrKeys() []string {
	keys := make([]string, 0, len(r.attrs))
	for k := range r.attrs {
		keys = append(keys, k)
	}
	sort.SliceStable(keys, func(i, j int) bool {
		if cmp := bt.NamePriority[keys[i]] - bt.NamePriority[keys[j]]; cmp != 0 {
			return cmp < 0
		}
		return keys[i] < keys[j]
	})
	return keys
}
func (r *Rule) Attr(key string) bzl.Expr {
	attr, ok := r.attrs[key]
	if !ok {
		return nil
	}
	return attr.RHS
}
func (r *Rule) AttrString(key string) string {
	attr, ok := r.attrs[key]
	if !ok {
		return ""
	}
	str, ok := attr.RHS.(*bzl.StringExpr)
	if !ok {
		return ""
	}
	return str.Value
}
func (r *Rule) AttrStrings(key string) []string {
	attr, ok := r.attrs[key]
	if !ok {
		return nil
	}
	list, ok := attr.RHS.(*bzl.ListExpr)
	if !ok {
		return nil
	}
	strs := make([]string, 0, len(list.List))
	for _, e := range list.List {
		if str, ok := e.(*bzl.StringExpr); ok {
			strs = append(strs, str.Value)
		}
	}
	return strs
}
func (r *Rule) DelAttr(key string) {
	delete(r.attrs, key)
	r.updated = true
}
func (r *Rule) SetAttr(key string, value interface{}) {
	rhs := ExprFromValue(value)
	if attr, ok := r.attrs[key]; ok {
		attr.RHS = rhs
	} else {
		r.attrs[key] = &bzl.AssignExpr{
			LHS: &bzl.Ident{Name: key},
			RHS: rhs,
			Op:  "=",
		}
	}
	r.updated = true
}
func (r *Rule) PrivateAttrKeys() []string {
	keys := make([]string, 0, len(r.private))
	for k := range r.private {
		keys = append(keys, k)
	}
	sort.Strings(keys)
	return keys
}
func (r *Rule) SetPrivateAttr(key string, value interface{}) {
	r.private[key] = value
}
func (r *Rule) Insert(f *File) {
	// TODO(jayconrod): should rules always be inserted at the end? Should there
	// be some sort order?
	var stmt []bzl.Expr
	if f.function == nil {
		stmt = f.File.Stmt
	} else {
		stmt = f.function.stmt.Body
	}
	r.index = len(stmt)
	r.inserted = true
	f.Rules = append(f.Rules, r)
}
func (r *Rule) IsEmpty(info KindInfo) bool {
	if info.NonEmptyAttrs == nil {
		return false
	}
	for k := range info.NonEmptyAttrs {
		if _, ok := r.attrs[k]; ok {
			return false
		}
	}
	return true
}
func CheckInternalVisibility(rel, visibility string) string {
	if i := strings.LastIndex(rel, "/internal/"); i >= 0 {
		visibility = fmt.Sprintf("//%s:__subpackages__", rel[:i])
	} else if strings.HasPrefix(rel, "internal/") {
		visibility = "//:__subpackages__"
	}
	return visibility
}
func New(repo, pkg, name string) Label {
	return Label{Repo: repo, Pkg: pkg, Name: name}
}
func (l Label) Rel(repo, pkg string) Label {
	if l.Relative || l.Repo != repo {
		return l
	}
	if l.Pkg == pkg {
		return Label{Name: l.Name, Relative: true}
	}
	return Label{Pkg: l.Pkg, Name: l.Name}
}
func (l Label) Equal(other Label) bool {
	return l.Repo == other.Repo &&
		l.Pkg == other.Pkg &&
		l.Name == other.Name &&
		l.Relative == other.Relative
}
func (l Label) Contains(other Label) bool {
	if l.Relative {
		log.Panicf("l must not be relative: %s", l)
	}
	if other.Relative {
		log.Panicf("other must not be relative: %s", other)
	}
	result := l.Repo == other.Repo && pathtools.HasPrefix(other.Pkg, l.Pkg)
	return result
}
func generateFromPath(w io.Writer, rootPath string) error {
	return filepath.Walk(rootPath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}
		if !strings.HasSuffix(path, ".proto") {
			return nil
		}
		relPath, err := filepath.Rel(rootPath, path)
		if err != nil || strings.HasPrefix(relPath, "..") {
			log.Panicf("file %q not in repository rootPath %q", path, rootPath)
		}
		relPath = filepath.ToSlash(relPath)

		if strings.HasPrefix(relPath, "google/api/experimental/") {
			// Special case: these protos need to be built together with protos in
			// google/api. They have the same 'option go_package'. The proto_library
			// rule requires them to be in the same Bazel package, so we don't
			// create a build file in experimental.
			packagePath := "google.golang.org/genproto/googleapis/api"
			protoLabel, goLabel := protoLabels("google/api/x", "api")
			fmt.Fprintf(w, "%s,%s,%s,%s\n", relPath, protoLabel, packagePath, goLabel)
			return nil
		}

		packagePath, packageName, err := loadGoPackage(path)
		if err != nil {
			log.Print(err)
			return nil
		}

		protoLabel, goLabel := protoLabels(relPath, packageName)

		fmt.Fprintf(w, "%s,%s,%s,%s\n", relPath, protoLabel, packagePath, goLabel)
		return nil
	})
}
func shouldCall(rel string, mode Mode, updateRels map[string]bool) bool {
	return mode != UpdateDirsMode || updateRels[rel]
}
func shouldUpdate(rel string, mode Mode, updateParent bool, updateRels map[string]bool) bool {
	return mode == VisitAllUpdateSubdirsMode && updateParent || updateRels[rel]
}
func shouldVisit(rel string, mode Mode, updateRels map[string]bool) bool {
	if mode != UpdateDirsMode {
		return true
	}
	_, ok := updateRels[rel]
	return ok
}
func SquashRules(src, dst *Rule, filename string) error {
	if dst.ShouldKeep() {
		return nil
	}

	for key, srcAttr := range src.attrs {
		srcValue := srcAttr.RHS
		if dstAttr, ok := dst.attrs[key]; !ok {
			dst.SetAttr(key, srcValue)
		} else if !ShouldKeep(dstAttr) {
			dstValue := dstAttr.RHS
			if squashedValue, err := squashExprs(srcValue, dstValue); err != nil {
				start, end := dstValue.Span()
				return fmt.Errorf("%s:%d.%d-%d.%d: could not squash expression", filename, start.Line, start.LineRune, end.Line, end.LineRune)
			} else {
				dst.SetAttr(key, squashedValue)
			}
		}
	}
	dst.expr.Comment().Before = append(dst.expr.Comment().Before, src.expr.Comment().Before...)
	dst.expr.Comment().Suffix = append(dst.expr.Comment().Suffix, src.expr.Comment().Suffix...)
	dst.expr.Comment().After = append(dst.expr.Comment().After, src.expr.Comment().After...)
	return nil
}
func runClient() error {
	startTime := time.Now()
	conn, err := net.Dial("unix", *socketPath)
	if err != nil {
		if err := startServer(); err != nil {
			return fmt.Errorf("error starting server: %v", err)
		}
		for retry := 0; retry < 3; retry++ {
			conn, err = net.Dial("unix", *socketPath)
			if err == nil {
				break
			}
			// Wait for server to start listening.
			time.Sleep(1 * time.Second)
		}
		if err != nil {
			return fmt.Errorf("failed to connect to server: %v", err)
		}
	}
	defer conn.Close()

	if _, err := io.Copy(os.Stderr, conn); err != nil {
		log.Print(err)
	}

	elapsedTime := time.Since(startTime)
	log.Printf("ran gazelle in %.3f s", elapsedTime.Seconds())
	return nil
}
func UpdateRepo(rc *RemoteCache, importPath string) (Repo, error) {
	root, name, err := rc.Root(importPath)
	if err != nil {
		return Repo{}, err
	}
	remote, vcs, err := rc.Remote(root)
	if err != nil {
		return Repo{}, err
	}
	commit, tag, err := rc.Head(remote, vcs)
	if err != nil {
		return Repo{}, err
	}
	repo := Repo{
		Name:     name,
		GoPrefix: root,
		Commit:   commit,
		Tag:      tag,
		Remote:   remote,
		VCS:      vcs,
	}
	return repo, nil
}
func NewRemoteCache(knownRepos []Repo) (r *RemoteCache, cleanup func() error) {
	r = &RemoteCache{
		RepoRootForImportPath: vcs.RepoRootForImportPath,
		HeadCmd:               defaultHeadCmd,
		root:                  remoteCacheMap{cache: make(map[string]*remoteCacheEntry)},
		remote:                remoteCacheMap{cache: make(map[string]*remoteCacheEntry)},
		head:                  remoteCacheMap{cache: make(map[string]*remoteCacheEntry)},
		mod:                   remoteCacheMap{cache: make(map[string]*remoteCacheEntry)},
	}
	r.ModInfo = func(importPath string) (string, error) {
		return defaultModInfo(r, importPath)
	}
	for _, repo := range knownRepos {
		r.root.cache[repo.GoPrefix] = &remoteCacheEntry{
			value: rootValue{
				root: repo.GoPrefix,
				name: repo.Name,
			},
		}
		if repo.Remote != "" {
			r.remote.cache[repo.GoPrefix] = &remoteCacheEntry{
				value: remoteValue{
					remote: repo.Remote,
					vcs:    repo.VCS,
				},
			}
		}
		r.mod.cache[repo.GoPrefix] = &remoteCacheEntry{
			value: modValue{
				path:  repo.GoPrefix,
				name:  repo.Name,
				known: true,
			},
		}
	}
	return r, r.cleanup
}
func (r *RemoteCache) Remote(root string) (remote, vcs string, err error) {
	v, err := r.remote.ensure(root, func() (interface{}, error) {
		repo, err := r.RepoRootForImportPath(root, false)
		if err != nil {
			return nil, err
		}
		return remoteValue{remote: repo.Repo, vcs: repo.VCS.Cmd}, nil
	})
	if err != nil {
		return "", "", err
	}
	value := v.(remoteValue)
	return value.remote, value.vcs, nil
}
func (m *remoteCacheMap) get(key string) (value interface{}, ok bool, err error) {
	m.mu.Lock()
	e, ok := m.cache[key]
	m.mu.Unlock()
	if !ok {
		return nil, ok, nil
	}
	if e.ready != nil {
		<-e.ready
	}
	return e.value, ok, e.err
}
func (m *remoteCacheMap) ensure(key string, load func() (interface{}, error)) (interface{}, error) {
	m.mu.Lock()
	e, ok := m.cache[key]
	if !ok {
		e = &remoteCacheEntry{ready: make(chan struct{})}
		m.cache[key] = e
		m.mu.Unlock()
		e.value, e.err = load()
		close(e.ready)
	} else {
		m.mu.Unlock()
		if e.ready != nil {
			<-e.ready
		}
	}
	return e.value, e.err
}
func RelBaseName(rel, prefix, root string) string {
	base := path.Base(rel)
	if base == "." || base == "/" {
		base = path.Base(prefix)
	}
	if base == "." || base == "/" {
		base = filepath.Base(root)
	}
	if base == "." || base == "/" {
		base = "root"
	}
	return base
}
func (c *Config) Clone() *Config {
	cc := *c
	cc.Exts = make(map[string]interface{})
	for k, v := range c.Exts {
		cc.Exts[k] = v
	}
	cc.KindMap = make(map[string]MappedKind)
	for k, v := range c.KindMap {
		cc.KindMap[k] = v
	}
	return &cc
}
func (c *Config) IsValidBuildFileName(name string) bool {
	for _, n := range c.ValidBuildFileNames {
		if name == n {
			return true
		}
	}
	return false
}
func (l tagLine) check(c *config.Config, os, arch string) bool {
	if len(l) == 0 {
		return false
	}
	for _, g := range l {
		if g.check(c, os, arch) {
			return true
		}
	}
	return false
}
func fileNameInfo(path_ string) fileInfo {
	name := filepath.Base(path_)
	var ext ext
	switch path.Ext(name) {
	case ".go":
		ext = goExt
	case ".c", ".cc", ".cpp", ".cxx", ".m", ".mm":
		ext = cExt
	case ".h", ".hh", ".hpp", ".hxx":
		ext = hExt
	case ".s":
		ext = sExt
	case ".S":
		ext = csExt
	case ".proto":
		ext = protoExt
	default:
		ext = unknownExt
	}
	if strings.HasPrefix(name, ".") || strings.HasPrefix(name, "_") {
		ext = unknownExt
	}

	// Determine test, goos, and goarch. This is intended to match the logic
	// in goodOSArchFile in go/build.
	var isTest bool
	var goos, goarch string
	l := strings.Split(name[:len(name)-len(path.Ext(name))], "_")
	if len(l) >= 2 && l[len(l)-1] == "test" {
		isTest = ext == goExt
		l = l[:len(l)-1]
	}
	switch {
	case len(l) >= 3 && rule.KnownOSSet[l[len(l)-2]] && rule.KnownArchSet[l[len(l)-1]]:
		goos = l[len(l)-2]
		goarch = l[len(l)-1]
	case len(l) >= 2 && rule.KnownOSSet[l[len(l)-1]]:
		goos = l[len(l)-1]
	case len(l) >= 2 && rule.KnownArchSet[l[len(l)-1]]:
		goarch = l[len(l)-1]
	}

	return fileInfo{
		path:   path_,
		name:   name,
		ext:    ext,
		isTest: isTest,
		goos:   goos,
		goarch: goarch,
	}
}
func otherFileInfo(path string) fileInfo {
	info := fileNameInfo(path)
	if info.ext == unknownExt {
		return info
	}

	tags, err := readTags(info.path)
	if err != nil {
		log.Printf("%s: error reading file: %v", info.path, err)
		return info
	}
	info.tags = tags
	return info
}
func protoFileInfo(path_ string, protoInfo proto.FileInfo) fileInfo {
	info := fileNameInfo(path_)

	// Look for "option go_package".  If there's no / in the package option, then
	// it's just a simple package name, not a full import path.
	for _, opt := range protoInfo.Options {
		if opt.Key != "go_package" {
			continue
		}
		if strings.LastIndexByte(opt.Value, '/') == -1 {
			info.packageName = opt.Value
		} else {
			if i := strings.LastIndexByte(opt.Value, ';'); i != -1 {
				info.importPath = opt.Value[:i]
				info.packageName = opt.Value[i+1:]
			} else {
				info.importPath = opt.Value
				info.packageName = path.Base(opt.Value)
			}
		}
	}

	// Set the Go package name from the proto package name if there was no
	// option go_package.
	if info.packageName == "" && protoInfo.PackageName != "" {
		info.packageName = strings.Replace(protoInfo.PackageName, ".", "_", -1)
	}

	info.imports = protoInfo.Imports
	info.hasServices = protoInfo.HasServices
	return info
}
func (ix *RuleIndex) AddRule(c *config.Config, r *rule.Rule, f *rule.File) {
	var imps []ImportSpec
	if rslv := ix.mrslv(r, f.Pkg); rslv != nil {
		imps = rslv.Imports(c, r, f)
	}
	// If imps == nil, the rule is not importable. If imps is the empty slice,
	// it may still be importable if it embeds importable libraries.
	if imps == nil {
		return
	}

	record := &ruleRecord{
		rule:       r,
		label:      label.New(c.RepoName, f.Pkg, r.Name()),
		file:       f,
		importedAs: imps,
	}
	if _, ok := ix.labelMap[record.label]; ok {
		log.Printf("multiple rules found with label %s", record.label)
		return
	}
	ix.rules = append(ix.rules, record)
	ix.labelMap[record.label] = record
}
func (ix *RuleIndex) Finish() {
	for _, r := range ix.rules {
		ix.collectEmbeds(r)
	}
	ix.buildImportIndex()
}
func (ix *RuleIndex) buildImportIndex() {
	ix.importMap = make(map[ImportSpec][]*ruleRecord)
	for _, r := range ix.rules {
		if r.embedded {
			continue
		}
		indexed := make(map[ImportSpec]bool)
		for _, imp := range r.importedAs {
			if indexed[imp] {
				continue
			}
			indexed[imp] = true
			ix.importMap[imp] = append(ix.importMap[imp], r)
		}
	}
}
func (r FindResult) IsSelfImport(from label.Label) bool {
	if from.Equal(r.Label) {
		return true
	}
	for _, e := range r.Embeds {
		if from.Equal(e) {
			return true
		}
	}
	return false
}
func applyKindMappings(mappedKinds []config.MappedKind, loads []rule.LoadInfo) []rule.LoadInfo {
	if len(mappedKinds) == 0 {
		return loads
	}

	// Add new RuleInfos or replace existing ones with merged ones.
	mappedLoads := make([]rule.LoadInfo, len(loads))
	copy(mappedLoads, loads)
	for _, mappedKind := range mappedKinds {
		mappedLoads = appendOrMergeKindMapping(mappedLoads, mappedKind)
	}
	return mappedLoads
}
func appendOrMergeKindMapping(mappedLoads []rule.LoadInfo, mappedKind config.MappedKind) []rule.LoadInfo {
	// If mappedKind.KindLoad already exists in the list, create a merged copy.
	for _, load := range mappedLoads {
		if load.Name == mappedKind.KindLoad {
			load.Symbols = append(load.Symbols, mappedKind.KindName)
			return mappedLoads
		}
	}

	// Add a new LoadInfo.
	return append(mappedLoads, rule.LoadInfo{
		Name:    mappedKind.KindLoad,
		Symbols: []string{mappedKind.KindName},
	})
}
func RuleName(names ...string) string {
	base := "root"
	for _, name := range names {
		notIdent := func(c rune) bool {
			return !('A' <= c && c <= 'Z' ||
				'a' <= c && c <= 'z' ||
				'0' <= c && c <= '9' ||
				c == '_')
		}
		if i := strings.LastIndexFunc(name, notIdent); i >= 0 {
			name = name[i+1:]
		}
		if name != "" {
			base = name
			break
		}
	}
	return base + "_proto"
}
func buildPackages(pc *ProtoConfig, dir, rel string, protoFiles, genFiles []string) []*Package {
	packageMap := make(map[string]*Package)
	for _, name := range protoFiles {
		info := protoFileInfo(dir, name)
		key := info.PackageName
		if pc.groupOption != "" {
			for _, opt := range info.Options {
				if opt.Key == pc.groupOption {
					key = opt.Value
					break
				}
			}
		}
		if packageMap[key] == nil {
			packageMap[key] = newPackage(info.PackageName)
		}
		packageMap[key].addFile(info)
	}

	switch pc.Mode {
	case DefaultMode:
		pkg, err := selectPackage(dir, rel, packageMap)
		if err != nil {
			log.Print(err)
		}
		if pkg == nil {
			return nil // empty rule created in generateEmpty
		}
		for _, name := range genFiles {
			pkg.addGenFile(dir, name)
		}
		return []*Package{pkg}

	case PackageMode:
		pkgs := make([]*Package, 0, len(packageMap))
		for _, pkg := range packageMap {
			pkgs = append(pkgs, pkg)
		}
		return pkgs

	default:
		return nil
	}
}
func selectPackage(dir, rel string, packageMap map[string]*Package) (*Package, error) {
	if len(packageMap) == 0 {
		return nil, nil
	}
	if len(packageMap) == 1 {
		for _, pkg := range packageMap {
			return pkg, nil
		}
	}
	defaultPackageName := strings.Replace(rel, "/", "_", -1)
	for _, pkg := range packageMap {
		if pkgName := goPackageName(pkg); pkgName != "" && pkgName == defaultPackageName {
			return pkg, nil
		}
	}
	return nil, fmt.Errorf("%s: directory contains multiple proto packages. Gazelle can only generate a proto_library for one package.", dir)
}
func generateProto(pc *ProtoConfig, rel string, pkg *Package, shouldSetVisibility bool) *rule.Rule {
	var name string
	if pc.Mode == DefaultMode {
		name = RuleName(goPackageName(pkg), pc.GoPrefix, rel)
	} else {
		name = RuleName(pkg.Options[pc.groupOption], pkg.Name, rel)
	}
	r := rule.NewRule("proto_library", name)
	srcs := make([]string, 0, len(pkg.Files))
	for f := range pkg.Files {
		srcs = append(srcs, f)
	}
	sort.Strings(srcs)
	if len(srcs) > 0 {
		r.SetAttr("srcs", srcs)
	}
	r.SetPrivateAttr(PackageKey, *pkg)
	imports := make([]string, 0, len(pkg.Imports))
	for i := range pkg.Imports {
		imports = append(imports, i)
	}
	sort.Strings(imports)
	// NOTE: This attribute should not be used outside this extension. It's still
	// convenient for testing though.
	r.SetPrivateAttr(config.GazelleImportsKey, imports)
	for k, v := range pkg.Options {
		r.SetPrivateAttr(k, v)
	}
	if shouldSetVisibility {
		vis := rule.CheckInternalVisibility(rel, "//visibility:public")
		r.SetAttr("visibility", []string{vis})
	}
	if pc.stripImportPrefix != "" {
		r.SetAttr("strip_import_prefix", pc.stripImportPrefix)
	}
	if pc.importPrefix != "" {
		r.SetAttr("import_prefix", pc.importPrefix)
	}
	return r
}
func generateEmpty(f *rule.File, regularFiles, genFiles []string) []*rule.Rule {
	if f == nil {
		return nil
	}
	knownFiles := make(map[string]bool)
	for _, f := range regularFiles {
		knownFiles[f] = true
	}
	for _, f := range genFiles {
		knownFiles[f] = true
	}
	var empty []*rule.Rule
outer:
	for _, r := range f.Rules {
		if r.Kind() != "proto_library" {
			continue
		}
		srcs := r.AttrStrings("srcs")
		if len(srcs) == 0 && r.Attr("srcs") != nil {
			// srcs is not a string list; leave it alone
			continue
		}
		for _, src := range r.AttrStrings("srcs") {
			if knownFiles[src] {
				continue outer
			}
		}
		empty = append(empty, rule.NewRule("proto_library", r.Name()))
	}
	return empty
}
func ImportRepoRules(filename string, repoCache *RemoteCache) ([]*rule.Rule, error) {
	format := getLockFileFormat(filename)
	if format == unknownFormat {
		return nil, fmt.Errorf(`%s: unrecognized lock file format. Expected "Gopkg.lock", "go.mod", or "Godeps.json"`, filename)
	}
	parser := lockFileParsers[format]
	repos, err := parser(filename, repoCache)
	if err != nil {
		return nil, fmt.Errorf("error parsing %q: %v", filename, err)
	}
	sort.Stable(byName(repos))

	rules := make([]*rule.Rule, 0, len(repos))
	for _, repo := range repos {
		rules = append(rules, GenerateRule(repo))
	}
	return rules, nil
}
func MergeRules(genRules []*rule.Rule, existingRules map[*rule.File][]string, destFile *rule.File, kinds map[string]rule.KindInfo) []*rule.File {
	sort.Stable(byRuleName(genRules))

	repoMap := make(map[string]*rule.File)
	for file, repoNames := range existingRules {
		if file.Path == destFile.Path && file.MacroName() != "" && file.MacroName() == destFile.MacroName() {
			file = destFile
		}
		for _, name := range repoNames {
			repoMap[name] = file
		}
	}

	rulesByFile := make(map[*rule.File][]*rule.Rule)
	for _, rule := range genRules {
		dest := destFile
		if file, ok := repoMap[rule.Name()]; ok {
			dest = file
		}
		rulesByFile[dest] = append(rulesByFile[dest], rule)
	}

	updatedFiles := make(map[string]*rule.File)
	for f, rules := range rulesByFile {
		merger.MergeFile(f, nil, rules, merger.PreResolve, kinds)
		f.Sync()
		if uf, ok := updatedFiles[f.Path]; ok {
			uf.SyncMacroFile(f)
		} else {
			updatedFiles[f.Path] = f
		}
	}
	files := make([]*rule.File, 0, len(updatedFiles))
	for _, f := range updatedFiles {
		files = append(files, f)
	}
	return files
}
func GenerateRule(repo Repo) *rule.Rule {
	r := rule.NewRule("go_repository", repo.Name)
	if repo.Commit != "" {
		r.SetAttr("commit", repo.Commit)
	}
	if repo.Tag != "" {
		r.SetAttr("tag", repo.Tag)
	}
	r.SetAttr("importpath", repo.GoPrefix)
	if repo.Remote != "" {
		r.SetAttr("remote", repo.Remote)
	}
	if repo.VCS != "" {
		r.SetAttr("vcs", repo.VCS)
	}
	if repo.Version != "" {
		r.SetAttr("version", repo.Version)
	}
	if repo.Sum != "" {
		r.SetAttr("sum", repo.Sum)
	}
	if repo.Replace != "" {
		r.SetAttr("replace", repo.Replace)
	}
	return r
}
func FindExternalRepo(repoRoot, name string) (string, error) {
	// See https://docs.bazel.build/versions/master/output_directories.html
	// for documentation on Bazel directory layout.
	// We expect the bazel-out symlink in the workspace root directory to point to
	// <output-base>/execroot/<workspace-name>/bazel-out
	// We expect the external repository to be checked out at
	// <output-base>/external/<name>
	// Note that users can change the prefix for most of the Bazel symlinks with
	// --symlink_prefix, but this does not include bazel-out.
	externalPath := strings.Join([]string{repoRoot, "bazel-out", "..", "..", "..", "external", name}, string(os.PathSeparator))
	cleanPath, err := filepath.EvalSymlinks(externalPath)
	if err != nil {
		return "", err
	}
	st, err := os.Stat(cleanPath)
	if err != nil {
		return "", err
	}
	if !st.IsDir() {
		return "", fmt.Errorf("%s: not a directory", externalPath)
	}
	return cleanPath, nil
}
func ListRepositories(workspace *rule.File) (repos []Repo, repoNamesByFile map[*rule.File][]string, err error) {
	repoNamesByFile = make(map[*rule.File][]string)
	repos, repoNamesByFile[workspace] = getRepos(workspace.Rules)
	for _, d := range workspace.Directives {
		switch d.Key {
		case "repository_macro":
			f, defName, err := parseRepositoryMacroDirective(d.Value)
			if err != nil {
				return nil, nil, err
			}
			f = filepath.Join(filepath.Dir(workspace.Path), filepath.Clean(f))
			macroFile, err := rule.LoadMacroFile(f, "", defName)
			if err != nil {
				return nil, nil, err
			}
			currRepos, names := getRepos(macroFile.Rules)
			repoNamesByFile[macroFile] = names
			repos = append(repos, currRepos...)
		}
	}

	return repos, repoNamesByFile, nil
}
func migrateLibraryEmbed(c *config.Config, f *rule.File) {
	for _, r := range f.Rules {
		if !isGoRule(r.Kind()) {
			continue
		}
		libExpr := r.Attr("library")
		if libExpr == nil || rule.ShouldKeep(libExpr) || r.Attr("embed") != nil {
			continue
		}
		r.DelAttr("library")
		r.SetAttr("embed", &bzl.ListExpr{List: []bzl.Expr{libExpr}})
	}
}
func migrateGrpcCompilers(c *config.Config, f *rule.File) {
	for _, r := range f.Rules {
		if r.Kind() != "go_grpc_library" || r.ShouldKeep() || r.Attr("compilers") != nil {
			continue
		}
		r.SetKind("go_proto_library")
		r.SetAttr("compilers", []string{grpcCompilerLabel})
	}
}
func squashCgoLibrary(c *config.Config, f *rule.File) {
	// Find the default cgo_library and go_library rules.
	var cgoLibrary, goLibrary *rule.Rule
	for _, r := range f.Rules {
		if r.Kind() == "cgo_library" && r.Name() == "cgo_default_library" && !r.ShouldKeep() {
			if cgoLibrary != nil {
				log.Printf("%s: when fixing existing file, multiple cgo_library rules with default name found", f.Path)
				continue
			}
			cgoLibrary = r
			continue
		}
		if r.Kind() == "go_library" && r.Name() == defaultLibName {
			if goLibrary != nil {
				log.Printf("%s: when fixing existing file, multiple go_library rules with default name referencing cgo_library found", f.Path)
			}
			goLibrary = r
			continue
		}
	}

	if cgoLibrary == nil {
		return
	}
	if !c.ShouldFix {
		log.Printf("%s: cgo_library is deprecated. Run 'gazelle fix' to squash with go_library.", f.Path)
		return
	}

	if goLibrary == nil {
		cgoLibrary.SetKind("go_library")
		cgoLibrary.SetName(defaultLibName)
		cgoLibrary.SetAttr("cgo", true)
		return
	}

	if err := rule.SquashRules(cgoLibrary, goLibrary, f.Path); err != nil {
		log.Print(err)
		return
	}
	goLibrary.DelAttr("embed")
	goLibrary.SetAttr("cgo", true)
	cgoLibrary.Delete()
}
func removeLegacyProto(c *config.Config, f *rule.File) {
	// Don't fix if the proto mode was set to something other than the default.
	if pcMode := getProtoMode(c); pcMode != proto.DefaultMode {
		return
	}

	// Scan for definitions to delete.
	var protoLoads []*rule.Load
	for _, l := range f.Loads {
		if l.Name() == "@io_bazel_rules_go//proto:go_proto_library.bzl" {
			protoLoads = append(protoLoads, l)
		}
	}
	var protoFilegroups, protoRules []*rule.Rule
	for _, r := range f.Rules {
		if r.Kind() == "filegroup" && r.Name() == legacyProtoFilegroupName {
			protoFilegroups = append(protoFilegroups, r)
		}
		if r.Kind() == "go_proto_library" {
			protoRules = append(protoRules, r)
		}
	}
	if len(protoLoads)+len(protoFilegroups) == 0 {
		return
	}
	if !c.ShouldFix {
		log.Printf("%s: go_proto_library.bzl is deprecated. Run 'gazelle fix' to replace old rules.", f.Path)
		return
	}

	// Delete legacy proto loads and filegroups. Only delete go_proto_library
	// rules if we deleted a load.
	for _, l := range protoLoads {
		l.Delete()
	}
	for _, r := range protoFilegroups {
		r.Delete()
	}
	if len(protoLoads) > 0 {
		for _, r := range protoRules {
			r.Delete()
		}
	}
}
func removeLegacyGazelle(c *config.Config, f *rule.File) {
	for _, l := range f.Loads {
		if l.Name() == "@io_bazel_rules_go//go:def.bzl" && l.Has("gazelle") {
			l.Remove("gazelle")
			if l.IsEmpty() {
				l.Delete()
			}
		}
	}
}
func selectPackage(c *config.Config, dir string, packageMap map[string]*goPackage) (*goPackage, error) {
	buildablePackages := make(map[string]*goPackage)
	for name, pkg := range packageMap {
		if pkg.isBuildable(c) {
			buildablePackages[name] = pkg
		}
	}

	if len(buildablePackages) == 0 {
		return nil, &build.NoGoError{Dir: dir}
	}

	if len(buildablePackages) == 1 {
		for _, pkg := range buildablePackages {
			return pkg, nil
		}
	}

	if pkg, ok := buildablePackages[defaultPackageName(c, dir)]; ok {
		return pkg, nil
	}

	err := &build.MultiplePackageError{Dir: dir}
	for name, pkg := range buildablePackages {
		// Add the first file for each package for the error message.
		// Error() method expects these lists to be the same length. File
		// lists must be non-empty. These lists are only created by
		// buildPackage for packages with .go files present.
		err.Packages = append(err.Packages, name)
		err.Files = append(err.Files, pkg.firstGoFile())
	}
	return nil, err
}
func (mr *metaResolver) AddBuiltin(kindName string, resolver resolve.Resolver) {
	mr.builtins[kindName] = resolver
}
func (mr *metaResolver) MappedKind(pkgRel string, kind config.MappedKind) {
	mr.mappedKinds[pkgRel] = append(mr.mappedKinds[pkgRel], kind)
}
func (mr metaResolver) Resolver(r *rule.Rule, pkgRel string) resolve.Resolver {
	for _, mappedKind := range mr.mappedKinds[pkgRel] {
		if mappedKind.KindName == r.Kind() {
			return mr.builtins[mappedKind.FromKind]
		}
	}
	return mr.builtins[r.Kind()]
}
func sortExprLabels(e bzl.Expr, _ []bzl.Expr) {
	list, ok := e.(*bzl.ListExpr)
	if !ok || len(list.List) == 0 {
		return
	}

	keys := make([]stringSortKey, len(list.List))
	for i, elem := range list.List {
		s, ok := elem.(*bzl.StringExpr)
		if !ok {
			return // don't sort lists unless all elements are strings
		}
		keys[i] = makeSortKey(i, s)
	}

	before := keys[0].x.Comment().Before
	keys[0].x.Comment().Before = nil
	sort.Sort(byStringExpr(keys))
	keys[0].x.Comment().Before = append(before, keys[0].x.Comment().Before...)
	for i, k := range keys {
		list.List[i] = k.x
	}
}
func checkRulesGoVersion(repoRoot string) {
	const message = `Gazelle may not be compatible with this version of rules_go.
Update io_bazel_rules_go to a newer version in your WORKSPACE file.`

	rulesGoPath, err := repo.FindExternalRepo(repoRoot, config.RulesGoRepoName)
	if err != nil {
		return
	}
	defBzlPath := filepath.Join(rulesGoPath, "go", "def.bzl")
	defBzlContent, err := ioutil.ReadFile(defBzlPath)
	if err != nil {
		return
	}
	versionRe := regexp.MustCompile(`(?m)^RULES_GO_VERSION = ['"]([0-9.]*)['"]`)
	match := versionRe.FindSubmatch(defBzlContent)
	if match == nil {
		log.Printf("RULES_GO_VERSION not found in @%s//go:def.bzl.\n%s", config.RulesGoRepoName, message)
		return
	}
	vstr := string(match[1])
	v, err := version.ParseVersion(vstr)
	if err != nil {
		log.Printf("RULES_GO_VERSION %q could not be parsed in @%s//go:def.bzl.\n%s", vstr, config.RulesGoRepoName, message)
	}
	if v.Compare(minimumRulesGoVersion) < 0 {
		log.Printf("Found RULES_GO_VERSION %s. Minimum compatible version is %s.\n%s", v, minimumRulesGoVersion, message)
	}
}
func (gc *goConfig) preprocessTags() {
	if gc.genericTags == nil {
		gc.genericTags = make(map[string]bool)
	}
	gc.genericTags["gc"] = true
}
func (gc *goConfig) setBuildTags(tags string) error {
	if tags == "" {
		return nil
	}
	for _, t := range strings.Split(tags, ",") {
		if strings.HasPrefix(t, "!") {
			return fmt.Errorf("build tags can't be negated: %s", t)
		}
		gc.genericTags[t] = true
	}
	return nil
}
func splitValue(value string) []string {
	parts := strings.Split(value, ",")
	values := make([]string, 0, len(parts))
	for _, part := range parts {
		values = append(values, strings.TrimSpace(part))
	}
	return values
}
func copyGoModToTemp(filename string) (tempDir string, err error) {
	goModOrig, err := os.Open(filename)
	if err != nil {
		return "", err
	}
	defer goModOrig.Close()

	tempDir, err = ioutil.TempDir("", "gazelle-temp-gomod")
	if err != nil {
		return "", err
	}

	goModCopy, err := os.Create(filepath.Join(tempDir, "go.mod"))
	if err != nil {
		os.Remove(tempDir)
		return "", err
	}
	defer func() {
		if cerr := goModCopy.Close(); err == nil && cerr != nil {
			err = cerr
		}
	}()

	_, err = io.Copy(goModCopy, goModOrig)
	if err != nil {
		os.RemoveAll(tempDir)
		return "", err
	}
	return tempDir, err
}
func findGoTool() string {
	path := "go" // rely on PATH by default
	if goroot, ok := os.LookupEnv("GOROOT"); ok {
		path = filepath.Join(goroot, "bin", "go")
	}
	if runtime.GOOS == "windows" {
		path += ".exe"
	}
	return path
}
func (pkg *goPackage) isBuildable(c *config.Config) bool {
	return pkg.firstGoFile() != "" || !pkg.proto.sources.isEmpty()
}
func startServer() error {
	exe, err := os.Executable()
	if err != nil {
		return err
	}
	args := []string{"-server"}
	args = append(args, os.Args[1:]...)
	cmd := exec.Command(exe, args...)
	log.Printf("starting server: %s", strings.Join(cmd.Args, " "))
	if err := cmd.Start(); err != nil {
		return err
	}
	if err := cmd.Process.Release(); err != nil {
		return err
	}
	return nil
}
func watchDir(root string, record func(string)) (cancel func(), err error) {
	w, err := fsnotify.NewWatcher()
	if err != nil {
		return nil, err
	}

	dirs, errs := listDirs(root)
	for _, err := range errs {
		log.Print(err)
	}
	gitDir := filepath.Join(root, ".git")
	for _, dir := range dirs {
		if dir == gitDir {
			continue
		}
		if err := w.Add(dir); err != nil {
			log.Print(err)
		}
	}

	done := make(chan struct{})
	go func() {
		for {
			select {
			case ev := <-w.Events:
				if shouldIgnore(ev.Name) {
					continue
				}
				if ev.Op == fsnotify.Create {
					if st, err := os.Lstat(ev.Name); err != nil {
						log.Print(err)
					} else if st.IsDir() {
						dirs, errs := listDirs(ev.Name)
						for _, err := range errs {
							log.Print(err)
						}
						for _, dir := range dirs {
							if err := w.Add(dir); err != nil {
								log.Print(err)
							}
							recordWrite(dir)
						}
					}
				} else {
					recordWrite(filepath.Dir(ev.Name))
				}
			case err := <-w.Errors:
				log.Print(err)
			case <-done:
				if err := w.Close(); err != nil {
					log.Print(err)
				}
				return
			}
		}
	}()
	return func() { close(done) }, nil
}
func listDirs(dir string) ([]string, []error) {
	var dirs []string
	var errs []error
	err := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			errs = append(errs, err)
			return nil
		}
		if info.IsDir() {
			dirs = append(dirs, path)
		}
		return nil
	})
	if err != nil {
		errs = append(errs, err)
	}
	return dirs, errs
}
func shouldIgnore(p string) bool {
	p = strings.TrimPrefix(filepath.ToSlash(p), "./")
	base := path.Base(p)
	return strings.HasPrefix(p, "tools/") || base == ".git" || base == "BUILD" || base == "BUILD.bazel"
}
func recordWrite(path string) {
	dirSetMutex.Lock()
	defer dirSetMutex.Unlock()
	dirSet[path] = true
}
func getAndClearWrittenDirs() []string {
	dirSetMutex.Lock()
	defer dirSetMutex.Unlock()
	dirs := make([]string, 0, len(dirSet))
	for d := range dirSet {
		dirs = append(dirs, d)
	}
	dirSet = make(map[string]bool)
	return dirs
}
func CombineHandlers(handlers ...http.HandlerFunc) http.HandlerFunc {
	return func(w http.ResponseWriter, req *http.Request) {
		for _, handler := range handlers {
			handler(w, req)
		}
	}
}
func VerifyContentType(contentType string) http.HandlerFunc {
	return func(w http.ResponseWriter, req *http.Request) {
		Expect(req.Header.Get("Content-Type")).Should(Equal(contentType))
	}
}
func VerifyMimeType(mimeType string) http.HandlerFunc {
	return func(w http.ResponseWriter, req *http.Request) {
		Expect(strings.Split(req.Header.Get("Content-Type"), ";")[0]).Should(Equal(mimeType))
	}
}
func VerifyBasicAuth(username string, password string) http.HandlerFunc {
	return func(w http.ResponseWriter, req *http.Request) {
		auth := req.Header.Get("Authorization")
		Expect(auth).ShouldNot(Equal(""), "Authorization header must be specified")

		decoded, err := base64.StdEncoding.DecodeString(auth[6:])
		Expect(err).ShouldNot(HaveOccurred())

		Expect(string(decoded)).Should(Equal(fmt.Sprintf("%s:%s", username, password)), "Authorization mismatch")
	}
}
func VerifyJSONRepresenting(object interface{}) http.HandlerFunc {
	data, err := json.Marshal(object)
	Expect(err).ShouldNot(HaveOccurred())
	return CombineHandlers(
		VerifyContentType("application/json"),
		VerifyJSON(string(data)),
	)
}
func VerifyFormKV(key string, values ...string) http.HandlerFunc {
	return VerifyForm(url.Values{key: values})
}
func RespondWithProto(statusCode int, message proto.Message, optionalHeader ...http.Header) http.HandlerFunc {
	return func(w http.ResponseWriter, req *http.Request) {
		data, err := proto.Marshal(message)
		Expect(err).ShouldNot(HaveOccurred())

		var headers http.Header
		if len(optionalHeader) == 1 {
			headers = optionalHeader[0]
		} else {
			headers = make(http.Header)
		}
		if _, found := headers["Content-Type"]; !found {
			headers["Content-Type"] = []string{"application/x-protobuf"}
		}
		copyHeader(headers, w.Header())

		w.WriteHeader(statusCode)
		w.Write(data)
	}
}
func RegisterFailHandlerWithT(t types.TWithHelper, handler types.GomegaFailHandler) {
	if handler == nil {
		globalFailWrapper = nil
		return
	}

	globalFailWrapper = &types.GomegaFailWrapper{
		Fail:        handler,
		TWithHelper: t,
	}
}
func EventuallyWithOffset(offset int, actual interface{}, intervals ...interface{}) AsyncAssertion {
	if globalFailWrapper == nil {
		panic(nilFailHandlerPanic)
	}
	timeoutInterval := defaultEventuallyTimeout
	pollingInterval := defaultEventuallyPollingInterval
	if len(intervals) > 0 {
		timeoutInterval = toDuration(intervals[0])
	}
	if len(intervals) > 1 {
		pollingInterval = toDuration(intervals[1])
	}
	return asyncassertion.New(asyncassertion.AsyncAssertionTypeEventually, actual, globalFailWrapper, timeoutInterval, pollingInterval, offset)
}
func ConsistentlyWithOffset(offset int, actual interface{}, intervals ...interface{}) AsyncAssertion {
	if globalFailWrapper == nil {
		panic(nilFailHandlerPanic)
	}
	timeoutInterval := defaultConsistentlyDuration
	pollingInterval := defaultConsistentlyPollingInterval
	if len(intervals) > 0 {
		timeoutInterval = toDuration(intervals[0])
	}
	if len(intervals) > 1 {
		pollingInterval = toDuration(intervals[1])
	}
	return asyncassertion.New(asyncassertion.AsyncAssertionTypeConsistently, actual, globalFailWrapper, timeoutInterval, pollingInterval, offset)
}
func (g *WithT) Expect(actual interface{}, extra ...interface{}) Assertion {
	return assertion.New(actual, testingtsupport.BuildTestingTGomegaFailWrapper(g.t), 0, extra...)
}
func (g *WithT) Eventually(actual interface{}, intervals ...interface{}) AsyncAssertion {
	timeoutInterval := defaultEventuallyTimeout
	pollingInterval := defaultEventuallyPollingInterval
	if len(intervals) > 0 {
		timeoutInterval = toDuration(intervals[0])
	}
	if len(intervals) > 1 {
		pollingInterval = toDuration(intervals[1])
	}
	return asyncassertion.New(asyncassertion.AsyncAssertionTypeEventually, actual, testingtsupport.BuildTestingTGomegaFailWrapper(g.t), timeoutInterval, pollingInterval, 0)
}
func (g *WithT) Consistently(actual interface{}, intervals ...interface{}) AsyncAssertion {
	timeoutInterval := defaultConsistentlyDuration
	pollingInterval := defaultConsistentlyPollingInterval
	if len(intervals) > 0 {
		timeoutInterval = toDuration(intervals[0])
	}
	if len(intervals) > 1 {
		pollingInterval = toDuration(intervals[1])
	}
	return asyncassertion.New(asyncassertion.AsyncAssertionTypeConsistently, actual, testingtsupport.BuildTestingTGomegaFailWrapper(g.t), timeoutInterval, pollingInterval, 0)
}
func TimeoutCloser(c io.Closer, timeout time.Duration) io.Closer {
	return timeoutReaderWriterCloser{c: c, d: timeout}
}
func TimeoutReader(r io.Reader, timeout time.Duration) io.Reader {
	return timeoutReaderWriterCloser{r: r, d: timeout}
}
func TimeoutWriter(w io.Writer, timeout time.Duration) io.Writer {
	return timeoutReaderWriterCloser{w: w, d: timeout}
}
func Nest(path string, err error) error {
	if ag, ok := err.(AggregateError); ok {
		var errs AggregateError
		for _, e := range ag {
			errs = append(errs, Nest(path, e))
		}
		return errs
	}
	if ne, ok := err.(*NestedError); ok {
		return &NestedError{
			Path: path + ne.Path,
			Err:  ne.Err,
		}
	}
	return &NestedError{
		Path: path,
		Err:  err,
	}
}
func SetMockService(m *MockService) {
	m.Cache = &cache.MockAppCacheService{}
	m.Plan = &app.MockPlanService{}
	m.Platform = &app.MockPlatformService{}
	m.PlatformImage = &image.MockPlatformImageService{}
	m.Team = &auth.MockTeamService{}
	m.UserQuota = &quota.MockQuotaService{}
	m.AppQuota = &quota.MockQuotaService{}
	m.Cluster = &provision.MockClusterService{}
	m.ServiceBroker = &service.MockServiceBrokerService{}
	m.ServiceBrokerCatalogCache = &service.MockServiceBrokerCatalogCacheService{}
	servicemanager.AppCache = m.Cache
	servicemanager.Plan = m.Plan
	servicemanager.Platform = m.Platform
	servicemanager.PlatformImage = m.PlatformImage
	servicemanager.Team = m.Team
	servicemanager.UserQuota = m.UserQuota
	servicemanager.AppQuota = m.AppQuota
	servicemanager.Cluster = m.Cluster
	servicemanager.ServiceBroker = m.ServiceBroker
	servicemanager.ServiceBrokerCatalogCache = m.ServiceBrokerCatalogCache
}
func FindMachineByIdOrAddress(id string, address string) (Machine, error) {
	coll, err := collection()
	if err != nil {
		return Machine{}, err
	}
	defer coll.Close()
	var result Machine
	query := bson.M{}
	if id != "" {
		query["_id"] = id
	} else {
		query["address"] = address
	}
	err = coll.Find(query).One(&result)
	if err == mgo.ErrNotFound {
		err = ErrMachineNotFound
	}
	return result, err
}
func (w *FlushingWriter) Write(data []byte) (written int, err error) {
	w.writeMutex.Lock()
	defer w.writeMutex.Unlock()
	w.wrote = true
	written, err = w.ResponseWriter.Write(data)
	if err != nil {
		return
	}
	if f, ok := w.ResponseWriter.(http.Flusher); ok {
		defer func() {
			if r := recover(); r != nil {
				msg := fmt.Sprintf("Error recovered on flushing writer: %#v", r)
				log.Debugf(msg)
				err = errors.Errorf(msg)
			}
		}()
		f.Flush()
	}
	return
}
func (w *FlushingWriter) Hijack() (net.Conn, *bufio.ReadWriter, error) {
	if hijacker, ok := w.ResponseWriter.(http.Hijacker); ok {
		return hijacker.Hijack()
	}
	return nil, nil, errors.New("cannot hijack connection")
}
func ListDeploys(filter *Filter, skip, limit int) ([]DeployData, error) {
	appsList, err := List(filter)
	if err != nil {
		return nil, err
	}
	apps := make([]string, len(appsList))
	for i, a := range appsList {
		apps[i] = a.GetName()
	}
	evts, err := event.List(&event.Filter{
		Target:    event.Target{Type: event.TargetTypeApp},
		Raw:       bson.M{"target.value": bson.M{"$in": apps}},
		KindNames: []string{permission.PermAppDeploy.FullName()},
		KindType:  event.KindTypePermission,
		Limit:     limit,
		Skip:      skip,
	})
	if err != nil {
		return nil, err
	}
	validImages, err := findValidImages(appsList...)
	if err != nil {
		return nil, err
	}
	list := make([]DeployData, len(evts))
	for i := range evts {
		list[i] = *eventToDeployData(&evts[i], validImages, false)
	}
	return list, nil
}
func (r *DelayedRouter) AddAll(version, path string, h http.Handler) *mux.Route {
	return r.addRoute(version, path, h, "GET", "POST", "PUT", "DELETE")
}
func samlRequestTimeout(schemeData map[string]string) int {
	p := schemeData["request_timeout"]
	timeout, _ := strconv.Atoi(p)
	return timeout
}
func RegisterHandler(path string, method string, h http.Handler) {
	RegisterHandlerVersion("1.0", path, method, h)
}
func RegisterHandlerVersion(version, path, method string, h http.Handler) {
	var th TsuruHandler
	th.version = version
	th.path = path
	th.method = method
	th.h = h
	tsuruHandlerList = append(tsuruHandlerList, th)
}
func Check(names ...string) []Result {
	results := make([]Result, 0, len(checkers))
	nameSet := set.FromSlice(names)
	isAll := nameSet.Includes("all")
	for _, checker := range checkers {
		if !isAll && !nameSet.Includes(checker.name) {
			continue
		}
		startTime := time.Now()
		if err := checker.check(); err != nil && err != ErrDisabledComponent {
			results = append(results, Result{
				Name:     checker.name,
				Status:   "fail - " + err.Error(),
				Duration: time.Since(startTime),
			})
		} else if err == nil {
			results = append(results, Result{
				Name:     checker.name,
				Status:   HealthCheckOK,
				Duration: time.Since(startTime),
			})
		}
	}
	return results
}
func DiscoverRepositoryPath(dir string) (string, error) {
	_, err := os.Stat(dir)
	if os.IsNotExist(err) {
		return "", ErrRepositoryNotFound
	}
	dir = filepath.Join(dir, ".git")
	for dir != "/.git" {
		fi, err := os.Stat(dir)
		if err == nil && fi.IsDir() {
			return dir, nil
		}
		dir = filepath.Join(dir, "..", "..", ".git")
	}
	return "", ErrRepositoryNotFound
}
func OpenRepository(p string) (*Repository, error) {
	if !strings.HasSuffix(p, ".git") && !strings.HasSuffix(p, ".git/") {
		p = filepath.Join(p, ".git")
	}
	p = strings.TrimRight(p, "/")
	fi, err := os.Stat(filepath.Join(p, "config"))
	if err == nil && !fi.IsDir() {
		return &Repository{path: p}, nil
	}
	return nil, ErrRepositoryNotFound
}
func (r *Repository) RemoteURL(name string) (string, error) {
	config, err := os.Open(filepath.Join(r.path, "config"))
	if err != nil {
		return "", err
	}
	defer config.Close()
	line := fmt.Sprintf("[remote %q]", name)
	scanner := bufio.NewScanner(config)
	scanner.Split(bufio.ScanLines)
	for scanner.Scan() {
		if scanner.Text() == line {
			scanner.Scan()
			return strings.Split(scanner.Text(), " = ")[1], nil
		}
	}
	return "", errRemoteNotFound{name}
}
func (s *appLister) List(selector labels.Selector) (ret []*v1.App, err error) {
	err = cache.ListAll(s.indexer, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.App))
	})
	return ret, err
}
func (s *appLister) Apps(namespace string) AppNamespaceLister {
	return appNamespaceLister{indexer: s.indexer, namespace: namespace}
}
func (s appNamespaceLister) List(selector labels.Selector) (ret []*v1.App, err error) {
	err = cache.ListAllByNamespace(s.indexer, s.namespace, selector, func(m interface{}) {
		ret = append(ret, m.(*v1.App))
	})
	return ret, err
}
func StreamJSONResponse(w io.Writer, response *http.Response) error {
	if response == nil {
		return errors.New("response cannot be nil")
	}
	defer response.Body.Close()
	var err error
	output := tsuruio.NewStreamWriter(w, nil)
	for n := int64(1); n > 0 && err == nil; n, err = io.Copy(output, response.Body) {
	}
	if err != nil {
		return err
	}
	unparsed := output.Remaining()
	if len(unparsed) > 0 {
		return errors.Errorf("unparsed message error: %s", string(unparsed))
	}
	return nil
}
func (s *Storage) DropDatabase(name string) error {
	return s.session.DB(name).DropDatabase()
}
func (s *Storage) Collection(name string) *Collection {
	return &Collection{Collection: s.session.DB(s.dbname).C(name)}
}
func (t *Target) SetLogger(l Logger) {
	t.mut.Lock()
	defer t.mut.Unlock()
	t.logger = l
}
func (t *Target) Error(v error) {
	t.mut.RLock()
	defer t.mut.RUnlock()
	if t.logger != nil {
		t.logger.Errorf("%+v", v)
	}
}
func (t *Target) Errorf(format string, v ...interface{}) {
	t.mut.RLock()
	defer t.mut.RUnlock()
	if t.logger != nil {
		t.logger.Errorf(format, v...)
		for _, item := range v {
			if _, hasStack := item.(withStack); hasStack {
				t.logger.Errorf("stack for error: %+v", item)
			}
		}
	}
}
func (t *Target) Fatal(v string) {
	t.mut.RLock()
	defer t.mut.RUnlock()
	if t.logger != nil {
		t.logger.Fatal(v)
	}
}
func (t *Target) Debugf(format string, v ...interface{}) {
	t.mut.RLock()
	defer t.mut.RUnlock()
	if t.logger != nil {
		t.logger.Debugf(format, v...)
	}
}
func (t *Target) GetStdLogger() *log.Logger {
	t.mut.RLock()
	defer t.mut.RUnlock()
	if t.logger != nil {
		return t.logger.GetStdLogger()
	}
	return nil
}
func (c *ClusterClient) Namespace() string {
	if c.CustomData != nil && c.CustomData[namespaceClusterKey] != "" {
		return c.CustomData[namespaceClusterKey]
	}
	return "tsuru"
}
func recreateContainers(p DockerProvisioner, w io.Writer, nodes ...cluster.Node) error {
	return ensureContainersStarted(p, w, true, nil, nodes...)
}
func checkProvisioner() error {
	if value, _ := config.Get("provisioner"); value == defaultProvisionerName || value == "" {
		return checkDocker()
	}
	return nil
}
func checkDocker() error {
	if _, err := config.Get("docker"); err != nil {
		return errors.New("Config error: you should configure docker.")
	}
	err := checkDockerBasicConfig()
	if err != nil {
		return err
	}
	err = checkScheduler()
	if err != nil {
		return err
	}
	err = checkRouter()
	if err != nil {
		return err
	}
	return checkCluster()
}
func checkScheduler() error {
	if servers, err := config.Get("docker:servers"); err == nil && servers != nil {
		return errors.Errorf(`Using docker:servers is deprecated, please remove it your config and use "tsuru docker-node-add" do add docker nodes.`)
	}
	isSegregate, err := config.GetBool("docker:segregate")
	if err == nil {
		if isSegregate {
			return config.NewWarning(`Setting "docker:segregate" is not necessary anymore, this is the default behavior from now on.`)
		} else {
			return errors.Errorf(`You must remove "docker:segregate" from your config.`)
		}
	}
	return nil
}
func checkRouter() error {
	defaultRouter, _ := config.GetString("docker:router")
	if defaultRouter == "" {
		return errors.Errorf(`You must configure a default router in "docker:router".`)
	}
	isHipacheOld := false
	if defaultRouter == "hipache" {
		hipacheOld, _ := config.Get("hipache")
		isHipacheOld = hipacheOld != nil
	}
	routerConf, _ := config.Get("routers:" + defaultRouter)
	if isHipacheOld {
		return config.NewWarning(`Setting "hipache:*" config entries is deprecated. You should configure your router with "routers:*". See http://docs.tsuru.io/en/latest/reference/config.html#routers for more details.`)
	}
	if routerConf == nil {
		return errors.Errorf(`You must configure your default router %q in "routers:%s".`, defaultRouter, defaultRouter)
	}
	routerType, _ := config.Get("routers:" + defaultRouter + ":type")
	if routerType == nil {
		return errors.Errorf(`You must configure your default router type in "routers:%s:type".`, defaultRouter)
	}
	return nil
}
func (u *Unit) Available() bool {
	return u.Status == StatusStarted ||
		u.Status == StatusStarting ||
		u.Status == StatusError
}
func Get(name string) (Provisioner, error) {
	pFunc, ok := provisioners[name]
	if !ok {
		return nil, errors.Errorf("unknown provisioner: %q", name)
	}
	return pFunc()
}
func Registry() ([]Provisioner, error) {
	registry := make([]Provisioner, 0, len(provisioners))
	for _, pFunc := range provisioners {
		p, err := pFunc()
		if err != nil {
			return nil, err
		}
		registry = append(registry, p)
	}
	return registry, nil
}
func (e *Error) Error() string {
	var err string
	if e.Err != nil {
		err = e.Err.Error() + ": " + e.Reason
	} else {
		err = e.Reason
	}
	return err
}
func validateVersion(supported, current string) bool {
	if supported == "" {
		return true
	}
	vSupported, err := goVersion.NewVersion(supported)
	if err != nil {
		return false
	}
	vCurrent, err := goVersion.NewVersion(current)
	if err != nil {
		return false
	}
	return vCurrent.Compare(vSupported) >= 0
}
func ReadTarget() (string, error) {
	if target := os.Getenv("TSURU_TARGET"); target != "" {
		targets, err := getTargets()
		if err == nil {
			if val, ok := targets[target]; ok {
				return val, nil
			}
		}
		return target, nil
	}
	targetPath := JoinWithUserDir(".tsuru", "target")
	target, err := readTarget(targetPath)
	if err == errUndefinedTarget {
		copyTargetFiles()
		target, err = readTarget(JoinWithUserDir(".tsuru_target"))
	}
	return target, err
}
func WriteTarget(t string) error {
	targetPath := JoinWithUserDir(".tsuru", "target")
	targetFile, err := filesystem().OpenFile(targetPath, syscall.O_WRONLY|syscall.O_CREAT|syscall.O_TRUNC, 0600)
	if err != nil {
		return err
	}
	defer targetFile.Close()
	n, err := targetFile.WriteString(t)
	if n != len(t) || err != nil {
		return errors.New("Failed to write the target file")
	}
	return nil
}
func WriteOnTargetList(label, target string) error {
	label = strings.TrimSpace(label)
	target = strings.TrimSpace(target)
	targetExist, err := CheckIfTargetLabelExists(label)
	if err != nil {
		return err
	}
	if targetExist {
		return errors.New("Target label provided already exists")
	}
	targetsPath := JoinWithUserDir(".tsuru", "targets")
	targetsFile, err := filesystem().OpenFile(targetsPath, syscall.O_RDWR|syscall.O_CREAT|syscall.O_APPEND, 0600)
	if err != nil {
		return err
	}
	defer targetsFile.Close()
	content := label + "\t" + target + "\n"
	n, err := targetsFile.WriteString(content)
	if n != len(content) || err != nil {
		return errors.New("Failed to write the target file")
	}
	return nil
}
func Conn() (*Storage, error) {
	var (
		strg Storage
		err  error
	)
	url, dbname := DbConfig("")
	strg.Storage, err = storage.Open(url, dbname)
	return &strg, err
}
func (s *Storage) Apps() *storage.Collection {
	nameIndex := mgo.Index{Key: []string{"name"}, Unique: true}
	c := s.Collection("apps")
	c.EnsureIndex(nameIndex)
	return c
}
func (s *Storage) PoolsConstraints() *storage.Collection {
	poolConstraintIndex := mgo.Index{Key: []string{"poolexpr", "field"}, Unique: true}
	c := s.Collection("pool_constraints")
	c.EnsureIndex(poolConstraintIndex)
	return c
}
func (s *Storage) Users() *storage.Collection {
	emailIndex := mgo.Index{Key: []string{"email"}, Unique: true}
	c := s.Collection("users")
	c.EnsureIndex(emailIndex)
	return c
}
func (s *Storage) SAMLRequests() *storage.Collection {
	id := mgo.Index{Key: []string{"id"}}
	coll := s.Collection("saml_requests")
	coll.EnsureIndex(id)
	return coll
}
func (s *LogStorage) AppLogCollection(appName string) *storage.Collection {
	if appName == "" {
		return nil
	}
	return s.Collection("logs_" + appName)
}
func (s *LogStorage) CreateAppLogCollection(appName string) (*storage.Collection, error) {
	c := s.AppLogCollection(appName)
	err := c.Create(&logCappedInfo)
	return c, err
}
func (s *LogStorage) LogsCollections() ([]*storage.Collection, error) {
	var names []struct {
		Name string
	}
	conn, err := Conn()
	if err != nil {
		return nil, err
	}
	defer conn.Close()
	err = conn.Apps().Find(nil).All(&names)
	if err != nil {
		return nil, err
	}
	var colls []*storage.Collection
	for _, name := range names {
		colls = append(colls, s.Collection("logs_"+name.Name))
	}
	return colls, nil
}
func ArchiveBuildCmds(app provision.App, archiveURL string) []string {
	return buildCmds(app, "build", "archive", archiveURL)
}
func ArchiveDeployCmds(app provision.App, archiveURL string) []string {
	return buildCmds(app, "deploy", "archive", archiveURL)
}
func DeployCmds(app provision.App) []string {
	uaCmds := unitAgentCmds(app)
	uaCmds = append(uaCmds, "deploy-only")
	finalCmd := strings.Join(uaCmds, " ")
	return []string{"/bin/sh", "-lc", finalCmd}
}
func runWithAgentCmds(app provision.App) ([]string, error) {
	runCmd, err := config.GetString("docker:run-cmd:bin")
	if err != nil {
		runCmd = "/var/lib/tsuru/start"
	}
	host, _ := config.GetString("host")
	token := app.Envs()["TSURU_APP_TOKEN"].Value
	return []string{"tsuru_unit_agent", host, token, app.GetName(), runCmd}, nil
}
func newApps(c *TsuruV1Client, namespace string) *apps {
	return &apps{
		client: c.RESTClient(),
		ns:     namespace,
	}
}
func (c *Container) Commit(client provision.BuilderDockerClient, limiter provision.ActionLimiter, writer io.Writer, isDeploy bool) (string, error) {
	log.Debugf("committing container %s", c.ID)
	repository, tag := image.SplitImageName(c.BuildingImage)
	opts := docker.CommitContainerOptions{Container: c.ID, Repository: repository, Tag: tag}
	done := limiter.Start(c.HostAddr)
	image, err := client.CommitContainer(opts)
	done()
	if err != nil {
		return "", log.WrapError(errors.Wrapf(err, "error in commit container %s", c.ID))
	}
	tags := []string{tag}
	if isDeploy && tag != "latest" {
		tags = append(tags, "latest")
		err = client.TagImage(fmt.Sprintf("%s:%s", repository, tag), docker.TagImageOptions{
			Repo:  repository,
			Tag:   "latest",
			Force: true,
		})
		if err != nil {
			return "", log.WrapError(errors.Wrapf(err, "error in tag container %s", c.ID))
		}
	}
	imgHistory, err := client.ImageHistory(c.BuildingImage)
	imgSize := ""
	if err == nil && len(imgHistory) > 0 {
		fullSize := imgHistory[0].Size
		if len(imgHistory) > 1 && strings.Contains(imgHistory[1].CreatedBy, "tail -f /dev/null") {
			fullSize += imgHistory[1].Size
		}
		imgSize = fmt.Sprintf("(%.02fMB)", float64(fullSize)/1024/1024)
	}
	fmt.Fprintf(writer, " ---> Sending image to repository %s\n", imgSize)
	log.Debugf("image %s generated from container %s", image.ID, c.ID)
	for _, tag := range tags {
		maxTry, _ := config.GetInt("docker:registry-max-try")
		if maxTry <= 0 {
			maxTry = 3
		}
		for i := 0; i < maxTry; i++ {
			err = dockercommon.PushImage(client, repository, tag, dockercommon.RegistryAuthConfig(repository))
			if err != nil {
				fmt.Fprintf(writer, "Could not send image, trying again. Original error: %s\n", err.Error())
				log.Errorf("error in push image %s: %s", c.BuildingImage, err)
				time.Sleep(time.Second)
				continue
			}
			break
		}
		if err != nil {
			return "", log.WrapError(errors.Wrapf(err, "error in push image %s", c.BuildingImage))
		}
	}
	return c.BuildingImage, nil
}
func processTags(tags []string) []string {
	if tags == nil {
		return nil
	}
	processedTags := []string{}
	usedTags := make(map[string]bool)
	for _, tag := range tags {
		tag = strings.TrimSpace(tag)
		if len(tag) > 0 && !usedTags[tag] {
			processedTags = append(processedTags, tag)
			usedTags[tag] = true
		}
	}
	return processedTags
}
func (s *segregatedScheduler) aggregateContainersBy(matcher bson.M) (map[string]int, error) {
	coll := s.provisioner.Collection()
	defer coll.Close()
	pipe := coll.Pipe([]bson.M{
		matcher,
		{"$group": bson.M{"_id": "$hostaddr", "count": bson.M{"$sum": 1}}},
	})
	var results []nodeAggregate
	err := pipe.All(&results)
	if err != nil {
		return nil, err
	}
	countMap := make(map[string]int)
	for _, result := range results {
		countMap[result.HostAddr] = result.Count
	}
	return countMap, nil
}
func (s *segregatedScheduler) chooseNodeToAdd(nodes []cluster.Node, contName string, appName, process string) (string, error) {
	log.Debugf("[scheduler] Possible nodes for container %s: %#v", contName, nodes)
	s.hostMutex.Lock()
	defer s.hostMutex.Unlock()
	chosenNode, _, err := s.minMaxNodes(nodes, appName, process)
	if err != nil {
		return "", err
	}
	log.Debugf("[scheduler] Chosen node for container %s: %#v", contName, chosenNode)
	if contName != "" {
		coll := s.provisioner.Collection()
		defer coll.Close()
		err = coll.Update(bson.M{"name": contName}, bson.M{"$set": bson.M{"hostaddr": net.URLToHost(chosenNode)}})
	}
	return chosenNode, err
}
func (s *segregatedScheduler) chooseContainerToRemove(nodes []cluster.Node, appName, process string) (string, error) {
	_, chosenNode, err := s.minMaxNodes(nodes, appName, process)
	if err != nil {
		return "", err
	}
	log.Debugf("[scheduler] Chosen node for remove a container: %#v", chosenNode)
	containerID, err := s.getContainerPreferablyFromHost(chosenNode, appName, process)
	if err != nil {
		return "", err
	}
	return containerID, err
}
func Get(name string) (Router, error) {
	routerType, prefix, err := Type(name)
	if err != nil {
		return nil, &ErrRouterNotFound{Name: name}
	}
	factory, ok := routers[routerType]
	if !ok {
		return nil, errors.Errorf("unknown router: %q.", routerType)
	}
	r, err := factory(name, prefix)
	if err != nil {
		return nil, err
	}
	return r, nil
}
func Default() (string, error) {
	plans, err := List()
	if err != nil {
		return "", err
	}
	if len(plans) == 0 {
		return "", ErrDefaultRouterNotFound
	}
	if len(plans) == 1 {
		return plans[0].Name, nil
	}
	for _, p := range plans {
		if p.Default {
			return p.Name, nil
		}
	}
	return "", ErrDefaultRouterNotFound
}
func Store(appName, routerName, kind string) error {
	coll, err := collection()
	if err != nil {
		return err
	}
	defer coll.Close()
	data := routerAppEntry{
		App:    appName,
		Router: routerName,
		Kind:   kind,
	}
	_, err = coll.Upsert(bson.M{"app": appName}, data)
	return err
}
func (c *Clientset) TsuruV1() tsuruv1.TsuruV1Interface {
	return &faketsuruv1.FakeTsuruV1{Fake: &c.Fake}
}
func (c *Clientset) Tsuru() tsuruv1.TsuruV1Interface {
	return &faketsuruv1.FakeTsuruV1{Fake: &c.Fake}
}
func NewAppInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer {
	return NewFilteredAppInformer(client, namespace, resyncPeriod, indexers, nil)
}
func NewFilteredAppInformer(client versioned.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&cache.ListWatch{
			ListFunc: func(options meta_v1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.TsuruV1().Apps(namespace).List(options)
			},
			WatchFunc: func(options meta_v1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.TsuruV1().Apps(namespace).Watch(options)
			},
		},
		&tsuru_v1.App{},
		resyncPeriod,
		indexers,
	)
}
func (c *FakeApps) Watch(opts v1.ListOptions) (watch.Interface, error) {
	return c.Fake.
		InvokesWatch(testing.NewWatchAction(appsResource, c.ns, opts))

}
func (c *FakeApps) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *tsuru_v1.App, err error) {
	obj, err := c.Fake.
		Invokes(testing.NewPatchSubresourceAction(appsResource, c.ns, name, data, subresources...), &tsuru_v1.App{})

	if obj == nil {
		return nil, err
	}
	return obj.(*tsuru_v1.App), err
}
func NewSharedInformerFactory(client versioned.Interface, defaultResync time.Duration) SharedInformerFactory {
	return NewFilteredSharedInformerFactory(client, defaultResync, v1.NamespaceAll, nil)
}
func NewFilteredSharedInformerFactory(client versioned.Interface, defaultResync time.Duration, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) SharedInformerFactory {
	return &sharedInformerFactory{
		client:           client,
		namespace:        namespace,
		tweakListOptions: tweakListOptions,
		defaultResync:    defaultResync,
		informers:        make(map[reflect.Type]cache.SharedIndexInformer),
		startedInformers: make(map[reflect.Type]bool),
	}
}
func (b *brokerClient) Proxy(path string, evt *event.Event, requestID string, w http.ResponseWriter, r *http.Request) error {
	return fmt.Errorf("service proxy is not available for broker services")
}
func (b *brokerClient) UnbindUnit(instance *ServiceInstance, app bind.App, unit bind.Unit) error {
	return nil
}
func (s *planService) Create(plan appTypes.Plan) error {
	if plan.Name == "" {
		return appTypes.PlanValidationError{Field: "name"}
	}
	if plan.CpuShare < 2 {
		return appTypes.ErrLimitOfCpuShare
	}
	if plan.Memory > 0 && plan.Memory < 4194304 {
		return appTypes.ErrLimitOfMemory
	}
	return s.storage.Insert(plan)
}
func (s *planService) Remove(planName string) error {
	return s.storage.Delete(appTypes.Plan{Name: planName})
}
func (s *planService) ensureDefault() error {
	plans, err := s.storage.FindAll()
	if err != nil {
		return err
	}
	if len(plans) > 0 {
		return nil
	}
	configMemory, _ := config.GetInt("docker:memory")
	configSwap, _ := config.GetInt("docker:swap")
	dp := appTypes.Plan{
		Name:     "autogenerated",
		Memory:   int64(configMemory) * 1024 * 1024,
		Swap:     int64(configSwap-configMemory) * 1024 * 1024,
		CpuShare: 100,
		Default:  true,
	}
	return s.storage.Insert(dp)
}
func DeleteInstance(si *ServiceInstance, evt *event.Event, requestID string) error {
	if len(si.Apps) > 0 {
		return ErrServiceInstanceBound
	}
	s, err := Get(si.ServiceName)
	if err != nil {
		return err
	}
	endpoint, err := s.getClient("production")
	if err == nil {
		endpoint.Destroy(si, evt, requestID)
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	return conn.ServiceInstances().Remove(bson.M{"name": si.Name, "service_name": si.ServiceName})
}
func (si *ServiceInstance) ToInfo() (ServiceInstanceWithInfo, error) {
	info, err := si.Info("")
	if err != nil {
		info = nil
	}
	return ServiceInstanceWithInfo{
		Id:          si.Id,
		Name:        si.Name,
		Teams:       si.Teams,
		PlanName:    si.PlanName,
		Apps:        si.Apps,
		ServiceName: si.ServiceName,
		Info:        info,
		TeamOwner:   si.TeamOwner,
	}, nil
}
func (si *ServiceInstance) Update(service Service, updateData ServiceInstance, evt *event.Event, requestID string) error {
	err := validateServiceInstanceTeamOwner(updateData)
	if err != nil {
		return err
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	tags := processTags(updateData.Tags)
	if tags == nil {
		updateData.Tags = si.Tags
	} else {
		updateData.Tags = tags
	}
	actions := []*action.Action{&updateServiceInstance, &notifyUpdateServiceInstance}
	pipeline := action.NewPipeline(actions...)
	return pipeline.Execute(service, *si, updateData, evt, requestID)
}
func (si *ServiceInstance) BindApp(app bind.App, params BindAppParameters, shouldRestart bool, writer io.Writer, evt *event.Event, requestID string) error {
	args := bindPipelineArgs{
		serviceInstance: si,
		app:             app,
		writer:          writer,
		shouldRestart:   shouldRestart,
		params:          params,
		event:           evt,
		requestID:       requestID,
	}
	actions := []*action.Action{
		bindAppDBAction,
		bindAppEndpointAction,
		setBoundEnvsAction,
		bindUnitsAction,
	}
	pipeline := action.NewPipeline(actions...)
	return pipeline.Execute(&args)
}
func (si *ServiceInstance) BindUnit(app bind.App, unit bind.Unit) error {
	s, err := Get(si.ServiceName)
	if err != nil {
		return err
	}
	endpoint, err := s.getClient("production")
	if err != nil {
		return err
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	updateOp := bson.M{
		"$addToSet": bson.M{
			"bound_units": bson.D([]bson.DocElem{
				{Name: "appname", Value: app.GetName()},
				{Name: "id", Value: unit.GetID()},
				{Name: "ip", Value: unit.GetIp()},
			}),
		},
	}
	err = conn.ServiceInstances().Update(bson.M{"name": si.Name, "service_name": si.ServiceName, "bound_units.id": bson.M{"$ne": unit.GetID()}}, updateOp)
	conn.Close()
	if err != nil {
		if err == mgo.ErrNotFound {
			return nil
		}
		return err
	}
	err = endpoint.BindUnit(si, app, unit)
	if err != nil {
		updateOp = bson.M{
			"$pull": bson.M{
				"bound_units": bson.D([]bson.DocElem{
					{Name: "appname", Value: app.GetName()},
					{Name: "id", Value: unit.GetID()},
					{Name: "ip", Value: unit.GetIp()},
				}),
			},
		}
		rollbackErr := si.updateData(updateOp)
		if rollbackErr != nil {
			log.Errorf("[bind unit] could not remove stil unbound unit from db after failure: %s", rollbackErr)
		}
		return err
	}
	return nil
}
func (si *ServiceInstance) UnbindApp(unbindArgs UnbindAppArgs) error {
	if si.FindApp(unbindArgs.App.GetName()) == -1 {
		return ErrAppNotBound
	}
	args := bindPipelineArgs{
		serviceInstance: si,
		app:             unbindArgs.App,
		writer:          unbindArgs.Event,
		shouldRestart:   unbindArgs.Restart,
		event:           unbindArgs.Event,
		requestID:       unbindArgs.RequestID,
		forceRemove:     unbindArgs.ForceRemove,
	}
	actions := []*action.Action{
		&unbindUnits,
		&unbindAppDB,
		&unbindAppEndpoint,
		&removeBoundEnvs,
	}
	pipeline := action.NewPipeline(actions...)
	return pipeline.Execute(&args)
}
func (si *ServiceInstance) Status(requestID string) (string, error) {
	s, err := Get(si.ServiceName)
	if err != nil {
		return "", err
	}
	endpoint, err := s.getClient("production")
	if err != nil {
		return "", err
	}
	return endpoint.Status(si, requestID)
}
func ProxyInstance(instance *ServiceInstance, path string, evt *event.Event, requestID string, w http.ResponseWriter, r *http.Request) error {
	service, err := Get(instance.ServiceName)
	if err != nil {
		return err
	}
	endpoint, err := service.getClient("production")
	if err != nil {
		return err
	}
	prefix := fmt.Sprintf("/resources/%s/", instance.GetIdentifier())
	path = strings.Trim(strings.TrimPrefix(path+"/", prefix), "/")
	for _, reserved := range reservedProxyPaths {
		if path == reserved && r.Method != "GET" {
			return &tsuruErrors.ValidationError{
				Message: fmt.Sprintf("proxy request %s %q is forbidden", r.Method, path),
			}
		}
	}
	return endpoint.Proxy(fmt.Sprintf("%s%s", prefix, path), evt, requestID, w, r)
}
func (s *QuotaService) Inc(appName string, quantity int) error {
	quota, err := s.Storage.Get(appName)
	if err != nil {
		return err
	}
	err = s.checkLimit(quota, quantity)
	if err != nil {
		return err
	}
	return s.Storage.Inc(appName, quantity)
}
func (s *QuotaService) SetLimit(appName string, limit int) error {
	q, err := s.Storage.Get(appName)
	if err != nil {
		return err
	}
	if limit < 0 {
		limit = -1
	} else if limit < q.InUse {
		return quota.ErrLimitLowerThanAllocated
	}
	return s.Storage.SetLimit(appName, limit)
}
func (s *QuotaService) Set(appName string, inUse int) error {
	q, err := s.Storage.Get(appName)
	if err != nil {
		return err
	}
	if inUse < 0 {
		return quota.ErrLessThanZero
	}
	if !q.IsUnlimited() && inUse > q.Limit {
		return &quota.QuotaExceededError{
			Requested: uint(inUse),
			Available: uint(q.Limit),
		}
	}
	return s.Storage.Set(appName, inUse)
}
func (s *QuotaService) Get(appName string) (*quota.Quota, error) {
	return s.Storage.Get(appName)
}
func RemoveImage(imageName string) error {
	registry, image, tag := parseImage(imageName)
	if registry == "" {
		registry, _ = config.GetString("docker:registry")
	}
	if registry == "" {
		// Nothing to do if no registry is set
		return nil
	}
	if image == "" {
		return errors.Errorf("empty image after parsing %q", imageName)
	}
	r := &dockerRegistry{server: registry}
	digest, err := r.getDigest(image, tag)
	if err != nil {
		return errors.Wrapf(err, "failed to get digest for image %s/%s:%s on registry", r.server, image, tag)
	}
	err = r.removeImage(image, digest)
	if err != nil {
		return errors.Wrapf(err, "failed to remove image %s/%s:%s/%s on registry", r.server, image, tag, digest)
	}
	return nil
}
func RemoveAppImages(appName string) error {
	registry, _ := config.GetString("docker:registry")
	if registry == "" {
		// Nothing to do if no registry is set
		return nil
	}
	r := &dockerRegistry{server: registry}
	image := fmt.Sprintf("tsuru/app-%s", appName)
	tags, err := r.getImageTags(image)
	if err != nil {
		return err
	}
	multi := tsuruErrors.NewMultiError()
	for _, tag := range tags {
		digest, err := r.getDigest(image, tag)
		if err != nil {
			multi.Add(errors.Wrapf(err, "failed to get digest for image %s/%s:%s on registry", r.server, image, tag))
			continue
		}
		err = r.removeImage(image, digest)
		if err != nil {
			multi.Add(errors.Wrapf(err, "failed to remove image %s/%s:%s/%s on registry", r.server, image, tag, digest))
			if errors.Cause(err) == ErrDeleteDisabled {
				break
			}
		}
	}
	return multi.ToError()
}
func (s *platformService) Create(opts appTypes.PlatformOptions) error {
	p := appTypes.Platform{Name: opts.Name}
	if err := s.validate(p); err != nil {
		return err
	}
	err := s.storage.Insert(p)
	if err != nil {
		return err
	}
	opts.ImageName, err = servicemanager.PlatformImage.NewImage(opts.Name)
	if err != nil {
		return err
	}
	err = builder.PlatformAdd(opts)
	if err != nil {
		if imgErr := servicemanager.PlatformImage.DeleteImages(opts.Name); imgErr != nil {
			log.Errorf("unable to remove platform images: %s", imgErr)
		}
		dbErr := s.storage.Delete(p)
		if dbErr != nil {
			return tsuruErrors.NewMultiError(
				errors.Wrapf(dbErr, "unable to rollback platform add"),
				errors.Wrapf(err, "original platform add error"),
			)
		}
		return err
	}
	return servicemanager.PlatformImage.AppendImage(opts.Name, opts.ImageName)
}
func (s *platformService) List(enabledOnly bool) ([]appTypes.Platform, error) {
	if enabledOnly {
		return s.storage.FindEnabled()
	}
	return s.storage.FindAll()
}
func (s *platformService) FindByName(name string) (*appTypes.Platform, error) {
	p, err := s.storage.FindByName(name)
	if err != nil {
		return nil, appTypes.ErrInvalidPlatform
	}
	return p, nil
}
func (s *platformService) Update(opts appTypes.PlatformOptions) error {
	if opts.Name == "" {
		return appTypes.ErrPlatformNameMissing
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	_, err = s.FindByName(opts.Name)
	if err != nil {
		return err
	}
	if opts.Input != nil {
		data, err := ioutil.ReadAll(opts.Input)
		if err != nil {
			return err
		}
		if len(data) == 0 {
			return appTypes.ErrMissingFileContent
		}
		opts.Data = data
		opts.ImageName, err = servicemanager.PlatformImage.NewImage(opts.Name)
		if err != nil {
			return err
		}
		err = builder.PlatformUpdate(opts)
		if err != nil {
			return err
		}
		err = servicemanager.PlatformImage.AppendImage(opts.Name, opts.ImageName)
		if err != nil {
			return err
		}
		var apps []App
		err = conn.Apps().Find(bson.M{"framework": opts.Name}).All(&apps)
		if err != nil {
			return err
		}
		for _, app := range apps {
			app.SetUpdatePlatform(true)
		}
	}
	if opts.Args["disabled"] != "" {
		disableBool, err := strconv.ParseBool(opts.Args["disabled"])
		if err != nil {
			return err
		}
		return s.storage.Update(appTypes.Platform{Name: opts.Name, Disabled: disableBool})
	}
	return nil
}
func (s *platformService) Remove(name string) error {
	if name == "" {
		return appTypes.ErrPlatformNameMissing
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	apps, _ := conn.Apps().Find(bson.M{"framework": name}).Count()
	if apps > 0 {
		return appTypes.ErrDeletePlatformWithApps
	}
	err = builder.PlatformRemove(name)
	if err != nil {
		log.Errorf("Failed to remove platform from builder: %s", err)
	}
	images, err := servicemanager.PlatformImage.ListImagesOrDefault(name)
	if err == nil {
		for _, img := range images {
			if regErr := registry.RemoveImage(img); regErr != nil {
				log.Errorf("Failed to remove platform image from registry: %s", regErr)
			}
		}
	} else {
		log.Errorf("Failed to retrieve platform images from storage: %s", err)
	}
	err = servicemanager.PlatformImage.DeleteImages(name)
	if err != nil {
		log.Errorf("Failed to remove platform images from storage: %s", err)
	}
	return s.storage.Delete(appTypes.Platform{Name: name})
}
func (s *platformService) Rollback(opts appTypes.PlatformOptions) error {
	if opts.Name == "" {
		return appTypes.ErrPlatformNameMissing
	}
	if opts.ImageName == "" {
		return appTypes.ErrPlatformImageMissing
	}
	_, err := s.FindByName(opts.Name)
	if err != nil {
		return err
	}
	image, err := servicemanager.PlatformImage.FindImage(opts.Name, opts.ImageName)
	if err != nil {
		return err
	}
	if image == "" {
		return fmt.Errorf("Image %s not found in platform %q", opts.ImageName, opts.Name)
	}
	opts.Data = []byte("FROM " + image)
	opts.ImageName, err = servicemanager.PlatformImage.NewImage(opts.Name)
	if err != nil {
		return err
	}
	err = builder.PlatformUpdate(opts)
	if err != nil {
		return err
	}
	err = servicemanager.PlatformImage.AppendImage(opts.Name, opts.ImageName)
	if err != nil {
		return err
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	var apps []App
	err = conn.Apps().Find(bson.M{"framework": opts.Name}).All(&apps)
	if err != nil {
		return err
	}
	for _, app := range apps {
		app.SetUpdatePlatform(true)
	}
	return nil
}
func GetPoolByName(name string) (*Pool, error) {
	conn, err := db.Conn()
	if err != nil {
		return nil, err
	}
	defer conn.Close()
	var p Pool
	err = conn.Pools().FindId(name).One(&p)
	if err != nil {
		if err == mgo.ErrNotFound {
			return nil, ErrPoolNotFound
		}
		return nil, err
	}
	return &p, nil
}
func Manager() RepositoryManager {
	managerName, err := config.GetString("repo-manager")
	if err != nil {
		managerName = defaultManager
	}
	if _, ok := managers[managerName]; !ok {
		managerName = "nop"
	}
	return managers[managerName]
}
func Register(name string, manager RepositoryManager) {
	if managers == nil {
		managers = make(map[string]RepositoryManager)
	}
	managers[name] = manager
}
func (b *bindSyncer) start() error {
	if b.started {
		return errors.New("syncer already started")
	}
	if b.appLister == nil {
		return errors.New("must set app lister function")
	}
	if b.interval == 0 {
		b.interval = 5 * time.Minute
	}
	b.shutdown = make(chan struct{}, 1)
	b.done = make(chan struct{})
	b.started = true
	log.Debugf("[bind-syncer] starting. Running every %s.\n", b.interval)
	go func(d time.Duration) {
		for {
			select {
			case <-time.After(d):
				start := time.Now()
				log.Debug("[bind-syncer] starting run")
				apps, err := b.appLister()
				if err != nil {
					log.Errorf("[bind-syncer] error listing apps: %v. Aborting sync.", err)
					syncDuration.Set(time.Since(start).Seconds())
					break
				}
				for _, a := range apps {
					err = b.sync(a)
					if err != nil {
						log.Errorf("[bind-syncer] error syncing app %q: %v", a.GetName(), err)
					}
					if len(b.shutdown) > 0 {
						break
					}
				}
				log.Debugf("[bind-syncer] finished running. Synced %d apps.", len(apps))
				d = b.interval
				syncDuration.Set(time.Since(start).Seconds())
			case <-b.shutdown:
				b.done <- struct{}{}
				return
			}
		}
	}(time.Millisecond * 100)
	return nil
}
func (b *bindSyncer) Shutdown(ctx context.Context) error {
	if !b.started {
		return nil
	}
	b.shutdown <- struct{}{}
	select {
	case <-b.done:
	case <-ctx.Done():
	}
	b.started = false
	return ctx.Err()
}
func GetForProvisioner(p provision.Provisioner) (Builder, error) {
	builder, err := get(p.GetName())
	if err != nil {
		if _, ok := p.(provision.BuilderDeployDockerClient); ok {
			return get("docker")
		} else if _, ok := p.(provision.BuilderDeployKubeClient); ok {
			return get("kubernetes")
		}
	}
	return builder, err
}
func get(name string) (Builder, error) {
	b, ok := builders[name]
	if !ok {
		return nil, errors.Errorf("unknown builder: %q", name)
	}
	return b, nil
}
func Registry() ([]Builder, error) {
	registry := make([]Builder, 0, len(builders))
	for _, b := range builders {
		registry = append(registry, b)
	}
	return registry, nil
}
func RegisterQueueTask(p DockerProvisioner) error {
	q, err := queue.Queue()
	if err != nil {
		return err
	}
	return q.RegisterTask(&runBs{provisioner: p})
}
func (v *version) Apps() AppInformer {
	return &appInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}
}
func (in *App) DeepCopy() *App {
	if in == nil {
		return nil
	}
	out := new(App)
	in.DeepCopyInto(out)
	return out
}
func (in *AppList) DeepCopy() *AppList {
	if in == nil {
		return nil
	}
	out := new(AppList)
	in.DeepCopyInto(out)
	return out
}
func (in *AppSpec) DeepCopy() *AppSpec {
	if in == nil {
		return nil
	}
	out := new(AppSpec)
	in.DeepCopyInto(out)
	return out
}
func (w *LogWriter) Write(data []byte) (int, error) {
	w.finLk.RLock()
	defer w.finLk.RUnlock()
	if w.closed {
		return len(data), nil
	}
	if w.msgCh == nil {
		return len(data), w.write(data)
	}
	copied := make([]byte, len(data))
	copy(copied, data)
	w.msgCh <- copied
	return len(data), nil
}
func (s NativeScheme) ResetPassword(user *auth.User, resetToken string) error {
	if resetToken == "" {
		return auth.ErrInvalidToken
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	passToken, err := getPasswordToken(resetToken)
	if err != nil {
		return err
	}
	if passToken.UserEmail != user.Email {
		return auth.ErrInvalidToken
	}
	password := generatePassword(12)
	user.Password = password
	hashPassword(user)
	go sendNewPassword(user, password)
	passToken.Used = true
	conn.PasswordTokens().UpdateId(passToken.Token, passToken)
	return user.Update()
}
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&App{},
		&AppList{},
	)

	scheme.AddKnownTypes(SchemeGroupVersion,
		&metav1.Status{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
func Register(s Shutdownable) {
	lock.Lock()
	defer lock.Unlock()
	registered = append(registered, s)
}
func Do(ctx context.Context, w io.Writer) error {
	lock.Lock()
	defer lock.Unlock()
	done := make(chan bool)
	wg := sync.WaitGroup{}
	for _, h := range registered {
		wg.Add(1)
		go func(h Shutdownable) {
			defer wg.Done()
			var name string
			if _, ok := h.(fmt.Stringer); ok {
				name = fmt.Sprintf("%s", h)
			} else {
				name = fmt.Sprintf("%T", h)
			}
			fmt.Fprintf(w, "[shutdown] running shutdown for %s...\n", name)
			err := h.Shutdown(ctx)
			if err != nil {
				fmt.Fprintf(w, "[shutdown] running shutdown for %s. ERROED: %v", name, err)
				return
			}
			fmt.Fprintf(w, "[shutdown] running shutdown for %s. DONE.\n", name)
		}(h)
	}
	go func() {
		wg.Wait()
		close(done)
	}()
	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-done:
	}
	return nil
}
func (s *platformImageService) ListImagesOrDefault(platformName string) ([]string, error) {
	imgs, err := s.ListImages(platformName)
	if err != nil && err == imageTypes.ErrPlatformImageNotFound {
		return []string{platformBasicImageName(platformName)}, nil
	}
	return imgs, err
}
func MigrateAppsCRDs() error {
	config.Set("kubernetes:use-pool-namespaces", false)
	defer config.Unset("kubernetes:use-pool-namespaces")
	prov := kubernetes.GetProvisioner()
	pools, err := pool.ListAllPools()
	if err != nil {
		return errors.Wrap(err, "failed to list pools")
	}
	var kubePools []string
	for _, p := range pools {
		if p.Provisioner == prov.GetName() {
			kubePools = append(kubePools, p.Name)
		}
	}
	apps, err := app.List(&app.Filter{Pools: kubePools})
	if err != nil {
		return errors.Wrap(err, "failed to list apps")
	}
	multiErr := tsuruerrors.NewMultiError()
	for _, a := range apps {
		errProv := prov.Provision(&a)
		if errProv != nil {
			multiErr.Add(errProv)
		}
	}
	return multiErr.ToError()
}
func Register(name string, fn MigrateFunc) error {
	return register(name, false, fn)
}
func RegisterOptional(name string, fn MigrateFunc) error {
	return register(name, true, fn)
}
func Run(args RunArgs) error {
	if args.Name != "" {
		return runOptional(args)
	}
	if args.Force {
		return ErrCannotForceMandatory
	}
	return run(args)
}
func (app *App) Units() ([]provision.Unit, error) {
	prov, err := app.getProvisioner()
	if err != nil {
		return []provision.Unit{}, err
	}
	units, err := prov.Units(app)
	if units == nil {
		// This is unusual but was done because previously this method didn't
		// return an error. This ensures we always return an empty list instead
		// of nil to preserve compatibility with old clients.
		units = []provision.Unit{}
	}
	return units, err
}
func (app *App) MarshalJSON() ([]byte, error) {
	repo, _ := repository.Manager().GetRepository(app.Name)
	result := make(map[string]interface{})
	result["name"] = app.Name
	result["platform"] = app.Platform
	if version := app.GetPlatformVersion(); version != "latest" {
		result["platform"] = fmt.Sprintf("%s:%s", app.Platform, version)
	}
	result["teams"] = app.Teams
	units, err := app.Units()
	result["units"] = units
	var errMsgs []string
	if err != nil {
		errMsgs = append(errMsgs, fmt.Sprintf("unable to list app units: %+v", err))
	}
	result["repository"] = repo.ReadWriteURL
	plan := map[string]interface{}{
		"name":     app.Plan.Name,
		"memory":   app.Plan.Memory,
		"swap":     app.Plan.Swap,
		"cpushare": app.Plan.CpuShare,
	}
	routers, err := app.GetRoutersWithAddr()
	if err != nil {
		errMsgs = append(errMsgs, fmt.Sprintf("unable to get app addresses: %+v", err))
	}
	if len(routers) > 0 {
		result["ip"] = routers[0].Address
		plan["router"] = routers[0].Name
		result["router"] = routers[0].Name
		result["routeropts"] = routers[0].Opts
	}
	result["cname"] = app.CName
	result["owner"] = app.Owner
	result["pool"] = app.Pool
	result["description"] = app.Description
	result["deploys"] = app.Deploys
	result["teamowner"] = app.TeamOwner
	result["plan"] = plan
	result["lock"] = app.Lock
	result["tags"] = app.Tags
	result["routers"] = routers
	if len(errMsgs) > 0 {
		result["error"] = strings.Join(errMsgs, "\n")
	}
	return json.Marshal(&result)
}
func AcquireApplicationLockWait(appName string, owner string, reason string, timeout time.Duration) (bool, error) {
	timeoutChan := time.After(timeout)
	for {
		appLock := appTypes.AppLock{
			Locked:      true,
			Reason:      reason,
			Owner:       owner,
			AcquireDate: time.Now().In(time.UTC),
		}
		conn, err := db.Conn()
		if err != nil {
			return false, err
		}
		err = conn.Apps().Update(bson.M{"name": appName, "lock.locked": bson.M{"$in": []interface{}{false, nil}}}, bson.M{"$set": bson.M{"lock": appLock}})
		conn.Close()
		if err == nil {
			return true, nil
		}
		if err != mgo.ErrNotFound {
			return false, err
		}
		select {
		case <-timeoutChan:
			return false, nil
		case <-time.After(300 * time.Millisecond):
		}
	}
}
func ReleaseApplicationLock(appName string) {
	var err error
	retries := 3
	for i := 0; i < retries; i++ {
		err = releaseApplicationLockOnce(appName)
		if err == nil {
			return
		}
		time.Sleep(time.Second * time.Duration(i+1))
	}
	log.Error(err)
}
func GetByName(name string) (*App, error) {
	var app App
	conn, err := db.Conn()
	if err != nil {
		return nil, err
	}
	defer conn.Close()
	err = conn.Apps().Find(bson.M{"name": name}).One(&app)
	if err == mgo.ErrNotFound {
		return nil, appTypes.ErrAppNotFound
	}
	return &app, err
}
func (app *App) AddUnits(n uint, process string, w io.Writer) error {
	if n == 0 {
		return errors.New("Cannot add zero units.")
	}
	units, err := app.Units()
	if err != nil {
		return err
	}
	for _, u := range units {
		if (u.Status == provision.StatusAsleep) || (u.Status == provision.StatusStopped) {
			return errors.New("Cannot add units to an app that has stopped or sleeping units")
		}
	}
	w = app.withLogWriter(w)
	err = action.NewPipeline(
		&reserveUnitsToAdd,
		&provisionAddUnits,
	).Execute(app, n, w, process)
	rebuild.RoutesRebuildOrEnqueue(app.Name)
	quotaErr := app.fixQuota()
	if err != nil {
		return err
	}
	return quotaErr
}
func (app *App) SetUnitStatus(unitName string, status provision.Status) error {
	units, err := app.Units()
	if err != nil {
		return err
	}
	for _, unit := range units {
		if strings.HasPrefix(unit.ID, unitName) {
			prov, err := app.getProvisioner()
			if err != nil {
				return err
			}
			unitProv, ok := prov.(provision.UnitStatusProvisioner)
			if !ok {
				return nil
			}
			return unitProv.SetUnitStatus(unit, status)
		}
	}
	return &provision.UnitNotFoundError{ID: unitName}
}
func UpdateNodeStatus(nodeData provision.NodeStatusData) ([]UpdateUnitsResult, error) {
	node, findNodeErr := findNodeForNodeData(nodeData)
	var nodeAddresses []string
	if findNodeErr == nil {
		nodeAddresses = []string{node.Address()}
	} else {
		nodeAddresses = nodeData.Addrs
	}
	if healer.HealerInstance != nil {
		err := healer.HealerInstance.UpdateNodeData(nodeAddresses, nodeData.Checks)
		if err != nil {
			log.Errorf("[update node status] unable to set node status in healer: %s", err)
		}
	}
	if findNodeErr == provision.ErrNodeNotFound {
		counterNodesNotFound.Inc()
		log.Errorf("[update node status] node not found with nodedata: %#v", nodeData)
		result := make([]UpdateUnitsResult, len(nodeData.Units))
		for i, unitData := range nodeData.Units {
			result[i] = UpdateUnitsResult{ID: unitData.ID, Found: false}
		}
		return result, nil
	}
	if findNodeErr != nil {
		return nil, findNodeErr
	}
	unitProv, ok := node.Provisioner().(provision.UnitStatusProvisioner)
	if !ok {
		return []UpdateUnitsResult{}, nil
	}
	result := make([]UpdateUnitsResult, len(nodeData.Units))
	for i, unitData := range nodeData.Units {
		unit := provision.Unit{ID: unitData.ID, Name: unitData.Name}
		err := unitProv.SetUnitStatus(unit, unitData.Status)
		_, isNotFound := err.(*provision.UnitNotFoundError)
		if err != nil && !isNotFound {
			return nil, err
		}
		result[i] = UpdateUnitsResult{ID: unitData.ID, Found: !isNotFound}
	}
	return result, nil
}
func (app *App) available() bool {
	units, err := app.Units()
	if err != nil {
		return false
	}
	for _, unit := range units {
		if unit.Available() {
			return true
		}
	}
	return false
}
func (app *App) Grant(team *authTypes.Team) error {
	if _, found := app.findTeam(team); found {
		return ErrAlreadyHaveAccess
	}
	app.Teams = append(app.Teams, team.Name)
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	err = conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$addToSet": bson.M{"teams": team.Name}})
	if err != nil {
		return err
	}
	users, err := auth.ListUsersWithPermissions(permission.Permission{
		Scheme:  permission.PermAppDeploy,
		Context: permission.Context(permTypes.CtxTeam, team.Name),
	})
	if err != nil {
		conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$pull": bson.M{"teams": team.Name}})
		return err
	}
	for _, user := range users {
		err = repository.Manager().GrantAccess(app.Name, user.Email)
		if err != nil {
			conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$pull": bson.M{"teams": team.Name}})
			return err
		}
	}
	return nil
}
func (app *App) Revoke(team *authTypes.Team) error {
	if len(app.Teams) == 1 {
		return ErrCannotOrphanApp
	}
	index, found := app.findTeam(team)
	if !found {
		return ErrNoAccess
	}
	last := len(app.Teams) - 1
	app.Teams[index] = app.Teams[last]
	app.Teams = app.Teams[:last]
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	err = conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$pull": bson.M{"teams": team.Name}})
	if err != nil {
		return err
	}
	users, err := auth.ListUsersWithPermissions(permission.Permission{
		Scheme:  permission.PermAppDeploy,
		Context: permission.Context(permTypes.CtxTeam, team.Name),
	})
	if err != nil {
		conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$addToSet": bson.M{"teams": team.Name}})
		return err
	}
	for _, user := range users {
		perms, err := user.Permissions()
		if err != nil {
			conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$addToSet": bson.M{"teams": team.Name}})
			return err
		}
		canDeploy := permission.CheckFromPermList(perms, permission.PermAppDeploy,
			append(permission.Contexts(permTypes.CtxTeam, app.Teams),
				permission.Context(permTypes.CtxApp, app.Name),
				permission.Context(permTypes.CtxPool, app.Pool),
			)...,
		)
		if canDeploy {
			continue
		}
		err = repository.Manager().RevokeAccess(app.Name, user.Email)
		if err != nil {
			conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$addToSet": bson.M{"teams": team.Name}})
			return err
		}
	}
	return nil
}
func (app *App) GetTeams() []authTypes.Team {
	t, _ := servicemanager.Team.FindByNames(app.Teams)
	return t
}
func (app *App) setEnv(env bind.EnvVar) {
	if app.Env == nil {
		app.Env = make(map[string]bind.EnvVar)
	}
	app.Env[env.Name] = env
	if env.Public {
		app.Log(fmt.Sprintf("setting env %s with value %s", env.Name, env.Value), "tsuru", "api")
	}
}
func (app *App) getEnv(name string) (bind.EnvVar, error) {
	if env, ok := app.Env[name]; ok {
		return env, nil
	}
	return bind.EnvVar{}, errors.New("Environment variable not declared for this app.")
}
func (app *App) validateNew() error {
	if app.Name == InternalAppName || !validation.ValidateName(app.Name) {
		msg := "Invalid app name, your app should have at most 40 " +
			"characters, containing only lower case letters, numbers or dashes, " +
			"starting with a letter."
		return &tsuruErrors.ValidationError{Message: msg}
	}
	return app.validate()
}
func (app *App) validate() error {
	err := app.validatePool()
	if err != nil {
		return err
	}
	return app.validatePlan()
}
func (app *App) InstanceEnvs(serviceName, instanceName string) map[string]bind.EnvVar {
	envs := make(map[string]bind.EnvVar)
	for _, env := range app.ServiceEnvs {
		if env.ServiceName == serviceName && env.InstanceName == instanceName {
			envs[env.Name] = env.EnvVar
		}
	}
	return envs
}
func (app *App) Run(cmd string, w io.Writer, args provision.RunArgs) error {
	if !args.Isolated && !app.available() {
		return errors.New("App must be available to run non-isolated commands")
	}
	app.Log(fmt.Sprintf("running '%s'", cmd), "tsuru", "api")
	logWriter := LogWriter{App: app, Source: "app-run"}
	logWriter.Async()
	defer logWriter.Close()
	return app.run(cmd, io.MultiWriter(w, &logWriter), args)
}
func (app *App) GetUnits() ([]bind.Unit, error) {
	provUnits, err := app.Units()
	if err != nil {
		return nil, err
	}
	units := make([]bind.Unit, len(provUnits))
	for i := range provUnits {
		units[i] = &provUnits[i]
	}
	return units, nil
}
func (app *App) GetUUID() (string, error) {
	if app.UUID != "" {
		return app.UUID, nil
	}
	uuidV4, err := uuid.NewV4()
	if err != nil {
		return "", errors.WithMessage(err, "failed to generate uuid v4")
	}
	conn, err := db.Conn()
	if err != nil {
		return "", err
	}
	defer conn.Close()
	err = conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$set": bson.M{"uuid": uuidV4.String()}})
	if err != nil {
		return "", err
	}
	app.UUID = uuidV4.String()
	return app.UUID, nil
}
func (app *App) Envs() map[string]bind.EnvVar {
	mergedEnvs := make(map[string]bind.EnvVar, len(app.Env)+len(app.ServiceEnvs)+1)
	for _, e := range app.Env {
		mergedEnvs[e.Name] = e
	}
	for _, e := range app.ServiceEnvs {
		mergedEnvs[e.Name] = e.EnvVar
	}
	mergedEnvs[TsuruServicesEnvVar] = serviceEnvsFromEnvVars(app.ServiceEnvs)
	return mergedEnvs
}
func (app *App) SetEnvs(setEnvs bind.SetEnvArgs) error {
	if len(setEnvs.Envs) == 0 {
		return nil
	}
	for _, env := range setEnvs.Envs {
		err := validateEnv(env.Name)
		if err != nil {
			return err
		}
	}
	if setEnvs.Writer != nil {
		fmt.Fprintf(setEnvs.Writer, "---- Setting %d new environment variables ----\n", len(setEnvs.Envs))
	}
	for _, env := range setEnvs.Envs {
		app.setEnv(env)
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	err = conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$set": bson.M{"env": app.Env}})
	if err != nil {
		return err
	}
	if setEnvs.ShouldRestart {
		return app.restartIfUnits(setEnvs.Writer)
	}
	return nil
}
func (app *App) UnsetEnvs(unsetEnvs bind.UnsetEnvArgs) error {
	if len(unsetEnvs.VariableNames) == 0 {
		return nil
	}
	if unsetEnvs.Writer != nil {
		fmt.Fprintf(unsetEnvs.Writer, "---- Unsetting %d environment variables ----\n", len(unsetEnvs.VariableNames))
	}
	for _, name := range unsetEnvs.VariableNames {
		delete(app.Env, name)
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	err = conn.Apps().Update(bson.M{"name": app.Name}, bson.M{"$set": bson.M{"env": app.Env}})
	if err != nil {
		return err
	}
	if unsetEnvs.ShouldRestart {
		return app.restartIfUnits(unsetEnvs.Writer)
	}
	return nil
}
func (app *App) AddCName(cnames ...string) error {
	actions := []*action.Action{
		&validateNewCNames,
		&setNewCNamesToProvisioner,
		&saveCNames,
		&updateApp,
	}
	err := action.NewPipeline(actions...).Execute(app, cnames)
	rebuild.RoutesRebuildOrEnqueue(app.Name)
	return err
}
func (app *App) Log(message, source, unit string) error {
	messages := strings.Split(message, "\n")
	logs := make([]interface{}, 0, len(messages))
	for _, msg := range messages {
		if msg != "" {
			l := Applog{
				Date:    time.Now().In(time.UTC),
				Message: msg,
				Source:  source,
				AppName: app.Name,
				Unit:    unit,
			}
			logs = append(logs, l)
		}
	}
	if len(logs) > 0 {
		conn, err := db.LogConn()
		if err != nil {
			return err
		}
		defer conn.Close()
		return conn.AppLogCollection(app.Name).Insert(logs...)
	}
	return nil
}
func (app *App) LastLogs(lines int, filterLog Applog) ([]Applog, error) {
	return app.lastLogs(lines, filterLog, false)
}
func List(filter *Filter) ([]App, error) {
	apps := []App{}
	query := filter.Query()
	conn, err := db.Conn()
	if err != nil {
		return nil, err
	}
	err = conn.Apps().Find(query).All(&apps)
	conn.Close()
	if err != nil {
		return nil, err
	}
	if filter != nil && len(filter.Statuses) > 0 {
		appsProvisionerMap := make(map[string][]provision.App)
		var prov provision.Provisioner
		for i := range apps {
			a := &apps[i]
			prov, err = a.getProvisioner()
			if err != nil {
				return nil, err
			}
			appsProvisionerMap[prov.GetName()] = append(appsProvisionerMap[prov.GetName()], a)
		}
		var provisionApps []provision.App
		for provName, apps := range appsProvisionerMap {
			prov, err = provision.Get(provName)
			if err != nil {
				return nil, err
			}
			if filterProv, ok := prov.(provision.AppFilterProvisioner); ok {
				apps, err = filterProv.FilterAppsByUnitStatus(apps, filter.Statuses)
				if err != nil {
					return nil, err
				}
			}
			provisionApps = append(provisionApps, apps...)
		}
		for i := range provisionApps {
			apps[i] = *(provisionApps[i].(*App))
		}
		apps = apps[:len(provisionApps)]
	}
	err = loadCachedAddrsInApps(apps)
	if err != nil {
		return nil, err
	}
	return apps, nil
}
func Swap(app1, app2 *App, cnameOnly bool) error {
	a1Routers := app1.GetRouters()
	a2Routers := app2.GetRouters()
	if len(a1Routers) != 1 || len(a2Routers) != 1 {
		return errors.New("swapping apps with multiple routers is not supported")
	}
	r1, err := router.Get(a1Routers[0].Name)
	if err != nil {
		return err
	}
	r2, err := router.Get(a2Routers[0].Name)
	if err != nil {
		return err
	}
	defer func(app1, app2 *App) {
		rebuild.RoutesRebuildOrEnqueue(app1.Name)
		rebuild.RoutesRebuildOrEnqueue(app2.Name)
		app1.GetRoutersWithAddr()
		app2.GetRoutersWithAddr()
	}(app1, app2)
	err = r1.Swap(app1.Name, app2.Name, cnameOnly)
	if err != nil {
		return err
	}
	conn, err := db.Conn()
	if err != nil {
		return err
	}
	defer conn.Close()
	app1.CName, app2.CName = app2.CName, app1.CName
	updateCName := func(app *App, r router.Router) error {
		return conn.Apps().Update(
			bson.M{"name": app.Name},
			bson.M{"$set": bson.M{"cname": app.CName}},
		)
	}
	err = updateCName(app1, r1)
	if err != nil {
		return err
	}
	return updateCName(app2, r2)
}
func (app *App) Start(w io.Writer, process string) error {
	w = app.withLogWriter(w)
	msg := fmt.Sprintf("\n ---> Starting the process %q", process)
	if process == "" {
		msg = fmt.Sprintf("\n ---> Starting the app %q", app.Name)
	}
	fmt.Fprintf(w, "%s\n", msg)
	prov, err := app.getProvisioner()
	if err != nil {
		return err
	}
	err = prov.Start(app, process)
	if err != nil {
		log.Errorf("[start] error on start the app %s - %s", app.Name, err)
		return err
	}
	rebuild.RoutesRebuildOrEnqueue(app.Name)
	return err
}
func GetDbDriver(name string) (*DbDriver, error) {
	driver, ok := dbDrivers[name]
	if !ok {
		return nil, errors.Errorf("Unknown database driver: %q.", name)
	}
	return &driver, nil
}
func GetCurrentDbDriver() (*DbDriver, error) {
	driverLock.RLock()
	if currentDbDriver != nil {
		driverLock.RUnlock()
		return currentDbDriver, nil
	}
	driverLock.RUnlock()
	driverLock.Lock()
	defer driverLock.Unlock()
	if currentDbDriver != nil {
		return currentDbDriver, nil
	}
	dbDriverName, err := config.GetString("database:driver")
	if err != nil || dbDriverName == "" {
		dbDriverName = DefaultDbDriverName
	}
	currentDbDriver, err = GetDbDriver(dbDriverName)
	if err != nil {
		return nil, err
	}
	return currentDbDriver, nil
}
func NewForConfig(c *rest.Config) (*Clientset, error) {
	configShallowCopy := *c
	if configShallowCopy.RateLimiter == nil && configShallowCopy.QPS > 0 {
		configShallowCopy.RateLimiter = flowcontrol.NewTokenBucketRateLimiter(configShallowCopy.QPS, configShallowCopy.Burst)
	}
	var cs Clientset
	var err error
	cs.tsuruV1, err = tsuruv1.NewForConfig(&configShallowCopy)
	if err != nil {
		return nil, err
	}

	cs.DiscoveryClient, err = discovery.NewDiscoveryClientForConfig(&configShallowCopy)
	if err != nil {
		glog.Errorf("failed to create the DiscoveryClient: %v", err)
		return nil, err
	}
	return &cs, nil
}
func (p *dockerProvisioner) GetAppFromUnitID(unitID string) (provision.App, error) {
	cnt, err := p.GetContainer(unitID)
	if err != nil {
		return nil, err
	}
	a, err := app.GetByName(cnt.AppName)
	if err != nil {
		return nil, err
	}
	return a, nil
}
func NewPipeline(actions ...*Action) *Pipeline {
	// Actions are usually global functions, copying them
	// guarantees each copy has an isolated Result.
	newActions := make([]*Action, len(actions))
	for i, action := range actions {
		newAction := &Action{
			Name:      action.Name,
			Forward:   action.Forward,
			Backward:  action.Backward,
			MinParams: action.MinParams,
			OnError:   action.OnError,
		}
		newActions[i] = newAction
	}
	return &Pipeline{actions: newActions}

}
func (p *Pipeline) Result() Result {
	action := p.actions[len(p.actions)-1]
	action.rMutex.Lock()
	defer action.rMutex.Unlock()
	return action.result
}
func (r *Request) DecodeJsonPayload(v interface{}) error {
	content, err := ioutil.ReadAll(r.Body)
	r.Body.Close()
	if err != nil {
		return err
	}
	if len(content) == 0 {
		return ErrJsonPayloadEmpty
	}
	err = json.Unmarshal(content, v)
	if err != nil {
		return err
	}
	return nil
}
func (r *Request) UrlFor(path string, queryParams map[string][]string) *url.URL {
	baseUrl := r.BaseUrl()
	baseUrl.Path = path
	if queryParams != nil {
		query := url.Values{}
		for k, v := range queryParams {
			for _, vv := range v {
				query.Add(k, vv)
			}
		}
		baseUrl.RawQuery = query.Encode()
	}
	return baseUrl
}
func (r *Request) GetCorsInfo() *CorsInfo {

	origin := r.Header.Get("Origin")

	var originUrl *url.URL
	var isCors bool

	if origin == "" {
		isCors = false
	} else if origin == "null" {
		isCors = true
	} else {
		var err error
		originUrl, err = url.ParseRequestURI(origin)
		isCors = err == nil && r.Host != originUrl.Host
	}

	reqMethod := r.Header.Get("Access-Control-Request-Method")

	reqHeaders := []string{}
	rawReqHeaders := r.Header[http.CanonicalHeaderKey("Access-Control-Request-Headers")]
	for _, rawReqHeader := range rawReqHeaders {
		if len(rawReqHeader) == 0 {
			continue
		}
		// net/http does not handle comma delimited headers for us
		for _, reqHeader := range strings.Split(rawReqHeader, ",") {
			reqHeaders = append(reqHeaders, http.CanonicalHeaderKey(strings.TrimSpace(reqHeader)))
		}
	}

	isPreflight := isCors && r.Method == "OPTIONS" && reqMethod != ""

	return &CorsInfo{
		IsCors:                      isCors,
		IsPreflight:                 isPreflight,
		Origin:                      origin,
		OriginUrl:                   originUrl,
		AccessControlRequestMethod:  strings.ToUpper(reqMethod),
		AccessControlRequestHeaders: reqHeaders,
	}
}
func (mw *CorsMiddleware) MiddlewareFunc(handler HandlerFunc) HandlerFunc {

	// precompute as much as possible at init time

	mw.allowedMethods = map[string]bool{}
	normedMethods := []string{}
	for _, allowedMethod := range mw.AllowedMethods {
		normed := strings.ToUpper(allowedMethod)
		mw.allowedMethods[normed] = true
		normedMethods = append(normedMethods, normed)
	}
	mw.allowedMethodsCsv = strings.Join(normedMethods, ",")

	mw.allowedHeaders = map[string]bool{}
	normedHeaders := []string{}
	for _, allowedHeader := range mw.AllowedHeaders {
		normed := http.CanonicalHeaderKey(allowedHeader)
		mw.allowedHeaders[normed] = true
		normedHeaders = append(normedHeaders, normed)
	}
	mw.allowedHeadersCsv = strings.Join(normedHeaders, ",")

	return func(writer ResponseWriter, request *Request) {

		corsInfo := request.GetCorsInfo()

		// non CORS requests
		if !corsInfo.IsCors {
			if mw.RejectNonCorsRequests {
				Error(writer, "Non CORS request", http.StatusForbidden)
				return
			}
			// continue, execute the wrapped middleware
			handler(writer, request)
			return
		}

		// Validate the Origin
		if mw.OriginValidator(corsInfo.Origin, request) == false {
			Error(writer, "Invalid Origin", http.StatusForbidden)
			return
		}

		if corsInfo.IsPreflight {

			// check the request methods
			if mw.allowedMethods[corsInfo.AccessControlRequestMethod] == false {
				Error(writer, "Invalid Preflight Request", http.StatusForbidden)
				return
			}

			// check the request headers
			for _, requestedHeader := range corsInfo.AccessControlRequestHeaders {
				if mw.allowedHeaders[requestedHeader] == false {
					Error(writer, "Invalid Preflight Request", http.StatusForbidden)
					return
				}
			}

			writer.Header().Set("Access-Control-Allow-Methods", mw.allowedMethodsCsv)
			writer.Header().Set("Access-Control-Allow-Headers", mw.allowedHeadersCsv)
			writer.Header().Set("Access-Control-Allow-Origin", corsInfo.Origin)
			if mw.AccessControlAllowCredentials == true {
				writer.Header().Set("Access-Control-Allow-Credentials", "true")
			}
			writer.Header().Set("Access-Control-Max-Age", strconv.Itoa(mw.AccessControlMaxAge))
			writer.WriteHeader(http.StatusOK)
			return
		}

		// Non-preflight requests
		for _, exposed := range mw.AccessControlExposeHeaders {
			writer.Header().Add("Access-Control-Expose-Headers", exposed)
		}
		writer.Header().Set("Access-Control-Allow-Origin", corsInfo.Origin)
		if mw.AccessControlAllowCredentials == true {
			writer.Header().Set("Access-Control-Allow-Credentials", "true")
		}
		// continure, execute the wrapped middleware
		handler(writer, request)
		return
	}
}
func (mw *RecorderMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {
	return func(w ResponseWriter, r *Request) {

		writer := &recorderResponseWriter{w, 0, false, 0}

		// call the handler
		h(writer, r)

		r.Env["STATUS_CODE"] = writer.statusCode
		r.Env["BYTES_WRITTEN"] = writer.bytesWritten
	}
}
func (w *recorderResponseWriter) WriteHeader(code int) {
	w.ResponseWriter.WriteHeader(code)
	if w.wroteHeader {
		return
	}
	w.statusCode = code
	w.wroteHeader = true
}
func MakeRouter(routes ...*Route) (App, error) {
	r := &router{
		Routes: routes,
	}
	err := r.start()
	if err != nil {
		return nil, err
	}
	return r, nil
}
func (rt *router) AppFunc() HandlerFunc {
	return func(writer ResponseWriter, request *Request) {

		// find the route
		route, params, pathMatched := rt.findRouteFromURL(request.Method, request.URL)
		if route == nil {

			if pathMatched {
				// no route found, but path was matched: 405 Method Not Allowed
				Error(writer, "Method not allowed", http.StatusMethodNotAllowed)
				return
			}

			// no route found, the path was not matched: 404 Not Found
			NotFound(writer, request)
			return
		}

		// a route was found, set the PathParams
		request.PathParams = params

		// run the user code
		handler := route.Func
		handler(writer, request)
	}
}
func escapedPath(urlObj *url.URL) string {
	// the escape method of url.URL should be public
	// that would avoid this split.
	parts := strings.SplitN(urlObj.RequestURI(), "?", 2)
	return parts[0]
}
func escapedPathExp(pathExp string) (string, error) {

	// PathExp validation
	if pathExp == "" {
		return "", errors.New("empty PathExp")
	}
	if pathExp[0] != '/' {
		return "", errors.New("PathExp must start with /")
	}
	if strings.Contains(pathExp, "?") {
		return "", errors.New("PathExp must not contain the query string")
	}

	// Get the right escaping
	// XXX a bit hacky

	pathExp = preEscape.Replace(pathExp)

	urlObj, err := url.Parse(pathExp)
	if err != nil {
		return "", err
	}

	// get the same escaping as find requests
	pathExp = urlObj.RequestURI()

	pathExp = postEscape.Replace(pathExp)

	return pathExp, nil
}
func (rt *router) start() error {

	rt.trie = trie.New()
	rt.index = map[*Route]int{}

	for i, route := range rt.Routes {

		// work with the PathExp urlencoded.
		pathExp, err := escapedPathExp(route.PathExp)
		if err != nil {
			return err
		}

		// insert in the Trie
		err = rt.trie.AddRoute(
			strings.ToUpper(route.HttpMethod), // work with the HttpMethod in uppercase
			pathExp,
			route,
		)
		if err != nil {
			return err
		}

		// index
		rt.index[route] = i
	}

	if rt.disableTrieCompression == false {
		rt.trie.Compress()
	}

	return nil
}
func (rt *router) ofFirstDefinedRoute(matches []*trie.Match) *trie.Match {
	minIndex := -1
	var bestMatch *trie.Match

	for _, result := range matches {
		route := result.Route.(*Route)
		routeIndex := rt.index[route]
		if minIndex == -1 || routeIndex < minIndex {
			minIndex = routeIndex
			bestMatch = result
		}
	}

	return bestMatch
}
func (rt *router) findRouteFromURL(httpMethod string, urlObj *url.URL) (*Route, map[string]string, bool) {

	// lookup the routes in the Trie
	matches, pathMatched := rt.trie.FindRoutesAndPathMatched(
		strings.ToUpper(httpMethod), // work with the httpMethod in uppercase
		escapedPath(urlObj),         // work with the path urlencoded
	)

	// short cuts
	if len(matches) == 0 {
		// no route found
		return nil, nil, pathMatched
	}

	if len(matches) == 1 {
		// one route found
		return matches[0].Route.(*Route), matches[0].Params, pathMatched
	}

	// multiple routes found, pick the first defined
	result := rt.ofFirstDefinedRoute(matches)
	return result.Route.(*Route), result.Params, pathMatched
}
func (mw *ContentTypeCheckerMiddleware) MiddlewareFunc(handler HandlerFunc) HandlerFunc {

	return func(w ResponseWriter, r *Request) {

		mediatype, params, _ := mime.ParseMediaType(r.Header.Get("Content-Type"))
		charset, ok := params["charset"]
		if !ok {
			charset = "UTF-8"
		}

		// per net/http doc, means that the length is known and non-null
		if r.ContentLength > 0 &&
			!(mediatype == "application/json" && strings.ToUpper(charset) == "UTF-8") {

			Error(w,
				"Bad Content-Type or charset, expected 'application/json'",
				http.StatusUnsupportedMediaType,
			)
			return
		}

		// call the wrapped handler
		handler(w, r)
	}
}
func (w *responseWriter) CloseNotify() <-chan bool {
	notifier := w.ResponseWriter.(http.CloseNotifier)
	return notifier.CloseNotify()
}
func (mw *AccessLogApacheMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {

	// set the default Logger
	if mw.Logger == nil {
		mw.Logger = log.New(os.Stderr, "", 0)
	}

	// set default format
	if mw.Format == "" {
		mw.Format = DefaultLogFormat
	}

	mw.convertFormat()

	return func(w ResponseWriter, r *Request) {

		// call the handler
		h(w, r)

		util := &accessLogUtil{w, r}

		mw.Logger.Print(mw.executeTextTemplate(util))
	}
}
func (mw *AccessLogApacheMiddleware) executeTextTemplate(util *accessLogUtil) string {
	buf := bytes.NewBufferString("")
	err := mw.textTemplate.Execute(buf, util)
	if err != nil {
		panic(err)
	}
	return buf.String()
}
func (u *accessLogUtil) RemoteUser() string {
	if u.R.Env["REMOTE_USER"] != nil {
		return u.R.Env["REMOTE_USER"].(string)
	}
	return ""
}
func (u *accessLogUtil) ApacheQueryString() string {
	if u.R.URL.RawQuery != "" {
		return "?" + u.R.URL.RawQuery
	}
	return ""
}
func (u *accessLogUtil) StartTime() *time.Time {
	if u.R.Env["START_TIME"] != nil {
		return u.R.Env["START_TIME"].(*time.Time)
	}
	return nil
}
func (u *accessLogUtil) ApacheRemoteAddr() string {
	remoteAddr := u.R.RemoteAddr
	if remoteAddr != "" {
		if ip, _, err := net.SplitHostPort(remoteAddr); err == nil {
			return ip
		}
	}
	return ""
}
func (u *accessLogUtil) ResponseTime() *time.Duration {
	if u.R.Env["ELAPSED_TIME"] != nil {
		return u.R.Env["ELAPSED_TIME"].(*time.Duration)
	}
	return nil
}
func (mw *JsonIndentMiddleware) MiddlewareFunc(handler HandlerFunc) HandlerFunc {

	if mw.Indent == "" {
		mw.Indent = "  "
	}

	return func(w ResponseWriter, r *Request) {

		writer := &jsonIndentResponseWriter{w, false, mw.Prefix, mw.Indent}
		// call the wrapped handler
		handler(writer, r)
	}
}
func (w *jsonIndentResponseWriter) EncodeJson(v interface{}) ([]byte, error) {
	b, err := json.MarshalIndent(v, w.prefix, w.indent)
	if err != nil {
		return nil, err
	}
	return b, nil
}
func (w *jsonIndentResponseWriter) WriteHeader(code int) {
	w.ResponseWriter.WriteHeader(code)
	w.wroteHeader = true
}
func (route *Route) MakePath(pathParams map[string]string) string {
	path := route.PathExp
	for paramName, paramValue := range pathParams {
		paramPlaceholder := ":" + paramName
		relaxedPlaceholder := "#" + paramName
		splatPlaceholder := "*" + paramName
		r := strings.NewReplacer(paramPlaceholder, paramValue, splatPlaceholder, paramValue, relaxedPlaceholder, paramValue)
		path = r.Replace(path)
	}
	return path
}
func (mw *RecoverMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {

	// set the default Logger
	if mw.Logger == nil {
		mw.Logger = log.New(os.Stderr, "", 0)
	}

	return func(w ResponseWriter, r *Request) {

		// catch user code's panic, and convert to http response
		defer func() {
			if reco := recover(); reco != nil {
				trace := debug.Stack()

				// log the trace
				message := fmt.Sprintf("%s\n%s", reco, trace)
				mw.logError(message)

				// write error response
				if mw.EnableResponseStackTrace {
					Error(w, message, http.StatusInternalServerError)
				} else {
					Error(w, "Internal Server Error", http.StatusInternalServerError)
				}
			}
		}()

		// call the handler
		h(w, r)
	}
}
func WrapMiddlewares(middlewares []Middleware, handler HandlerFunc) HandlerFunc {
	wrapped := handler
	for i := len(middlewares) - 1; i >= 0; i-- {
		wrapped = middlewares[i].MiddlewareFunc(wrapped)
	}
	return wrapped
}
func (mw *GzipMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {
	return func(w ResponseWriter, r *Request) {
		// gzip support enabled
		canGzip := strings.Contains(r.Header.Get("Accept-Encoding"), "gzip")
		// client accepts gzip ?
		writer := &gzipResponseWriter{w, false, canGzip, nil}
		defer func() {
			// need to close gzip writer
			if writer.gzipWriter != nil {
				writer.gzipWriter.Close()
			}
		}()
		// call the handler with the wrapped writer
		h(writer, r)
	}
}
func (w *gzipResponseWriter) WriteHeader(code int) {

	// Always set the Vary header, even if this particular request
	// is not gzipped.
	w.Header().Add("Vary", "Accept-Encoding")

	if w.canGzip {
		w.Header().Set("Content-Encoding", "gzip")
	}

	w.ResponseWriter.WriteHeader(code)
	w.wroteHeader = true
}
func (w *gzipResponseWriter) Hijack() (net.Conn, *bufio.ReadWriter, error) {
	hijacker := w.ResponseWriter.(http.Hijacker)
	return hijacker.Hijack()
}
func (w *gzipResponseWriter) Write(b []byte) (int, error) {

	if !w.wroteHeader {
		w.WriteHeader(http.StatusOK)
	}

	writer := w.ResponseWriter.(http.ResponseWriter)

	if w.canGzip {
		// Write can be called multiple times for a given response.
		// (see the streaming example:
		// https://github.com/ant0ine/go-json-rest-examples/tree/master/streaming)
		// The gzipWriter is instantiated only once, and flushed after
		// each write.
		if w.gzipWriter == nil {
			w.gzipWriter = gzip.NewWriter(writer)
		}
		count, errW := w.gzipWriter.Write(b)
		errF := w.gzipWriter.Flush()
		if errW != nil {
			return count, errW
		}
		if errF != nil {
			return count, errF
		}
		return count, nil
	}

	return writer.Write(b)
}
func (mw *AuthBasicMiddleware) MiddlewareFunc(handler HandlerFunc) HandlerFunc {

	if mw.Realm == "" {
		log.Fatal("Realm is required")
	}

	if mw.Authenticator == nil {
		log.Fatal("Authenticator is required")
	}

	if mw.Authorizator == nil {
		mw.Authorizator = func(userId string, request *Request) bool {
			return true
		}
	}

	return func(writer ResponseWriter, request *Request) {

		authHeader := request.Header.Get("Authorization")
		if authHeader == "" {
			mw.unauthorized(writer)
			return
		}

		providedUserId, providedPassword, err := mw.decodeBasicAuthHeader(authHeader)

		if err != nil {
			Error(writer, "Invalid authentication", http.StatusBadRequest)
			return
		}

		if !mw.Authenticator(providedUserId, providedPassword) {
			mw.unauthorized(writer)
			return
		}

		if !mw.Authorizator(providedUserId, request) {
			mw.unauthorized(writer)
			return
		}

		request.Env["REMOTE_USER"] = providedUserId

		handler(writer, request)
	}
}
func (n *node) printDebug(level int) {
	level++
	// *splat branch
	if n.SplatChild != nil {
		printFPadding(level, "*splat\n")
		n.SplatChild.printDebug(level)
	}
	// :param branch
	if n.ParamChild != nil {
		printFPadding(level, ":param\n")
		n.ParamChild.printDebug(level)
	}
	// #param branch
	if n.RelaxedChild != nil {
		printFPadding(level, "#relaxed\n")
		n.RelaxedChild.printDebug(level)
	}
	// main branch
	for key, node := range n.Children {
		printFPadding(level, "\"%s\"\n", key)
		node.printDebug(level)
	}
}
func (t *Trie) AddRoute(httpMethod, pathExp string, route interface{}) error {
	return t.root.addRoute(httpMethod, pathExp, route, []string{})
}
func (t *Trie) printDebug() {
	fmt.Print("<trie>\n")
	t.root.printDebug(0)
	fmt.Print("</trie>\n")
}
func (t *Trie) FindRoutes(httpMethod, path string) []*Match {
	context := newFindContext()
	matches := []*Match{}
	context.matchFunc = func(httpMethod, path string, node *node) {
		if node.HttpMethodToRoute[httpMethod] != nil {
			// path and method match, found a route !
			matches = append(
				matches,
				&Match{
					Route:  node.HttpMethodToRoute[httpMethod],
					Params: context.paramsAsMap(),
				},
			)
		}
	}
	t.root.find(httpMethod, path, context)
	return matches
}
func (t *Trie) FindRoutesAndPathMatched(httpMethod, path string) ([]*Match, bool) {
	context := newFindContext()
	pathMatched := false
	matches := []*Match{}
	context.matchFunc = func(httpMethod, path string, node *node) {
		pathMatched = true
		if node.HttpMethodToRoute[httpMethod] != nil {
			// path and method match, found a route !
			matches = append(
				matches,
				&Match{
					Route:  node.HttpMethodToRoute[httpMethod],
					Params: context.paramsAsMap(),
				},
			)
		}
	}
	t.root.find(httpMethod, path, context)
	return matches, pathMatched
}
func (t *Trie) FindRoutesForPath(path string) []*Match {
	context := newFindContext()
	matches := []*Match{}
	context.matchFunc = func(httpMethod, path string, node *node) {
		params := context.paramsAsMap()
		for _, route := range node.HttpMethodToRoute {
			matches = append(
				matches,
				&Match{
					Route:  route,
					Params: params,
				},
			)
		}
	}
	t.root.find("", path, context)
	return matches
}
func (api *Api) Use(middlewares ...Middleware) {
	api.stack = append(api.stack, middlewares...)
}
func (api *Api) MakeHandler() http.Handler {
	var appFunc HandlerFunc
	if api.app != nil {
		appFunc = api.app.AppFunc()
	} else {
		appFunc = func(w ResponseWriter, r *Request) {}
	}
	return http.HandlerFunc(
		adapterFunc(
			WrapMiddlewares(api.stack, appFunc),
		),
	)
}
func (mw *PoweredByMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {

	poweredBy := xPoweredByDefault
	if mw.XPoweredBy != "" {
		poweredBy = mw.XPoweredBy
	}

	return func(w ResponseWriter, r *Request) {

		w.Header().Add("X-Powered-By", poweredBy)

		// call the handler
		h(w, r)

	}
}
func (mw *StatusMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {

	mw.start = time.Now()
	mw.pid = os.Getpid()
	mw.responseCounts = map[string]int{}
	mw.totalResponseTime = time.Time{}

	return func(w ResponseWriter, r *Request) {

		// call the handler
		h(w, r)

		if r.Env["STATUS_CODE"] == nil {
			log.Fatal("StatusMiddleware: Env[\"STATUS_CODE\"] is nil, " +
				"RecorderMiddleware may not be in the wrapped Middlewares.")
		}
		statusCode := r.Env["STATUS_CODE"].(int)

		if r.Env["ELAPSED_TIME"] == nil {
			log.Fatal("StatusMiddleware: Env[\"ELAPSED_TIME\"] is nil, " +
				"TimerMiddleware may not be in the wrapped Middlewares.")
		}
		responseTime := r.Env["ELAPSED_TIME"].(*time.Duration)

		mw.lock.Lock()
		mw.responseCounts[fmt.Sprintf("%d", statusCode)]++
		mw.totalResponseTime = mw.totalResponseTime.Add(*responseTime)
		mw.lock.Unlock()
	}
}
func (mw *StatusMiddleware) GetStatus() *Status {

	mw.lock.RLock()

	now := time.Now()

	uptime := now.Sub(mw.start)

	totalCount := 0
	for _, count := range mw.responseCounts {
		totalCount += count
	}

	totalResponseTime := mw.totalResponseTime.Sub(time.Time{})

	averageResponseTime := time.Duration(0)
	if totalCount > 0 {
		avgNs := int64(totalResponseTime) / int64(totalCount)
		averageResponseTime = time.Duration(avgNs)
	}

	status := &Status{
		Pid:                    mw.pid,
		UpTime:                 uptime.String(),
		UpTimeSec:              uptime.Seconds(),
		Time:                   now.String(),
		TimeUnix:               now.Unix(),
		StatusCodeCount:        mw.responseCounts,
		TotalCount:             totalCount,
		TotalResponseTime:      totalResponseTime.String(),
		TotalResponseTimeSec:   totalResponseTime.Seconds(),
		AverageResponseTime:    averageResponseTime.String(),
		AverageResponseTimeSec: averageResponseTime.Seconds(),
	}

	mw.lock.RUnlock()

	return status
}
func (mw *JsonpMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {

	if mw.CallbackNameKey == "" {
		mw.CallbackNameKey = "callback"
	}

	return func(w ResponseWriter, r *Request) {

		callbackName := r.URL.Query().Get(mw.CallbackNameKey)
		// TODO validate the callbackName ?

		if callbackName != "" {
			// the client request JSONP, instantiate JsonpMiddleware.
			writer := &jsonpResponseWriter{w, false, callbackName}
			// call the handler with the wrapped writer
			h(writer, r)
		} else {
			// do nothing special
			h(w, r)
		}

	}
}
func (w *jsonpResponseWriter) Flush() {
	if !w.wroteHeader {
		w.WriteHeader(http.StatusOK)
	}
	flusher := w.ResponseWriter.(http.Flusher)
	flusher.Flush()
}
func (mw *AccessLogJsonMiddleware) MiddlewareFunc(h HandlerFunc) HandlerFunc {

	// set the default Logger
	if mw.Logger == nil {
		mw.Logger = log.New(os.Stderr, "", 0)
	}

	return func(w ResponseWriter, r *Request) {

		// call the handler
		h(w, r)

		mw.Logger.Printf("%s", makeAccessLogJsonRecord(r).asJson())
	}
}
func (s *S3) Fetch() (io.Reader, error) {
	//delay fetches after first
	if s.delay {
		time.Sleep(s.Interval)
	}
	s.delay = true
	//status check using HEAD
	head, err := s.client.HeadObject(&s3.HeadObjectInput{Bucket: &s.Bucket, Key: &s.Key})
	if err != nil {
		return nil, fmt.Errorf("HEAD request failed (%s)", err)
	}
	if s.lastETag == *head.ETag {
		return nil, nil //skip, file match
	}
	s.lastETag = *head.ETag
	//binary fetch using GET
	get, err := s.client.GetObject(&s3.GetObjectInput{Bucket: &s.Bucket, Key: &s.Key})
	if err != nil {
		return nil, fmt.Errorf("GET request failed (%s)", err)
	}
	//extract gz files
	if strings.HasSuffix(s.Key, ".gz") && aws.StringValue(get.ContentEncoding) != "gzip" {
		return gzip.NewReader(get.Body)
	}
	//success!
	return get.Body, nil
}
func sanityCheck() bool {
	//sanity check
	if token := os.Getenv(envBinCheck); token != "" {
		fmt.Fprint(os.Stdout, token)
		return true
	}
	//legacy sanity check using old env var
	if token := os.Getenv(envBinCheckLegacy); token != "" {
		fmt.Fprint(os.Stdout, token)
		return true
	}
	return false
}
func (l *overseerListener) release(timeout time.Duration) {
	//stop accepting connections - release fd
	l.closeError = l.Listener.Close()
	//start timer, close by force if deadline not met
	waited := make(chan bool)
	go func() {
		l.wg.Wait()
		waited <- true
	}()
	go func() {
		select {
		case <-time.After(timeout):
			close(l.closeByForce)
		case <-waited:
			//no need to force close
		}
	}()
}
func (mp *master) fetchLoop() {
	min := mp.Config.MinFetchInterval
	time.Sleep(min)
	for {
		t0 := time.Now()
		mp.fetch()
		//duration fetch of fetch
		diff := time.Now().Sub(t0)
		if diff < min {
			delay := min - diff
			//ensures at least MinFetchInterval delay.
			//should be throttled by the fetcher!
			time.Sleep(delay)
		}
	}
}
func (mp *master) forkLoop() error {
	//loop, restart command
	for {
		if err := mp.fork(); err != nil {
			return err
		}
	}
}
func (f *File) Init() error {
	if f.Path == "" {
		return fmt.Errorf("Path required")
	}
	if f.Interval < 1*time.Second {
		f.Interval = 1 * time.Second
	}
	if err := f.updateHash(); err != nil {
		return err
	}
	return nil
}
func (f *File) Fetch() (io.Reader, error) {
	//only delay after first fetch
	if f.delay {
		time.Sleep(f.Interval)
	}
	f.delay = true
	lastHash := f.hash
	if err := f.updateHash(); err != nil {
		return nil, err
	}
	// no change
	if lastHash == f.hash {
		return nil, nil
	}
	// changed!
	file, err := os.Open(f.Path)
	if err != nil {
		return nil, err
	}
	//check every 1/4s for 5s to
	//ensure its not mid-copy
	const rate = 250 * time.Millisecond
	const total = int(5 * time.Second / rate)
	attempt := 1
	for {
		if attempt == total {
			file.Close()
			return nil, errors.New("file is currently being changed")
		}
		attempt++
		//sleep
		time.Sleep(rate)
		//check hash!
		if err := f.updateHash(); err != nil {
			file.Close()
			return nil, err
		}
		//check until no longer changing
		if lastHash == f.hash {
			break
		}
		lastHash = f.hash
	}
	return file, nil
}
func (h *HTTP) Fetch() (io.Reader, error) {
	//delay fetches after first
	if h.delay {
		time.Sleep(h.Interval)
	}
	h.delay = true
	//status check using HEAD
	resp, err := http.Head(h.URL)
	if err != nil {
		return nil, fmt.Errorf("HEAD request failed (%s)", err)
	}
	resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("HEAD request failed (status code %d)", resp.StatusCode)
	}
	//if all headers match, skip update
	matches, total := 0, 0
	for _, header := range h.CheckHeaders {
		if curr := resp.Header.Get(header); curr != "" {
			if last, ok := h.lasts[header]; ok && last == curr {
				matches++
			}
			h.lasts[header] = curr
			total++
		}
	}
	if matches == total {
		return nil, nil //skip, file match
	}
	//binary fetch using GET
	resp, err = http.Get(h.URL)
	if err != nil {
		return nil, fmt.Errorf("GET request failed (%s)", err)
	}
	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("GET request failed (status code %d)", resp.StatusCode)
	}
	//extract gz files
	if strings.HasSuffix(h.URL, ".gz") && resp.Header.Get("Content-Encoding") != "gzip" {
		return gzip.NewReader(resp.Body)
	}
	//success!
	return resp.Body, nil
}
func NewConfig() *Config {
	c := &Config{
		Config: *sarama.NewConfig(),
	}
	c.Group.PartitionStrategy = StrategyRange
	c.Group.Offsets.Retry.Max = 3
	c.Group.Offsets.Synchronization.DwellTime = c.Consumer.MaxProcessingTime
	c.Group.Session.Timeout = 30 * time.Second
	c.Group.Heartbeat.Interval = 3 * time.Second
	c.Config.Version = minVersion
	return c
}
func (c *Config) Validate() error {
	if c.Group.Heartbeat.Interval%time.Millisecond != 0 {
		sarama.Logger.Println("Group.Heartbeat.Interval only supports millisecond precision; nanoseconds will be truncated.")
	}
	if c.Group.Session.Timeout%time.Millisecond != 0 {
		sarama.Logger.Println("Group.Session.Timeout only supports millisecond precision; nanoseconds will be truncated.")
	}
	if c.Group.PartitionStrategy != StrategyRange && c.Group.PartitionStrategy != StrategyRoundRobin {
		sarama.Logger.Println("Group.PartitionStrategy is not supported; range will be assumed.")
	}
	if !c.Version.IsAtLeast(minVersion) {
		sarama.Logger.Println("Version is not supported; 0.9. will be assumed.")
		c.Version = minVersion
	}
	if err := c.Config.Validate(); err != nil {
		return err
	}

	// validate the Group values
	switch {
	case c.Group.Offsets.Retry.Max < 0:
		return sarama.ConfigurationError("Group.Offsets.Retry.Max must be >= 0")
	case c.Group.Offsets.Synchronization.DwellTime <= 0:
		return sarama.ConfigurationError("Group.Offsets.Synchronization.DwellTime must be > 0")
	case c.Group.Offsets.Synchronization.DwellTime > 10*time.Minute:
		return sarama.ConfigurationError("Group.Offsets.Synchronization.DwellTime must be <= 10m")
	case c.Group.Heartbeat.Interval <= 0:
		return sarama.ConfigurationError("Group.Heartbeat.Interval must be > 0")
	case c.Group.Session.Timeout <= 0:
		return sarama.ConfigurationError("Group.Session.Timeout must be > 0")
	case !c.Metadata.Full && c.Group.Topics.Whitelist != nil:
		return sarama.ConfigurationError("Metadata.Full must be enabled when Group.Topics.Whitelist is used")
	case !c.Metadata.Full && c.Group.Topics.Blacklist != nil:
		return sarama.ConfigurationError("Metadata.Full must be enabled when Group.Topics.Blacklist is used")
	}

	// ensure offset is correct
	switch c.Consumer.Offsets.Initial {
	case sarama.OffsetOldest, sarama.OffsetNewest:
	default:
		return sarama.ConfigurationError("Consumer.Offsets.Initial must be either OffsetOldest or OffsetNewest")
	}

	return nil
}
func NewClient(addrs []string, config *Config) (*Client, error) {
	if config == nil {
		config = NewConfig()
	}

	if err := config.Validate(); err != nil {
		return nil, err
	}

	client, err := sarama.NewClient(addrs, &config.Config)
	if err != nil {
		return nil, err
	}

	return &Client{Client: client, config: *config}, nil
}
func (c *partitionConsumer) AsyncClose() {
	c.closeOnce.Do(func() {
		c.closeErr = c.PartitionConsumer.Close()
		close(c.dying)
	})
}
func (c *partitionConsumer) Close() error {
	c.AsyncClose()
	<-c.dead
	return c.closeErr
}
func (c *partitionConsumer) MarkOffset(offset int64, metadata string) {
	c.mu.Lock()
	if next := offset + 1; next > c.state.Info.Offset {
		c.state.Info.Offset = next
		c.state.Info.Metadata = metadata
		c.state.Dirty = true
	}
	c.mu.Unlock()
}
func NewConsumer(addrs []string, groupID string, topics []string, config *Config) (*Consumer, error) {
	client, err := NewClient(addrs, config)
	if err != nil {
		return nil, err
	}

	consumer, err := NewConsumerFromClient(client, groupID, topics)
	if err != nil {
		return nil, err
	}
	consumer.ownClient = true
	return consumer, nil
}
func (c *Consumer) MarkOffsets(s *OffsetStash) {
	s.mu.Lock()
	defer s.mu.Unlock()

	for tp, info := range s.offsets {
		if sub := c.subs.Fetch(tp.Topic, tp.Partition); sub != nil {
			sub.MarkOffset(info.Offset, info.Metadata)
		}
		delete(s.offsets, tp)
	}
}
func (c *Consumer) ResetOffset(msg *sarama.ConsumerMessage, metadata string) {
	if sub := c.subs.Fetch(msg.Topic, msg.Partition); sub != nil {
		sub.ResetOffset(msg.Offset, metadata)
	}
}
func (c *Consumer) Close() (err error) {
	c.closeOnce.Do(func() {
		close(c.dying)
		<-c.dead

		if e := c.release(); e != nil {
			err = e
		}
		if e := c.consumer.Close(); e != nil {
			err = e
		}
		close(c.messages)
		close(c.errors)

		if e := c.leaveGroup(); e != nil {
			err = e
		}
		close(c.partitions)
		close(c.notifications)

		// drain
		for range c.messages {
		}
		for range c.errors {
		}
		for p := range c.partitions {
			_ = p.Close()
		}
		for range c.notifications {
		}

		c.client.release()
		if c.ownClient {
			if e := c.client.Close(); e != nil {
				err = e
			}
		}
	})
	return
}
func (c *Consumer) hbLoop(stopped <-chan none) {
	ticker := time.NewTicker(c.client.config.Group.Heartbeat.Interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			switch err := c.heartbeat(); err {
			case nil, sarama.ErrNoError:
			case sarama.ErrNotCoordinatorForConsumer, sarama.ErrRebalanceInProgress:
				return
			default:
				c.handleError(&Error{Ctx: "heartbeat", error: err})
				return
			}
		case <-stopped:
			return
		case <-c.dying:
			return
		}
	}
}
func (c *Consumer) twLoop(stopped <-chan none) {
	ticker := time.NewTicker(c.client.config.Metadata.RefreshFrequency / 2)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			topics, err := c.client.Topics()
			if err != nil {
				c.handleError(&Error{Ctx: "topics", error: err})
				return
			}

			for _, topic := range topics {
				if !c.isKnownCoreTopic(topic) &&
					!c.isKnownExtraTopic(topic) &&
					c.isPotentialExtraTopic(topic) {
					return
				}
			}
		case <-stopped:
			return
		case <-c.dying:
			return
		}
	}
}
func (c *Consumer) cmLoop(stopped <-chan none) {
	ticker := time.NewTicker(c.client.config.Consumer.Offsets.CommitInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			if err := c.commitOffsetsWithRetry(c.client.config.Group.Offsets.Retry.Max); err != nil {
				c.handleError(&Error{Ctx: "commit", error: err})
				return
			}
		case <-stopped:
			return
		case <-c.dying:
			return
		}
	}
}
func (c *Consumer) fetchOffsets(subs map[string][]int32) (map[string]map[int32]offsetInfo, error) {
	offsets := make(map[string]map[int32]offsetInfo, len(subs))
	req := &sarama.OffsetFetchRequest{
		Version:       1,
		ConsumerGroup: c.groupID,
	}

	for topic, partitions := range subs {
		offsets[topic] = make(map[int32]offsetInfo, len(partitions))
		for _, partition := range partitions {
			offsets[topic][partition] = offsetInfo{Offset: -1}
			req.AddPartition(topic, partition)
		}
	}

	broker, err := c.client.Coordinator(c.groupID)
	if err != nil {
		c.closeCoordinator(broker, err)
		return nil, err
	}

	resp, err := broker.FetchOffset(req)
	if err != nil {
		c.closeCoordinator(broker, err)
		return nil, err
	}

	for topic, partitions := range subs {
		for _, partition := range partitions {
			block := resp.GetBlock(topic, partition)
			if block == nil {
				return nil, sarama.ErrIncompleteResponse
			}

			if block.Err == sarama.ErrNoError {
				offsets[topic][partition] = offsetInfo{Offset: block.Offset, Metadata: block.Metadata}
			} else {
				return nil, block.Err
			}
		}
	}
	return offsets, nil
}
func (s *OffsetStash) MarkOffset(msg *sarama.ConsumerMessage, metadata string) {
	s.MarkPartitionOffset(msg.Topic, msg.Partition, msg.Offset, metadata)
}
func (s *OffsetStash) ResetOffset(msg *sarama.ConsumerMessage, metadata string) {
	s.ResetPartitionOffset(msg.Topic, msg.Partition, msg.Offset, metadata)
}
func (s *OffsetStash) Offsets() map[string]int64 {
	s.mu.Lock()
	defer s.mu.Unlock()

	res := make(map[string]int64, len(s.offsets))
	for tp, info := range s.offsets {
		res[tp.String()] = info.Offset
	}
	return res
}
func (r *InstanceGroup) Actual(immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("instanceGroup.Actual")
	if r.CachedActual != nil {
		logger.Debug("Using cached instance [actual]")
		return immutable, r.CachedActual, nil
	}
	newResource := &InstanceGroup{
		Shared: Shared{
			Name:    r.Name,
			CloudID: r.ServerPool.Identifier,
		},
	}

	project, err := Sdk.Service.Projects.Get(immutable.ProviderConfig().CloudId).Do()
	if err != nil && project != nil {
		instances, err := Sdk.Service.Instances.List(immutable.ProviderConfig().CloudId, immutable.ProviderConfig().Location).Do()
		if err != nil {
			return nil, nil, err
		}

		count := len(instances.Items)
		if count > 0 {
			newResource.Count = count

			instance := instances.Items[0]
			newResource.Name = instance.Name
			newResource.CloudID = string(instance.Id)
			newResource.Size = instance.Kind
			newResource.Image = r.Image
			newResource.Location = instance.Zone
		}
	}

	newResource.BootstrapScripts = r.ServerPool.BootstrapScripts
	newResource.SSHFingerprint = immutable.ProviderConfig().SSH.PublicKeyFingerprint
	newResource.Name = r.Name
	r.CachedActual = newResource
	return immutable, newResource, nil
}
func (r *InstanceGroup) Expected(immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("instanceGroup.Expected")
	if r.CachedExpected != nil {
		logger.Debug("Using instance subnet [expected]")
		return immutable, r.CachedExpected, nil
	}
	expected := &InstanceGroup{
		Shared: Shared{
			Name:    r.Name,
			CloudID: r.ServerPool.Identifier,
		},
		Size:             r.ServerPool.Size,
		Location:         immutable.ProviderConfig().Location,
		Image:            r.ServerPool.Image,
		Count:            r.ServerPool.MaxCount,
		SSHFingerprint:   immutable.ProviderConfig().SSH.PublicKeyFingerprint,
		BootstrapScripts: r.ServerPool.BootstrapScripts,
	}
	r.CachedExpected = expected
	return immutable, expected, nil
}
func (r *InstanceGroup) Delete(actual cloud.Resource, immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("instanceGroup.Delete")
	deleteResource := actual.(*InstanceGroup)
	if deleteResource.Name == "" {
		return nil, nil, fmt.Errorf("Unable to delete instance resource without Name [%s]", deleteResource.Name)
	}

	logger.Success("Deleting InstanceGroup manager [%s]", r.ServerPool.Name)
	_, err := Sdk.Service.InstanceGroupManagers.Get(immutable.ProviderConfig().CloudId, immutable.ProviderConfig().Location, strings.ToLower(r.ServerPool.Name)).Do()
	if err == nil {
		_, err := Sdk.Service.InstanceGroupManagers.Delete(immutable.ProviderConfig().CloudId, immutable.ProviderConfig().Location, strings.ToLower(r.ServerPool.Name)).Do()
		if err != nil {
			return nil, nil, err
		}
	}

	_, err = Sdk.Service.InstanceTemplates.Get(immutable.ProviderConfig().CloudId, strings.ToLower(r.ServerPool.Name)).Do()
	if err == nil {
		err := r.retryDeleteInstanceTemplate(immutable)
		if err != nil {
			return nil, nil, err
		}
	}

	// Kubernetes API
	providerConfig := immutable.ProviderConfig()
	providerConfig.KubernetesAPI.Endpoint = ""
	immutable.SetProviderConfig(providerConfig)
	renderedCluster, err := r.immutableRender(actual, immutable)
	if err != nil {
		return nil, nil, err
	}
	return renderedCluster, actual, nil
}
func GetReconciler(known *cluster.Cluster, runtimeParameters *RuntimeParameters) (reconciler cloud.Reconciler, err error) {
	switch known.ProviderConfig().Cloud {
	case cluster.CloudGoogle:
		sdk, err := googleSDK.NewSdk()
		if err != nil {
			return nil, err
		}
		gr.Sdk = sdk
		return cloud.NewAtomicReconciler(known, compute.NewGoogleComputeModel(known)), nil
	case cluster.CloudDigitalOcean:
		sdk, err := godoSdk.NewSdk()
		if err != nil {
			return nil, err
		}
		dr.Sdk = sdk
		return cloud.NewAtomicReconciler(known, droplet.NewDigitalOceanDropletModel(known)), nil
	case cluster.CloudAmazon:
		awsProfile := ""
		if runtimeParameters != nil {
			awsProfile = runtimeParameters.AwsProfile
		}
		sdk, err := awsSdkGo.NewSdk(known.ProviderConfig().Location, awsProfile)
		if err != nil {
			return nil, err
		}
		ar.Sdk = sdk
		return cloud.NewAtomicReconciler(known, awspub.NewAmazonPublicModel(known)), nil
	case cluster.CloudAzure:
		sdk, err := azureSDK.NewSdk()
		if err != nil {
			return nil, err
		}
		azr.Sdk = sdk
		return cloud.NewAtomicReconciler(known, azpub.NewAzurePublicModel(known)), nil
	case cluster.CloudOVH:
		sdk, err := openstackSdk.NewSdk(known.ProviderConfig().Location)
		if err != nil {
			return nil, err
		}
		osr.Sdk = sdk
		return cloud.NewAtomicReconciler(known, osovh.NewOvhPublicModel(known)), nil
	case cluster.CloudPacket:
		sdk, err := packetSDK.NewSdk()
		if err != nil {
			return nil, err
		}
		packetr.Sdk = sdk
		return cloud.NewAtomicReconciler(known, packetpub.NewPacketPublicModel(known)), nil
	case cluster.CloudECS:
		sdk, err := openstackSdk.NewSdk(known.ProviderConfig().Location)
		if err != nil {
			return nil, err
		}
		osr.Sdk = sdk
		return cloud.NewAtomicReconciler(known, osecs.NewEcsPublicModel(known)), nil
	default:
		return nil, fmt.Errorf("Invalid cloud type: %s", known.ProviderConfig().Cloud)
	}
}
func GetVersion() *Version {
	return &Version{
		Version:   KubicornVersion,
		GitCommit: GitSha,
		BuildDate: time.Now().UTC().String(),
		GoVersion: runtime.Version(),
		GOOS:      runtime.GOOS,
		GOArch:    runtime.GOARCH,
	}
}
func GetVersionJSON() string {
	verBytes, err := json.Marshal(GetVersion())
	if err != nil {
		logger.Critical("Unable to marshal version struct: %v", err)
	}
	return string(verBytes)
}
func (r *ResourceGroup) Actual(immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("resourcegroup.Actual")
	newResource := &ResourceGroup{
		Shared: Shared{
			Name:       r.Name,
			Tags:       r.Tags,
			Identifier: immutable.ProviderConfig().GroupIdentifier,
		},
		Location: r.Location,
	}

	if r.Identifier != "" {
		group, err := Sdk.ResourceGroup.Get(immutable.Name)
		if err != nil {
			return nil, nil, err
		}
		newResource.Location = *group.Location
		newResource.Name = *group.Name
		newResource.Identifier = *group.ID
	}

	newCluster := r.immutableRender(newResource, immutable)
	return newCluster, newResource, nil
}
func (r *ResourceGroup) Expected(immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("resourcegroup.Expected")
	newResource := &ResourceGroup{
		Shared: Shared{
			Name:       immutable.Name,
			Tags:       r.Tags,
			Identifier: immutable.ProviderConfig().GroupIdentifier,
		},
		Location: immutable.ProviderConfig().Location,
	}
	newCluster := r.immutableRender(newResource, immutable)
	return newCluster, newResource, nil
}
func CreateCmd() *cobra.Command {
	var co = &cli.CreateOptions{}
	var createCmd = &cobra.Command{
		Use:   "create [NAME] [-p|--profile PROFILENAME] [-c|--cloudid CLOUDID]",
		Short: "Create a Kubicorn API model from a profile",
		Long: `Use this command to create a Kubicorn API model in a defined state store.

	This command will create a cluster API model as a YAML manifest in a state store.
	Once the API model has been created, a user can optionally change the model to their liking.
	After a model is defined and configured properly, the user can then apply the model.`,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				co.Name = viper.GetString(keyKubicornName)
				if co.Name == "" {
					co.Name = namer.RandomName()
				}
			case 1:
				co.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := RunCreate(co); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}

		},
	}

	fs := createCmd.Flags()

	bindCommonStateStoreFlags(&co.StateStoreOptions, fs)
	bindCommonAwsFlags(&co.AwsOptions, fs)

	fs.StringVarP(&co.Profile, keyProfile, "p", viper.GetString(keyProfile), descProfile)
	fs.StringVarP(&co.CloudID, keyCloudID, "c", viper.GetString(keyCloudID), descCloudID)
	fs.StringVar(&co.KubeConfigLocalFile, keyKubeConfigLocalFile, viper.GetString(keyKubeConfigLocalFile), descKubeConfigLocalFile)
	fs.StringArrayVarP(&co.Set, keySet, "C", viper.GetStringSlice(keySet), descSet)
	fs.StringArrayVarP(&co.MasterSet, keyMasterSet, "M", viper.GetStringSlice(keyMasterSet), descMasterSet)
	fs.StringArrayVarP(&co.NodeSet, keyNodeSet, "N", viper.GetStringSlice(keyNodeSet), descNodeSet)
	fs.StringVarP(&co.GitRemote, keyGitConfig, "g", viper.GetString(keyGitConfig), descGitConfig)
	fs.StringArrayVar(&co.AwsOptions.PolicyAttachments, keyPolicyAttachments, co.AwsOptions.PolicyAttachments, descPolicyAttachments)

	flagApplyAnnotations(createCmd, "profile", "__kubicorn_parse_profiles")
	flagApplyAnnotations(createCmd, "cloudid", "__kubicorn_parse_cloudid")

	createCmd.SetUsageTemplate(cli.UsageTemplate)

	return createCmd
}
func NewUbuntuCluster(name string) *cluster.Cluster {

	controlPlaneProviderConfig := &cluster.ControlPlaneProviderConfig{
		Cloud:    cluster.CloudAzure,
		Location: "eastus",
		SSH: &cluster.SSH{
			PublicKeyPath: "~/.ssh/id_rsa.pub",
			User:          "root",
		},
		KubernetesAPI: &cluster.KubernetesAPI{
			Port: "443",
		},
		Values: &cluster.Values{
			ItemMap: map[string]string{
				"INJECTEDTOKEN": kubeadm.GetRandomToken(),
			},
		},
	}
	machineSetsProviderConfigs := []*cluster.MachineProviderConfig{
		{
			ServerPool: &cluster.ServerPool{
				Type:             cluster.ServerPoolTypeMaster,
				Name:             fmt.Sprintf("%s-master", name),
				MaxCount:         1,
				Image:            "UbuntuServer",
				Size:             "Standard_DS3_v2 ",
				BootstrapScripts: []string{},
				Firewalls: []*cluster.Firewall{
					{
						Name: fmt.Sprintf("%s-master", name),
						IngressRules: []*cluster.IngressRule{
							{
								IngressToPort:   "22",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "tcp",
							},
							{
								IngressToPort:   "443",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "tcp",
							},
							{
								IngressToPort:   "1194",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "udp",
							},
						},
						EgressRules: []*cluster.EgressRule{
							{
								EgressToPort:      "all", // By default all egress from VM
								EgressDestination: "0.0.0.0/0",
								EgressProtocol:    "tcp",
							},
							{
								EgressToPort:      "all", // By default all egress from VM
								EgressDestination: "0.0.0.0/0",
								EgressProtocol:    "udp",
							},
						},
					},
				},
			},
		},
		{
			ServerPool: &cluster.ServerPool{
				Type:             cluster.ServerPoolTypeNode,
				Name:             fmt.Sprintf("%s-node", name),
				MaxCount:         1,
				Image:            "UbuntuServer",
				Size:             "Standard_DS3_v2 ",
				BootstrapScripts: []string{},
				Firewalls: []*cluster.Firewall{
					{
						Name: fmt.Sprintf("%s-node", name),
						IngressRules: []*cluster.IngressRule{
							{
								IngressToPort:   "22",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "tcp",
							},
							{
								IngressToPort:   "1194",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "udp",
							},
						},
						EgressRules: []*cluster.EgressRule{
							{
								EgressToPort:      "all", // By default all egress from VM
								EgressDestination: "0.0.0.0/0",
								EgressProtocol:    "tcp",
							},
							{
								EgressToPort:      "all", // By default all egress from VM
								EgressDestination: "0.0.0.0/0",
								EgressProtocol:    "udp",
							},
						},
					},
				},
			},
		},
	}
	c := cluster.NewCluster(name)
	c.SetProviderConfig(controlPlaneProviderConfig)
	c.NewMachineSetsFromProviderConfigs(machineSetsProviderConfigs)
	return c
}
func (c *Cluster) ProviderConfig() *ControlPlaneProviderConfig {
	//providerConfig providerConfig
	raw := c.ClusterAPI.Spec.ProviderConfig
	providerConfig := &ControlPlaneProviderConfig{}
	err := json.Unmarshal([]byte(raw), providerConfig)
	if err != nil {
		logger.Critical("Unable to unmarshal provider config: %v", err)
	}
	return providerConfig
}
func (c *Cluster) SetProviderConfig(config *ControlPlaneProviderConfig) error {
	bytes, err := json.Marshal(config)
	if err != nil {
		logger.Critical("Unable to marshal provider config: %v", err)
		return err
	}
	str := string(bytes)
	c.ClusterAPI.Spec.ProviderConfig = str
	return nil
}
func (c *Cluster) MachineProviderConfigs() []*MachineProviderConfig {
	var providerConfigs []*MachineProviderConfig
	for _, machineSet := range c.MachineSets {
		raw := machineSet.Spec.Template.Spec.ProviderConfig
		providerConfig := &MachineProviderConfig{}
		err := json.Unmarshal([]byte(raw), providerConfig)
		if err != nil {
			logger.Critical("Unable to unmarshal provider config: %v", err)
		}
		providerConfigs = append(providerConfigs, providerConfig)
	}
	return providerConfigs
}
func (c *Cluster) SetMachineProviderConfigs(providerConfigs []*MachineProviderConfig) {
	for _, providerConfig := range providerConfigs {
		name := providerConfig.ServerPool.Name
		found := false
		for _, machineSet := range c.MachineSets {
			if machineSet.Name == name {
				//logger.Debug("Matched machine set to provider config: %s", name)
				bytes, err := json.Marshal(providerConfig)
				if err != nil {
					logger.Critical("unable to marshal machine provider config: %v")
					continue
				}
				str := string(bytes)
				machineSet.Spec.Template.Spec.ProviderConfig = str
				found = true
			}
		}

		// TODO
		// @kris-nova
		// Right now if we have a machine provider config and we can't match it
		// we log a warning and move on. We might want to change this to create
		// the machineSet moving forward..
		if !found {
			logger.Warning("Unable to match provider config to machine set: %s", name)
		}

	}

}
func NewCluster(name string) *Cluster {
	return &Cluster{
		Name: name,
		ClusterAPI: &clusterv1.Cluster{
			ObjectMeta: metav1.ObjectMeta{
				Name: name,
			},
			Spec: clusterv1.ClusterSpec{},
		},
		ControlPlane: &clusterv1.MachineSet{},
	}
}
func DeployControllerCmd() *cobra.Command {
	var dco = &cli.DeployControllerOptions{}
	var deployControllerCmd = &cobra.Command{
		Use:   "deploycontroller <NAME>",
		Short: "Deploy a controller for a given cluster",
		Long: `Use this command to deploy a controller for a given cluster.

As long as a controller is defined, this will create the deployment and the namespace.`,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				dco.Name = viper.GetString(keyKubicornName)
			case 1:
				dco.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := runDeployController(dco); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}

		},
	}

	fs := deployControllerCmd.Flags()

	bindCommonStateStoreFlags(&dco.StateStoreOptions, fs)
	bindCommonAwsFlags(&dco.AwsOptions, fs)

	fs.StringVar(&dco.GitRemote, keyGitConfig, viper.GetString(keyGitConfig), descGitConfig)

	return deployControllerCmd
}
func NewRetrier(retries, sleepSeconds int, retryable Retryable) *Retrier {
	return &Retrier{
		retries:      retries,
		sleepSeconds: sleepSeconds,
		retryable:    retryable,
	}
}
func (r *Retrier) RunRetry() error {
	// Start signal handler.
	sigHandler := signals.NewSignalHandler(10)
	go sigHandler.Register()

	finish := make(chan bool, 1)
	go func() {
		select {
		case <-finish:
			return
		case <-time.After(10 * time.Second):
			return
		default:
			for {
				if sigHandler.GetState() != 0 {
					logger.Critical("detected signal. retry failed.")
					os.Exit(1)
				}
			}
		}
	}()

	for i := 0; i < r.retries; i++ {
		err := r.retryable.Try()
		if err != nil {
			logger.Info("Retryable error: %v", err)
			time.Sleep(time.Duration(r.sleepSeconds) * time.Second)
			continue
		}
		finish <- true
		return nil
	}

	finish <- true
	return fmt.Errorf("unable to succeed at retry after %d attempts at %d seconds", r.retries, r.sleepSeconds)
}
func MustGenerateRandomBytes(length int) []byte {
	res, err := GenerateRandomBytes(length)

	if err != nil {
		panic("Could not generate random bytes")
	}

	return res
}
func ExplainCmd() *cobra.Command {
	var exo = &cli.ExplainOptions{}

	var cmd = &cobra.Command{
		Use:   "explain",
		Short: "Explain cluster",
		Long:  `Output expected and actual state of the given cluster`,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				exo.Name = viper.GetString(keyKubicornName)
			case 1:
				exo.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := runExplain(exo); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}
		},
	}

	fs := cmd.Flags()

	bindCommonStateStoreFlags(&exo.StateStoreOptions, fs)
	bindCommonAwsFlags(&exo.AwsOptions, fs)

	fs.StringVarP(&exo.Output, keyOutput, "o", viper.GetString(keyOutput), descOutput)
	fs.StringVar(&exo.GitRemote, keyGitConfig, viper.GetString(keyGitConfig), descGitConfig)

	return cmd
}
func TimeOrderedUUID() string {
	unixTime := uint32(time.Now().UTC().Unix())
	return fmt.Sprintf("%08x-%04x-%04x-%04x-%04x%08x",
		unixTime,
		rand.MustGenerateRandomBytes(2),
		rand.MustGenerateRandomBytes(2),
		rand.MustGenerateRandomBytes(2),
		rand.MustGenerateRandomBytes(2),
		rand.MustGenerateRandomBytes(4))
}
func GetConfigCmd() *cobra.Command {
	var cro = &cli.GetConfigOptions{}
	var getConfigCmd = &cobra.Command{
		Use:   "getconfig <NAME>",
		Short: "Manage Kubernetes configuration",
		Long: `Use this command to pull a kubeconfig file from a cluster so you can use kubectl.
	
	This command will attempt to find a cluster, and append a local kubeconfig file with a kubeconfig `,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				cro.Name = viper.GetString(keyKubicornName)
			case 1:
				cro.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := runGetConfig(cro); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}

		},
	}

	fs := getConfigCmd.Flags()

	bindCommonStateStoreFlags(&cro.StateStoreOptions, fs)
	bindCommonAwsFlags(&cro.AwsOptions, fs)

	fs.StringVar(&cro.GitRemote, keyGitConfig, viper.GetString(keyGitConfig), descGitConfig)

	return getConfigCmd
}
func RunAnnotated(task Task, description string, symbol string, options ...interface{}) error {
	doneCh := make(chan bool)
	errCh := make(chan error)

	l := logger.Log
	t := DefaultTicker

	for _, o := range options {
		if value, ok := o.(logger.Logger); ok {
			l = value
		} else if value, ok := o.(*time.Ticker); ok {
			t = value
		}
	}

	go func() {
		errCh <- task()
	}()

	l(description)
	logActivity(symbol, l, t, doneCh)

	err := <-errCh
	doneCh <- true

	return err
}
func ListCmd() *cobra.Command {
	var lo = &cli.ListOptions{}
	var cmd = &cobra.Command{
		Use:   "list",
		Short: "List available states",
		Long:  `List the states available in the _state directory`,
		Run: func(cmd *cobra.Command, args []string) {
			if err := runList(lo); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}
		},
	}

	fs := cmd.Flags()

	bindCommonStateStoreFlags(&lo.StateStoreOptions, fs)
	bindCommonAwsFlags(&lo.AwsOptions, fs)

	fs.BoolVarP(&noHeaders, keyNoHeaders, "n", viper.GetBool(keyNoHeaders), desNoHeaders)

	return cmd
}
func NewUbuntuCluster(name string) *cluster.Cluster {

	controlPlaneProviderConfig := &cluster.ControlPlaneProviderConfig{
		Cloud: cluster.CloudPacket,
		Project: &cluster.Project{
			Name: fmt.Sprintf("kubicorn-%s", name),
		},
		Location: "ewr1",
		SSH: &cluster.SSH{
			PublicKeyPath: "~/.ssh/id_rsa.pub",
			User:          "root",
		},
		KubernetesAPI: &cluster.KubernetesAPI{
			Port: "443",
		},
		Values: &cluster.Values{
			ItemMap: map[string]string{
				"INJECTEDTOKEN": kubeadm.GetRandomToken(),
			},
		},
	}
	machineSetsProviderConfigs := []*cluster.MachineProviderConfig{
		{
			ServerPool: &cluster.ServerPool{
				Type:     cluster.ServerPoolTypeMaster,
				Name:     fmt.Sprintf("%s.master", name),
				MaxCount: 1,
				MinCount: 1,
				Image:    "ubuntu_16_04",
				Size:     "baremetal_1",
				BootstrapScripts: []string{
					"bootstrap/packet_k8s_ubuntu_16.04_master.sh",
				},
			},
		},
		{
			ServerPool: &cluster.ServerPool{
				Type:     cluster.ServerPoolTypeNode,
				Name:     fmt.Sprintf("%s.node", name),
				MaxCount: 1,
				MinCount: 1,
				Image:    "ubuntu_16_04",
				Size:     "baremetal_2",
				BootstrapScripts: []string{
					"bootstrap/packet_k8s_ubuntu_16.04_node.sh",
				},
			},
		},
	}
	c := cluster.NewCluster(name)
	c.SetProviderConfig(controlPlaneProviderConfig)
	c.NewMachineSetsFromProviderConfigs(machineSetsProviderConfigs)
	return c
}
func EditCmd() *cobra.Command {
	var eo = &cli.EditOptions{}
	var editCmd = &cobra.Command{
		Use:   "edit <NAME>",
		Short: "Edit a cluster state",
		Long:  `Use this command to edit a state.`,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				eo.Name = viper.GetString(keyKubicornName)
			case 1:
				eo.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := runEdit(eo); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}

		},
	}

	fs := editCmd.Flags()

	bindCommonStateStoreFlags(&eo.StateStoreOptions, fs)
	bindCommonAwsFlags(&eo.AwsOptions, fs)

	fs.StringVarP(&eo.Editor, keyEditor, "e", viper.GetString(keyEditor), descEditor)
	fs.StringVar(&eo.GitRemote, keyGitConfig, viper.GetString(keyGitConfig), descGitConfig)

	return editCmd
}
func (k *Keyring) RemoveKey(key ssh.PublicKey) error {
	return k.Agent.Remove(key)
}
func (k *Keyring) RemoveKeyUsingFile(pubkey string) error {
	p, err := ioutil.ReadFile(pubkey)
	if err != nil {
		return err
	}

	key, _, _, _, _ := ssh.ParseAuthorizedKey(p)
	if err != nil {
		return err
	}

	return k.RemoveKey(key)
}
func (r *Firewall) Actual(immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("firewall.Actual")

	newResource := defaultFirewallStruct()
	// Digital Firewalls.Get requires firewall ID, which we will not always have.thats why using List.
	firewalls, _, err := Sdk.Client.Firewalls.List(context.TODO(), &godo.ListOptions{})
	if err != nil {
		return nil, nil, fmt.Errorf("failed to get firwalls info")
	}
	for _, firewall := range firewalls {
		if firewall.Name == r.Name { // In digitalOcean Firwall names are unique.
			// gotcha get all details from this firewall and populate actual.
			firewallBytes, err := json.Marshal(firewall)
			if err != nil {
				return nil, nil, fmt.Errorf("failed to marshal DO firewall details err: %v", err)
			}
			if err := json.Unmarshal(firewallBytes, newResource); err != nil {
				return nil, nil, fmt.Errorf("failed to unmarhal DO firewall details err: %v", err)
			}
			// hack: DO api doesn't take "0" as portRange, but returns "0" for port range in firewall.List.
			for i := 0; i < len(newResource.OutboundRules); i++ {
				if newResource.OutboundRules[i].PortRange == "0" {
					newResource.OutboundRules[i].PortRange = "all"
				}
			}
			for i := 0; i < len(newResource.InboundRules); i++ {
				if newResource.InboundRules[i].PortRange == "0" {
					newResource.InboundRules[i].PortRange = "all"
				}
			}
		}
	}

	newCluster := r.immutableRender(newResource, immutable)
	return newCluster, newResource, nil
}
func (r *Firewall) Expected(immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("firewall.Expected")
	newResource := &Firewall{
		Shared: Shared{
			Name:    r.Name,
			CloudID: r.ServerPool.Identifier,
		},
		InboundRules:  r.InboundRules,
		OutboundRules: r.OutboundRules,
		DropletIDs:    r.DropletIDs,
		Tags:          r.Tags,
		FirewallID:    r.FirewallID,
		Status:        r.Status,
		Created:       r.Created,
	}

	//logger.Info("Expected firewall returned is %+v", immutable)
	newCluster := r.immutableRender(newResource, immutable)
	return newCluster, newResource, nil

}
func (r *Firewall) Apply(actual, expected cloud.Resource, immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("firewall.Apply")
	expectedResource := expected.(*Firewall)
	actualResource := actual.(*Firewall)

	isEqual, err := compare.IsEqual(actualResource, expectedResource)
	if err != nil {
		return nil, nil, err
	}
	if isEqual {
		return immutable, expected, nil
	}

	firewallRequest := godo.FirewallRequest{
		Name:          expectedResource.Name,
		InboundRules:  convertInRuleType(expectedResource.InboundRules),
		OutboundRules: convertOutRuleType(expectedResource.OutboundRules),
		DropletIDs:    expectedResource.DropletIDs,
		Tags:          expectedResource.Tags,
	}

	// Make sure Droplets are fully created before applying a firewall
	machineProviderConfigs := immutable.MachineProviderConfigs()
	for _, machineProviderConfig := range machineProviderConfigs {
		for i := 0; i <= TagsGetAttempts; i++ {
			active := true
			droplets, _, err := Sdk.Client.Droplets.ListByTag(context.TODO(), machineProviderConfig.ServerPool.Name, &godo.ListOptions{})
			if err != nil {
				logger.Debug("Hanging for droplets to get created.. (%v)", err)
				time.Sleep(time.Duration(TagsGetTimeout) * time.Second)
				continue
			}
			if len(droplets) == 0 {
				continue
			}
			for _, d := range droplets {
				if d.Status != "active" {
					active = false
					break
				}
			}
			if !active {
				logger.Debug("Waiting for droplets to become active..")
				time.Sleep(time.Duration(TagsGetTimeout) * time.Second)
				continue
			}
			break
		}
	}

	firewall, _, err := Sdk.Client.Firewalls.Create(context.TODO(), &firewallRequest)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to create the firewall err: %v", err)
	}
	logger.Success("Created Firewall [%s]", firewall.ID)
	newResource := &Firewall{
		Shared: Shared{
			CloudID: firewall.ID,
			Name:    r.Name,
			Tags:    r.Tags,
		},
		DropletIDs:    r.DropletIDs,
		FirewallID:    firewall.ID,
		InboundRules:  r.InboundRules,
		OutboundRules: r.OutboundRules,
		Created:       r.Created,
	}

	newCluster := r.immutableRender(newResource, immutable)
	return newCluster, newResource, nil
}
func (r *Firewall) Delete(actual cloud.Resource, immutable *cluster.Cluster) (*cluster.Cluster, cloud.Resource, error) {
	logger.Debug("firewall.Delete")
	deleteResource, ok := actual.(*Firewall)
	if !ok {
		return nil, nil, fmt.Errorf("failed to type convert actual Firewall type ")
	}
	if deleteResource.Name == "" {
		return immutable, nil, nil
		return nil, nil, fmt.Errorf("Unable to delete firewall resource without Name [%s]", deleteResource.Name)
	}
	if _, err := Sdk.Client.Firewalls.Delete(context.TODO(), deleteResource.FirewallID); err != nil {
		return nil, nil, fmt.Errorf("failed to delete firewall [%s] err: %v", deleteResource.Name, err)
	}
	logger.Success("Deleted firewall [%s]", deleteResource.FirewallID)

	newResource := &Firewall{
		Shared: Shared{
			Name: r.Name,
			Tags: r.Tags,
		},
		InboundRules:  r.InboundRules,
		OutboundRules: r.OutboundRules,
		Created:       r.Created,
	}

	newCluster := r.immutableRender(newResource, immutable)
	return newCluster, newResource, nil
}
func DeleteCmd() *cobra.Command {
	var do = &cli.DeleteOptions{}
	var deleteCmd = &cobra.Command{
		Use:   "delete <NAME>",
		Short: "Delete a Kubernetes cluster",
		Long: `Use this command to delete cloud resources.
	
	This command will attempt to build the resource graph based on an API model.
	Once the graph is built, the delete will attempt to delete the resources from the cloud.
	After the delete is complete, the state store will be left in tact and could potentially be applied later.
	
	To delete the resource AND the API model in the state store, use --purge.`,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				do.Name = viper.GetString(keyKubicornName)
			case 1:
				do.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := runDelete(do); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}

		},
	}

	fs := deleteCmd.Flags()

	bindCommonStateStoreFlags(&do.StateStoreOptions, fs)
	bindCommonAwsFlags(&do.AwsOptions, fs)

	fs.StringVar(&do.AwsProfile, keyAwsProfile, viper.GetString(keyAwsProfile), descAwsProfile)
	fs.StringVar(&do.GitRemote, keyGitConfig, viper.GetString(keyGitConfig), descGitConfig)
	fs.BoolVarP(&do.Purge, keyPurge, "p", viper.GetBool(keyPurge), descPurge)

	return deleteCmd
}
func (options Options) NewStateStore() (state.ClusterStorer, error) {
	var stateStore state.ClusterStorer

	switch options.StateStore {
	case "fs":
		logger.Info("Selected [fs] state store")
		stateStore = fs.NewFileSystemStore(&fs.FileSystemStoreOptions{
			BasePath:    options.StateStorePath,
			ClusterName: options.Name,
		})
	case "crd":
		logger.Info("Selected [crd] state store")
		stateStore = crd.NewCRDStore(&crd.CRDStoreOptions{
			BasePath:    options.StateStorePath,
			ClusterName: options.Name,
		})
	case "git":
		logger.Info("Selected [git] state store")
		if options.GitRemote == "" {
			return nil, errors.New("empty GitRemote url. Must specify the link to the remote git repo")
		}
		user, _ := gg.Global("user.name")
		email, _ := gg.Email()

		stateStore = git.NewJSONGitStore(&git.JSONGitStoreOptions{
			BasePath:    options.StateStorePath,
			ClusterName: options.Name,
			CommitConfig: &git.JSONGitCommitConfig{
				Name:   user,
				Email:  email,
				Remote: options.GitRemote,
			},
		})
	case "jsonfs":
		logger.Info("Selected [jsonfs] state store")
		stateStore = jsonfs.NewJSONFileSystemStore(&jsonfs.JSONFileSystemStoreOptions{
			BasePath:    options.StateStorePath,
			ClusterName: options.Name,
		})
	case "s3":
		logger.Info("Selected [s3] state store")
		client, err := minio.New(options.BucketEndpointURL, options.S3AccessKey, options.S3SecretKey, options.BucketSSL)
		if err != nil {
			return nil, err
		}
		stateStore = s3.NewJSONFS3Store(&s3.JSONS3StoreOptions{
			BasePath:    options.StateStorePath,
			ClusterName: options.Name,
			Client:      client,
			BucketOptions: &s3.S3BucketOptions{
				EndpointURL: options.BucketEndpointURL,
				BucketName:  options.BucketName,
			},
		})
	default:
		return nil, fmt.Errorf("state store [%s] has an invalid type [%s]", options.Name, options.StateStore)
	}

	return stateStore, nil
}
func (git *JSONGitStore) Commit(c *cluster.Cluster) error {
	if c == nil {
		return fmt.Errorf("Nil cluster spec")
	}
	bytes, err := json.Marshal(c)
	if err != nil {
		return err
	}

	//writes latest changes to git repo.
	git.Write(state.ClusterJSONFile, bytes)

	//commits the changes
	r, err := g.NewFilesystemRepository(state.ClusterJSONFile)
	if err != nil {
		return err
	}

	// Add a new remote, with the default fetch refspec
	_, err = r.CreateRemote(&config.RemoteConfig{
		Name: git.ClusterName,
		URL:  git.options.CommitConfig.Remote,
	})
	_, err = r.Commits()
	if err != nil {
		return err
	}
	return nil
}
func ApplyCmd() *cobra.Command {
	var ao = &cli.ApplyOptions{}
	var applyCmd = &cobra.Command{
		Use:   "apply <NAME>",
		Short: "Apply a cluster resource to a cloud",
		Long: `Use this command to apply an API model in a cloud.

	This command will attempt to find an API model in a defined state store, and then apply any changes needed directly to a cloud.
	The apply will run once, and ultimately time out if something goes wrong.`,
		Run: func(cmd *cobra.Command, args []string) {
			switch len(args) {
			case 0:
				ao.Name = viper.GetString(keyKubicornName)
			case 1:
				ao.Name = args[0]
			default:
				logger.Critical("Too many arguments.")
				os.Exit(1)
			}

			if err := runApply(ao); err != nil {
				logger.Critical(err.Error())
				os.Exit(1)
			}
		},
	}

	fs := applyCmd.Flags()

	bindCommonStateStoreFlags(&ao.StateStoreOptions, fs)
	bindCommonAwsFlags(&ao.AwsOptions, fs)

	fs.StringArrayVarP(&ao.Set, keyKubicornSet, "e", viper.GetStringSlice(keyKubicornSet), descSet)
	fs.StringVar(&ao.AwsProfile, keyAwsProfile, viper.GetString(keyAwsProfile), descAwsProfile)
	fs.StringVar(&ao.GitRemote, keyGitConfig, viper.GetString(keyGitConfig), descGitConfig)

	return applyCmd
}
func ExpandPath(path string) string {
	switch path {
	case ".":
		wd, err := os.Getwd()
		if err != nil {
			logger.Critical("Unable to get current working directory: %v", err)
			return ""
		}
		path = wd
	case "~":
		homeVar := os.Getenv("HOME")
		if homeVar == "" {
			homeUser, err := user.Current()
			if err != nil {
				logger.Critical("Unable to use user.Current() for user. Maybe a cross compile issue: %v", err)
				return ""
			}
			path = homeUser.HomeDir
		}
	}

	return path
}
func CompletionCmd() *cobra.Command {
	return &cobra.Command{
		Use:   "completion",
		Short: "Generate completion code for bash and zsh shells.",
		Long: `completion is used to output completion code for bash and zsh shells.
	
	Before using completion features, you have to source completion code
	from your .profile. This is done by adding following line to one of above files:
		source <(kubicorn completion SHELL)
	Valid arguments for SHELL are: "bash" and "zsh".
	Notes:
	1) zsh completions requires zsh 5.2 or newer.
		
	2) macOS users have to install bash-completion framework to utilize
	completion features. This can be done using homebrew:
		brew install bash-completion
	Once installed, you must load bash_completion by adding following
	line to your .profile or .bashrc/.zshrc:
		source $(brew --prefix)/etc/bash_completion`,
		RunE: func(cmd *cobra.Command, args []string) error {
			if logger.Fabulous {
				cmd.SetOutput(logger.FabulousWriter)
			}

			if viper.GetString(keyTrueColor) != "" {
				cmd.SetOutput(logger.FabulousWriter)
			}

			switch len(args) {
			case 0:
				return fmt.Errorf("shell argument is not specified")
			default:
				switch args[0] {
				case "bash":
					return runBashGeneration()
				case "zsh":
					return runZshGeneration()
				default:
					return fmt.Errorf("invalid shell argument")
				}
			}
		},
	}
}
func AdoptCmd() *cobra.Command {
	return &cobra.Command{
		Use:   "adopt",
		Short: "Adopt a Kubernetes cluster into a Kubicorn state store",
		Long: `Use this command to audit and adopt a Kubernetes cluster into a Kubicorn state store.
	
	This command will query cloud resources and attempt to build a representation of the cluster in the Kubicorn API model.
	Once the cluster has been adopted, a user can manage and scale their Kubernetes cluster with Kubicorn.`,
		Run: func(cmd *cobra.Command, args []string) {
			fmt.Println("adopt called")
		},
	}
}
func StrEnvDef(env string, def string) string {
	val := os.Getenv(env)
	if val == "" {
		return def
	}
	return val
}
func IntEnvDef(env string, def int) int {
	val := os.Getenv(env)
	if val == "" {
		return def
	}
	ival, err := strconv.Atoi(val)
	if err != nil {
		return def
	}
	return ival
}
func BoolEnvDef(env string, def bool) bool {
	val := os.Getenv(env)
	if val == "" {
		return def
	}
	b, err := strconv.ParseBool(val)
	if err != nil {
		return def
	}
	return b
}
func readFromFS(sourcePath string) (string, error) {

	// If sourcePath starts with ~ we search for $HOME
	// and preppend it to the absolutePath overwriting the first character
	// TODO: Add Windows support
	if strings.HasPrefix(sourcePath, "~") {
		homeDir := os.Getenv("HOME")
		if homeDir == "" {
			return "", fmt.Errorf("Could not find $HOME")
		}
		sourcePath = filepath.Join(homeDir, sourcePath[1:])
	}

	bytes, err := ioutil.ReadFile(sourcePath)
	if err != nil {
		return "", err
	}
	return string(bytes), nil
}
func VersionCmd() *cobra.Command {
	return &cobra.Command{
		Use:   "version",
		Short: "Verify Kubicorn version",
		Long: `Use this command to check the version of Kubicorn.
	
	This command will return the version of the Kubicorn binary.`,
		Run: func(cmd *cobra.Command, args []string) {
			fmt.Printf("%s\n", version.GetVersionJSON())
		},
	}
}
func NewSignalHandler(timeoutSeconds int) *Handler {
	signals := make(chan os.Signal)
	signal.Notify(signals, os.Interrupt, os.Kill)
	return &Handler{
		timeoutSeconds: timeoutSeconds,
		signals:        signals,
		signalReceived: 0,
	}
}
func (h *Handler) Register() {
	go func() {
		h.timer = time.NewTimer(time.Duration(h.timeoutSeconds) * time.Second)
		for {
			select {
			case s := <-h.signals:
				switch {
				case s == os.Interrupt:
					if h.signalReceived == 0 {
						h.signalReceived = 1
						logger.Debug("SIGINT Received")
						continue
					}
					h.signalReceived = signalTerminate
					debug.PrintStack()
					os.Exit(130)
					break
				case s == syscall.SIGQUIT:
					h.signalReceived = signalAbort
					break
				case s == syscall.SIGTERM:
					h.signalReceived = signalTerminate
					os.Exit(3)
					break
				}
			case <-h.timer.C:
				os.Exit(4)
				break
			}
		}

	}()
}
func NewUbuntuCluster(name string) *cluster.Cluster {
	var (
		masterName = fmt.Sprintf("%s-master", name)
		nodeName   = fmt.Sprintf("%s-node", name)
	)
	controlPlaneProviderConfig := &cluster.ControlPlaneProviderConfig{
		Cloud:    cluster.CloudECS,
		Location: "nl-ams1",
		SSH: &cluster.SSH{
			PublicKeyPath: "~/.ssh/id_rsa.pub",
			User:          "ubuntu",
		},
		Values: &cluster.Values{
			ItemMap: map[string]string{
				"INJECTEDTOKEN": kubeadm.GetRandomToken(),
			},
		},
		KubernetesAPI: &cluster.KubernetesAPI{
			Port: "443",
		},
		Network: &cluster.Network{
			Type: cluster.NetworkTypePublic,
			InternetGW: &cluster.InternetGW{
				Name: "default",
			},
		},
	}
	machineSetsProviderConfigs := []*cluster.MachineProviderConfig{
		{
			ServerPool: &cluster.ServerPool{
				Type:     cluster.ServerPoolTypeMaster,
				Name:     masterName,
				MaxCount: 1,
				Image:    "GNU/Linux Ubuntu Server 16.04 Xenial Xerus x64",
				Size:     "e3standard.x3",
				BootstrapScripts: []string{
					"bootstrap/ecs_k8s_ubuntu_16.04_master.sh",
				},
				Subnets: []*cluster.Subnet{
					{
						Name: "internal",
						CIDR: "192.168.200.0/24",
					},
				},
				Firewalls: []*cluster.Firewall{
					{
						Name: masterName,
						IngressRules: []*cluster.IngressRule{
							{
								IngressFromPort: "22",
								IngressToPort:   "22",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "tcp",
							},
							{
								IngressFromPort: "443",
								IngressToPort:   "443",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "tcp",
							},
							{
								IngressSource: "192.168.200.0/24",
							},
						},
					},
				},
			},
		},
		{
			ServerPool: &cluster.ServerPool{
				Type:     cluster.ServerPoolTypeNode,
				Name:     nodeName,
				MaxCount: 2,
				Image:    "GNU/Linux Ubuntu Server 16.04 Xenial Xerus x64",
				Size:     "e3standard.x3",
				BootstrapScripts: []string{
					"bootstrap/ecs_k8s_ubuntu_16.04_node.sh",
				},
				Firewalls: []*cluster.Firewall{
					{
						Name: nodeName,
						IngressRules: []*cluster.IngressRule{
							{
								IngressFromPort: "22",
								IngressToPort:   "22",
								IngressSource:   "0.0.0.0/0",
								IngressProtocol: "tcp",
							},
							{
								IngressSource: "192.168.200.0/24",
							},
						},
					},
				},
			},
		},
	}
	c := cluster.NewCluster(name)
	c.SetProviderConfig(controlPlaneProviderConfig)
	c.NewMachineSetsFromProviderConfigs(machineSetsProviderConfigs)
	return c
}
func (now *Now) BeginningOfHour() time.Time {
	y, m, d := now.Date()
	return time.Date(y, m, d, now.Time.Hour(), 0, 0, 0, now.Time.Location())
}
func (now *Now) BeginningOfDay() time.Time {
	y, m, d := now.Date()
	return time.Date(y, m, d, 0, 0, 0, 0, now.Time.Location())
}
func (now *Now) BeginningOfWeek() time.Time {
	t := now.BeginningOfDay()
	weekday := int(t.Weekday())

	if WeekStartDay != time.Sunday {
		weekStartDayInt := int(WeekStartDay)

		if weekday < weekStartDayInt {
			weekday = weekday + 7 - weekStartDayInt
		} else {
			weekday = weekday - weekStartDayInt
		}
	}
	return t.AddDate(0, 0, -weekday)
}
func (now *Now) BeginningOfMonth() time.Time {
	y, m, _ := now.Date()
	return time.Date(y, m, 1, 0, 0, 0, 0, now.Location())
}
func (now *Now) BeginningOfQuarter() time.Time {
	month := now.BeginningOfMonth()
	offset := (int(month.Month()) - 1) % 3
	return month.AddDate(0, -offset, 0)
}
func (now *Now) BeginningOfYear() time.Time {
	y, _, _ := now.Date()
	return time.Date(y, time.January, 1, 0, 0, 0, 0, now.Location())
}
func (now *Now) EndOfMinute() time.Time {
	return now.BeginningOfMinute().Add(time.Minute - time.Nanosecond)
}
func (now *Now) EndOfHour() time.Time {
	return now.BeginningOfHour().Add(time.Hour - time.Nanosecond)
}
func (now *Now) EndOfDay() time.Time {
	y, m, d := now.Date()
	return time.Date(y, m, d, 23, 59, 59, int(time.Second-time.Nanosecond), now.Location())
}
func (now *Now) EndOfWeek() time.Time {
	return now.BeginningOfWeek().AddDate(0, 0, 7).Add(-time.Nanosecond)
}
func (now *Now) EndOfMonth() time.Time {
	return now.BeginningOfMonth().AddDate(0, 1, 0).Add(-time.Nanosecond)
}
func (now *Now) EndOfQuarter() time.Time {
	return now.BeginningOfQuarter().AddDate(0, 3, 0).Add(-time.Nanosecond)
}
func (now *Now) EndOfYear() time.Time {
	return now.BeginningOfYear().AddDate(1, 0, 0).Add(-time.Nanosecond)
}
func (now *Now) MustParse(strs ...string) (t time.Time) {
	t, err := now.Parse(strs...)
	if err != nil {
		panic(err)
	}
	return t
}
func (now *Now) Between(begin, end string) bool {
	beginTime := now.MustParse(begin)
	endTime := now.MustParse(end)
	return now.After(beginTime) && now.Before(endTime)
}
func ParseInLocation(loc *time.Location, strs ...string) (time.Time, error) {
	return New(time.Now().In(loc)).Parse(strs...)
}
func MustParse(strs ...string) time.Time {
	return New(time.Now()).MustParse(strs...)
}
func MustParseInLocation(loc *time.Location, strs ...string) time.Time {
	return New(time.Now().In(loc)).MustParse(strs...)
}
func Between(time1, time2 string) bool {
	return New(time.Now()).Between(time1, time2)
}
func NewChannelMemoryBackend(size int) *ChannelMemoryBackend {
	backend := &ChannelMemoryBackend{
		maxSize:  size,
		incoming: make(chan *Record, 1024),
		events:   make(chan event),
	}
	backend.Start()
	return backend
}
func (b *ChannelMemoryBackend) Start() {
	b.mu.Lock()
	defer b.mu.Unlock()

	// Launch the goroutine unless it's already running.
	if b.running != true {
		b.running = true
		b.stopWg.Add(1)
		go b.process()
	}
}
func (b *ChannelMemoryBackend) Flush() {
	b.flushWg.Add(1)
	b.events <- eventFlush
	b.flushWg.Wait()
}
func (b *ChannelMemoryBackend) Stop() {
	b.mu.Lock()
	if b.running == true {
		b.running = false
		b.events <- eventStop
	}
	b.mu.Unlock()
	b.stopWg.Wait()
}
func (r *Record) Formatted(calldepth int) string {
	if r.formatted == "" {
		var buf bytes.Buffer
		r.formatter.Format(calldepth+1, r, &buf)
		r.formatted = buf.String()
	}
	return r.formatted
}
func (r *Record) Message() string {
	if r.message == nil {
		// Redact the arguments that implements the Redactor interface
		for i, arg := range r.Args {
			if redactor, ok := arg.(Redactor); ok == true {
				r.Args[i] = redactor.Redacted()
			}
		}
		var buf bytes.Buffer
		if r.fmt != nil {
			fmt.Fprintf(&buf, *r.fmt, r.Args...)
		} else {
			// use Fprintln to make sure we always get space between arguments
			fmt.Fprintln(&buf, r.Args...)
			buf.Truncate(buf.Len() - 1) // strip newline
		}
		msg := buf.String()
		r.message = &msg
	}
	return *r.message
}
func (l *Logger) SetBackend(backend LeveledBackend) {
	l.backend = backend
	l.haveBackend = true
}
func MustGetLogger(module string) *Logger {
	logger, err := GetLogger(module)
	if err != nil {
		panic("logger: " + module + ": " + err.Error())
	}
	return logger
}
func Reset() {
	// TODO make a global Init() method to be less magic? or make it such that
	// if there's no backends at all configured, we could use some tricks to
	// automatically setup backends based if we have a TTY or not.
	sequenceNo = 0
	b := SetBackend(NewLogBackend(os.Stderr, "", log.LstdFlags))
	b.SetLevel(DEBUG, "")
	SetFormatter(DefaultFormatter)
	timeNow = time.Now
}
func (l *Logger) IsEnabledFor(level Level) bool {
	return defaultBackend.IsEnabledFor(level, l.Module)
}
func (l *Logger) Criticalf(format string, args ...interface{}) {
	l.log(CRITICAL, &format, args...)
}
func (l *Logger) Warningf(format string, args ...interface{}) {
	l.log(WARNING, &format, args...)
}
func (l *Logger) Noticef(format string, args ...interface{}) {
	l.log(NOTICE, &format, args...)
}
func (l *Logger) Infof(format string, args ...interface{}) {
	l.log(INFO, &format, args...)
}
func SetFormatter(f Formatter) {
	formatter.Lock()
	defer formatter.Unlock()
	formatter.def = f
}
func MustStringFormatter(format string) Formatter {
	f, err := NewStringFormatter(format)
	if err != nil {
		panic("Failed to initialized string formatter: " + err.Error())
	}
	return f
}
func formatFuncName(v fmtVerb, f string) string {
	i := strings.LastIndex(f, "/")
	j := strings.Index(f[i+1:], ".")
	if j < 1 {
		return "???"
	}
	pkg, fun := f[:i+j+1], f[i+j+2:]
	switch v {
	case fmtVerbLongpkg:
		return pkg
	case fmtVerbShortpkg:
		return path.Base(pkg)
	case fmtVerbLongfunc:
		return fun
	case fmtVerbShortfunc:
		i = strings.LastIndex(fun, ".")
		return fun[i+1:]
	}
	panic("unexpected func formatter")
}
func (bf *backendFormatter) Log(level Level, calldepth int, r *Record) error {
	// Make a shallow copy of the record and replace any formatter
	r2 := *r
	r2.formatter = bf.f
	return bf.b.Log(level, calldepth+1, &r2)
}
func LogLevel(level string) (Level, error) {
	for i, name := range levelNames {
		if strings.EqualFold(name, level) {
			return Level(i), nil
		}
	}
	return ERROR, ErrInvalidLogLevel
}
func AddModuleLevel(backend Backend) LeveledBackend {
	var leveled LeveledBackend
	var ok bool
	if leveled, ok = backend.(LeveledBackend); !ok {
		leveled = &moduleLeveled{
			levels:  make(map[string]Level),
			backend: backend,
		}
	}
	return leveled
}
func (l *moduleLeveled) GetLevel(module string) Level {
	level, exists := l.levels[module]
	if exists == false {
		level, exists = l.levels[""]
		// no configuration exists, default to debug
		if exists == false {
			level = DEBUG
		}
	}
	return level
}
func (l *moduleLeveled) SetLevel(level Level, module string) {
	l.levels[module] = level
}
func (l *moduleLeveled) IsEnabledFor(level Level, module string) bool {
	return level <= l.GetLevel(module)
}
func MultiLogger(backends ...Backend) LeveledBackend {
	var leveledBackends []LeveledBackend
	for _, backend := range backends {
		leveledBackends = append(leveledBackends, AddModuleLevel(backend))
	}
	return &multiLogger{leveledBackends}
}
func (b *multiLogger) Log(level Level, calldepth int, rec *Record) (err error) {
	for _, backend := range b.backends {
		if backend.IsEnabledFor(level, rec.Module) {
			// Shallow copy of the record for the formatted cache on Record and get the
			// record formatter from the backend.
			r2 := *rec
			if e := backend.Log(level, calldepth+1, &r2); e != nil {
				err = e
			}
		}
	}
	return
}
func (b *multiLogger) GetLevel(module string) Level {
	var level Level
	for _, backend := range b.backends {
		if backendLevel := backend.GetLevel(module); backendLevel > level {
			level = backendLevel
		}
	}
	return level
}
func (b *multiLogger) SetLevel(level Level, module string) {
	for _, backend := range b.backends {
		backend.SetLevel(level, module)
	}
}
func (b *multiLogger) IsEnabledFor(level Level, module string) bool {
	for _, backend := range b.backends {
		if backend.IsEnabledFor(level, module) {
			return true
		}
	}
	return false
}
func ConvertColors(colors []int, bold bool) []string {
	converted := []string{}
	for _, i := range colors {
		if bold {
			converted = append(converted, ColorSeqBold(color(i)))
		} else {
			converted = append(converted, ColorSeq(color(i)))
		}
	}

	return converted
}
func NewSyslogBackend(prefix string) (b *SyslogBackend, err error) {
	var w *syslog.Writer
	w, err = syslog.New(syslog.LOG_CRIT, prefix)
	return &SyslogBackend{w}, err
}
func NewSyslogBackendPriority(prefix string, priority syslog.Priority) (b *SyslogBackend, err error) {
	var w *syslog.Writer
	w, err = syslog.New(priority, prefix)
	return &SyslogBackend{w}, err
}
func SetBackend(backends ...Backend) LeveledBackend {
	var backend Backend
	if len(backends) == 1 {
		backend = backends[0]
	} else {
		backend = MultiLogger(backends...)
	}

	defaultBackend = AddModuleLevel(backend)
	return defaultBackend
}
func NewCommander(topLevelFlags *flag.FlagSet, name string) *Commander {
	cdr := &Commander{
		topFlags: topLevelFlags,
		name:     name,
		Output:   os.Stdout,
		Error:    os.Stderr,
	}
	topLevelFlags.Usage = func() { cdr.explain(cdr.Error) }
	return cdr
}
func (cdr *Commander) Execute(ctx context.Context, args ...interface{}) ExitStatus {
	if cdr.topFlags.NArg() < 1 {
		cdr.topFlags.Usage()
		return ExitUsageError
	}

	name := cdr.topFlags.Arg(0)

	for _, group := range cdr.commands {
		for _, cmd := range group.commands {
			if name != cmd.Name() {
				continue
			}
			f := flag.NewFlagSet(name, flag.ContinueOnError)
			f.Usage = func() { explain(cdr.Error, cmd) }
			cmd.SetFlags(f)
			if f.Parse(cdr.topFlags.Args()[1:]) != nil {
				return ExitUsageError
			}
			return cmd.Execute(ctx, f, args...)
		}
	}

	// Cannot find this command.
	cdr.topFlags.Usage()
	return ExitUsageError
}
func (cdr *Commander) explain(w io.Writer) {
	fmt.Fprintf(w, "Usage: %s <flags> <subcommand> <subcommand args>\n\n", cdr.name)
	sort.Sort(byGroupName(cdr.commands))
	for _, group := range cdr.commands {
		explainGroup(w, group)
	}
	if cdr.topFlags == nil {
		fmt.Fprintln(w, "\nNo top level flags.")
		return
	}
	if len(cdr.important) == 0 {
		fmt.Fprintf(w, "\nUse \"%s flags\" for a list of top-level flags\n", cdr.name)
		return
	}

	fmt.Fprintf(w, "\nTop-level flags (use \"%s flags\" for a full list):\n", cdr.name)
	for _, name := range cdr.important {
		f := cdr.topFlags.Lookup(name)
		if f == nil {
			panic(fmt.Sprintf("Important flag (%s) is not defined", name))
		}
		fmt.Fprintf(w, "  -%s=%s: %s\n", f.Name, f.DefValue, f.Usage)
	}
}
func explainGroup(w io.Writer, group *commandGroup) {
	if len(group.commands) == 0 {
		return
	}
	if group.name == "" {
		fmt.Fprintf(w, "Subcommands:\n")
	} else {
		fmt.Fprintf(w, "Subcommands for %s:\n", group.name)
	}
	sort.Sort(group)

	aliases := make(map[string][]string)
	for _, cmd := range group.commands {
		if alias, ok := cmd.(*aliaser); ok {
			root := dealias(alias).Name()

			if _, ok := aliases[root]; !ok {
				aliases[root] = []string{}
			}
			aliases[root] = append(aliases[root], alias.Name())
		}
	}

	for _, cmd := range group.commands {
		if _, ok := cmd.(*aliaser); ok {
			continue
		}

		name := cmd.Name()
		names := []string{name}

		if a, ok := aliases[name]; ok {
			names = append(names, a...)
		}

		fmt.Fprintf(w, "\t%-15s  %s\n", strings.Join(names, ", "), cmd.Synopsis())
	}
	fmt.Fprintln(w)
}
func explain(w io.Writer, cmd Command) {
	fmt.Fprintf(w, "%s", cmd.Usage())
	subflags := flag.NewFlagSet(cmd.Name(), flag.PanicOnError)
	subflags.SetOutput(w)
	cmd.SetFlags(subflags)
	subflags.PrintDefaults()
}
func dealias(cmd Command) Command {
	if alias, ok := cmd.(*aliaser); ok {
		return dealias(alias.Command)
	}

	return cmd
}
func Execute(ctx context.Context, args ...interface{}) ExitStatus {
	return DefaultCommander.Execute(ctx, args...)
}
func LoadFromData(data []byte) (c *ConfigFile, err error) {
	// Save memory data to temporary file to support further operations.
	tmpName := path.Join(os.TempDir(), "goconfig", fmt.Sprintf("%d", time.Now().Nanosecond()))
	if err = os.MkdirAll(path.Dir(tmpName), os.ModePerm); err != nil {
		return nil, err
	}
	if err = ioutil.WriteFile(tmpName, data, 0655); err != nil {
		return nil, err
	}

	c = newConfigFile([]string{tmpName})
	err = c.read(bytes.NewBuffer(data))
	return c, err
}
func LoadFromReader(in io.Reader) (c *ConfigFile, err error) {
	c = newConfigFile([]string{""})
	err = c.read(in)
	return c, err
}
func (c *ConfigFile) ReloadData(in io.Reader) (err error) {
	var cfg *ConfigFile
	if len(c.fileNames) != 1 {
		return fmt.Errorf("Multiple files loaded, unable to mix in-memory and file data")
	}

	cfg, err = LoadFromReader(in)
	if err == nil {
		*c = *cfg
	}
	return err
}
func (c *ConfigFile) AppendFiles(files ...string) error {
	if len(c.fileNames) == 1 && c.fileNames[0] == "" {
		return fmt.Errorf("Cannot append file data to in-memory data")
	}
	c.fileNames = append(c.fileNames, files...)
	return c.Reload()
}
func (c *ConfigFile) GetKeyList(section string) []string {
	// Blank section name represents DEFAULT section.
	if len(section) == 0 {
		section = DEFAULT_SECTION
	}

	if c.BlockMode {
		c.lock.RLock()
		defer c.lock.RUnlock()
	}

	// Check if section exists.
	if _, ok := c.data[section]; !ok {
		return nil
	}

	// Non-default section has a blank key as section keeper.
	list := make([]string, 0, len(c.keyList[section]))
	for _, key := range c.keyList[section] {
		if key != " " {
			list = append(list, key)
		}
	}
	return list
}
func SaveConfigData(c *ConfigFile, out io.Writer) (err error) {
	equalSign := "="
	if PrettyFormat {
		equalSign = " = "
	}

	buf := bytes.NewBuffer(nil)
	for _, section := range c.sectionList {
		// Write section comments.
		if len(c.GetSectionComments(section)) > 0 {
			if _, err = buf.WriteString(c.GetSectionComments(section) + LineBreak); err != nil {
				return err
			}
		}

		if section != DEFAULT_SECTION {
			// Write section name.
			if _, err = buf.WriteString("[" + section + "]" + LineBreak); err != nil {
				return err
			}
		}

		for _, key := range c.keyList[section] {
			if key != " " {
				// Write key comments.
				if len(c.GetKeyComments(section, key)) > 0 {
					if _, err = buf.WriteString(c.GetKeyComments(section, key) + LineBreak); err != nil {
						return err
					}
				}

				keyName := key
				// Check if it's auto increment.
				if keyName[0] == '#' {
					keyName = "-"
				}
				//[SWH|+]:支持键名包含等号和冒号
				if strings.Contains(keyName, `=`) || strings.Contains(keyName, `:`) {
					if strings.Contains(keyName, "`") {
						if strings.Contains(keyName, `"`) {
							keyName = `"""` + keyName + `"""`
						} else {
							keyName = `"` + keyName + `"`
						}
					} else {
						keyName = "`" + keyName + "`"
					}
				}
				value := c.data[section][key]
				// In case key value contains "`" or "\"".
				if strings.Contains(value, "`") {
					if strings.Contains(value, `"`) {
						value = `"""` + value + `"""`
					} else {
						value = `"` + value + `"`
					}
				}

				// Write key and value.
				if _, err = buf.WriteString(keyName + equalSign + value + LineBreak); err != nil {
					return err
				}
			}
		}

		// Put a line between sections.
		if _, err = buf.WriteString(LineBreak); err != nil {
			return err
		}
	}

	if _, err := buf.WriteTo(out); err != nil {
		return err
	}
	return nil
}
func SaveConfigFile(c *ConfigFile, filename string) (err error) {
	// Write configuration file by filename.
	var f *os.File
	if f, err = os.Create(filename); err != nil {
		return err
	}

	if err := SaveConfigData(c, f); err != nil {
		return err
	}
	return f.Close()
}
func (s *selectable) Find(selector string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.CSS, selector).Single())
}
func (s *selectable) FindByXPath(selector string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.XPath, selector).Single())
}
func (s *selectable) FindByLink(text string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Link, text).Single())
}
func (s *selectable) FindByLabel(text string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Label, text).Single())
}
func (s *selectable) FindByName(name string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Name, name).Single())
}
func (s *selectable) FindByClass(text string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Class, text).Single())
}
func (s *selectable) FindByID(id string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.ID, id).Single())
}
func (s *selectable) First(selector string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.CSS, selector).At(0))
}
func (s *selectable) FirstByXPath(selector string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.XPath, selector).At(0))
}
func (s *selectable) FirstByLink(text string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Link, text).At(0))
}
func (s *selectable) FirstByLabel(text string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Label, text).At(0))
}
func (s *selectable) FirstByName(name string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Name, name).At(0))
}
func (s *selectable) All(selector string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.CSS, selector))
}
func (s *selectable) AllByXPath(selector string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.XPath, selector))
}
func (s *selectable) AllByLink(text string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.Link, text))
}
func (s *selectable) AllByLabel(text string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.Label, text))
}
func (s *selectable) AllByName(name string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.Name, name))
}
func (s *selectable) AllByClass(text string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.Class, text))
}
func (s *selectable) AllByID(text string) *MultiSelection {
	return newMultiSelection(s.session, s.selectors.Append(target.ID, text))
}
func (s *selectable) FindForAppium(selectorType string, text string) *Selection {
	return newSelection(s.session, s.selectors.Append(target.Class, text).At(0))
}
func Timeout(seconds int) Option {
	return func(c *config) {
		c.Timeout = time.Duration(seconds) * time.Second
	}
}
func ChromeOptions(opt string, value interface{}) Option {
	return func(c *config) {
		if c.ChromeOptions == nil {
			c.ChromeOptions = make(map[string]interface{})
		}
		c.ChromeOptions[opt] = value
	}
}
func JoinPage(url string, options ...Option) *Page {
	pageOptions := config{}.Merge(options)
	session := api.NewWithClient(url, pageOptions.HTTPClient)
	return newPage(session)
}
func (p *Page) Destroy() error {
	if err := p.session.Delete(); err != nil {
		return fmt.Errorf("failed to destroy session: %s", err)
	}
	return nil
}
func (p *Page) Reset() error {
	p.ConfirmPopup()

	url, err := p.URL()
	if err != nil {
		return err
	}
	if url == "about:blank" {
		return nil
	}

	if err := p.ClearCookies(); err != nil {
		return err
	}

	if err := p.session.DeleteLocalStorage(); err != nil {
		if err := p.RunScript("localStorage.clear();", nil, nil); err != nil {
			return err
		}
	}

	if err := p.session.DeleteSessionStorage(); err != nil {
		if err := p.RunScript("sessionStorage.clear();", nil, nil); err != nil {
			return err
		}
	}

	return p.Navigate("about:blank")
}
func (p *Page) Navigate(url string) error {
	if err := p.session.SetURL(url); err != nil {
		return fmt.Errorf("failed to navigate: %s", err)
	}
	return nil
}
func (p *Page) GetCookies() ([]*http.Cookie, error) {
	apiCookies, err := p.session.GetCookies()
	if err != nil {
		return nil, fmt.Errorf("failed to get cookies: %s", err)
	}
	cookies := []*http.Cookie{}
	for _, apiCookie := range apiCookies {
		expSeconds := int64(apiCookie.Expiry)
		expNano := int64(apiCookie.Expiry-float64(expSeconds)) * 1000000000
		cookie := &http.Cookie{
			Name:     apiCookie.Name,
			Value:    apiCookie.Value,
			Path:     apiCookie.Path,
			Domain:   apiCookie.Domain,
			Secure:   apiCookie.Secure,
			HttpOnly: apiCookie.HTTPOnly,
			Expires:  time.Unix(expSeconds, expNano),
		}
		cookies = append(cookies, cookie)
	}
	return cookies, nil
}
func (p *Page) SetCookie(cookie *http.Cookie) error {
	if cookie == nil {
		return errors.New("nil cookie is invalid")
	}

	var expiry int64
	if !cookie.Expires.IsZero() {
		expiry = cookie.Expires.Unix()
	}

	apiCookie := &api.Cookie{
		Name:     cookie.Name,
		Value:    cookie.Value,
		Path:     cookie.Path,
		Domain:   cookie.Domain,
		Secure:   cookie.Secure,
		HTTPOnly: cookie.HttpOnly,
		Expiry:   float64(expiry),
	}

	if err := p.session.SetCookie(apiCookie); err != nil {
		return fmt.Errorf("failed to set cookie: %s", err)
	}
	return nil
}
func (p *Page) DeleteCookie(name string) error {
	if err := p.session.DeleteCookie(name); err != nil {
		return fmt.Errorf("failed to delete cookie %s: %s", name, err)
	}
	return nil
}
func (p *Page) ClearCookies() error {
	if err := p.session.DeleteCookies(); err != nil {
		return fmt.Errorf("failed to clear cookies: %s", err)
	}
	return nil
}
func (p *Page) URL() (string, error) {
	url, err := p.session.GetURL()
	if err != nil {
		return "", fmt.Errorf("failed to retrieve URL: %s", err)
	}
	return url, nil
}
func (p *Page) Size(width, height int) error {
	window, err := p.session.GetWindow()
	if err != nil {
		return fmt.Errorf("failed to retrieve window: %s", err)
	}

	if err := window.SetSize(width, height); err != nil {
		return fmt.Errorf("failed to set window size: %s", err)
	}

	return nil
}
func (p *Page) Screenshot(filename string) error {
	absFilePath, err := filepath.Abs(filename)
	if err != nil {
		return fmt.Errorf("failed to find absolute path for filename: %s", err)
	}

	screenshot, err := p.session.GetScreenshot()
	if err != nil {
		return fmt.Errorf("failed to retrieve screenshot: %s", err)
	}

	if err := ioutil.WriteFile(absFilePath, screenshot, 0666); err != nil {
		return fmt.Errorf("failed to save screenshot: %s", err)
	}

	return nil
}
func (p *Page) Title() (string, error) {
	title, err := p.session.GetTitle()
	if err != nil {
		return "", fmt.Errorf("failed to retrieve page title: %s", err)
	}
	return title, nil
}
func (p *Page) HTML() (string, error) {
	html, err := p.session.GetSource()
	if err != nil {
		return "", fmt.Errorf("failed to retrieve page HTML: %s", err)
	}
	return html, nil
}
func (p *Page) PopupText() (string, error) {
	text, err := p.session.GetAlertText()
	if err != nil {
		return "", fmt.Errorf("failed to retrieve popup text: %s", err)
	}
	return text, nil
}
func (p *Page) EnterPopupText(text string) error {
	if err := p.session.SetAlertText(text); err != nil {
		return fmt.Errorf("failed to enter popup text: %s", err)
	}
	return nil
}
func (p *Page) ConfirmPopup() error {
	if err := p.session.AcceptAlert(); err != nil {
		return fmt.Errorf("failed to confirm popup: %s", err)
	}
	return nil
}
func (p *Page) CancelPopup() error {
	if err := p.session.DismissAlert(); err != nil {
		return fmt.Errorf("failed to cancel popup: %s", err)
	}
	return nil
}
func (p *Page) SwitchToParentFrame() error {
	if err := p.session.FrameParent(); err != nil {
		return fmt.Errorf("failed to switch to parent frame: %s", err)
	}
	return nil
}
func (p *Page) SwitchToRootFrame() error {
	if err := p.session.Frame(nil); err != nil {
		return fmt.Errorf("failed to switch to original page frame: %s", err)
	}
	return nil
}
func (p *Page) NextWindow() error {
	windows, err := p.session.GetWindows()
	if err != nil {
		return fmt.Errorf("failed to find available windows: %s", err)
	}

	var windowIDs []string
	for _, window := range windows {
		windowIDs = append(windowIDs, window.ID)
	}

	// order not defined according to W3 spec
	sort.Strings(windowIDs)

	activeWindow, err := p.session.GetWindow()
	if err != nil {
		return fmt.Errorf("failed to find active window: %s", err)
	}

	for position, windowID := range windowIDs {
		if windowID == activeWindow.ID {
			activeWindow.ID = windowIDs[(position+1)%len(windowIDs)]
			break
		}
	}

	if err := p.session.SetWindow(activeWindow); err != nil {
		return fmt.Errorf("failed to change active window: %s", err)
	}

	return nil
}
func (p *Page) CloseWindow() error {
	if err := p.session.DeleteWindow(); err != nil {
		return fmt.Errorf("failed to close active window: %s", err)
	}
	return nil
}
func (p *Page) WindowCount() (int, error) {
	windows, err := p.session.GetWindows()
	if err != nil {
		return 0, fmt.Errorf("failed to find available windows: %s", err)
	}
	return len(windows), nil
}
func (p *Page) LogTypes() ([]string, error) {
	types, err := p.session.GetLogTypes()
	if err != nil {
		return nil, fmt.Errorf("failed to retrieve log types: %s", err)
	}
	return types, nil
}
func (p *Page) MoveMouseBy(xOffset, yOffset int) error {
	if err := p.session.MoveTo(nil, api.XYOffset{X: xOffset, Y: yOffset}); err != nil {
		return fmt.Errorf("failed to move mouse: %s", err)
	}

	return nil
}
func (p *Page) DoubleClick() error {
	if err := p.session.DoubleClick(); err != nil {
		return fmt.Errorf("failed to double click: %s", err)
	}

	return nil
}
func (p *Page) Click(event Click, button Button) error {
	var err error
	switch event {
	case SingleClick:
		err = p.session.Click(api.Button(button))
	case HoldClick:
		err = p.session.ButtonDown(api.Button(button))
	case ReleaseClick:
		err = p.session.ButtonUp(api.Button(button))
	default:
		err = errors.New("invalid touch event")
	}
	if err != nil {
		return fmt.Errorf("failed to %s %s: %s", event, button, err)
	}

	return nil
}
func (s *Selection) Click() error {
	return s.forEachElement(func(selectedElement element.Element) error {
		if err := selectedElement.Click(); err != nil {
			return fmt.Errorf("failed to click on %s: %s", s, err)
		}
		return nil
	})
}
func (s *Selection) DoubleClick() error {
	return s.forEachElement(func(selectedElement element.Element) error {
		if err := s.session.MoveTo(selectedElement.(*api.Element), nil); err != nil {
			return fmt.Errorf("failed to move mouse to %s: %s", s, err)
		}
		if err := s.session.DoubleClick(); err != nil {
			return fmt.Errorf("failed to double-click on %s: %s", s, err)
		}
		return nil
	})
}
func (s *Selection) Fill(text string) error {
	return s.forEachElement(func(selectedElement element.Element) error {
		if err := selectedElement.Clear(); err != nil {
			return fmt.Errorf("failed to clear %s: %s", s, err)
		}
		if err := selectedElement.Value(text); err != nil {
			return fmt.Errorf("failed to enter text into %s: %s", s, err)
		}
		return nil
	})
}
func (s *Selection) Tap(event Tap) error {
	var touchFunc func(*api.Element) error
	switch event {
	case SingleTap:
		touchFunc = s.session.TouchClick
	case DoubleTap:
		touchFunc = s.session.TouchDoubleClick
	case LongTap:
		touchFunc = s.session.TouchLongClick
	default:
		return fmt.Errorf("failed to %s on %s: invalid tap event", event, s)
	}

	return s.forEachElement(func(selectedElement element.Element) error {
		if err := touchFunc(selectedElement.(*api.Element)); err != nil {
			return fmt.Errorf("failed to %s on %s: %s", event, s, err)
		}
		return nil
	})
}
func (s *Selection) Touch(event Touch) error {
	var touchFunc func(x, y int) error
	switch event {
	case HoldFinger:
		touchFunc = s.session.TouchDown
	case ReleaseFinger:
		touchFunc = s.session.TouchUp
	case MoveFinger:
		touchFunc = s.session.TouchMove
	default:
		return fmt.Errorf("failed to %s on %s: invalid touch event", event, s)
	}

	return s.forEachElement(func(selectedElement element.Element) error {
		x, y, err := selectedElement.GetLocation()
		if err != nil {
			return fmt.Errorf("failed to retrieve location of %s: %s", s, err)
		}
		if err := touchFunc(x, y); err != nil {
			return fmt.Errorf("failed to flick finger on %s: %s", s, err)
		}
		return nil
	})
}
func (s *Selection) FlickFinger(xOffset, yOffset int, speed uint) error {
	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	if err := s.session.TouchFlick(selectedElement.(*api.Element), api.XYOffset{X: xOffset, Y: yOffset}, api.ScalarSpeed(speed)); err != nil {
		return fmt.Errorf("failed to flick finger on %s: %s", s, err)
	}
	return nil
}
func (s *Selection) ScrollFinger(xOffset, yOffset int) error {
	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	if err := s.session.TouchScroll(selectedElement.(*api.Element), api.XYOffset{X: xOffset, Y: yOffset}); err != nil {
		return fmt.Errorf("failed to scroll finger on %s: %s", s, err)
	}
	return nil
}
func NewCapabilities(features ...string) Capabilities {
	c := Capabilities{}
	for _, feature := range features {
		c.With(feature)
	}
	return c
}
func (c Capabilities) JSON() (string, error) {
	capabilitiesJSON, err := json.Marshal(c)
	return string(capabilitiesJSON), err
}
func HaveTitle(title string) types.GomegaMatcher {
	return &internal.ValueMatcher{Method: "Title", Property: "title", Expected: title}
}
func HaveURL(url string) types.GomegaMatcher {
	return &internal.ValueMatcher{Method: "URL", Property: "URL", Expected: url}
}
func HavePopupText(text string) types.GomegaMatcher {
	return &internal.ValueMatcher{Method: "PopupText", Property: "popup text", Expected: text}
}
func HaveLoggedError(messages ...string) types.GomegaMatcher {
	return &internal.LogMatcher{
		ExpectedMessages: messages,
		Levels:           []string{"WARNING", "SEVERE"},
		Name:             "error",
		Type:             "browser",
	}
}
func (s *Selection) Text() (string, error) {
	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return "", fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	text, err := selectedElement.GetText()
	if err != nil {
		return "", fmt.Errorf("failed to retrieve text for %s: %s", s, err)
	}
	return text, nil
}
func (s *Selection) Active() (bool, error) {
	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return false, fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	activeElement, err := s.session.GetActiveElement()
	if err != nil {
		return false, fmt.Errorf("failed to retrieve active element: %s", err)
	}

	equal, err := selectedElement.IsEqualTo(activeElement)
	if err != nil {
		return false, fmt.Errorf("failed to compare selection to active element: %s", err)
	}

	return equal, nil
}
func (s *Selection) Attribute(attribute string) (string, error) {
	return s.hasProperty(element.Element.GetAttribute, attribute, "attribute")
}
func (s *Selection) CSS(property string) (string, error) {
	return s.hasProperty(element.Element.GetCSS, property, "CSS property")
}
func (s *Selection) Selected() (bool, error) {
	return s.hasState(element.Element.IsSelected, "selected")
}
func (s *Selection) Visible() (bool, error) {
	return s.hasState(element.Element.IsDisplayed, "visible")
}
func (s *Selection) Enabled() (bool, error) {
	return s.hasState(element.Element.IsEnabled, "enabled")
}
func HaveCount(count int) types.GomegaMatcher {
	return &internal.ValueMatcher{Method: "Count", Property: "element count", Expected: count}
}
func HaveAttribute(attribute string, value string) types.GomegaMatcher {
	return &internal.HaveAttributeMatcher{ExpectedAttribute: attribute, ExpectedValue: value}
}
func EdgeDriver(options ...Option) *WebDriver {
	var binaryName string
	if runtime.GOOS == "windows" {
		binaryName = "MicrosoftWebDriver.exe"
	} else {
		return nil
	}
	command := []string{binaryName, "--port={{.Port}}"}
	// Using {{.Address}} means using 127.0.0.1
	// But MicrosoftWebDriver only supports localhost, not 127.0.0.1
	return NewWebDriver("http://localhost:{{.Port}}", command, options...)
}
func Selendroid(jarFile string, options ...Option) *WebDriver {
	absJARPath, err := filepath.Abs(jarFile)
	if err != nil {
		return nil
	}

	command := []string{
		"java",
		"-jar", absJARPath,
		"-port", "{{.Port}}",
	}
	options = append([]Option{Timeout(90), Browser("android")}, options...)
	return NewWebDriver("http://{{.Address}}/wd/hub", command, options...)
}
func (s *Selection) SwitchToFrame() error {
	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	if err := s.session.Frame(selectedElement.(*api.Element)); err != nil {
		return fmt.Errorf("failed to switch to frame referred to by %s: %s", s, err)
	}
	return nil
}
func (s *Selection) Count() (int, error) {
	elements, err := s.elements.Get()
	if err != nil {
		return 0, fmt.Errorf("failed to select elements from %s: %s", s, err)
	}

	return len(elements), nil
}
func (s *Selection) EqualsElement(other interface{}) (bool, error) {
	otherSelection, ok := other.(*Selection)
	if !ok {
		multiSelection, ok := other.(*MultiSelection)
		if !ok {
			return false, fmt.Errorf("must be *Selection or *MultiSelection")
		}
		otherSelection = &multiSelection.Selection
	}

	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return false, fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	otherElement, err := otherSelection.elements.GetExactlyOne()
	if err != nil {
		return false, fmt.Errorf("failed to select element from %s: %s", other, err)
	}

	equal, err := selectedElement.IsEqualTo(otherElement.(*api.Element))
	if err != nil {
		return false, fmt.Errorf("failed to compare %s to %s: %s", s, other, err)
	}

	return equal, nil
}
func (s *Selection) MouseToElement() error {
	selectedElement, err := s.elements.GetExactlyOne()
	if err != nil {
		return fmt.Errorf("failed to select element from %s: %s", s, err)
	}

	if err := s.session.MoveTo(selectedElement.(*api.Element), nil); err != nil {
		return fmt.Errorf("failed to move mouse to element for %s: %s", s, err)
	}

	return nil
}
func loggingMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		log.Printf("[DEBUG] http reverse proxy received connection from %s on path %s\n", r.RemoteAddr, r.RequestURI)
		next.ServeHTTP(w, r)
	})
}
func chainHandlers(mw ...Middleware) Middleware {
	return func(final http.Handler) http.Handler {
		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
			last := final
			for i := len(mw) - 1; i >= 0; i-- {
				last = mw[i](last)
			}
			last.ServeHTTP(w, r)
		})
	}
}
func HTTPReverseProxy(options Options) (int, error) {
	port := options.ProxyPort
	var err error

	proxy := httputil.NewSingleHostReverseProxy(&url.URL{
		Scheme: options.TargetScheme,
		Host:   options.TargetAddress,
	})

	if port == 0 {
		port, err = utils.GetFreePort()
		if err != nil {
			log.Println("[ERROR] unable to start reverse proxy server:", err)
			return 0, err
		}
	}

	wrapper := chainHandlers(append(options.Middleware, loggingMiddleware)...)

	log.Println("[DEBUG] starting reverse proxy on port", port)
	go http.ListenAndServe(fmt.Sprintf(":%d", port), wrapper(proxy))

	return port, nil
}
func (i *Installer) CheckInstallation() error {

	for binary, versionRange := range versionMap {
		log.Println("[INFO] checking", binary, "within range", versionRange)

		version, err := i.GetVersionForBinary(binary)
		if err != nil {
			return err
		}

		if err = i.CheckVersion(binary, version); err != nil {
			return err
		}
	}

	return nil
}
func (i *Installer) CheckVersion(binary, version string) error {
	log.Println("[DEBUG] checking version for binary", binary, "version", version)
	v, err := goversion.NewVersion(version)
	if err != nil {
		log.Println("[DEBUG] err", err)
		return err
	}

	versionRange, ok := versionMap[binary]
	if !ok {
		return fmt.Errorf("unable to find version range for binary %s", binary)
	}

	log.Println("[DEBUG] checking if version", v, "within semver range", versionRange)
	constraints, err := goversion.NewConstraint(versionRange)
	if constraints.Check(v) {
		log.Println("[DEBUG]", v, "satisfies constraints", v, constraints)
		return nil
	}

	return fmt.Errorf("version %s of %s does not match constraint %s", version, binary, versionRange)
}
func (i *Installer) GetVersionForBinary(binary string) (version string, err error) {
	log.Println("[DEBUG] running binary", binary)

	content, err := i.commander.Output(binary, "version")
	elements := strings.Split(strings.TrimSpace(string(content)), "\n")
	version = strings.TrimSpace(elements[len(elements)-1])

	return version, err
}
func (c *Client) getUser(id string) (*ex.User, error) {

	u := fmt.Sprintf("%s/users/%s", c.Host, id)
	req, err := http.NewRequest("GET", u, nil)

	// NOTE: by default, request bodies are expected to be sent with a Content-Type
	// of application/json. If you don't explicitly set the content-type, you
	// will get a mismatch during Verification.
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", c.token)

	res, err := http.DefaultClient.Do(req)

	if res.StatusCode != 200 || err != nil {
		return nil, fmt.Errorf("get user failed")
	}

	data, err := ioutil.ReadAll(res.Body)
	if err != nil {
		return nil, err
	}

	var response ex.User
	err = json.Unmarshal(data, &response)
	if err != nil {
		return nil, err
	}

	return &response, err
}
func (c *Client) login(username string, password string) (*ex.User, error) {
	loginRequest := fmt.Sprintf(`
    {
      "username":"%s",
      "password": "%s"
    }`, username, password)

	res, err := http.Post(fmt.Sprintf("%s/login/10?foo=anything", c.Host), "application/json; charset=utf-8", bytes.NewReader([]byte(loginRequest)))
	if res.StatusCode != 200 || err != nil {
		return nil, fmt.Errorf("login failed")
	}

	data, err := ioutil.ReadAll(res.Body)
	if err != nil {
		return nil, err
	}

	var response loginResponse
	err = json.Unmarshal(data, &response)
	if err != nil {
		return nil, err
	}

	return &response.User, err
}
func (c *Client) loginHandler(w http.ResponseWriter, r *http.Request) {
	username := r.FormValue("username")
	password := r.FormValue("password")

	user, err := c.login(username, password)
	if err == nil && user != nil {
		c.user = user
		c.err = nil
		http.Redirect(w, r, "/", http.StatusFound)
		return
	}
	c.err = fmt.Errorf("Invalid username/password")
	http.Redirect(w, r, "/", http.StatusFound)
	return
}
func (c *Client) logoutHandler(w http.ResponseWriter, r *http.Request) {
	c.user = nil
	c.err = nil
	http.Redirect(w, r, "/", http.StatusFound)
	return
}
func (c *Client) viewHandler(w http.ResponseWriter, r *http.Request) {
	data := templateData{
		User:  c.user,
		Error: c.err,
	}
	renderTemplate(w, "login", data)
}
func (c *Client) Run() {
	http.HandleFunc("/login", c.loginHandler)
	http.HandleFunc("/logout", c.logoutHandler)
	http.HandleFunc("/", c.viewHandler)
	fmt.Println("User svc client running on port 8081")
	http.ListenAndServe(":8081", nil)
}
func (i *Interaction) WithRequest(request Request) *Interaction {
	i.Request = request

	// Check if someone tried to add an object as a string representation
	// as per original allowed implementation, e.g.
	// { "foo": "bar", "baz": like("bat") }
	if isJSONFormattedObject(request.Body) {
		log.Println("[WARN] request body appears to be a JSON formatted object, " +
			"no structural matching will occur. Support for structured strings has been" +
			"deprecated as of 0.13.0")
	}

	return i
}
func (i *Interaction) WillRespondWith(response Response) *Interaction {
	i.Response = response

	return i
}
func isJSONFormattedObject(stringOrObject interface{}) bool {
	switch content := stringOrObject.(type) {
	case []byte:
	case string:
		var obj interface{}
		err := json.Unmarshal([]byte(content), &obj)

		if err != nil {
			return false
		}

		// Check if a map type
		if _, ok := obj.(map[string]interface{}); ok {
			return true
		}
	}

	return false
}
func (u *UserRepository) ByUsername(username string) (*User, error) {
	if user, ok := u.Users[username]; ok {
		return user, nil
	}
	return nil, ErrNotFound
}
func (u *UserRepository) ByID(ID int) (*User, error) {
	for _, user := range u.Users {
		if user.ID == ID {
			return user, nil
		}
	}
	return nil, ErrNotFound
}
func UserLogin(w http.ResponseWriter, r *http.Request) {
	var login types.LoginRequest
	w.Header().Set("Content-Type", "application/json; charset=utf-8")
	w.Header().Set("X-Api-Correlation-Id", "1234")

	body, err := ioutil.ReadAll(r.Body)
	defer r.Body.Close()

	if err != nil {
		w.WriteHeader(http.StatusServiceUnavailable)
		return
	}

	err = json.Unmarshal(body, &login)
	if err != nil {
		w.WriteHeader(http.StatusServiceUnavailable)
		return
	}

	user, err := userRepository.ByUsername(login.Username)
	if err != nil {
		w.WriteHeader(http.StatusNotFound)
	} else if user.Username != login.Username || user.Password != login.Password {
		w.WriteHeader(http.StatusUnauthorized)
	} else {
		w.Header().Set("X-Auth-Token", getAuthToken())
		w.WriteHeader(http.StatusOK)
		res := types.LoginResponse{User: user}
		resBody, _ := json.Marshal(res)
		w.Write(resBody)
	}
}
func newClient(MockServiceManager client.Service, verificationServiceManager client.Service, messageServiceManager client.Service, publishServiceManager client.Service) *PactClient {
	MockServiceManager.Setup()
	verificationServiceManager.Setup()
	messageServiceManager.Setup()
	publishServiceManager.Setup()

	return &PactClient{
		pactMockSvcManager:     MockServiceManager,
		verificationSvcManager: verificationServiceManager,
		messageSvcManager:      messageServiceManager,
		publishSvcManager:      publishServiceManager,
		TimeoutDuration:        10 * time.Second,
	}
}
func NewClient() *PactClient {
	return newClient(&client.MockService{}, &client.VerificationService{}, &client.MessageService{}, &client.PublishService{})
}
func (p *PactClient) ListServers() []*types.MockServer {
	log.Println("[DEBUG] client: starting a server")

	var servers []*types.MockServer

	for port, s := range p.pactMockSvcManager.List() {
		servers = append(servers, &types.MockServer{
			Pid:  s.Process.Pid,
			Port: port,
		})
	}

	return servers
}
func (p *PactClient) UpdateMessagePact(request types.PactMessageRequest) error {
	log.Println("[DEBUG] client: adding pact message...")

	// Convert request into flags, and validate request
	err := request.Validate()
	if err != nil {
		return err
	}

	svc := p.messageSvcManager.NewService(request.Args)
	cmd := svc.Command()

	stdOutPipe, err := cmd.StdoutPipe()
	if err != nil {
		return err
	}
	stdErrPipe, err := cmd.StderrPipe()
	if err != nil {
		return err
	}
	err = cmd.Start()
	if err != nil {
		return err
	}
	stdOut, err := ioutil.ReadAll(stdOutPipe)
	if err != nil {
		return err
	}
	stdErr, err := ioutil.ReadAll(stdErrPipe)
	if err != nil {
		return err
	}

	err = cmd.Wait()

	if err == nil {
		return nil
	}

	return fmt.Errorf("error creating message: %s\n\nSTDERR:\n%s\n\nSTDOUT:\n%s", err, stdErr, stdOut)
}
func (p *PactClient) PublishPacts(request types.PublishRequest) error {
	svc := p.publishSvcManager.NewService(request.Args)
	log.Println("[DEBUG] about to publish pacts")
	cmd := svc.Start()

	log.Println("[DEBUG] waiting for response")
	err := cmd.Wait()

	log.Println("[DEBUG] response from publish", err)

	return err
}
func getPort(rawURL string) int {
	parsedURL, err := url.Parse(rawURL)
	if err == nil {
		splitHost := strings.Split(parsedURL.Host, ":")
		if len(splitHost) == 2 {
			port, err := strconv.Atoi(splitHost[1])
			if err == nil {
				return port
			}
		}
		if parsedURL.Scheme == "https" {
			return 443
		}
		return 80
	}

	return -1
}
func getAddress(rawURL string) string {
	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		return ""
	}

	splitHost := strings.Split(parsedURL.Host, ":")
	return splitHost[0]
}
func sanitiseRubyResponse(response string) string {
	log.Println("[TRACE] response from Ruby process pre-sanitisation:", response)

	r := regexp.MustCompile("(?m)^\\s*#.*$")
	s := r.ReplaceAllString(response, "")

	r = regexp.MustCompile("(?m).*bundle exec rake pact:verify.*$")
	s = r.ReplaceAllString(s, "")

	r = regexp.MustCompile("\\n+")
	s = r.ReplaceAllString(s, "\n")

	return s
}
func (p *Publisher) Publish(request types.PublishRequest) error {
	log.Println("[DEBUG] pact publisher: publish pact")

	if p.pactClient == nil {
		c := NewClient()
		p.pactClient = c
	}

	err := request.Validate()

	if err != nil {
		return err
	}

	return p.pactClient.PublishPacts(request)
}
func FindPortInRange(s string) (int, error) {
	// Take care of csv and single value
	if !strings.Contains(s, "-") {
		ports := strings.Split(strings.TrimSpace(s), ",")
		for _, p := range ports {
			i, err := strconv.Atoi(p)
			if err != nil {
				return 0, err
			}
			err = checkPort(i)
			if err != nil {
				continue
			}
			return i, nil
		}
		return 0, errors.New("all passed ports are unusable")
	}
	// Now take care of ranges
	ports := strings.Split(strings.TrimSpace(s), "-")
	if len(ports) != 2 {
		return 0, errors.New("invalid range passed")
	}
	lower, err := strconv.Atoi(ports[0])
	if err != nil {
		return 0, err
	}
	upper, err := strconv.Atoi(ports[1])
	if err != nil {
		return 0, err
	}
	if upper < lower {
		return 0, errors.New("invalid range passed")
	}
	for i := lower; i <= upper; i++ {
		err = checkPort(i)
		if err != nil {
			continue
		}
		return i, nil
	}
	return 0, errors.New("all passed ports are unusable")
}
func EachLike(content interface{}, minRequired int) Matcher {
	return eachLike{
		Contents: content,
		Min:      minRequired,
	}
}
func Term(generate string, matcher string) Matcher {
	return term{
		Data: termData{
			Generate: generate,
			Matcher: termMatcher{
				Type:  "Regexp",
				O:     0,
				Regex: matcher,
			},
		},
	}
}
func (m *MapMatcher) UnmarshalJSON(bytes []byte) (err error) {
	sk := make(map[string]string)
	err = json.Unmarshal(bytes, &sk)
	if err != nil {
		return
	}

	*m = make(map[string]Matcher)
	for k, v := range sk {
		(*m)[k] = String(v)
	}

	return
}
func objectToString(obj interface{}) string {
	switch content := obj.(type) {
	case string:
		return content
	default:
		jsonString, err := json.Marshal(obj)
		if err != nil {
			log.Println("[DEBUG] objectToString: error unmarshaling object into string:", err.Error())
			return ""
		}
		return string(jsonString)
	}
}
func match(srcType reflect.Type, params params) Matcher {
	switch kind := srcType.Kind(); kind {
	case reflect.Ptr:
		return match(srcType.Elem(), params)
	case reflect.Slice, reflect.Array:
		return EachLike(match(srcType.Elem(), getDefaults()), params.slice.min)
	case reflect.Struct:
		result := StructMatcher{}

		for i := 0; i < srcType.NumField(); i++ {
			field := srcType.Field(i)
			result[field.Tag.Get("json")] = match(field.Type, pluckParams(field.Type, field.Tag.Get("pact")))
		}
		return result
	case reflect.String:
		if params.str.regEx != "" {
			return Term(params.str.example, params.str.regEx)
		}
		if params.str.example != "" {
			return Like(params.str.example)
		}

		return Like("string")
	case reflect.Bool:
		return Like(true)
	case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64,
		reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:
		return Like(1)
	case reflect.Float32, reflect.Float64:
		return Like(1.1)
	default:
		panic(fmt.Sprintf("match: unhandled type: %v", srcType))
	}
}
func (p *Pact) AddMessage() *Message {
	log.Println("[DEBUG] pact add message")

	m := &Message{}
	p.MessageInteractions = append(p.MessageInteractions, m)
	return m
}
func (p *Pact) AddInteraction() *Interaction {
	p.Setup(true)
	log.Println("[DEBUG] pact add interaction")
	i := &Interaction{}
	p.Interactions = append(p.Interactions, i)
	return i
}
func (p *Pact) Teardown() *Pact {
	log.Println("[DEBUG] teardown")
	if p.Server != nil {
		server, err := p.pactClient.StopServer(p.Server)

		if err != nil {
			log.Println("error:", err)
		}
		p.Server = server
	}
	return p
}
func (p *Pact) Verify(integrationTest func() error) error {
	p.Setup(true)
	log.Println("[DEBUG] pact verify")

	// Check if we are verifying messages or if we actually have interactions
	if len(p.Interactions) == 0 {
		return errors.New("there are no interactions to be verified")
	}

	mockServer := &MockService{
		BaseURL:  fmt.Sprintf("http://%s:%d", p.Host, p.Server.Port),
		Consumer: p.Consumer,
		Provider: p.Provider,
	}

	for _, interaction := range p.Interactions {
		err := mockServer.AddInteraction(interaction)
		if err != nil {
			return err
		}
	}

	// Run the integration test
	err := integrationTest()
	if err != nil {
		return err
	}

	// Run Verification Process
	err = mockServer.Verify()
	if err != nil {
		return err
	}

	// Clear out interations
	p.Interactions = make([]*Interaction, 0)

	return mockServer.DeleteInteractions()
}
func stateHandlerMiddleware(stateHandlers types.StateHandlers) proxy.Middleware {
	return func(next http.Handler) http.Handler {
		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
			if r.URL.Path == "/__setup" {
				var s *types.ProviderState
				decoder := json.NewDecoder(r.Body)
				decoder.Decode(&s)

				// Setup any provider state
				for _, state := range s.States {
					sf, stateFound := stateHandlers[state]

					if !stateFound {
						log.Printf("[WARN] state handler not found for state: %v", state)
					} else {
						// Execute state handler
						if err := sf(); err != nil {
							log.Printf("[ERROR] state handler for '%v' errored: %v", state, err)
							w.WriteHeader(http.StatusInternalServerError)
							return
						}
					}
				}

				w.WriteHeader(http.StatusOK)
				return
			}

			log.Println("[DEBUG] skipping state handler for request", r.RequestURI)

			// Pass through to application
			next.ServeHTTP(w, r)
		})
	}
}
func (p *Pact) VerifyMessageProviderRaw(request VerifyMessageRequest) (types.ProviderVerifierResponse, error) {
	p.Setup(false)
	response := types.ProviderVerifierResponse{}

	// Starts the message wrapper API with hooks back to the message handlers
	// This maps the 'description' field of a message pact, to a function handler
	// that will implement the message producer. This function must return an object and optionally
	// and error. The object will be marshalled to JSON for comparison.
	mux := http.NewServeMux()

	port, err := utils.GetFreePort()
	if err != nil {
		return response, fmt.Errorf("unable to allocate a port for verification: %v", err)
	}

	// Construct verifier request
	verificationRequest := types.VerifyRequest{
		ProviderBaseURL:            fmt.Sprintf("http://localhost:%d", port),
		PactURLs:                   request.PactURLs,
		BrokerURL:                  request.BrokerURL,
		Tags:                       request.Tags,
		BrokerUsername:             request.BrokerUsername,
		BrokerPassword:             request.BrokerPassword,
		BrokerToken:                request.BrokerToken,
		PublishVerificationResults: request.PublishVerificationResults,
		ProviderVersion:            request.ProviderVersion,
		Provider:                   p.Provider,
	}

	mux.HandleFunc("/", messageVerificationHandler(request.MessageHandlers, request.StateHandlers))

	ln, err := net.Listen("tcp", fmt.Sprintf(":%d", port))
	if err != nil {
		log.Fatal(err)
	}
	defer ln.Close()

	log.Printf("[DEBUG] API handler starting: port %d (%s)", port, ln.Addr())
	go http.Serve(ln, mux)

	portErr := waitForPort(port, "tcp", "localhost", p.ClientTimeout,
		fmt.Sprintf(`Timed out waiting for pact proxy on port %d - check for errors`, port))

	if portErr != nil {
		log.Fatal("Error:", err)
		return response, portErr
	}

	log.Println("[DEBUG] pact provider verification")
	return p.pactClient.VerifyProvider(verificationRequest)
}
func (p *Pact) VerifyMessageConsumerRaw(message *Message, handler MessageConsumer) error {
	log.Printf("[DEBUG] verify message")
	p.Setup(false)

	// Reify the message back to its "example/generated" form
	reified, err := p.pactClient.ReifyMessage(&types.PactReificationRequest{
		Message: message.Content,
	})

	if err != nil {
		return fmt.Errorf("unable to convert consumer test to a valid JSON representation: %v", err)
	}

	t := reflect.TypeOf(message.Type)
	if t != nil && t.Name() != "interface" {
		log.Println("[DEBUG] narrowing type to", t.Name())
		err = json.Unmarshal(reified.ResponseRaw, &message.Type)

		if err != nil {
			return fmt.Errorf("unable to narrow type to %v: %v", t.Name(), err)
		}
	}

	// Yield message, and send through handler function
	generatedMessage :=
		Message{
			Content:     message.Type,
			States:      message.States,
			Description: message.Description,
			Metadata:    message.Metadata,
		}

	err = handler(generatedMessage)
	if err != nil {
		return err
	}

	// If no errors, update Message Pact
	return p.pactClient.UpdateMessagePact(types.PactMessageRequest{
		Message:  message,
		Consumer: p.Consumer,
		Provider: p.Provider,
		PactDir:  p.PactDir,
	})
}
func (p *mockClient) VerifyProvider(request types.VerifyRequest) (types.ProviderVerifierResponse, error) {
	return p.VerifyProviderResponse, p.VerifyProviderError
}
func (m *MockService) NewService(args []string) Service {
	m.Args = []string{
		"service",
	}
	m.Args = append(m.Args, args...)

	m.Cmd = getMockServiceCommandPath()
	return m
}
func (s *ServiceManager) Setup() {
	log.Println("[DEBUG] setting up a service manager")

	s.commandCreatedChan = make(chan *exec.Cmd)
	s.commandCompleteChan = make(chan *exec.Cmd)
	s.processMap = processMap{processes: make(map[int]*exec.Cmd)}

	// Listen for service create/kill
	go s.addServiceMonitor()
	go s.removeServiceMonitor()
}
func (s *ServiceManager) addServiceMonitor() {
	log.Println("[DEBUG] starting service creation monitor")
	for {
		select {
		case p := <-s.commandCreatedChan:
			if p != nil && p.Process != nil {
				s.processMap.Set(p.Process.Pid, p)
			}
		}
	}
}
func (s *ServiceManager) removeServiceMonitor() {
	log.Println("[DEBUG] starting service removal monitor")
	var p *exec.Cmd
	for {
		select {
		case p = <-s.commandCompleteChan:
			if p != nil && p.Process != nil {
				p.Process.Signal(os.Interrupt)
				s.processMap.Delete(p.Process.Pid)
			}
		}
	}
}
func (s *ServiceManager) List() map[int]*exec.Cmd {
	log.Println("[DEBUG] listing services")
	return s.processMap.processes
}
func (s *ServiceManager) Command() *exec.Cmd {
	cmd := exec.Command(s.Cmd, s.Args...)
	env := os.Environ()
	env = append(env, s.Env...)
	cmd.Env = env

	return cmd
}
func (m *MockService) call(method string, url string, content interface{}) error {
	body, err := json.Marshal(content)
	if err != nil {
		fmt.Println(err)
		return err
	}

	client := &http.Client{}
	var req *http.Request
	if method == "POST" {
		req, err = http.NewRequest(method, url, bytes.NewReader(body))
	} else {
		req, err = http.NewRequest(method, url, nil)
	}
	if err != nil {
		return err
	}

	req.Header.Set("X-Pact-Mock-Service", "true")
	req.Header.Set("Content-Type", "application/json")

	res, err := client.Do(req)
	if err != nil {
		return err
	}

	responseBody, err := ioutil.ReadAll(res.Body)
	res.Body.Close()
	if res.StatusCode < 200 || res.StatusCode >= 300 {
		return errors.New(string(responseBody))
	}
	return err
}
func (m *MockService) DeleteInteractions() error {
	log.Println("[DEBUG] mock service delete interactions")
	url := fmt.Sprintf("%s/interactions", m.BaseURL)
	return m.call("DELETE", url, nil)
}
func (m *MockService) AddInteraction(interaction *Interaction) error {
	log.Println("[DEBUG] mock service add interaction")
	url := fmt.Sprintf("%s/interactions", m.BaseURL)
	return m.call("POST", url, interaction)
}
func (m *MockService) WritePact() error {
	log.Println("[DEBUG] mock service write pact")

	if m.Consumer == "" || m.Provider == "" {
		return errors.New("Consumer and Provider name need to be provided")
	}
	if m.PactFileWriteMode == "" {
		m.PactFileWriteMode = "overwrite"
	}

	pact := map[string]interface{}{
		"consumer": map[string]string{
			"name": m.Consumer,
		},
		"provider": map[string]string{
			"name": m.Provider,
		},
		"pactFileWriteMode": m.PactFileWriteMode,
	}

	url := fmt.Sprintf("%s/pact", m.BaseURL)
	return m.call("POST", url, pact)
}
func (p *Message) ExpectsToReceive(description string) *Message {
	p.Description = description
	return p
}
func (p *Message) WithMetadata(metadata MapMatcher) *Message {
	p.Metadata = metadata
	return p
}
func (p *Message) AsType(t interface{}) *Message {
	fmt.Println("[DEBUG] setting Message decoding to type:", reflect.TypeOf(t))
	p.Type = t

	return p
}
func UserLogin(c *gin.Context) {
	c.Header("X-Api-Correlation-Id", "1234")

	var json Login
	if c.BindJSON(&json) == nil {
		user, err := userRepository.ByUsername(json.User)
		if err != nil {
			c.JSON(http.StatusNotFound, gin.H{"status": "file not found"})
		} else if user.Username != json.User || user.Password != json.Password {
			c.JSON(http.StatusUnauthorized, gin.H{"status": "unauthorized"})
		} else {
			c.Header("X-Auth-Token", getAuthToken())
			c.JSON(http.StatusOK, types.LoginResponse{User: user})
		}
	}
}
func (s *S3) Region() string {
	region := os.Getenv("AWS_REGION")
	switch s.Domain {
	case "s3.amazonaws.com", "s3-external-1.amazonaws.com":
		return "us-east-1"
	case "s3-accelerate.amazonaws.com":
		if region == "" {
			panic("can't find endpoint region")
		}
		return region
	default:
		regions := regionMatcher.FindStringSubmatch(s.Domain)
		if len(regions) < 2 {
			if region == "" {
				panic("can't find endpoint region")
			}
			return region
		}
		return regions[1]
	}
}
func New(domain string, keys Keys) *S3 {
	if domain == "" {
		domain = DefaultDomain
	}
	return &S3{domain, keys}
}
func (s *S3) Bucket(name string) *Bucket {
	return &Bucket{
		S3:     s,
		Name:   name,
		Config: DefaultConfig,
	}
}
func (b *Bucket) PutWriter(path string, h http.Header, c *Config) (w io.WriteCloser, err error) {
	if c == nil {
		c = b.conf()
	}
	u, err := b.url(path, c)
	if err != nil {
		return nil, err
	}

	return newPutter(*u, h, c, b)
}
func (b *Bucket) url(bPath string, c *Config) (*url.URL, error) {

	// parse versionID parameter from path, if included
	// See https://github.com/rlmcpherson/s3gof3r/issues/84 for rationale
	purl, err := url.Parse(bPath)
	if err != nil {
		return nil, err
	}
	var vals url.Values
	if v := purl.Query().Get(versionParam); v != "" {
		vals = make(url.Values)
		vals.Add(versionParam, v)
		bPath = strings.Split(bPath, "?")[0] // remove versionID from path
	}

	// handling for bucket names containing periods / explicit PathStyle addressing
	// http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html for details
	if strings.Contains(b.Name, ".") || c.PathStyle {
		return &url.URL{
			Host:     b.S3.Domain,
			Scheme:   c.Scheme,
			Path:     path.Clean(fmt.Sprintf("/%s/%s", b.Name, bPath)),
			RawQuery: vals.Encode(),
		}, nil
	} else {
		return &url.URL{
			Scheme:   c.Scheme,
			Path:     path.Clean(fmt.Sprintf("/%s", bPath)),
			Host:     path.Clean(fmt.Sprintf("%s.%s", b.Name, b.S3.Domain)),
			RawQuery: vals.Encode(),
		}, nil
	}
}
func SetLogger(out io.Writer, prefix string, flag int, debug bool) {
	logger = internalLogger{
		log.New(out, prefix, flag),
		debug,
	}
}
func InstanceKeys() (keys Keys, err error) {

	rolePath := "http://169.254.169.254/latest/meta-data/iam/security-credentials/"
	var creds mdCreds

	// request the role name for the instance
	// assumes there is only one
	resp, err := ClientWithTimeout(2 * time.Second).Get(rolePath)
	if err != nil {
		return
	}
	defer checkClose(resp.Body, err)
	if resp.StatusCode != 200 {
		err = newRespError(resp)
		return
	}
	role, err := ioutil.ReadAll(resp.Body)

	if err != nil {
		return
	}

	// request the credential metadata for the role
	resp, err = http.Get(rolePath + string(role))
	if err != nil {
		return
	}
	defer checkClose(resp.Body, err)
	if resp.StatusCode != 200 {
		err = newRespError(resp)
		return
	}
	metadata, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return
	}

	if err = json.Unmarshal([]byte(metadata), &creds); err != nil {
		return
	}
	keys = Keys{
		AccessKey:     creds.AccessKeyID,
		SecretKey:     creds.SecretAccessKey,
		SecurityToken: creds.Token,
	}

	return
}
func EnvKeys() (keys Keys, err error) {
	keys = Keys{
		AccessKey:     os.Getenv("AWS_ACCESS_KEY_ID"),
		SecretKey:     os.Getenv("AWS_SECRET_ACCESS_KEY"),
		SecurityToken: os.Getenv("AWS_SECURITY_TOKEN"),
	}
	if keys.AccessKey == "" || keys.SecretKey == "" {
		err = fmt.Errorf("keys not set in environment: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY")
	}
	return
}
func (b *Bucket) Sign(req *http.Request) {
	if req.Header == nil {
		req.Header = http.Header{}
	}
	if b.S3.Keys.SecurityToken != "" {
		req.Header.Set("X-Amz-Security-Token", b.S3.Keys.SecurityToken)
	}
	req.Header.Set("User-Agent", "S3Gof3r")
	s := &signer{
		Time:    time.Now(),
		Request: req,
		Region:  b.S3.Region(),
		Keys:    b.S3.Keys,
	}
	s.sign()
}
func getAWSKeys() (keys s3gof3r.Keys, err error) {

	keys, err = s3gof3r.EnvKeys()
	if err == nil {
		return
	}
	keys, err = s3gof3r.InstanceKeys()
	if err == nil {
		return
	}
	err = errors.New("no AWS keys found")
	return
}
func homeDir() (string, error) {
	if h := os.Getenv("HOME"); h != "" {
		return h, nil
	}
	h, err := exec.Command("sh", "-c", "eval echo ~$USER").Output()
	if err == nil && len(h) > 0 {
		return strings.TrimSpace(string(h)), nil
	}
	return "", fmt.Errorf("home directory not found for current user")
}
func ACL(h http.Header, acl string) http.Header {
	if acl != "" {
		h.Set("x-amz-acl", acl)
	}
	return h
}
func (p *putter) putPart(part *part) error {
	v := url.Values{}
	v.Set("partNumber", strconv.Itoa(part.PartNumber))
	v.Set("uploadId", p.UploadID)
	if _, err := part.r.Seek(0, 0); err != nil { // move back to beginning, if retrying
		return err
	}
	req, err := http.NewRequest("PUT", p.url.String()+"?"+v.Encode(), part.r)
	if err != nil {
		return err
	}
	req.ContentLength = part.len
	req.Header.Set(md5Header, part.md5)
	req.Header.Set(sha256Header, part.sha256)
	p.b.Sign(req)
	resp, err := p.c.Client.Do(req)
	if err != nil {
		return err
	}
	defer checkClose(resp.Body, err)
	if resp.StatusCode != 200 {
		return newRespError(resp)
	}
	s := resp.Header.Get("etag")
	if len(s) < 2 {
		return fmt.Errorf("Got Bad etag:%s", s)
	}
	s = s[1 : len(s)-1] // includes quote chars for some reason
	if part.ETag != s {
		return fmt.Errorf("Response etag does not match. Remote:%s Calculated:%s", s, p.ETag)
	}
	return nil
}
func (p *putter) abort() {
	v := url.Values{}
	v.Set("uploadId", p.UploadID)
	s := p.url.String() + "?" + v.Encode()
	resp, err := p.retryRequest("DELETE", s, nil, nil)
	if err != nil {
		logger.Printf("Error aborting multipart upload: %v\n", err)
		return
	}
	defer checkClose(resp.Body, err)
	if resp.StatusCode != 204 {
		logger.Printf("Error aborting multipart upload: %v", newRespError(resp))
	}
	return
}
func growPartSize(partIndex int, partSize, putsz int64) bool {
	return (maxObjSize-putsz)/(maxNPart-int64(partIndex)) > partSize
}
func CheckDisallowed(prefix string, spec interface{}) error {
	infos, err := gatherInfo(prefix, spec)
	if err != nil {
		return err
	}

	vars := make(map[string]struct{})
	for _, info := range infos {
		vars[info.Key] = struct{}{}
	}

	if prefix != "" {
		prefix = strings.ToUpper(prefix) + "_"
	}

	for _, env := range os.Environ() {
		if !strings.HasPrefix(env, prefix) {
			continue
		}
		v := strings.SplitN(env, "=", 2)[0]
		if _, found := vars[v]; !found {
			return fmt.Errorf("unknown environment variable %s", v)
		}
	}

	return nil
}
func Process(prefix string, spec interface{}) error {
	infos, err := gatherInfo(prefix, spec)

	for _, info := range infos {

		// `os.Getenv` cannot differentiate between an explicitly set empty value
		// and an unset value. `os.LookupEnv` is preferred to `syscall.Getenv`,
		// but it is only available in go1.5 or newer. We're using Go build tags
		// here to use os.LookupEnv for >=go1.5
		value, ok := lookupEnv(info.Key)
		if !ok && info.Alt != "" {
			value, ok = lookupEnv(info.Alt)
		}

		def := info.Tags.Get("default")
		if def != "" && !ok {
			value = def
		}

		req := info.Tags.Get("required")
		if !ok && def == "" {
			if isTrue(req) {
				return fmt.Errorf("required key %s missing value", info.Key)
			}
			continue
		}

		err = processField(value, info.Field)
		if err != nil {
			return &ParseError{
				KeyName:   info.Key,
				FieldName: info.Name,
				TypeName:  info.Field.Type().String(),
				Value:     value,
				Err:       err,
			}
		}
	}

	return err
}
func MustProcess(prefix string, spec interface{}) {
	if err := Process(prefix, spec); err != nil {
		panic(err)
	}
}
func toTypeDescription(t reflect.Type) string {
	switch t.Kind() {
	case reflect.Array, reflect.Slice:
		return fmt.Sprintf("Comma-separated list of %s", toTypeDescription(t.Elem()))
	case reflect.Map:
		return fmt.Sprintf(
			"Comma-separated list of %s:%s pairs",
			toTypeDescription(t.Key()),
			toTypeDescription(t.Elem()),
		)
	case reflect.Ptr:
		return toTypeDescription(t.Elem())
	case reflect.Struct:
		if implementsInterface(t) && t.Name() != "" {
			return t.Name()
		}
		return ""
	case reflect.String:
		name := t.Name()
		if name != "" && name != "string" {
			return name
		}
		return "String"
	case reflect.Bool:
		name := t.Name()
		if name != "" && name != "bool" {
			return name
		}
		return "True or False"
	case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:
		name := t.Name()
		if name != "" && !strings.HasPrefix(name, "int") {
			return name
		}
		return "Integer"
	case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:
		name := t.Name()
		if name != "" && !strings.HasPrefix(name, "uint") {
			return name
		}
		return "Unsigned Integer"
	case reflect.Float32, reflect.Float64:
		name := t.Name()
		if name != "" && !strings.HasPrefix(name, "float") {
			return name
		}
		return "Float"
	}
	return fmt.Sprintf("%+v", t)
}
func Usage(prefix string, spec interface{}) error {
	// The default is to output the usage information as a table
	// Create tabwriter instance to support table output
	tabs := tabwriter.NewWriter(os.Stdout, 1, 0, 4, ' ', 0)

	err := Usagef(prefix, spec, tabs, DefaultTableFormat)
	tabs.Flush()
	return err
}
func Usagef(prefix string, spec interface{}, out io.Writer, format string) error {

	// Specify the default usage template functions
	functions := template.FuncMap{
		"usage_key":         func(v varInfo) string { return v.Key },
		"usage_description": func(v varInfo) string { return v.Tags.Get("desc") },
		"usage_type":        func(v varInfo) string { return toTypeDescription(v.Field.Type()) },
		"usage_default":     func(v varInfo) string { return v.Tags.Get("default") },
		"usage_required": func(v varInfo) (string, error) {
			req := v.Tags.Get("required")
			if req != "" {
				reqB, err := strconv.ParseBool(req)
				if err != nil {
					return "", err
				}
				if reqB {
					req = "true"
				}
			}
			return req, nil
		},
	}

	tmpl, err := template.New("envconfig").Funcs(functions).Parse(format)
	if err != nil {
		return err
	}

	return Usaget(prefix, spec, out, tmpl)
}
func Usaget(prefix string, spec interface{}, out io.Writer, tmpl *template.Template) error {
	// gather first
	infos, err := gatherInfo(prefix, spec)
	if err != nil {
		return err
	}

	return tmpl.Execute(out, infos)
}
func (t *Time) Scan(value interface{}) error {
	var err error
	switch x := value.(type) {
	case time.Time:
		t.Time = x
	case nil:
		t.Valid = false
		return nil
	default:
		err = fmt.Errorf("null: cannot scan type %T into null.Time: %v", value, value)
	}
	t.Valid = err == nil
	return err
}
func (t Time) Value() (driver.Value, error) {
	if !t.Valid {
		return nil, nil
	}
	return t.Time, nil
}
func NewTime(t time.Time, valid bool) Time {
	return Time{
		Time:  t,
		Valid: valid,
	}
}
func TimeFromPtr(t *time.Time) Time {
	if t == nil {
		return NewTime(time.Time{}, false)
	}
	return NewTime(*t, true)
}
func (t Time) ValueOrZero() time.Time {
	if !t.Valid {
		return time.Time{}
	}
	return t.Time
}
func (t Time) MarshalJSON() ([]byte, error) {
	if !t.Valid {
		return []byte("null"), nil
	}
	return t.Time.MarshalJSON()
}
func (t *Time) SetValid(v time.Time) {
	t.Time = v
	t.Valid = true
}
func (t Time) Ptr() *time.Time {
	if !t.Valid {
		return nil
	}
	return &t.Time
}
func NewBool(b bool, valid bool) Bool {
	return Bool{
		NullBool: sql.NullBool{
			Bool:  b,
			Valid: valid,
		},
	}
}
func BoolFromPtr(b *bool) Bool {
	if b == nil {
		return NewBool(false, false)
	}
	return NewBool(*b, true)
}
func (b *Bool) UnmarshalJSON(data []byte) error {
	var err error
	var v interface{}
	if err = json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch x := v.(type) {
	case bool:
		b.Bool = x
	case map[string]interface{}:
		err = json.Unmarshal(data, &b.NullBool)
	case nil:
		b.Valid = false
		return nil
	default:
		err = fmt.Errorf("json: cannot unmarshal %v into Go value of type null.Bool", reflect.TypeOf(v).Name())
	}
	b.Valid = err == nil
	return err
}
func (b *Bool) UnmarshalText(text []byte) error {
	str := string(text)
	switch str {
	case "", "null":
		b.Valid = false
		return nil
	case "true":
		b.Bool = true
	case "false":
		b.Bool = false
	default:
		b.Valid = false
		return errors.New("invalid input:" + str)
	}
	b.Valid = true
	return nil
}
func (b Bool) MarshalJSON() ([]byte, error) {
	if !b.Valid {
		return []byte("null"), nil
	}
	if !b.Bool {
		return []byte("false"), nil
	}
	return []byte("true"), nil
}
func (b *Bool) SetValid(v bool) {
	b.Bool = v
	b.Valid = true
}
func NewString(s string, valid bool) String {
	return String{
		NullString: sql.NullString{
			String: s,
			Valid:  valid,
		},
	}
}
func (s *String) UnmarshalJSON(data []byte) error {
	var err error
	var v interface{}
	if err = json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch x := v.(type) {
	case string:
		s.String = x
	case map[string]interface{}:
		err = json.Unmarshal(data, &s.NullString)
	case nil:
		s.Valid = false
		return nil
	default:
		err = fmt.Errorf("json: cannot unmarshal %v into Go value of type zero.String", reflect.TypeOf(v).Name())
	}
	s.Valid = (err == nil) && (s.String != "")
	return err
}
func (s String) MarshalText() ([]byte, error) {
	if !s.Valid {
		return []byte{}, nil
	}
	return []byte(s.String), nil
}
func (s *String) UnmarshalText(text []byte) error {
	s.String = string(text)
	s.Valid = s.String != ""
	return nil
}
func (s *String) SetValid(v string) {
	s.String = v
	s.Valid = true
}
func StringFromPtr(s *string) String {
	if s == nil {
		return NewString("", false)
	}
	return NewString(*s, true)
}
func (s String) MarshalJSON() ([]byte, error) {
	if !s.Valid {
		return []byte("null"), nil
	}
	return json.Marshal(s.String)
}
func NewInt(i int64, valid bool) Int {
	return Int{
		NullInt64: sql.NullInt64{
			Int64: i,
			Valid: valid,
		},
	}
}
func IntFromPtr(i *int64) Int {
	if i == nil {
		return NewInt(0, false)
	}
	n := NewInt(*i, true)
	return n
}
func (i *Int) UnmarshalJSON(data []byte) error {
	var err error
	var v interface{}
	if err = json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch x := v.(type) {
	case float64:
		// Unmarshal again, directly to int64, to avoid intermediate float64
		err = json.Unmarshal(data, &i.Int64)
	case string:
		str := string(x)
		if len(str) == 0 {
			i.Valid = false
			return nil
		}
		i.Int64, err = strconv.ParseInt(str, 10, 64)
	case map[string]interface{}:
		err = json.Unmarshal(data, &i.NullInt64)
	case nil:
		i.Valid = false
		return nil
	default:
		err = fmt.Errorf("json: cannot unmarshal %v into Go value of type zero.Int", reflect.TypeOf(v).Name())
	}
	i.Valid = (err == nil) && (i.Int64 != 0)
	return err
}
func (i Int) MarshalText() ([]byte, error) {
	n := i.Int64
	if !i.Valid {
		n = 0
	}
	return []byte(strconv.FormatInt(n, 10)), nil
}
func (i *Int) SetValid(n int64) {
	i.Int64 = n
	i.Valid = true
}
func (i *Int) UnmarshalText(text []byte) error {
	str := string(text)
	if str == "" || str == "null" {
		i.Valid = false
		return nil
	}
	var err error
	i.Int64, err = strconv.ParseInt(string(text), 10, 64)
	i.Valid = err == nil
	return err
}
func (b Bool) MarshalText() ([]byte, error) {
	if !b.Valid || !b.Bool {
		return []byte("false"), nil
	}
	return []byte("true"), nil
}
func (f *Float) SetValid(v float64) {
	f.Float64 = v
	f.Valid = true
}
func (t Time) MarshalJSON() ([]byte, error) {
	if !t.Valid {
		return (time.Time{}).MarshalJSON()
	}
	return t.Time.MarshalJSON()
}
func (f *Float) UnmarshalJSON(data []byte) error {
	var err error
	var v interface{}
	if err = json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch x := v.(type) {
	case float64:
		f.Float64 = float64(x)
	case string:
		str := string(x)
		if len(str) == 0 {
			f.Valid = false
			return nil
		}
		f.Float64, err = strconv.ParseFloat(str, 64)
	case map[string]interface{}:
		err = json.Unmarshal(data, &f.NullFloat64)
	case nil:
		f.Valid = false
		return nil
	default:
		err = fmt.Errorf("json: cannot unmarshal %v into Go value of type null.Float", reflect.TypeOf(v).Name())
	}
	f.Valid = err == nil
	return err
}
func (f *Float) UnmarshalText(text []byte) error {
	str := string(text)
	if str == "" || str == "null" {
		f.Valid = false
		return nil
	}
	var err error
	f.Float64, err = strconv.ParseFloat(string(text), 64)
	f.Valid = err == nil
	return err
}
func (f Float) MarshalJSON() ([]byte, error) {
	if !f.Valid {
		return []byte("null"), nil
	}
	if math.IsInf(f.Float64, 0) || math.IsNaN(f.Float64) {
		return nil, &json.UnsupportedValueError{
			Value: reflect.ValueOf(f.Float64),
			Str:   strconv.FormatFloat(f.Float64, 'g', -1, 64),
		}
	}
	return []byte(strconv.FormatFloat(f.Float64, 'f', -1, 64)), nil
}
func newGossipChannel(channelName string, ourself *localPeer, r *routes, g Gossiper, logger Logger) *gossipChannel {
	return &gossipChannel{
		name:     channelName,
		ourself:  ourself,
		routes:   r,
		gossiper: g,
		logger:   logger,
	}
}
func (c *gossipChannel) GossipUnicast(dstPeerName PeerName, msg []byte) error {
	return c.relayUnicast(dstPeerName, gobEncode(c.name, c.ourself.Name, dstPeerName, msg))
}
func (c *gossipChannel) GossipBroadcast(update GossipData) {
	c.relayBroadcast(c.ourself.Name, update)
}
func (c *gossipChannel) Send(data GossipData) {
	c.relay(c.ourself.Name, data)
}
func (c *gossipChannel) SendDown(conn Connection, data GossipData) {
	c.senderFor(conn).Send(data)
}
func gobEncode(items ...interface{}) []byte {
	buf := new(bytes.Buffer)
	enc := gob.NewEncoder(buf)
	for _, i := range items {
		if err := enc.Encode(i); err != nil {
			panic(err)
		}
	}
	return buf.Bytes()
}
func newTokenBucket(capacity int64, tokenInterval time.Duration) *tokenBucket {
	tb := tokenBucket{
		capacity:       capacity,
		tokenInterval:  tokenInterval,
		refillDuration: tokenInterval * time.Duration(capacity)}

	tb.earliestUnspentToken = tb.capacityToken()

	return &tb
}
func (tb *tokenBucket) wait() {
	// If earliest unspent token is in the future, sleep until then
	time.Sleep(time.Until(tb.earliestUnspentToken))

	// Alternatively, enforce bucket capacity if necessary
	capacityToken := tb.capacityToken()
	if tb.earliestUnspentToken.Before(capacityToken) {
		tb.earliestUnspentToken = capacityToken
	}

	// 'Remove' a token from the bucket
	tb.earliestUnspentToken = tb.earliestUnspentToken.Add(tb.tokenInterval)
}
func (tb *tokenBucket) capacityToken() time.Time {
	return time.Now().Add(-tb.refillDuration).Truncate(tb.tokenInterval)
}
func PrefixRangeEnd(prefix []byte) []byte {
	// https://github.com/coreos/etcd/blob/17e32b6/clientv3/op.go#L187
	end := make([]byte, len(prefix))
	copy(end, prefix)
	for i := len(end) - 1; i >= 0; i-- {
		if end[i] < 0xff {
			end[i] = end[i] + 1
			end = end[:i+1]
			return end
		}
	}
	// next prefix does not exist (e.g., 0xffff);
	// default to WithFromKey policy
	return []byte{0}
}
func newLocalPeer(name PeerName, nickName string, router *Router) *localPeer {
	actionChan := make(chan localPeerAction, ChannelSize)
	peer := &localPeer{
		Peer:       newPeer(name, nickName, randomPeerUID(), 0, randomPeerShortID()),
		router:     router,
		actionChan: actionChan,
	}
	go peer.actorLoop(actionChan)
	return peer
}
func (peer *localPeer) getConnections() connectionSet {
	connections := make(connectionSet)
	peer.RLock()
	defer peer.RUnlock()
	for _, conn := range peer.connections {
		connections[conn] = struct{}{}
	}
	return connections
}
func (peer *localPeer) createConnection(localAddr string, peerAddr string, acceptNewPeer bool, logger Logger) error {
	if err := peer.checkConnectionLimit(); err != nil {
		return err
	}
	localTCPAddr, err := net.ResolveTCPAddr("tcp", localAddr)
	if err != nil {
		return err
	}
	remoteTCPAddr, err := net.ResolveTCPAddr("tcp", peerAddr)
	if err != nil {
		return err
	}
	tcpConn, err := net.DialTCP("tcp", localTCPAddr, remoteTCPAddr)
	if err != nil {
		return err
	}
	connRemote := newRemoteConnection(peer.Peer, nil, peerAddr, true, false)
	startLocalConnection(connRemote, tcpConn, peer.router, acceptNewPeer, logger)
	return nil
}
func (peer *localPeer) doAddConnection(conn ourConnection, isRestartedPeer bool) error {
	resultChan := make(chan error)
	peer.actionChan <- func() {
		resultChan <- peer.handleAddConnection(conn, isRestartedPeer)
	}
	return <-resultChan
}
func startLocalConnection(connRemote *remoteConnection, tcpConn *net.TCPConn, router *Router, acceptNewPeer bool, logger Logger) {
	if connRemote.local != router.Ourself.Peer {
		panic("attempt to create local connection from a peer which is not ourself")
	}
	errorChan := make(chan error, 1)
	finished := make(chan struct{})
	conn := &LocalConnection{
		remoteConnection: *connRemote, // NB, we're taking a copy of connRemote here.
		router:           router,
		tcpConn:          tcpConn,
		trustRemote:      router.trusts(connRemote),
		uid:              randUint64(),
		errorChan:        errorChan,
		finished:         finished,
		logger:           logger,
	}
	conn.senders = newGossipSenders(conn, finished)
	go conn.run(errorChan, finished, acceptNewPeer)
}
func (conn *LocalConnection) SendProtocolMsg(m protocolMsg) error {
	if err := conn.sendProtocolMsg(m); err != nil {
		conn.shutdown(err)
		return err
	}
	return nil
}
func NewStatus(router *Router) *Status {
	return &Status{
		Protocol:           Protocol,
		ProtocolMinVersion: int(router.ProtocolMinVersion),
		ProtocolMaxVersion: ProtocolMaxVersion,
		Encryption:         router.usingPassword(),
		PeerDiscovery:      router.PeerDiscovery,
		Name:               router.Ourself.Name.String(),
		NickName:           router.Ourself.NickName,
		Port:               router.Port,
		Peers:              makePeerStatusSlice(router.Peers),
		UnicastRoutes:      makeUnicastRouteStatusSlice(router.Routes),
		BroadcastRoutes:    makeBroadcastRouteStatusSlice(router.Routes),
		Connections:        makeLocalConnectionStatusSlice(router.ConnectionMaker),
		TerminationCount:   router.ConnectionMaker.terminationCount,
		Targets:            router.ConnectionMaker.Targets(false),
		OverlayDiagnostics: router.Overlay.Diagnostics(),
		TrustedSubnets:     makeTrustedSubnetsSlice(router.TrustedSubnets),
	}
}
func makePeerStatusSlice(peers *Peers) []PeerStatus {
	var slice []PeerStatus

	peers.forEach(func(peer *Peer) {
		var connections []connectionStatus
		if peer == peers.ourself.Peer {
			for conn := range peers.ourself.getConnections() {
				connections = append(connections, makeConnectionStatus(conn))
			}
		} else {
			// Modifying peer.connections requires a write lock on
			// Peers, and since we are holding a read lock (due to the
			// ForEach), access without locking the peer is safe.
			for _, conn := range peer.connections {
				connections = append(connections, makeConnectionStatus(conn))
			}
		}
		slice = append(slice, PeerStatus{
			peer.Name.String(),
			peer.NickName,
			peer.UID,
			peer.ShortID,
			peer.Version,
			connections,
		})
	})

	return slice
}
func makeUnicastRouteStatusSlice(r *routes) []unicastRouteStatus {
	r.RLock()
	defer r.RUnlock()

	var slice []unicastRouteStatus
	for dest, via := range r.unicast {
		slice = append(slice, unicastRouteStatus{dest.String(), via.String()})
	}
	return slice
}
func makeBroadcastRouteStatusSlice(r *routes) []broadcastRouteStatus {
	r.RLock()
	defer r.RUnlock()

	var slice []broadcastRouteStatus
	for source, via := range r.broadcast {
		var hops []string
		for _, hop := range via {
			hops = append(hops, hop.String())
		}
		slice = append(slice, broadcastRouteStatus{source.String(), hops})
	}
	return slice
}
func makeLocalConnectionStatusSlice(cm *connectionMaker) []LocalConnectionStatus {
	resultChan := make(chan []LocalConnectionStatus)
	cm.actionChan <- func() bool {
		var slice []LocalConnectionStatus
		for conn := range cm.connections {
			state := "pending"
			if conn.isEstablished() {
				state = "established"
			}
			lc, _ := conn.(*LocalConnection)
			attrs := lc.OverlayConn.Attrs()
			name, ok := attrs["name"]
			if !ok {
				name = "none"
			}
			info := fmt.Sprintf("%-6v %v", name, conn.Remote())
			if lc.router.usingPassword() {
				if lc.untrusted() {
					info = fmt.Sprintf("%-11v %v", "encrypted", info)
					if attrs != nil {
						attrs["encrypted"] = true
					}
				} else {
					info = fmt.Sprintf("%-11v %v", "unencrypted", info)
				}
			}
			slice = append(slice, LocalConnectionStatus{conn.remoteTCPAddress(), conn.isOutbound(), state, info, attrs})
		}
		for address, target := range cm.targets {
			add := func(state, info string) {
				slice = append(slice, LocalConnectionStatus{address, true, state, info, nil})
			}
			switch target.state {
			case targetWaiting:
				until := "never"
				if !target.tryAfter.IsZero() {
					until = target.tryAfter.String()
				}
				if target.lastError == nil { // shouldn't happen
					add("waiting", "until: "+until)
				} else {
					add("failed", target.lastError.Error()+", retry: "+until)
				}
			case targetAttempting:
				if target.lastError == nil {
					add("connecting", "")
				} else {
					add("retrying", target.lastError.Error())
				}
			case targetConnected:
			case targetSuspended:
			}
		}
		resultChan <- slice
		return false
	}
	return <-resultChan
}
func makeTrustedSubnetsSlice(trustedSubnets []*net.IPNet) []string {
	trustedSubnetStrs := []string{}
	for _, trustedSubnet := range trustedSubnets {
		trustedSubnetStrs = append(trustedSubnetStrs, trustedSubnet.String())
	}
	return trustedSubnetStrs
}
func (s *etcdStore) Range(ctx context.Context, req *etcdserverpb.RangeRequest) (*etcdserverpb.RangeResponse, error) {
	ireq := etcdserverpb.InternalRaftRequest{ID: <-s.idgen, Range: req}
	msgc, errc, err := s.proposeInternalRaftRequest(ireq)
	if err != nil {
		return nil, err
	}
	select {
	case <-ctx.Done():
		s.cancelInternalRaftRequest(ireq)
		return nil, ctx.Err()
	case msg := <-msgc:
		return msg.(*etcdserverpb.RangeResponse), nil
	case err := <-errc:
		return nil, err
	case <-s.quitc:
		return nil, errStopped
	}
}
func (s *etcdStore) Put(ctx context.Context, req *etcdserverpb.PutRequest) (*etcdserverpb.PutResponse, error) {
	ireq := etcdserverpb.InternalRaftRequest{ID: <-s.idgen, Put: req}
	msgc, errc, err := s.proposeInternalRaftRequest(ireq)
	if err != nil {
		return nil, err
	}
	select {
	case <-ctx.Done():
		s.cancelInternalRaftRequest(ireq)
		return nil, ctx.Err()
	case msg := <-msgc:
		return msg.(*etcdserverpb.PutResponse), nil
	case err := <-errc:
		return nil, err
	case <-s.quitc:
		return nil, errStopped
	}
}
func (s *etcdStore) DeleteRange(ctx context.Context, req *etcdserverpb.DeleteRangeRequest) (*etcdserverpb.DeleteRangeResponse, error) {
	ireq := etcdserverpb.InternalRaftRequest{ID: <-s.idgen, DeleteRange: req}
	msgc, errc, err := s.proposeInternalRaftRequest(ireq)
	if err != nil {
		return nil, err
	}
	select {
	case <-ctx.Done():
		s.cancelInternalRaftRequest(ireq)
		return nil, ctx.Err()
	case msg := <-msgc:
		return msg.(*etcdserverpb.DeleteRangeResponse), nil
	case err := <-errc:
		return nil, err
	case <-s.quitc:
		return nil, errStopped
	}
}
func (s *etcdStore) Txn(ctx context.Context, req *etcdserverpb.TxnRequest) (*etcdserverpb.TxnResponse, error) {
	ireq := etcdserverpb.InternalRaftRequest{ID: <-s.idgen, Txn: req}
	msgc, errc, err := s.proposeInternalRaftRequest(ireq)
	if err != nil {
		return nil, err
	}
	select {
	case <-ctx.Done():
		s.cancelInternalRaftRequest(ireq)
		return nil, ctx.Err()
	case msg := <-msgc:
		return msg.(*etcdserverpb.TxnResponse), nil
	case err := <-errc:
		return nil, err
	case <-s.quitc:
		return nil, errStopped
	}
}
func (s *etcdStore) Compact(ctx context.Context, req *etcdserverpb.CompactionRequest) (*etcdserverpb.CompactionResponse, error) {
	// We don't have snapshotting yet, so compact just puts us in a bad state.
	// TODO(pb): fix this when we implement snapshotting.
	return nil, errors.New("not implemented")
}
func (s *etcdStore) proposeInternalRaftRequest(req etcdserverpb.InternalRaftRequest) (<-chan proto.Message, <-chan error, error) {
	data, err := req.Marshal()
	if err != nil {
		return nil, nil, err
	}
	if len(data) > maxRequestBytes {
		return nil, nil, errTooBig
	}
	msgc, errc, err := s.registerPending(req.ID)
	if err != nil {
		return nil, nil, err
	}
	s.proposalc <- data
	return msgc, errc, nil
}
func applyCompare(kv mvcc.KV, c *etcdserverpb.Compare) (int64, bool) {
	ckvs, rev, err := kv.Range(c.Key, nil, 1, 0)
	if err != nil {
		if err == mvcc.ErrTxnIDMismatch {
			panic("unexpected txn ID mismatch error")
		}
		return rev, false
	}
	var ckv mvccpb.KeyValue
	if len(ckvs) != 0 {
		ckv = ckvs[0]
	} else {
		// Use the zero value of ckv normally. However...
		if c.Target == etcdserverpb.Compare_VALUE {
			// Always fail if we're comparing a value on a key that doesn't exist.
			// We can treat non-existence as the empty set explicitly, such that
			// even a key with a value of length 0 bytes is still a real key
			// that was written that way
			return rev, false
		}
	}

	// -1 is less, 0 is equal, 1 is greater
	var result int
	switch c.Target {
	case etcdserverpb.Compare_VALUE:
		tv, _ := c.TargetUnion.(*etcdserverpb.Compare_Value)
		if tv != nil {
			result = bytes.Compare(ckv.Value, tv.Value)
		}
	case etcdserverpb.Compare_CREATE:
		tv, _ := c.TargetUnion.(*etcdserverpb.Compare_CreateRevision)
		if tv != nil {
			result = compareInt64(ckv.CreateRevision, tv.CreateRevision)
		}

	case etcdserverpb.Compare_MOD:
		tv, _ := c.TargetUnion.(*etcdserverpb.Compare_ModRevision)
		if tv != nil {
			result = compareInt64(ckv.ModRevision, tv.ModRevision)
		}
	case etcdserverpb.Compare_VERSION:
		tv, _ := c.TargetUnion.(*etcdserverpb.Compare_Version)
		if tv != nil {
			result = compareInt64(ckv.Version, tv.Version)
		}
	}

	switch c.Result {
	case etcdserverpb.Compare_EQUAL:
		if result != 0 {
			return rev, false
		}
	case etcdserverpb.Compare_GREATER:
		if result != 1 {
			return rev, false
		}
	case etcdserverpb.Compare_LESS:
		if result != -1 {
			return rev, false
		}
	}
	return rev, true
}
func (peers *Peers) Descriptions() []PeerDescription {
	peers.RLock()
	defer peers.RUnlock()
	descriptions := make([]PeerDescription, 0, len(peers.byName))
	for _, peer := range peers.byName {
		descriptions = append(descriptions, PeerDescription{
			Name:           peer.Name,
			NickName:       peer.peerSummary.NickName,
			UID:            peer.UID,
			Self:           peer.Name == peers.ourself.Name,
			NumConnections: len(peer.connections),
		})
	}
	return descriptions
}
func (peers *Peers) OnGC(callback func(*Peer)) {
	peers.Lock()
	defer peers.Unlock()

	// Although the array underlying peers.onGC might be accessed
	// without holding the lock in unlockAndNotify, we don't
	// support removing callbacks, so a simple append here is
	// safe.
	peers.onGC = append(peers.onGC, callback)
}
func (peers *Peers) OnInvalidateShortIDs(callback func()) {
	peers.Lock()
	defer peers.Unlock()

	// Safe, as in OnGC
	peers.onInvalidateShortIDs = append(peers.onInvalidateShortIDs, callback)
}
func (peers *Peers) chooseShortID() (PeerShortID, bool) {
	rng := rand.New(rand.NewSource(int64(randUint64())))

	// First, just try picking some short IDs at random, and
	// seeing if they are available:
	for i := 0; i < 10; i++ {
		shortID := PeerShortID(rng.Intn(1 << peerShortIDBits))
		if peers.byShortID[shortID].peer == nil {
			return shortID, true
		}
	}

	// Looks like most short IDs are used. So count the number of
	// unused ones, and pick one at random.
	available := int(1 << peerShortIDBits)
	for _, entry := range peers.byShortID {
		if entry.peer != nil {
			available--
		}
	}

	if available == 0 {
		// All short IDs are used.
		return 0, false
	}

	n := rng.Intn(available)
	var i PeerShortID
	for {
		if peers.byShortID[i].peer == nil {
			if n == 0 {
				return i, true
			}

			n--
		}

		i++
	}
}
func (peers *Peers) fetchWithDefault(peer *Peer) *Peer {
	peers.Lock()
	var pending peersPendingNotifications
	defer peers.unlockAndNotify(&pending)

	if existingPeer, found := peers.byName[peer.Name]; found {
		existingPeer.localRefCount++
		return existingPeer
	}

	peers.byName[peer.Name] = peer
	peers.addByShortID(peer, &pending)
	peer.localRefCount++
	return peer
}
func (peers *Peers) Fetch(name PeerName) *Peer {
	peers.RLock()
	defer peers.RUnlock()
	return peers.byName[name]
}
func (peers *Peers) fetchAndAddRef(name PeerName) *Peer {
	peers.Lock()
	defer peers.Unlock()
	peer := peers.byName[name]
	if peer != nil {
		peer.localRefCount++
	}
	return peer
}
func (peers *Peers) FetchByShortID(shortID PeerShortID) *Peer {
	peers.RLock()
	defer peers.RUnlock()
	return peers.byShortID[shortID].peer
}
func (peers *Peers) GarbageCollect() {
	peers.Lock()
	var pending peersPendingNotifications
	defer peers.unlockAndNotify(&pending)

	peers.garbageCollect(&pending)
}
func newRoutes(ourself *localPeer, peers *Peers) *routes {
	recalculate := make(chan *struct{}, 1)
	wait := make(chan chan struct{})
	action := make(chan func())
	r := &routes{
		ourself:      ourself,
		peers:        peers,
		unicast:      unicastRoutes{ourself.Name: UnknownPeerName},
		unicastAll:   unicastRoutes{ourself.Name: UnknownPeerName},
		broadcast:    broadcastRoutes{ourself.Name: []PeerName{}},
		broadcastAll: broadcastRoutes{ourself.Name: []PeerName{}},
		recalc:       recalculate,
		wait:         wait,
		action:       action,
	}
	go r.run(recalculate, wait, action)
	return r
}
func (r *routes) OnChange(callback func()) {
	r.Lock()
	defer r.Unlock()
	r.onChange = append(r.onChange, callback)
}
func (r *routes) Unicast(name PeerName) (PeerName, bool) {
	r.RLock()
	defer r.RUnlock()
	hop, found := r.unicast[name]
	return hop, found
}
func (r *routes) UnicastAll(name PeerName) (PeerName, bool) {
	r.RLock()
	defer r.RUnlock()
	hop, found := r.unicastAll[name]
	return hop, found
}
func (r *routes) Broadcast(name PeerName) []PeerName {
	return r.lookupOrCalculate(name, &r.broadcast, true)
}
func (r *routes) BroadcastAll(name PeerName) []PeerName {
	return r.lookupOrCalculate(name, &r.broadcastAll, false)
}
func NewPeer(name mesh.PeerName, uid mesh.PeerUID, logger mesh.Logger) *Peer {
	p := &Peer{
		name:    name,
		uid:     uid,
		gossip:  nil, // initially no gossip
		recv:    make(chan pkt),
		actions: make(chan func()),
		quit:    make(chan struct{}),
		logger:  logger,
	}
	go p.loop()
	return p
}
func (p *Peer) Register(gossip mesh.Gossip) {
	p.actions <- func() { p.gossip = gossip }
}
func (p *Peer) ReadFrom(b []byte) (n int, remote net.Addr, err error) {
	c := make(chan struct{})
	p.actions <- func() {
		go func() { // so as not to block loop
			defer close(c)
			select {
			case pkt := <-p.recv:
				n = copy(b, pkt.Buf)
				remote = MeshAddr{PeerName: pkt.SrcName, PeerUID: pkt.SrcUID}
				if n < len(pkt.Buf) {
					err = ErrShortRead
				}
			case <-p.quit:
				err = ErrPeerClosed
			}
		}()
	}
	<-c
	return n, remote, err
}
func (p *Peer) WriteTo(b []byte, dst net.Addr) (n int, err error) {
	c := make(chan struct{})
	p.actions <- func() {
		defer close(c)
		if p.gossip == nil {
			err = ErrGossipNotRegistered
			return
		}
		meshAddr, ok := dst.(MeshAddr)
		if !ok {
			err = ErrNotMeshAddr
			return
		}
		pkt := pkt{SrcName: p.name, SrcUID: p.uid, Buf: b}
		if meshAddr.PeerName == p.name {
			p.recv <- pkt
			return
		}
		// TODO(pb): detect and support broadcast
		buf := pkt.encode()
		n = len(buf)
		err = p.gossip.GossipUnicast(meshAddr.PeerName, buf)
	}
	<-c
	return n, err
}
func (p *Peer) LocalAddr() net.Addr {
	return MeshAddr{PeerName: p.name, PeerUID: p.uid}
}
func (p *Peer) OnGossip(buf []byte) (delta mesh.GossipData, err error) {
	return pktSlice{makePkt(buf)}, nil
}
func (p *Peer) OnGossipBroadcast(_ mesh.PeerName, buf []byte) (received mesh.GossipData, err error) {
	pkt := makePkt(buf)
	p.recv <- pkt // to ReadFrom
	return pktSlice{pkt}, nil
}
func (p *Peer) OnGossipUnicast(_ mesh.PeerName, buf []byte) error {
	pkt := makePkt(buf)
	p.recv <- pkt // to ReadFrom
	return nil
}
func NewDefaultServer(
	minPeerCount int,
	terminatec <-chan struct{},
	terminatedc chan<- error,
	logger mesh.Logger,
) Server {
	var (
		peerName = mustPeerName()
		nickName = mustHostname()
		host     = "0.0.0.0"
		port     = 6379
		password = ""
		channel  = "metcd"
	)
	router := mesh.NewRouter(mesh.Config{
		Host:               host,
		Port:               port,
		ProtocolMinVersion: mesh.ProtocolMinVersion,
		Password:           []byte(password),
		ConnLimit:          64,
		PeerDiscovery:      true,
		TrustedSubnets:     []*net.IPNet{},
	}, peerName, nickName, mesh.NullOverlay{}, logger)

	// Create a meshconn.Peer and connect it to a channel.
	peer := meshconn.NewPeer(router.Ourself.Peer.Name, router.Ourself.UID, logger)
	gossip := router.NewGossip(channel, peer)
	peer.Register(gossip)

	// Start the router and join the mesh.
	// Note that we don't ever stop the router.
	// This may or may not be a problem.
	// TODO(pb): determine if this is a super huge problem
	router.Start()

	return NewServer(router, peer, minPeerCount, terminatec, terminatedc, logger)
}
func PeerNameFromUserInput(userInput string) (PeerName, error) {
	// fixed-length identity
	nameByteAry := sha256.Sum256([]byte(userInput))
	return PeerNameFromBin(nameByteAry[:NameSize]), nil
}
func (name PeerName) bytes() []byte {
	res, err := hex.DecodeString(string(name))
	if err != nil {
		panic("unable to decode name to bytes: " + name)
	}
	return res
}
func NewRouter(config Config, name PeerName, nickName string, overlay Overlay, logger Logger) (*Router, error) {
	router := &Router{Config: config, gossipChannels: make(gossipChannels)}

	if overlay == nil {
		overlay = NullOverlay{}
	}

	router.Overlay = overlay
	router.Ourself = newLocalPeer(name, nickName, router)
	router.Peers = newPeers(router.Ourself)
	router.Peers.OnGC(func(peer *Peer) {
		logger.Printf("Removed unreachable peer %s", peer)
	})
	router.Routes = newRoutes(router.Ourself, router.Peers)
	router.ConnectionMaker = newConnectionMaker(router.Ourself, router.Peers, net.JoinHostPort(router.Host, "0"), router.Port, router.PeerDiscovery, logger)
	router.logger = logger
	gossip, err := router.NewGossip("topology", router)
	if err != nil {
		return nil, err
	}
	router.topologyGossip = gossip
	router.acceptLimiter = newTokenBucket(acceptMaxTokens, acceptTokenDelay)
	return router, nil
}
func (router *Router) sendAllGossip() {
	for channel := range router.gossipChannelSet() {
		if gossip := channel.gossiper.Gossip(); gossip != nil {
			channel.Send(gossip)
		}
	}
}
func (router *Router) sendAllGossipDown(conn Connection) {
	for channel := range router.gossipChannelSet() {
		if gossip := channel.gossiper.Gossip(); gossip != nil {
			channel.SendDown(conn, gossip)
		}
	}
}
func (router *Router) broadcastTopologyUpdate(update []*Peer) {
	names := make(peerNameSet)
	for _, p := range update {
		names[p.Name] = struct{}{}
	}
	router.topologyGossip.GossipBroadcast(&topologyGossipData{peers: router.Peers, update: names})
}
func (router *Router) OnGossipUnicast(sender PeerName, msg []byte) error {
	return fmt.Errorf("unexpected topology gossip unicast: %v", msg)
}
func (router *Router) OnGossipBroadcast(_ PeerName, update []byte) (GossipData, error) {
	origUpdate, _, err := router.applyTopologyUpdate(update)
	if err != nil || len(origUpdate) == 0 {
		return nil, err
	}
	return &topologyGossipData{peers: router.Peers, update: origUpdate}, nil
}
func (router *Router) Gossip() GossipData {
	return &topologyGossipData{peers: router.Peers, update: router.Peers.names()}
}
func (router *Router) OnGossip(update []byte) (GossipData, error) {
	_, newUpdate, err := router.applyTopologyUpdate(update)
	if err != nil || len(newUpdate) == 0 {
		return nil, err
	}
	return &topologyGossipData{peers: router.Peers, update: newUpdate}, nil
}
func (d *topologyGossipData) Encode() [][]byte {
	return [][]byte{d.peers.encodePeers(d.update)}
}
func newState(self mesh.PeerName) *state {
	return &state{
		set:  map[mesh.PeerName]int{},
		self: self,
	}
}
func (st *state) Merge(other mesh.GossipData) (complete mesh.GossipData) {
	return st.mergeComplete(other.(*state).copy().set)
}
func (st *state) mergeReceived(set map[mesh.PeerName]int) (received mesh.GossipData) {
	st.mtx.Lock()
	defer st.mtx.Unlock()

	for peer, v := range set {
		if v <= st.set[peer] {
			delete(set, peer) // optimization: make the forwarded data smaller
			continue
		}
		st.set[peer] = v
	}

	return &state{
		set: set, // all remaining elements were novel to us
	}
}
func (st *state) mergeComplete(set map[mesh.PeerName]int) (complete mesh.GossipData) {
	st.mtx.Lock()
	defer st.mtx.Unlock()

	for peer, v := range set {
		if v > st.set[peer] {
			st.set[peer] = v
		}
	}

	return &state{
		set: st.set, // n.b. can't .copy() due to lock contention
	}
}
func (*surrogateGossiper) OnGossipBroadcast(_ PeerName, update []byte) (GossipData, error) {
	return newSurrogateGossipData(update), nil
}
func (s *surrogateGossiper) OnGossip(update []byte) (GossipData, error) {
	hash := fnv.New64a()
	_, _ = hash.Write(update)
	updateHash := hash.Sum64()
	s.Lock()
	defer s.Unlock()
	for _, p := range s.prevUpdates {
		if updateHash == p.hash && bytes.Equal(update, p.update) {
			return nil, nil
		}
	}
	// Delete anything that's older than the gossip interval, so we don't grow forever
	// (this time limit is arbitrary; surrogateGossiper should pass on new gossip immediately
	// so there should be no reason for a duplicate to show up after a long time)
	updateTime := now()
	deleteBefore := updateTime.Add(-gossipInterval)
	keepFrom := len(s.prevUpdates)
	for i, p := range s.prevUpdates {
		if p.t.After(deleteBefore) {
			keepFrom = i
			break
		}
	}
	s.prevUpdates = append(s.prevUpdates[keepFrom:], prevUpdate{update, updateHash, updateTime})
	return newSurrogateGossipData(update), nil
}
func generateKeyPair() (publicKey, privateKey *[32]byte, err error) {
	return box.GenerateKey(rand.Reader)
}
func formSessionKey(remotePublicKey, localPrivateKey *[32]byte, secretKey []byte) *[32]byte {
	var sharedKey [32]byte
	box.Precompute(&sharedKey, remotePublicKey, localPrivateKey)
	sharedKeySlice := sharedKey[:]
	sharedKeySlice = append(sharedKeySlice, secretKey...)
	sessionKey := sha256.Sum256(sharedKeySlice)
	return &sessionKey
}
func newTCPCryptoState(sessionKey *[32]byte, outbound bool) *tcpCryptoState {
	s := &tcpCryptoState{sessionKey: sessionKey}
	if outbound {
		s.nonce[0] |= (1 << 7)
	}
	s.nonce[0] |= (1 << 6)
	return s
}
func (sender *gobTCPSender) Send(msg []byte) error {
	return sender.encoder.Encode(msg)
}
func (sender *lengthPrefixTCPSender) Send(msg []byte) error {
	l := len(msg)
	if l > maxTCPMsgSize {
		return fmt.Errorf("outgoing message exceeds maximum size: %d > %d", l, maxTCPMsgSize)
	}
	// We copy the message so we can send it in a single Write
	// operation, thus making this thread-safe without locking.
	prefixedMsg := make([]byte, 4+l)
	binary.BigEndian.PutUint32(prefixedMsg, uint32(l))
	copy(prefixedMsg[4:], msg)
	_, err := sender.writer.Write(prefixedMsg)
	return err
}
func (sender *encryptedTCPSender) Send(msg []byte) error {
	sender.Lock()
	defer sender.Unlock()
	encodedMsg := secretbox.Seal(nil, msg, &sender.state.nonce, sender.state.sessionKey)
	sender.state.advance()
	return sender.sender.Send(encodedMsg)
}
func (receiver *gobTCPReceiver) Receive() ([]byte, error) {
	var msg []byte
	err := receiver.decoder.Decode(&msg)
	return msg, err
}
func (receiver *lengthPrefixTCPReceiver) Receive() ([]byte, error) {
	lenPrefix := make([]byte, 4)
	if _, err := io.ReadFull(receiver.reader, lenPrefix); err != nil {
		return nil, err
	}
	l := binary.BigEndian.Uint32(lenPrefix)
	if l > maxTCPMsgSize {
		return nil, fmt.Errorf("incoming message exceeds maximum size: %d > %d", l, maxTCPMsgSize)
	}
	msg := make([]byte, l)
	_, err := io.ReadFull(receiver.reader, msg)
	return msg, err
}
func (receiver *encryptedTCPReceiver) Receive() ([]byte, error) {
	msg, err := receiver.receiver.Receive()
	if err != nil {
		return nil, err
	}

	decodedMsg, success := secretbox.Open(nil, msg, &receiver.state.nonce, receiver.state.sessionKey)
	if !success {
		return nil, fmt.Errorf("Unable to decrypt TCP msg")
	}

	receiver.state.advance()
	return decodedMsg, nil
}
func newPeer(self mesh.PeerName, logger *log.Logger) *peer {
	actions := make(chan func())
	p := &peer{
		st:      newState(self),
		send:    nil, // must .register() later
		actions: actions,
		quit:    make(chan struct{}),
		logger:  logger,
	}
	go p.loop(actions)
	return p
}
func (p *peer) incr() (result int) {
	c := make(chan struct{})
	p.actions <- func() {
		defer close(c)
		st := p.st.incr()
		if p.send != nil {
			p.send.GossipBroadcast(st)
		} else {
			p.logger.Printf("no sender configured; not broadcasting update right now")
		}
		result = st.get()
	}
	<-c
	return result
}
func (p *peer) Gossip() (complete mesh.GossipData) {
	complete = p.st.copy()
	p.logger.Printf("Gossip => complete %v", complete.(*state).set)
	return complete
}
func (p *peer) OnGossipUnicast(src mesh.PeerName, buf []byte) error {
	var set map[mesh.PeerName]int
	if err := gob.NewDecoder(bytes.NewReader(buf)).Decode(&set); err != nil {
		return err
	}

	complete := p.st.mergeComplete(set)
	p.logger.Printf("OnGossipUnicast %s %v => complete %v", src, set, complete)
	return nil
}
func makeRaftPeer(addr net.Addr) raft.Peer {
	return raft.Peer{
		ID:      uint64(addr.(meshconn.MeshAddr).PeerUID),
		Context: nil, // TODO(pb): ??
	}
}
func (peer *Peer) String() string {
	return fmt.Sprint(peer.Name, "(", peer.NickName, ")")
}
func (peer *Peer) forEachConnectedPeer(establishedAndSymmetric bool, exclude map[PeerName]PeerName, f func(*Peer)) {
	for remoteName, conn := range peer.connections {
		if establishedAndSymmetric && !conn.isEstablished() {
			continue
		}
		if _, found := exclude[remoteName]; found {
			continue
		}
		remotePeer := conn.Remote()
		if remoteConn, found := remotePeer.connections[peer.Name]; !establishedAndSymmetric || (found && remoteConn.isEstablished()) {
			f(remotePeer)
		}
	}
}
func parsePeerUID(s string) (PeerUID, error) {
	uid, err := strconv.ParseUint(s, 10, 64)
	return PeerUID(uid), err
}
func (lop listOfPeers) Swap(i, j int) {
	lop[i], lop[j] = lop[j], lop[i]
}
func (lop listOfPeers) Less(i, j int) bool {
	return lop[i].Name < lop[j].Name
}
func (params protocolIntroParams) doIntro() (res protocolIntroResults, err error) {
	if err = params.Conn.SetDeadline(time.Now().Add(headerTimeout)); err != nil {
		return
	}

	if res.Version, err = params.exchangeProtocolHeader(); err != nil {
		return
	}

	var pubKey, privKey *[32]byte
	if params.Password != nil {
		if pubKey, privKey, err = generateKeyPair(); err != nil {
			return
		}
	}

	if err = params.Conn.SetWriteDeadline(time.Time{}); err != nil {
		return
	}
	if err = params.Conn.SetReadDeadline(time.Now().Add(tcpHeartbeat * 2)); err != nil {
		return
	}

	switch res.Version {
	case 1:
		err = res.doIntroV1(params, pubKey, privKey)
	case 2:
		err = res.doIntroV2(params, pubKey, privKey)
	default:
		panic("unhandled protocol version")
	}

	return
}
func filterV1Features(intro map[string]string) map[string]string {
	safe := make(map[string]string)
	for _, k := range protocolV1Features {
		if val, ok := intro[k]; ok {
			safe[k] = val
		}
	}

	return safe
}
func newConnectionMaker(ourself *localPeer, peers *Peers, localAddr string, port int, discovery bool, logger Logger) *connectionMaker {
	actionChan := make(chan connectionMakerAction, ChannelSize)
	cm := &connectionMaker{
		ourself:     ourself,
		peers:       peers,
		localAddr:   localAddr,
		port:        port,
		discovery:   discovery,
		directPeers: peerAddrs{},
		targets:     make(map[string]*target),
		connections: make(map[Connection]struct{}),
		actionChan:  actionChan,
		logger:      logger,
	}
	go cm.queryLoop(actionChan)
	return cm
}
func (cm *connectionMaker) connectionAborted(address string, err error) {
	cm.actionChan <- func() bool {
		target := cm.targets[address]
		target.state = targetWaiting
		target.lastError = err
		target.nextTryLater()
		return true
	}
}
func newGossipSender(
	makeMsg func(msg []byte) protocolMsg,
	makeBroadcastMsg func(srcName PeerName, msg []byte) protocolMsg,
	sender protocolSender,
	stop <-chan struct{},
) *gossipSender {
	more := make(chan struct{}, 1)
	flush := make(chan chan<- bool)
	s := &gossipSender{
		makeMsg:          makeMsg,
		makeBroadcastMsg: makeBroadcastMsg,
		sender:           sender,
		broadcasts:       make(map[PeerName]GossipData),
		more:             more,
		flush:            flush,
	}
	go s.run(stop, more, flush)
	return s
}
func (s *gossipSender) Send(data GossipData) {
	s.Lock()
	defer s.Unlock()
	if s.empty() {
		defer s.prod()
	}
	if s.gossip == nil {
		s.gossip = data
	} else {
		s.gossip = s.gossip.Merge(data)
	}
}
func (s *gossipSender) Broadcast(srcName PeerName, data GossipData) {
	s.Lock()
	defer s.Unlock()
	if s.empty() {
		defer s.prod()
	}
	d, found := s.broadcasts[srcName]
	if !found {
		s.broadcasts[srcName] = data
	} else {
		s.broadcasts[srcName] = d.Merge(data)
	}
}
func (s *gossipSender) Flush() bool {
	ch := make(chan bool)
	s.flush <- ch
	return <-ch
}
func (gs *gossipSenders) Sender(channelName string, makeGossipSender func(sender protocolSender, stop <-chan struct{}) *gossipSender) *gossipSender {
	gs.Lock()
	defer gs.Unlock()
	s, found := gs.senders[channelName]
	if !found {
		s = makeGossipSender(gs.sender, gs.stop)
		gs.senders[channelName] = s
	}
	return s
}
func (gs *gossipSenders) Flush() bool {
	sent := false
	gs.Lock()
	defer gs.Unlock()
	for _, sender := range gs.senders {
		sent = sender.Flush() || sent
	}
	return sent
}
func findMainPath() string {
	pc := make([]uintptr, 100)
	n := runtime.Callers(2, pc)
	frames := runtime.CallersFrames(pc[:n])
	for {
		frame, more := frames.Next()
		// Tests won't have package main, instead they have testing.tRunner
		if frame.Function == "main.main" || frame.Function == "testing.tRunner" {
			return frame.File
		}
		if !more {
			break
		}
	}
	return ""
}
func Create(c context.Context, clientID string) (token string, err error) {
	req := &pb.CreateChannelRequest{
		ApplicationKey: &clientID,
	}
	resp := &pb.CreateChannelResponse{}
	err = internal.Call(c, service, "CreateChannel", req, resp)
	token = resp.GetToken()
	return token, remapError(err)
}
func Send(c context.Context, clientID, message string) error {
	req := &pb.SendMessageRequest{
		ApplicationKey: &clientID,
		Message:        &message,
	}
	resp := &basepb.VoidProto{}
	return remapError(internal.Call(c, service, "SendChannelMessage", req, resp))
}
func SendJSON(c context.Context, clientID string, value interface{}) error {
	m, err := json.Marshal(value)
	if err != nil {
		return err
	}
	return Send(c, clientID, string(m))
}
func remapError(err error) error {
	if e, ok := err.(*internal.APIError); ok {
		if e.Service == "xmpp" {
			e.Service = "channel"
		}
	}
	return err
}
func NamespacedContext(ctx netcontext.Context, namespace string) netcontext.Context {
	return withNamespace(ctx, namespace)
}
func protoToItem(p *pb.MemcacheGetResponse_Item) *Item {
	return &Item{
		Key:   string(p.Key),
		Value: p.Value,
		Flags: p.GetFlags(),
		casID: p.GetCasId(),
	}
}
func singleError(err error) error {
	if me, ok := err.(appengine.MultiError); ok {
		return me[0]
	}
	return err
}
func Get(c context.Context, key string) (*Item, error) {
	m, err := GetMulti(c, []string{key})
	if err != nil {
		return nil, err
	}
	if _, ok := m[key]; !ok {
		return nil, ErrCacheMiss
	}
	return m[key], nil
}
func GetMulti(c context.Context, key []string) (map[string]*Item, error) {
	if len(key) == 0 {
		return nil, nil
	}
	keyAsBytes := make([][]byte, len(key))
	for i, k := range key {
		keyAsBytes[i] = []byte(k)
	}
	req := &pb.MemcacheGetRequest{
		Key:    keyAsBytes,
		ForCas: proto.Bool(true),
	}
	res := &pb.MemcacheGetResponse{}
	if err := internal.Call(c, "memcache", "Get", req, res); err != nil {
		return nil, err
	}
	m := make(map[string]*Item, len(res.Item))
	for _, p := range res.Item {
		t := protoToItem(p)
		m[t.Key] = t
	}
	return m, nil
}
func Delete(c context.Context, key string) error {
	return singleError(DeleteMulti(c, []string{key}))
}
func DeleteMulti(c context.Context, key []string) error {
	if len(key) == 0 {
		return nil
	}
	req := &pb.MemcacheDeleteRequest{
		Item: make([]*pb.MemcacheDeleteRequest_Item, len(key)),
	}
	for i, k := range key {
		req.Item[i] = &pb.MemcacheDeleteRequest_Item{Key: []byte(k)}
	}
	res := &pb.MemcacheDeleteResponse{}
	if err := internal.Call(c, "memcache", "Delete", req, res); err != nil {
		return err
	}
	if len(res.DeleteStatus) != len(key) {
		return ErrServerError
	}
	me, any := make(appengine.MultiError, len(key)), false
	for i, s := range res.DeleteStatus {
		switch s {
		case pb.MemcacheDeleteResponse_DELETED:
			// OK
		case pb.MemcacheDeleteResponse_NOT_FOUND:
			me[i] = ErrCacheMiss
			any = true
		default:
			me[i] = ErrServerError
			any = true
		}
	}
	if any {
		return me
	}
	return nil
}
func Increment(c context.Context, key string, delta int64, initialValue uint64) (newValue uint64, err error) {
	return incr(c, key, delta, &initialValue)
}
func IncrementExisting(c context.Context, key string, delta int64) (newValue uint64, err error) {
	return incr(c, key, delta, nil)
}
func set(c context.Context, item []*Item, value [][]byte, policy pb.MemcacheSetRequest_SetPolicy) error {
	if len(item) == 0 {
		return nil
	}
	req := &pb.MemcacheSetRequest{
		Item: make([]*pb.MemcacheSetRequest_Item, len(item)),
	}
	for i, t := range item {
		p := &pb.MemcacheSetRequest_Item{
			Key: []byte(t.Key),
		}
		if value == nil {
			p.Value = t.Value
		} else {
			p.Value = value[i]
		}
		if t.Flags != 0 {
			p.Flags = proto.Uint32(t.Flags)
		}
		if t.Expiration != 0 {
			// In the .proto file, MemcacheSetRequest_Item uses a fixed32 (i.e. unsigned)
			// for expiration time, while MemcacheGetRequest_Item uses int32 (i.e. signed).
			// Throughout this .go file, we use int32.
			// Also, in the proto, the expiration value is either a duration (in seconds)
			// or an absolute Unix timestamp (in seconds), depending on whether the
			// value is less than or greater than or equal to 30 years, respectively.
			if t.Expiration < time.Second {
				// Because an Expiration of 0 means no expiration, we take
				// care here to translate an item with an expiration
				// Duration between 0-1 seconds as immediately expiring
				// (saying it expired a few seconds ago), rather than
				// rounding it down to 0 and making it live forever.
				p.ExpirationTime = proto.Uint32(uint32(time.Now().Unix()) - 5)
			} else if t.Expiration >= thirtyYears {
				p.ExpirationTime = proto.Uint32(uint32(time.Now().Unix()) + uint32(t.Expiration/time.Second))
			} else {
				p.ExpirationTime = proto.Uint32(uint32(t.Expiration / time.Second))
			}
		}
		if t.casID != 0 {
			p.CasId = proto.Uint64(t.casID)
			p.ForCas = proto.Bool(true)
		}
		p.SetPolicy = policy.Enum()
		req.Item[i] = p
	}
	res := &pb.MemcacheSetResponse{}
	if err := internal.Call(c, "memcache", "Set", req, res); err != nil {
		return err
	}
	if len(res.SetStatus) != len(item) {
		return ErrServerError
	}
	me, any := make(appengine.MultiError, len(item)), false
	for i, st := range res.SetStatus {
		var err error
		switch st {
		case pb.MemcacheSetResponse_STORED:
			// OK
		case pb.MemcacheSetResponse_NOT_STORED:
			err = ErrNotStored
		case pb.MemcacheSetResponse_EXISTS:
			err = ErrCASConflict
		default:
			err = ErrServerError
		}
		if err != nil {
			me[i] = err
			any = true
		}
	}
	if any {
		return me
	}
	return nil
}
func (cd Codec) Get(c context.Context, key string, v interface{}) (*Item, error) {
	i, err := Get(c, key)
	if err != nil {
		return nil, err
	}
	if err := cd.Unmarshal(i.Value, v); err != nil {
		return nil, err
	}
	return i, nil
}
func Stats(c context.Context) (*Statistics, error) {
	req := &pb.MemcacheStatsRequest{}
	res := &pb.MemcacheStatsResponse{}
	if err := internal.Call(c, "memcache", "Stats", req, res); err != nil {
		return nil, err
	}
	if res.Stats == nil {
		return nil, ErrNoStats
	}
	return &Statistics{
		Hits:     *res.Stats.Hits,
		Misses:   *res.Stats.Misses,
		ByteHits: *res.Stats.ByteHits,
		Items:    *res.Stats.Items,
		Bytes:    *res.Stats.Bytes,
		Oldest:   int64(*res.Stats.OldestItemAge),
	}, nil
}
func Flush(c context.Context) error {
	req := &pb.MemcacheFlushRequest{}
	res := &pb.MemcacheFlushResponse{}
	return internal.Call(c, "memcache", "FlushAll", req, res)
}
func RunInBackground(c context.Context, f func(c context.Context)) error {
	req := &pb.StartBackgroundRequestRequest{}
	res := &pb.StartBackgroundRequestResponse{}
	if err := internal.Call(c, "system", "StartBackgroundRequest", req, res); err != nil {
		return err
	}
	sendc <- send{res.GetRequestId(), f}
	return nil
}
func List(c context.Context) ([]string, error) {
	req := &pb.GetModulesRequest{}
	res := &pb.GetModulesResponse{}
	err := internal.Call(c, "modules", "GetModules", req, res)
	return res.Module, err
}
func SetNumInstances(c context.Context, module, version string, instances int) error {
	req := &pb.SetNumInstancesRequest{}
	if module != "" {
		req.Module = &module
	}
	if version != "" {
		req.Version = &version
	}
	req.Instances = proto.Int64(int64(instances))
	res := &pb.SetNumInstancesResponse{}
	return internal.Call(c, "modules", "SetNumInstances", req, res)
}
func Versions(c context.Context, module string) ([]string, error) {
	req := &pb.GetVersionsRequest{}
	if module != "" {
		req.Module = &module
	}
	res := &pb.GetVersionsResponse{}
	err := internal.Call(c, "modules", "GetVersions", req, res)
	return res.GetVersion(), err
}
func DefaultVersion(c context.Context, module string) (string, error) {
	req := &pb.GetDefaultVersionRequest{}
	if module != "" {
		req.Module = &module
	}
	res := &pb.GetDefaultVersionResponse{}
	err := internal.Call(c, "modules", "GetDefaultVersion", req, res)
	return res.GetVersion(), err
}
func Start(c context.Context, module, version string) error {
	req := &pb.StartModuleRequest{}
	if module != "" {
		req.Module = &module
	}
	if version != "" {
		req.Version = &version
	}
	res := &pb.StartModuleResponse{}
	return internal.Call(c, "modules", "StartModule", req, res)
}
func Stop(c context.Context, module, version string) error {
	req := &pb.StopModuleRequest{}
	if module != "" {
		req.Module = &module
	}
	if version != "" {
		req.Version = &version
	}
	res := &pb.StopModuleResponse{}
	return internal.Call(c, "modules", "StopModule", req, res)
}
func (q *Query) Ancestor(ancestor *Key) *Query {
	q = q.clone()
	if ancestor == nil {
		q.err = errors.New("datastore: nil query ancestor")
		return q
	}
	q.ancestor = ancestor
	return q
}
func (q *Query) EventualConsistency() *Query {
	q = q.clone()
	q.eventual = true
	return q
}
func (q *Query) Project(fieldNames ...string) *Query {
	q = q.clone()
	q.projection = append([]string(nil), fieldNames...)
	return q
}
func (q *Query) Distinct() *Query {
	q = q.clone()
	q.distinct = true
	return q
}
func (q *Query) DistinctOn(fieldNames ...string) *Query {
	q = q.clone()
	q.distinctOn = fieldNames
	return q
}
func (q *Query) KeysOnly() *Query {
	q = q.clone()
	q.keysOnly = true
	return q
}
func (q *Query) Limit(limit int) *Query {
	q = q.clone()
	if limit < math.MinInt32 || limit > math.MaxInt32 {
		q.err = errors.New("datastore: query limit overflow")
		return q
	}
	q.limit = int32(limit)
	return q
}
func (q *Query) Offset(offset int) *Query {
	q = q.clone()
	if offset < 0 {
		q.err = errors.New("datastore: negative query offset")
		return q
	}
	if offset > math.MaxInt32 {
		q.err = errors.New("datastore: query offset overflow")
		return q
	}
	q.offset = int32(offset)
	return q
}
func (q *Query) BatchSize(size int) *Query {
	q = q.clone()
	if size <= 0 || size > math.MaxInt32 {
		q.err = errors.New("datastore: query batch size overflow")
		return q
	}
	q.count = int32(size)
	return q
}
func (q *Query) Start(c Cursor) *Query {
	q = q.clone()
	if c.cc == nil {
		q.err = errors.New("datastore: invalid cursor")
		return q
	}
	q.start = c.cc
	return q
}
func (q *Query) End(c Cursor) *Query {
	q = q.clone()
	if c.cc == nil {
		q.err = errors.New("datastore: invalid cursor")
		return q
	}
	q.end = c.cc
	return q
}
func (q *Query) Count(c context.Context) (int, error) {
	// Check that the query is well-formed.
	if q.err != nil {
		return 0, q.err
	}

	// Run a copy of the query, with keysOnly true (if we're not a projection,
	// since the two are incompatible), and an adjusted offset. We also set the
	// limit to zero, as we don't want any actual entity data, just the number
	// of skipped results.
	newQ := q.clone()
	newQ.keysOnly = len(newQ.projection) == 0
	newQ.limit = 0
	if q.limit < 0 {
		// If the original query was unlimited, set the new query's offset to maximum.
		newQ.offset = math.MaxInt32
	} else {
		newQ.offset = q.offset + q.limit
		if newQ.offset < 0 {
			// Do the best we can, in the presence of overflow.
			newQ.offset = math.MaxInt32
		}
	}
	req := &pb.Query{}
	if err := newQ.toProto(req, internal.FullyQualifiedAppID(c)); err != nil {
		return 0, err
	}
	res := &pb.QueryResult{}
	if err := internal.Call(c, "datastore_v3", "RunQuery", req, res); err != nil {
		return 0, err
	}

	// n is the count we will return. For example, suppose that our original
	// query had an offset of 4 and a limit of 2008: the count will be 2008,
	// provided that there are at least 2012 matching entities. However, the
	// RPCs will only skip 1000 results at a time. The RPC sequence is:
	//   call RunQuery with (offset, limit) = (2012, 0)  // 2012 == newQ.offset
	//   response has (skippedResults, moreResults) = (1000, true)
	//   n += 1000  // n == 1000
	//   call Next     with (offset, limit) = (1012, 0)  // 1012 == newQ.offset - n
	//   response has (skippedResults, moreResults) = (1000, true)
	//   n += 1000  // n == 2000
	//   call Next     with (offset, limit) = (12, 0)    // 12 == newQ.offset - n
	//   response has (skippedResults, moreResults) = (12, false)
	//   n += 12    // n == 2012
	//   // exit the loop
	//   n -= 4     // n == 2008
	var n int32
	for {
		// The QueryResult should have no actual entity data, just skipped results.
		if len(res.Result) != 0 {
			return 0, errors.New("datastore: internal error: Count request returned too much data")
		}
		n += res.GetSkippedResults()
		if !res.GetMoreResults() {
			break
		}
		if err := callNext(c, res, newQ.offset-n, q.count); err != nil {
			return 0, err
		}
	}
	n -= q.offset
	if n < 0 {
		// If the offset was greater than the number of matching entities,
		// return 0 instead of negative.
		n = 0
	}
	return int(n), nil
}
func (q *Query) Run(c context.Context) *Iterator {
	if q.err != nil {
		return &Iterator{err: q.err}
	}
	t := &Iterator{
		c:      c,
		limit:  q.limit,
		count:  q.count,
		q:      q,
		prevCC: q.start,
	}
	var req pb.Query
	if err := q.toProto(&req, internal.FullyQualifiedAppID(c)); err != nil {
		t.err = err
		return t
	}
	if err := internal.Call(c, "datastore_v3", "RunQuery", &req, &t.res); err != nil {
		t.err = err
		return t
	}
	offset := q.offset - t.res.GetSkippedResults()
	var count int32
	if t.count > 0 && (t.limit < 0 || t.count < t.limit) {
		count = t.count
	} else {
		count = t.limit
	}
	for offset > 0 && t.res.GetMoreResults() {
		t.prevCC = t.res.CompiledCursor
		if err := callNext(t.c, &t.res, offset, count); err != nil {
			t.err = err
			break
		}
		skip := t.res.GetSkippedResults()
		if skip < 0 {
			t.err = errors.New("datastore: internal error: negative number of skipped_results")
			break
		}
		offset -= skip
	}
	if offset < 0 {
		t.err = errors.New("datastore: internal error: query offset was overshot")
	}
	return t
}
func (t *Iterator) Next(dst interface{}) (*Key, error) {
	k, e, err := t.next()
	if err != nil {
		return nil, err
	}
	if dst != nil && !t.q.keysOnly {
		err = loadEntity(dst, e)
	}
	return k, err
}
func (t *Iterator) Cursor() (Cursor, error) {
	if t.err != nil && t.err != Done {
		return Cursor{}, t.err
	}
	// If we are at either end of the current batch of results,
	// return the compiled cursor at that end.
	skipped := t.res.GetSkippedResults()
	if t.i == 0 && skipped == 0 {
		if t.prevCC == nil {
			// A nil pointer (of type *pb.CompiledCursor) means no constraint:
			// passing it as the end cursor of a new query means unlimited results
			// (glossing over the integer limit parameter for now).
			// A non-nil pointer to an empty pb.CompiledCursor means the start:
			// passing it as the end cursor of a new query means 0 results.
			// If prevCC was nil, then the original query had no start cursor, but
			// Iterator.Cursor should return "the start" instead of unlimited.
			return Cursor{&zeroCC}, nil
		}
		return Cursor{t.prevCC}, nil
	}
	if t.i == len(t.res.Result) {
		return Cursor{t.res.CompiledCursor}, nil
	}
	// Otherwise, re-run the query offset to this iterator's position, starting from
	// the most recent compiled cursor. This is done on a best-effort basis, as it
	// is racy; if a concurrent process has added or removed entities, then the
	// cursor returned may be inconsistent.
	q := t.q.clone()
	q.start = t.prevCC
	q.offset = skipped + int32(t.i)
	q.limit = 0
	q.keysOnly = len(q.projection) == 0
	t1 := q.Run(t.c)
	_, _, err := t1.next()
	if err != Done {
		if err == nil {
			err = fmt.Errorf("datastore: internal error: zero-limit query did not have zero results")
		}
		return Cursor{}, err
	}
	return Cursor{t1.res.CompiledCursor}, nil
}
func (c Cursor) String() string {
	if c.cc == nil {
		return ""
	}
	b, err := proto.Marshal(c.cc)
	if err != nil {
		// The only way to construct a Cursor with a non-nil cc field is to
		// unmarshal from the byte representation. We panic if the unmarshal
		// succeeds but the marshaling of the unchanged protobuf value fails.
		panic(fmt.Sprintf("datastore: internal error: malformed cursor: %v", err))
	}
	return strings.TrimRight(base64.URLEncoding.EncodeToString(b), "=")
}
func DecodeCursor(s string) (Cursor, error) {
	if s == "" {
		return Cursor{&zeroCC}, nil
	}
	if n := len(s) % 4; n != 0 {
		s += strings.Repeat("=", 4-n)
	}
	b, err := base64.URLEncoding.DecodeString(s)
	if err != nil {
		return Cursor{}, err
	}
	cc := &pb.CompiledCursor{}
	if err := proto.Unmarshal(b, cc); err != nil {
		return Cursor{}, err
	}
	return Cursor{cc}, nil
}
func saveEntity(defaultAppID string, key *Key, src interface{}) (*pb.EntityProto, error) {
	var err error
	var props []Property
	if e, ok := src.(PropertyLoadSaver); ok {
		props, err = e.Save()
	} else {
		props, err = SaveStruct(src)
	}
	if err != nil {
		return nil, err
	}
	return propertiesToProto(defaultAppID, key, props)
}
func Namespace(c context.Context, namespace string) (context.Context, error) {
	if !validNamespace.MatchString(namespace) {
		return nil, fmt.Errorf("appengine: namespace %q does not match /%s/", namespace, validNamespace)
	}
	return internal.NamespacedContext(c, namespace), nil
}
func (cfg *TypeConfig) typeof(name string) string {
	if cfg.Var != nil {
		if t := cfg.Var[name]; t != "" {
			return t
		}
	}
	if cfg.Func != nil {
		if t := cfg.Func[name]; t != "" {
			return "func()" + t
		}
	}
	return ""
}
func (typ *Type) dot(cfg *TypeConfig, name string) string {
	if typ.Field != nil {
		if t := typ.Field[name]; t != "" {
			return t
		}
	}
	if typ.Method != nil {
		if t := typ.Method[name]; t != "" {
			return t
		}
	}

	for _, e := range typ.Embed {
		etyp := cfg.Type[e]
		if etyp != nil {
			if t := etyp.dot(cfg, name); t != "" {
				return t
			}
		}
	}

	return ""
}
func joinFunc(in, out []string) string {
	outs := ""
	if len(out) == 1 {
		outs = " " + out[0]
	} else if len(out) > 1 {
		outs = " (" + join(out) + ")"
	}
	return "func(" + join(in) + ")" + outs
}
func validPropertyName(name string) bool {
	if name == "" {
		return false
	}
	for _, s := range strings.Split(name, ".") {
		if s == "" {
			return false
		}
		first := true
		for _, c := range s {
			if first {
				first = false
				if c != '_' && !unicode.IsLetter(c) {
					return false
				}
			} else {
				if c != '_' && !unicode.IsLetter(c) && !unicode.IsDigit(c) {
					return false
				}
			}
		}
	}
	return true
}
func getStructCodec(t reflect.Type) (*structCodec, error) {
	structCodecsMutex.Lock()
	defer structCodecsMutex.Unlock()
	return getStructCodecLocked(t)
}
func LoadStruct(dst interface{}, p []Property) error {
	x, err := newStructPLS(dst)
	if err != nil {
		return err
	}
	return x.Load(p)
}
func SaveStruct(src interface{}) ([]Property, error) {
	x, err := newStructPLS(src)
	if err != nil {
		return nil, err
	}
	return x.Save()
}
func ServingURL(c context.Context, key appengine.BlobKey, opts *ServingURLOptions) (*url.URL, error) {
	req := &pb.ImagesGetUrlBaseRequest{
		BlobKey: (*string)(&key),
	}
	if opts != nil && opts.Secure {
		req.CreateSecureUrl = &opts.Secure
	}
	res := &pb.ImagesGetUrlBaseResponse{}
	if err := internal.Call(c, "images", "GetUrlBase", req, res); err != nil {
		return nil, err
	}

	// The URL may have suffixes added to dynamically resize or crop:
	// - adding "=s32" will serve the image resized to 32 pixels, preserving the aspect ratio.
	// - adding "=s32-c" is the same as "=s32" except it will be cropped.
	u := *res.Url
	if opts != nil && opts.Size > 0 {
		u += fmt.Sprintf("=s%d", opts.Size)
		if opts.Crop {
			u += "-c"
		}
	}
	return url.Parse(u)
}
func DeleteServingURL(c context.Context, key appengine.BlobKey) error {
	req := &pb.ImagesDeleteUrlBaseRequest{
		BlobKey: (*string)(&key),
	}
	res := &pb.ImagesDeleteUrlBaseResponse{}
	return internal.Call(c, "images", "DeleteUrlBase", req, res)
}
func CurrentOAuth(c context.Context, scopes ...string) (*User, error) {
	req := &pb.GetOAuthUserRequest{}
	if len(scopes) != 1 || scopes[0] != "" {
		// The signature for this function used to be CurrentOAuth(Context, string).
		// Ignore the singular "" scope to preserve existing behavior.
		req.Scopes = scopes
	}

	res := &pb.GetOAuthUserResponse{}

	err := internal.Call(c, "user", "GetOAuthUser", req, res)
	if err != nil {
		return nil, err
	}
	return &User{
		Email:      *res.Email,
		AuthDomain: *res.AuthDomain,
		Admin:      res.GetIsAdmin(),
		ID:         *res.UserId,
		ClientID:   res.GetClientId(),
	}, nil
}
func OAuthConsumerKey(c context.Context) (string, error) {
	req := &pb.CheckOAuthSignatureRequest{}
	res := &pb.CheckOAuthSignatureResponse{}

	err := internal.Call(c, "user", "CheckOAuthSignature", req, res)
	if err != nil {
		return "", err
	}
	return *res.OauthConsumerKey, err
}
func (u *User) String() string {
	if u.AuthDomain != "" && strings.HasSuffix(u.Email, "@"+u.AuthDomain) {
		return u.Email[:len(u.Email)-len("@"+u.AuthDomain)]
	}
	if u.FederatedIdentity != "" {
		return u.FederatedIdentity
	}
	return u.Email
}
func LoginURL(c context.Context, dest string) (string, error) {
	return LoginURLFederated(c, dest, "")
}
func LoginURLFederated(c context.Context, dest, identity string) (string, error) {
	req := &pb.CreateLoginURLRequest{
		DestinationUrl: proto.String(dest),
	}
	if identity != "" {
		req.FederatedIdentity = proto.String(identity)
	}
	res := &pb.CreateLoginURLResponse{}
	if err := internal.Call(c, "user", "CreateLoginURL", req, res); err != nil {
		return "", err
	}
	return *res.LoginUrl, nil
}
func LogoutURL(c context.Context, dest string) (string, error) {
	req := &pb.CreateLogoutURLRequest{
		DestinationUrl: proto.String(dest),
	}
	res := &pb.CreateLogoutURLResponse{}
	if err := internal.Call(c, "user", "CreateLogoutURL", req, res); err != nil {
		return "", err
	}
	return *res.LogoutUrl, nil
}
func insertContext(f *ast.File, call *ast.CallExpr, ctx *ast.Ident) {
	if ctx == nil {
		// context is unknown, so use a plain "ctx".
		ctx = ast.NewIdent("ctx")
	} else {
		// Create a fresh *ast.Ident so we drop the position information.
		ctx = ast.NewIdent(ctx.Name)
	}

	call.Args = append([]ast.Expr{ctx}, call.Args...)
}
func NewClient(host string, client *http.Client) (*Client, error) {
	// Add an appcfg header to outgoing requests.
	wrapClient := new(http.Client)
	*wrapClient = *client
	t := client.Transport
	if t == nil {
		t = http.DefaultTransport
	}
	wrapClient.Transport = &headerAddingRoundTripper{t}

	url := url.URL{
		Scheme: "https",
		Host:   host,
		Path:   "/_ah/remote_api",
	}
	if host == "localhost" || strings.HasPrefix(host, "localhost:") {
		url.Scheme = "http"
	}
	u := url.String()
	appID, err := getAppID(wrapClient, u)
	if err != nil {
		return nil, fmt.Errorf("unable to contact server: %v", err)
	}
	return &Client{
		hc:    wrapClient,
		url:   u,
		appID: appID,
	}, nil
}
func (c *Client) NewContext(parent context.Context) context.Context {
	ctx := internal.WithCallOverride(parent, c.call)
	ctx = internal.WithLogOverride(ctx, c.logf)
	ctx = internal.WithAppIDOverride(ctx, c.appID)
	return ctx
}
func NewRemoteContext(host string, client *http.Client) (context.Context, error) {
	c, err := NewClient(host, client)
	if err != nil {
		return nil, err
	}
	return c.NewContext(context.Background()), nil
}
func Debugf(ctx context.Context, format string, args ...interface{}) {
	internal.Logf(ctx, 0, format, args...)
}
func guestbookKey(ctx context.Context) *datastore.Key {
	// The string "default_guestbook" here could be varied to have multiple guestbooks.
	return datastore.NewKey(ctx, "Guestbook", "default_guestbook", 0, nil)
}
func (opt *RetryOptions) toRetryParameters() *pb.TaskQueueRetryParameters {
	params := &pb.TaskQueueRetryParameters{}
	if opt.RetryLimit > 0 {
		params.RetryLimit = proto.Int32(opt.RetryLimit)
	}
	if opt.AgeLimit > 0 {
		params.AgeLimitSec = proto.Int64(int64(opt.AgeLimit.Seconds()))
	}
	if opt.MinBackoff > 0 {
		params.MinBackoffSec = proto.Float64(opt.MinBackoff.Seconds())
	}
	if opt.MaxBackoff > 0 {
		params.MaxBackoffSec = proto.Float64(opt.MaxBackoff.Seconds())
	}
	if opt.MaxDoublings > 0 || (opt.MaxDoublings == 0 && opt.ApplyZeroMaxDoublings) {
		params.MaxDoublings = proto.Int32(opt.MaxDoublings)
	}
	return params
}
func NewPOSTTask(path string, params url.Values) *Task {
	h := make(http.Header)
	h.Set("Content-Type", "application/x-www-form-urlencoded")
	return &Task{
		Path:    path,
		Payload: []byte(params.Encode()),
		Header:  h,
		Method:  "POST",
	}
}
func ParseRequestHeaders(h http.Header) *RequestHeaders {
	ret := &RequestHeaders{
		QueueName: h.Get("X-AppEngine-QueueName"),
		TaskName:  h.Get("X-AppEngine-TaskName"),
	}

	ret.TaskRetryCount, _ = strconv.ParseInt(h.Get("X-AppEngine-TaskRetryCount"), 10, 64)
	ret.TaskExecutionCount, _ = strconv.ParseInt(h.Get("X-AppEngine-TaskExecutionCount"), 10, 64)

	etaSecs, _ := strconv.ParseInt(h.Get("X-AppEngine-TaskETA"), 10, 64)
	if etaSecs != 0 {
		ret.TaskETA = time.Unix(etaSecs, 0)
	}

	ret.TaskPreviousResponse, _ = strconv.Atoi(h.Get("X-AppEngine-TaskPreviousResponse"))
	ret.TaskRetryReason = h.Get("X-AppEngine-TaskRetryReason")
	if h.Get("X-AppEngine-FailFast") != "" {
		ret.FailFast = true
	}

	return ret
}
func Add(c context.Context, task *Task, queueName string) (*Task, error) {
	req, err := newAddReq(c, task, queueName)
	if err != nil {
		return nil, err
	}
	res := &pb.TaskQueueAddResponse{}
	if err := internal.Call(c, "taskqueue", "Add", req, res); err != nil {
		apiErr, ok := err.(*internal.APIError)
		if ok && alreadyAddedErrors[pb.TaskQueueServiceError_ErrorCode(apiErr.Code)] {
			return nil, ErrTaskAlreadyAdded
		}
		return nil, err
	}
	resultTask := *task
	resultTask.Method = task.method()
	if task.Name == "" {
		resultTask.Name = string(res.ChosenTaskName)
	}
	return &resultTask, nil
}
func AddMulti(c context.Context, tasks []*Task, queueName string) ([]*Task, error) {
	req := &pb.TaskQueueBulkAddRequest{
		AddRequest: make([]*pb.TaskQueueAddRequest, len(tasks)),
	}
	me, any := make(appengine.MultiError, len(tasks)), false
	for i, t := range tasks {
		req.AddRequest[i], me[i] = newAddReq(c, t, queueName)
		any = any || me[i] != nil
	}
	if any {
		return nil, me
	}
	res := &pb.TaskQueueBulkAddResponse{}
	if err := internal.Call(c, "taskqueue", "BulkAdd", req, res); err != nil {
		return nil, err
	}
	if len(res.Taskresult) != len(tasks) {
		return nil, errors.New("taskqueue: server error")
	}
	tasksOut := make([]*Task, len(tasks))
	for i, tr := range res.Taskresult {
		tasksOut[i] = new(Task)
		*tasksOut[i] = *tasks[i]
		tasksOut[i].Method = tasksOut[i].method()
		if tasksOut[i].Name == "" {
			tasksOut[i].Name = string(tr.ChosenTaskName)
		}
		if *tr.Result != pb.TaskQueueServiceError_OK {
			if alreadyAddedErrors[*tr.Result] {
				me[i] = ErrTaskAlreadyAdded
			} else {
				me[i] = &internal.APIError{
					Service: "taskqueue",
					Code:    int32(*tr.Result),
				}
			}
			any = true
		}
	}
	if any {
		return tasksOut, me
	}
	return tasksOut, nil
}
func Delete(c context.Context, task *Task, queueName string) error {
	err := DeleteMulti(c, []*Task{task}, queueName)
	if me, ok := err.(appengine.MultiError); ok {
		return me[0]
	}
	return err
}
func DeleteMulti(c context.Context, tasks []*Task, queueName string) error {
	taskNames := make([][]byte, len(tasks))
	for i, t := range tasks {
		taskNames[i] = []byte(t.Name)
	}
	if queueName == "" {
		queueName = "default"
	}
	req := &pb.TaskQueueDeleteRequest{
		QueueName: []byte(queueName),
		TaskName:  taskNames,
	}
	res := &pb.TaskQueueDeleteResponse{}
	if err := internal.Call(c, "taskqueue", "Delete", req, res); err != nil {
		return err
	}
	if a, b := len(req.TaskName), len(res.Result); a != b {
		return fmt.Errorf("taskqueue: internal error: requested deletion of %d tasks, got %d results", a, b)
	}
	me, any := make(appengine.MultiError, len(res.Result)), false
	for i, ec := range res.Result {
		if ec != pb.TaskQueueServiceError_OK {
			me[i] = &internal.APIError{
				Service: "taskqueue",
				Code:    int32(ec),
			}
			any = true
		}
	}
	if any {
		return me
	}
	return nil
}
func Lease(c context.Context, maxTasks int, queueName string, leaseTime int) ([]*Task, error) {
	return lease(c, maxTasks, queueName, leaseTime, false, nil)
}
func LeaseByTag(c context.Context, maxTasks int, queueName string, leaseTime int, tag string) ([]*Task, error) {
	return lease(c, maxTasks, queueName, leaseTime, true, []byte(tag))
}
func Purge(c context.Context, queueName string) error {
	if queueName == "" {
		queueName = "default"
	}
	req := &pb.TaskQueuePurgeQueueRequest{
		QueueName: []byte(queueName),
	}
	res := &pb.TaskQueuePurgeQueueResponse{}
	return internal.Call(c, "taskqueue", "PurgeQueue", req, res)
}
func ModifyLease(c context.Context, task *Task, queueName string, leaseTime int) error {
	if queueName == "" {
		queueName = "default"
	}
	req := &pb.TaskQueueModifyTaskLeaseRequest{
		QueueName:    []byte(queueName),
		TaskName:     []byte(task.Name),
		EtaUsec:      proto.Int64(task.ETA.UnixNano() / 1e3), // Used to verify ownership.
		LeaseSeconds: proto.Float64(float64(leaseTime)),
	}
	res := &pb.TaskQueueModifyTaskLeaseResponse{}
	if err := internal.Call(c, "taskqueue", "ModifyTaskLease", req, res); err != nil {
		return err
	}
	task.ETA = time.Unix(0, *res.UpdatedEtaUsec*1e3)
	return nil
}
func QueueStats(c context.Context, queueNames []string) ([]QueueStatistics, error) {
	req := &pb.TaskQueueFetchQueueStatsRequest{
		QueueName: make([][]byte, len(queueNames)),
	}
	for i, q := range queueNames {
		if q == "" {
			q = "default"
		}
		req.QueueName[i] = []byte(q)
	}
	res := &pb.TaskQueueFetchQueueStatsResponse{}
	if err := internal.Call(c, "taskqueue", "FetchQueueStats", req, res); err != nil {
		return nil, err
	}
	qs := make([]QueueStatistics, len(res.Queuestats))
	for i, qsg := range res.Queuestats {
		qs[i] = QueueStatistics{
			Tasks: int(*qsg.NumTasks),
		}
		if eta := *qsg.OldestEtaUsec; eta > -1 {
			qs[i].OldestETA = time.Unix(0, eta*1e3)
		}
		if si := qsg.ScannerInfo; si != nil {
			qs[i].Executed1Minute = int(*si.ExecutedLastMinute)
			qs[i].InFlight = int(si.GetRequestsInFlight())
			qs[i].EnforcedRate = si.GetEnforcedRate()
		}
	}
	return qs, nil
}
func IsTimeoutError(err error) bool {
	if err == context.DeadlineExceeded {
		return true
	}
	if t, ok := err.(interface {
		IsTimeout() bool
	}); ok {
		return t.IsTimeout()
	}
	return false
}
func Func(key string, i interface{}) *Function {
	f := &Function{fv: reflect.ValueOf(i)}

	// Derive unique, somewhat stable key for this func.
	_, file, _, _ := runtime.Caller(1)
	fk, err := fileKey(file)
	if err != nil {
		// Not fatal, but log the error
		stdlog.Printf("delay: %v", err)
	}
	f.key = fk + ":" + key

	t := f.fv.Type()
	if t.Kind() != reflect.Func {
		f.err = errors.New("not a function")
		return f
	}
	if t.NumIn() == 0 || !isContext(t.In(0)) {
		f.err = errFirstArg
		return f
	}

	// Register the function's arguments with the gob package.
	// This is required because they are marshaled inside a []interface{}.
	// gob.Register only expects to be called during initialization;
	// that's fine because this function expects the same.
	for i := 0; i < t.NumIn(); i++ {
		// Only concrete types may be registered. If the argument has
		// interface type, the client is resposible for registering the
		// concrete types it will hold.
		if t.In(i).Kind() == reflect.Interface {
			continue
		}
		gob.Register(reflect.Zero(t.In(i)).Interface())
	}

	if old := funcs[f.key]; old != nil {
		old.err = fmt.Errorf("multiple functions registered for %s in %s", key, file)
	}
	funcs[f.key] = f
	return f
}
func (f *Function) Task(args ...interface{}) (*taskqueue.Task, error) {
	if f.err != nil {
		return nil, fmt.Errorf("delay: func is invalid: %v", f.err)
	}

	nArgs := len(args) + 1 // +1 for the context.Context
	ft := f.fv.Type()
	minArgs := ft.NumIn()
	if ft.IsVariadic() {
		minArgs--
	}
	if nArgs < minArgs {
		return nil, fmt.Errorf("delay: too few arguments to func: %d < %d", nArgs, minArgs)
	}
	if !ft.IsVariadic() && nArgs > minArgs {
		return nil, fmt.Errorf("delay: too many arguments to func: %d > %d", nArgs, minArgs)
	}

	// Check arg types.
	for i := 1; i < nArgs; i++ {
		at := reflect.TypeOf(args[i-1])
		var dt reflect.Type
		if i < minArgs {
			// not a variadic arg
			dt = ft.In(i)
		} else {
			// a variadic arg
			dt = ft.In(minArgs).Elem()
		}
		// nil arguments won't have a type, so they need special handling.
		if at == nil {
			// nil interface
			switch dt.Kind() {
			case reflect.Chan, reflect.Func, reflect.Interface, reflect.Map, reflect.Ptr, reflect.Slice:
				continue // may be nil
			}
			return nil, fmt.Errorf("delay: argument %d has wrong type: %v is not nilable", i, dt)
		}
		switch at.Kind() {
		case reflect.Chan, reflect.Func, reflect.Interface, reflect.Map, reflect.Ptr, reflect.Slice:
			av := reflect.ValueOf(args[i-1])
			if av.IsNil() {
				// nil value in interface; not supported by gob, so we replace it
				// with a nil interface value
				args[i-1] = nil
			}
		}
		if !at.AssignableTo(dt) {
			return nil, fmt.Errorf("delay: argument %d has wrong type: %v is not assignable to %v", i, at, dt)
		}
	}

	inv := invocation{
		Key:  f.key,
		Args: args,
	}

	buf := new(bytes.Buffer)
	if err := gob.NewEncoder(buf).Encode(inv); err != nil {
		return nil, fmt.Errorf("delay: gob encoding failed: %v", err)
	}

	return &taskqueue.Task{
		Path:    path,
		Payload: buf.Bytes(),
	}, nil
}
func RequestHeaders(c context.Context) (*taskqueue.RequestHeaders, error) {
	if ret, ok := c.Value(headersContextKey).(*taskqueue.RequestHeaders); ok {
		return ret, nil
	}
	return nil, errOutsideDelayFunc
}
func WithContext(parent context.Context, req *http.Request) context.Context {
	return internal.WithContext(parent, req)
}
func WithAPICallFunc(ctx context.Context, f APICallFunc) context.Context {
	return internal.WithCallOverride(ctx, internal.CallOverrideFunc(f))
}
func APICall(ctx context.Context, service, method string, in, out proto.Message) error {
	return internal.Call(ctx, service, method, in, out)
}
func ModuleHostname(c context.Context, module, version, instance string) (string, error) {
	req := &modpb.GetHostnameRequest{}
	if module != "" {
		req.Module = &module
	}
	if version != "" {
		req.Version = &version
	}
	if instance != "" {
		req.Instance = &instance
	}
	res := &modpb.GetHostnameResponse{}
	if err := internal.Call(c, "modules", "GetHostname", req, res); err != nil {
		return "", err
	}
	return *res.Hostname, nil
}
func AccessToken(c context.Context, scopes ...string) (token string, expiry time.Time, err error) {
	req := &pb.GetAccessTokenRequest{Scope: scopes}
	res := &pb.GetAccessTokenResponse{}

	err = internal.Call(c, "app_identity_service", "GetAccessToken", req, res)
	if err != nil {
		return "", time.Time{}, err
	}
	return res.GetAccessToken(), time.Unix(res.GetExpirationTime(), 0), nil
}
func PublicCertificates(c context.Context) ([]Certificate, error) {
	req := &pb.GetPublicCertificateForAppRequest{}
	res := &pb.GetPublicCertificateForAppResponse{}
	if err := internal.Call(c, "app_identity_service", "GetPublicCertificatesForApp", req, res); err != nil {
		return nil, err
	}
	var cs []Certificate
	for _, pc := range res.PublicCertificateList {
		cs = append(cs, Certificate{
			KeyName: pc.GetKeyName(),
			Data:    []byte(pc.GetX509CertificatePem()),
		})
	}
	return cs, nil
}
func ServiceAccount(c context.Context) (string, error) {
	req := &pb.GetServiceAccountNameRequest{}
	res := &pb.GetServiceAccountNameResponse{}

	err := internal.Call(c, "app_identity_service", "GetServiceAccountName", req, res)
	if err != nil {
		return "", err
	}
	return res.GetServiceAccountName(), err
}
func SignBytes(c context.Context, bytes []byte) (keyName string, signature []byte, err error) {
	req := &pb.SignForAppRequest{BytesToSign: bytes}
	res := &pb.SignForAppResponse{}

	if err := internal.Call(c, "app_identity_service", "SignForApp", req, res); err != nil {
		return "", nil, err
	}
	return res.GetKeyName(), res.GetSignatureBytes(), nil
}
func (r *reader) fetch(off int64) error {
	req := &blobpb.FetchDataRequest{
		BlobKey:    proto.String(string(r.blobKey)),
		StartIndex: proto.Int64(off),
		EndIndex:   proto.Int64(off + readBufferSize - 1), // EndIndex is inclusive.
	}
	res := &blobpb.FetchDataResponse{}
	if err := internal.Call(r.c, "blobstore", "FetchData", req, res); err != nil {
		return err
	}
	if len(res.Data) == 0 {
		return io.EOF
	}
	r.buf, r.r, r.off = res.Data, 0, off
	return nil
}
func (r *reader) seek(off int64) (int64, error) {
	delta := off - r.off
	if delta >= 0 && delta < int64(len(r.buf)) {
		r.r = int(delta)
		return off, nil
	}
	r.buf, r.r, r.off = nil, 0, off
	return off, nil
}
func multiKeyToProto(appID string, key []*Key) []*pb.Reference {
	ret := make([]*pb.Reference, len(key))
	for i, k := range key {
		ret[i] = keyToProto(appID, k)
	}
	return ret
}
func referenceValueToKey(r *pb.PropertyValue_ReferenceValue) (k *Key, err error) {
	appID := r.GetApp()
	namespace := r.GetNameSpace()
	for _, e := range r.Pathelement {
		k = &Key{
			kind:      e.GetType(),
			stringID:  e.GetName(),
			intID:     e.GetId(),
			parent:    k,
			appID:     appID,
			namespace: namespace,
		}
		if !k.valid() {
			return nil, ErrInvalidKey
		}
	}
	return
}
func keyToReferenceValue(defaultAppID string, k *Key) *pb.PropertyValue_ReferenceValue {
	ref := keyToProto(defaultAppID, k)
	pe := make([]*pb.PropertyValue_ReferenceValue_PathElement, len(ref.Path.Element))
	for i, e := range ref.Path.Element {
		pe[i] = &pb.PropertyValue_ReferenceValue_PathElement{
			Type: e.Type,
			Id:   e.Id,
			Name: e.Name,
		}
	}
	return &pb.PropertyValue_ReferenceValue{
		App:         ref.App,
		NameSpace:   ref.NameSpace,
		Pathelement: pe,
	}
}
func Put(c context.Context, key *Key, src interface{}) (*Key, error) {
	k, err := PutMulti(c, []*Key{key}, []interface{}{src})
	if err != nil {
		if me, ok := err.(appengine.MultiError); ok {
			return nil, me[0]
		}
		return nil, err
	}
	return k[0], nil
}
func PutMulti(c context.Context, key []*Key, src interface{}) ([]*Key, error) {
	v := reflect.ValueOf(src)
	multiArgType, _ := checkMultiArg(v)
	if multiArgType == multiArgTypeInvalid {
		return nil, errors.New("datastore: src has invalid type")
	}
	if len(key) != v.Len() {
		return nil, errors.New("datastore: key and src slices have different length")
	}
	if len(key) == 0 {
		return nil, nil
	}
	appID := internal.FullyQualifiedAppID(c)
	if err := multiValid(key); err != nil {
		return nil, err
	}
	req := &pb.PutRequest{}
	for i := range key {
		elem := v.Index(i)
		if multiArgType == multiArgTypePropertyLoadSaver || multiArgType == multiArgTypeStruct {
			elem = elem.Addr()
		}
		sProto, err := saveEntity(appID, key[i], elem.Interface())
		if err != nil {
			return nil, err
		}
		req.Entity = append(req.Entity, sProto)
	}
	res := &pb.PutResponse{}
	if err := internal.Call(c, "datastore_v3", "Put", req, res); err != nil {
		return nil, err
	}
	if len(key) != len(res.Key) {
		return nil, errors.New("datastore: internal error: server returned the wrong number of keys")
	}
	ret := make([]*Key, len(key))
	for i := range ret {
		var err error
		ret[i], err = protoToKey(res.Key[i])
		if err != nil || ret[i].Incomplete() {
			return nil, errors.New("datastore: internal error: server returned an invalid key")
		}
	}
	return ret, nil
}
func Delete(c context.Context, key *Key) error {
	err := DeleteMulti(c, []*Key{key})
	if me, ok := err.(appengine.MultiError); ok {
		return me[0]
	}
	return err
}
func DeleteMulti(c context.Context, key []*Key) error {
	if len(key) == 0 {
		return nil
	}
	if err := multiValid(key); err != nil {
		return err
	}
	req := &pb.DeleteRequest{
		Key: multiKeyToProto(internal.FullyQualifiedAppID(c), key),
	}
	res := &pb.DeleteResponse{}
	return internal.Call(c, "datastore_v3", "Delete", req, res)
}
func deploy() error {
	vlogf("Running command %v", flag.Args())
	cmd := exec.Command(flag.Arg(0), flag.Args()[1:]...)
	cmd.Stdin, cmd.Stdout, cmd.Stderr = os.Stdin, os.Stdout, os.Stderr
	if err := cmd.Run(); err != nil {
		return fmt.Errorf("unable to run %q: %v", strings.Join(flag.Args(), " "), err)
	}
	return nil
}
func (qr *Result) Next() (*Record, error) {
	if qr.err != nil {
		return nil, qr.err
	}
	if len(qr.logs) > 0 {
		lr := qr.logs[0]
		qr.logs = qr.logs[1:]
		return lr, nil
	}

	if qr.request.Offset == nil && qr.resultsSeen {
		return nil, Done
	}

	if err := qr.run(); err != nil {
		// Errors here may be retried, so don't store the error.
		return nil, err
	}

	return qr.Next()
}
func protoToAppLogs(logLines []*pb.LogLine) []AppLog {
	appLogs := make([]AppLog, len(logLines))

	for i, line := range logLines {
		appLogs[i] = AppLog{
			Time:    time.Unix(0, *line.Time*1e3),
			Level:   int(*line.Level),
			Message: *line.LogMessage,
		}
	}

	return appLogs
}
func protoToRecord(rl *pb.RequestLog) *Record {
	offset, err := proto.Marshal(rl.Offset)
	if err != nil {
		offset = nil
	}
	return &Record{
		AppID:             *rl.AppId,
		ModuleID:          rl.GetModuleId(),
		VersionID:         *rl.VersionId,
		RequestID:         rl.RequestId,
		Offset:            offset,
		IP:                *rl.Ip,
		Nickname:          rl.GetNickname(),
		AppEngineRelease:  string(rl.GetAppEngineRelease()),
		StartTime:         time.Unix(0, *rl.StartTime*1e3),
		EndTime:           time.Unix(0, *rl.EndTime*1e3),
		Latency:           time.Duration(*rl.Latency) * time.Microsecond,
		MCycles:           *rl.Mcycles,
		Method:            *rl.Method,
		Resource:          *rl.Resource,
		HTTPVersion:       *rl.HttpVersion,
		Status:            *rl.Status,
		ResponseSize:      *rl.ResponseSize,
		Referrer:          rl.GetReferrer(),
		UserAgent:         rl.GetUserAgent(),
		URLMapEntry:       *rl.UrlMapEntry,
		Combined:          *rl.Combined,
		Host:              rl.GetHost(),
		Cost:              rl.GetCost(),
		TaskQueueName:     rl.GetTaskQueueName(),
		TaskName:          rl.GetTaskName(),
		WasLoadingRequest: rl.GetWasLoadingRequest(),
		PendingTime:       time.Duration(rl.GetPendingTime()) * time.Microsecond,
		Finished:          rl.GetFinished(),
		AppLogs:           protoToAppLogs(rl.Line),
		InstanceID:        string(rl.GetCloneKey()),
	}
}
func (params *Query) Run(c context.Context) *Result {
	req, err := makeRequest(params, internal.FullyQualifiedAppID(c), appengine.VersionID(c))
	return &Result{
		context: c,
		request: req,
		err:     err,
	}
}
func (r *Result) run() error {
	res := &pb.LogReadResponse{}
	if err := internal.Call(r.context, "logservice", "Read", r.request, res); err != nil {
		return err
	}

	r.logs = make([]*Record, len(res.Log))
	r.request.Offset = res.Offset
	r.resultsSeen = true

	for i, log := range res.Log {
		r.logs[i] = protoToRecord(log)
	}

	return nil
}
func Current(c context.Context) *User {
	h := internal.IncomingHeaders(c)
	u := &User{
		Email:             h.Get("X-AppEngine-User-Email"),
		AuthDomain:        h.Get("X-AppEngine-Auth-Domain"),
		ID:                h.Get("X-AppEngine-User-Id"),
		Admin:             h.Get("X-AppEngine-User-Is-Admin") == "1",
		FederatedIdentity: h.Get("X-AppEngine-Federated-Identity"),
		FederatedProvider: h.Get("X-AppEngine-Federated-Provider"),
	}
	if u.Email == "" && u.FederatedIdentity == "" {
		return nil
	}
	return u
}
func IsAdmin(c context.Context) bool {
	h := internal.IncomingHeaders(c)
	return h.Get("X-AppEngine-User-Is-Admin") == "1"
}
func isErrFieldMismatch(err error) bool {
	_, ok := err.(*datastore.ErrFieldMismatch)
	return ok
}
func Stat(c context.Context, blobKey appengine.BlobKey) (*BlobInfo, error) {
	c, _ = appengine.Namespace(c, "") // Blobstore is always in the empty string namespace
	dskey := datastore.NewKey(c, blobInfoKind, string(blobKey), 0, nil)
	bi := &BlobInfo{
		BlobKey: blobKey,
	}
	if err := datastore.Get(c, dskey, bi); err != nil && !isErrFieldMismatch(err) {
		return nil, err
	}
	return bi, nil
}
func Send(response http.ResponseWriter, blobKey appengine.BlobKey) {
	hdr := response.Header()
	hdr.Set("X-AppEngine-BlobKey", string(blobKey))

	if hdr.Get("Content-Type") == "" {
		// This value is known to dev_appserver to mean automatic.
		// In production this is remapped to the empty value which
		// means automatic.
		hdr.Set("Content-Type", "application/vnd.google.appengine.auto")
	}
}
func UploadURL(c context.Context, successPath string, opts *UploadURLOptions) (*url.URL, error) {
	req := &blobpb.CreateUploadURLRequest{
		SuccessPath: proto.String(successPath),
	}
	if opts != nil {
		if n := opts.MaxUploadBytes; n != 0 {
			req.MaxUploadSizeBytes = &n
		}
		if n := opts.MaxUploadBytesPerBlob; n != 0 {
			req.MaxUploadSizePerBlobBytes = &n
		}
		if s := opts.StorageBucket; s != "" {
			req.GsBucketName = &s
		}
	}
	res := &blobpb.CreateUploadURLResponse{}
	if err := internal.Call(c, "blobstore", "CreateUploadURL", req, res); err != nil {
		return nil, err
	}
	return url.Parse(*res.Url)
}
func Delete(c context.Context, blobKey appengine.BlobKey) error {
	return DeleteMulti(c, []appengine.BlobKey{blobKey})
}
func DeleteMulti(c context.Context, blobKey []appengine.BlobKey) error {
	s := make([]string, len(blobKey))
	for i, b := range blobKey {
		s[i] = string(b)
	}
	req := &blobpb.DeleteBlobRequest{
		BlobKey: s,
	}
	res := &basepb.VoidProto{}
	if err := internal.Call(c, "blobstore", "DeleteBlob", req, res); err != nil {
		return err
	}
	return nil
}
func NewReader(c context.Context, blobKey appengine.BlobKey) Reader {
	return openBlob(c, blobKey)
}
func Handle(f func(c context.Context, m *Message)) {
	http.HandleFunc("/_ah/xmpp/message/chat/", func(_ http.ResponseWriter, r *http.Request) {
		f(appengine.NewContext(r), &Message{
			Sender: r.FormValue("from"),
			To:     []string{r.FormValue("to")},
			Body:   r.FormValue("body"),
		})
	})
}
func (m *Message) Send(c context.Context) error {
	req := &pb.XmppMessageRequest{
		Jid:    m.To,
		Body:   &m.Body,
		RawXml: &m.RawXML,
	}
	if m.Type != "" && m.Type != "chat" {
		req.Type = &m.Type
	}
	if m.Sender != "" {
		req.FromJid = &m.Sender
	}
	res := &pb.XmppMessageResponse{}
	if err := internal.Call(c, "xmpp", "SendMessage", req, res); err != nil {
		return err
	}

	if len(res.Status) != len(req.Jid) {
		return fmt.Errorf("xmpp: sent message to %d JIDs, but only got %d statuses back", len(req.Jid), len(res.Status))
	}
	me, any := make(appengine.MultiError, len(req.Jid)), false
	for i, st := range res.Status {
		if st != pb.XmppMessageResponse_NO_ERROR {
			me[i] = errors.New(st.String())
			any = true
		}
	}
	if any {
		return me
	}
	return nil
}
func Invite(c context.Context, to, from string) error {
	req := &pb.XmppInviteRequest{
		Jid: &to,
	}
	if from != "" {
		req.FromJid = &from
	}
	res := &pb.XmppInviteResponse{}
	return internal.Call(c, "xmpp", "SendInvite", req, res)
}
func (p *Presence) Send(c context.Context) error {
	req := &pb.XmppSendPresenceRequest{
		Jid: &p.To,
	}
	if p.State != "" {
		req.Show = &p.State
	}
	if p.Type != "" {
		req.Type = &p.Type
	}
	if p.Sender != "" {
		req.FromJid = &p.Sender
	}
	if p.Status != "" {
		req.Status = &p.Status
	}
	res := &pb.XmppSendPresenceResponse{}
	return internal.Call(c, "xmpp", "SendPresence", req, res)
}
func GetPresence(c context.Context, to string, from string) (string, error) {
	req := &pb.PresenceRequest{
		Jid: &to,
	}
	if from != "" {
		req.FromJid = &from
	}
	res := &pb.PresenceResponse{}
	if err := internal.Call(c, "xmpp", "GetPresence", req, res); err != nil {
		return "", err
	}
	if !*res.IsAvailable || res.Presence == nil {
		return "", ErrPresenceUnavailable
	}
	presence, ok := presenceMap[*res.Presence]
	if ok {
		return presence, nil
	}
	return "", fmt.Errorf("xmpp: unknown presence %v", *res.Presence)
}
func GetPresenceMulti(c context.Context, to []string, from string) ([]string, error) {
	req := &pb.BulkPresenceRequest{
		Jid: to,
	}
	if from != "" {
		req.FromJid = &from
	}
	res := &pb.BulkPresenceResponse{}

	if err := internal.Call(c, "xmpp", "BulkGetPresence", req, res); err != nil {
		return nil, err
	}

	presences := make([]string, 0, len(res.PresenceResponse))
	errs := appengine.MultiError{}

	addResult := func(presence string, err error) {
		presences = append(presences, presence)
		errs = append(errs, err)
	}

	anyErr := false
	for _, subres := range res.PresenceResponse {
		if !subres.GetValid() {
			anyErr = true
			addResult("", ErrInvalidJID)
			continue
		}
		if !*subres.IsAvailable || subres.Presence == nil {
			anyErr = true
			addResult("", ErrPresenceUnavailable)
			continue
		}
		presence, ok := presenceMap[*subres.Presence]
		if ok {
			addResult(presence, nil)
		} else {
			anyErr = true
			addResult("", fmt.Errorf("xmpp: unknown presence %q", *subres.Presence))
		}
	}
	if anyErr {
		return presences, errs
	}
	return presences, nil
}
func newStructFLS(p interface{}) (FieldLoadSaver, error) {
	v := reflect.ValueOf(p)
	if v.Kind() != reflect.Ptr || v.IsNil() || v.Elem().Kind() != reflect.Struct {
		return nil, ErrInvalidDocumentType
	}
	codec, err := loadCodec(v.Elem().Type())
	if err != nil {
		return nil, err
	}
	return structFLS{v.Elem(), codec}, nil
}
func SaveStruct(src interface{}) ([]Field, error) {
	f, _, err := saveStructWithMeta(src)
	return f, err
}
func Namespaces(ctx context.Context) ([]string, error) {
	// TODO(djd): Support range queries.
	q := NewQuery(namespaceKind).KeysOnly()
	keys, err := q.GetAll(ctx, nil)
	if err != nil {
		return nil, err
	}
	// The empty namespace key uses a numeric ID (==1), but luckily
	// the string ID defaults to "" for numeric IDs anyway.
	return keyNames(keys), nil
}
func Kinds(ctx context.Context) ([]string, error) {
	// TODO(djd): Support range queries.
	q := NewQuery(kindKind).KeysOnly()
	keys, err := q.GetAll(ctx, nil)
	if err != nil {
		return nil, err
	}
	return keyNames(keys), nil
}
func RunInTransaction(c context.Context, f func(tc context.Context) error, opts *TransactionOptions) error {
	xg := false
	if opts != nil {
		xg = opts.XG
	}
	readOnly := false
	if opts != nil {
		readOnly = opts.ReadOnly
	}
	attempts := 3
	if opts != nil && opts.Attempts > 0 {
		attempts = opts.Attempts
	}
	var t *pb.Transaction
	var err error
	for i := 0; i < attempts; i++ {
		if t, err = internal.RunTransactionOnce(c, f, xg, readOnly, t); err != internal.ErrConcurrentTransaction {
			return err
		}
	}
	return ErrConcurrentTransaction
}
func imports(f *ast.File, path string) bool {
	return importSpec(f, path) != nil
}
func importSpec(f *ast.File, path string) *ast.ImportSpec {
	for _, s := range f.Imports {
		if importPath(s) == path {
			return s
		}
	}
	return nil
}
func declImports(gen *ast.GenDecl, path string) bool {
	if gen.Tok != token.IMPORT {
		return false
	}
	for _, spec := range gen.Specs {
		impspec := spec.(*ast.ImportSpec)
		if importPath(impspec) == path {
			return true
		}
	}
	return false
}
func isPkgDot(t ast.Expr, pkg, name string) bool {
	sel, ok := t.(*ast.SelectorExpr)
	return ok && isTopName(sel.X, pkg) && sel.Sel.String() == name
}
func isTopName(n ast.Expr, name string) bool {
	id, ok := n.(*ast.Ident)
	return ok && id.Name == name && id.Obj == nil
}
func isName(n ast.Expr, name string) bool {
	id, ok := n.(*ast.Ident)
	return ok && id.String() == name
}
func isCall(t ast.Expr, pkg, name string) bool {
	call, ok := t.(*ast.CallExpr)
	return ok && isPkgDot(call.Fun, pkg, name)
}
func refersTo(n ast.Node, x *ast.Ident) bool {
	id, ok := n.(*ast.Ident)
	// The test of id.Name == x.Name handles top-level unresolved
	// identifiers, which all have Obj == nil.
	return ok && id.Obj == x.Obj && id.Name == x.Name
}
func isEmptyString(n ast.Expr) bool {
	lit, ok := n.(*ast.BasicLit)
	return ok && lit.Kind == token.STRING && len(lit.Value) == 2
}
func countUses(x *ast.Ident, scope []ast.Stmt) int {
	count := 0
	ff := func(n interface{}) {
		if n, ok := n.(ast.Node); ok && refersTo(n, x) {
			count++
		}
	}
	for _, n := range scope {
		walk(n, ff)
	}
	return count
}
func assignsTo(x *ast.Ident, scope []ast.Stmt) bool {
	assigned := false
	ff := func(n interface{}) {
		if assigned {
			return
		}
		switch n := n.(type) {
		case *ast.UnaryExpr:
			// use of &x
			if n.Op == token.AND && refersTo(n.X, x) {
				assigned = true
				return
			}
		case *ast.AssignStmt:
			for _, l := range n.Lhs {
				if refersTo(l, x) {
					assigned = true
					return
				}
			}
		}
	}
	for _, n := range scope {
		if assigned {
			break
		}
		walk(n, ff)
	}
	return assigned
}
func newPkgDot(pos token.Pos, pkg, name string) ast.Expr {
	return &ast.SelectorExpr{
		X: &ast.Ident{
			NamePos: pos,
			Name:    pkg,
		},
		Sel: &ast.Ident{
			NamePos: pos,
			Name:    name,
		},
	}
}
func renameTop(f *ast.File, old, new string) bool {
	var fixed bool

	// Rename any conflicting imports
	// (assuming package name is last element of path).
	for _, s := range f.Imports {
		if s.Name != nil {
			if s.Name.Name == old {
				s.Name.Name = new
				fixed = true
			}
		} else {
			_, thisName := path.Split(importPath(s))
			if thisName == old {
				s.Name = ast.NewIdent(new)
				fixed = true
			}
		}
	}

	// Rename any top-level declarations.
	for _, d := range f.Decls {
		switch d := d.(type) {
		case *ast.FuncDecl:
			if d.Recv == nil && d.Name.Name == old {
				d.Name.Name = new
				d.Name.Obj.Name = new
				fixed = true
			}
		case *ast.GenDecl:
			for _, s := range d.Specs {
				switch s := s.(type) {
				case *ast.TypeSpec:
					if s.Name.Name == old {
						s.Name.Name = new
						s.Name.Obj.Name = new
						fixed = true
					}
				case *ast.ValueSpec:
					for _, n := range s.Names {
						if n.Name == old {
							n.Name = new
							n.Obj.Name = new
							fixed = true
						}
					}
				}
			}
		}
	}

	// Rename top-level old to new, both unresolved names
	// (probably defined in another file) and names that resolve
	// to a declaration we renamed.
	walk(f, func(n interface{}) {
		id, ok := n.(*ast.Ident)
		if ok && isTopName(id, old) {
			id.Name = new
			fixed = true
		}
		if ok && id.Obj != nil && id.Name == old && id.Obj.Name == new {
			id.Name = id.Obj.Name
			fixed = true
		}
	})

	return fixed
}
func matchLen(x, y string) int {
	i := 0
	for i < len(x) && i < len(y) && x[i] == y[i] {
		i++
	}
	return i
}
func deleteImport(f *ast.File, path string) (deleted bool) {
	oldImport := importSpec(f, path)

	// Find the import node that imports path, if any.
	for i, decl := range f.Decls {
		gen, ok := decl.(*ast.GenDecl)
		if !ok || gen.Tok != token.IMPORT {
			continue
		}
		for j, spec := range gen.Specs {
			impspec := spec.(*ast.ImportSpec)
			if oldImport != impspec {
				continue
			}

			// We found an import spec that imports path.
			// Delete it.
			deleted = true
			copy(gen.Specs[j:], gen.Specs[j+1:])
			gen.Specs = gen.Specs[:len(gen.Specs)-1]

			// If this was the last import spec in this decl,
			// delete the decl, too.
			if len(gen.Specs) == 0 {
				copy(f.Decls[i:], f.Decls[i+1:])
				f.Decls = f.Decls[:len(f.Decls)-1]
			} else if len(gen.Specs) == 1 {
				gen.Lparen = token.NoPos // drop parens
			}
			if j > 0 {
				// We deleted an entry but now there will be
				// a blank line-sized hole where the import was.
				// Close the hole by making the previous
				// import appear to "end" where this one did.
				gen.Specs[j-1].(*ast.ImportSpec).EndPos = impspec.End()
			}
			break
		}
	}

	// Delete it from f.Imports.
	for i, imp := range f.Imports {
		if imp == oldImport {
			copy(f.Imports[i:], f.Imports[i+1:])
			f.Imports = f.Imports[:len(f.Imports)-1]
			break
		}
	}

	return
}
func rewriteImport(f *ast.File, oldPath, newPath string) (rewrote bool) {
	for _, imp := range f.Imports {
		if importPath(imp) == oldPath {
			rewrote = true
			// record old End, because the default is to compute
			// it using the length of imp.Path.Value.
			imp.EndPos = imp.End()
			imp.Path.Value = strconv.Quote(newPath)
		}
	}
	return
}
func DefaultTicket() string {
	defaultTicketOnce.Do(func() {
		if IsDevAppServer() {
			defaultTicket = "testapp" + defaultTicketSuffix
			return
		}
		appID := partitionlessAppID()
		escAppID := strings.Replace(strings.Replace(appID, ":", "_", -1), ".", "_", -1)
		majVersion := VersionID(nil)
		if i := strings.Index(majVersion, "."); i > 0 {
			majVersion = majVersion[:i]
		}
		defaultTicket = fmt.Sprintf("%s/%s.%s.%s", escAppID, ModuleName(nil), majVersion, InstanceID())
	})
	return defaultTicket
}
func (c *context) flushLog(force bool) (flushed bool) {
	c.pendingLogs.Lock()
	// Grab up to 30 MB. We can get away with up to 32 MB, but let's be cautious.
	n, rem := 0, 30<<20
	for ; n < len(c.pendingLogs.lines); n++ {
		ll := c.pendingLogs.lines[n]
		// Each log line will require about 3 bytes of overhead.
		nb := proto.Size(ll) + 3
		if nb > rem {
			break
		}
		rem -= nb
	}
	lines := c.pendingLogs.lines[:n]
	c.pendingLogs.lines = c.pendingLogs.lines[n:]
	c.pendingLogs.Unlock()

	if len(lines) == 0 && !force {
		// Nothing to flush.
		return false
	}

	rescueLogs := false
	defer func() {
		if rescueLogs {
			c.pendingLogs.Lock()
			c.pendingLogs.lines = append(lines, c.pendingLogs.lines...)
			c.pendingLogs.Unlock()
		}
	}()

	buf, err := proto.Marshal(&logpb.UserAppLogGroup{
		LogLine: lines,
	})
	if err != nil {
		log.Printf("internal.flushLog: marshaling UserAppLogGroup: %v", err)
		rescueLogs = true
		return false
	}

	req := &logpb.FlushRequest{
		Logs: buf,
	}
	res := &basepb.VoidProto{}
	c.pendingLogs.Lock()
	c.pendingLogs.flushes++
	c.pendingLogs.Unlock()
	if err := Call(toContext(c), "logservice", "Flush", req, res); err != nil {
		log.Printf("internal.flushLog: Flush RPC: %v", err)
		rescueLogs = true
		return false
	}
	return true
}
func withDeadline(parent context.Context, deadline time.Time) (context.Context, context.CancelFunc) {
	if deadline.IsZero() {
		return parent, func() {}
	}
	return context.WithDeadline(parent, deadline)
}
func (cn *Conn) KeepAlive() error {
	req := &pb.GetSocketNameRequest{
		SocketDescriptor: &cn.desc,
	}
	res := &pb.GetSocketNameReply{}
	return internal.Call(cn.ctx, "remote_socket", "GetSocketName", req, res)
}
func applyTransaction(pb proto.Message, t *pb.Transaction) {
	v := reflect.ValueOf(pb)
	if f, ok := transactionSetters[v.Type()]; ok {
		f.Call([]reflect.Value{v, reflect.ValueOf(t)})
	}
}
func analyze(tags []string) (*app, error) {
	ctxt := buildContext(tags)
	hasMain, appFiles, err := checkMain(ctxt)
	if err != nil {
		return nil, err
	}
	gopath := filepath.SplitList(ctxt.GOPATH)
	im, err := imports(ctxt, *rootDir, gopath)
	return &app{
		hasMain:  hasMain,
		appFiles: appFiles,
		imports:  im,
	}, err
}
func buildContext(tags []string) *build.Context {
	return &build.Context{
		GOARCH:    build.Default.GOARCH,
		GOOS:      build.Default.GOOS,
		GOROOT:    build.Default.GOROOT,
		GOPATH:    build.Default.GOPATH,
		Compiler:  build.Default.Compiler,
		BuildTags: append(build.Default.BuildTags, tags...),
	}
}
func synthesizeMain(tw *tar.Writer, appFiles []string) error {
	appMap := make(map[string]bool)
	for _, f := range appFiles {
		appMap[f] = true
	}
	var f string
	for i := 0; i < 100; i++ {
		f = fmt.Sprintf("app_main%d.go", i)
		if !appMap[filepath.Join(*rootDir, f)] {
			break
		}
	}
	if appMap[filepath.Join(*rootDir, f)] {
		return fmt.Errorf("unable to find unique name for %v", f)
	}
	hdr := &tar.Header{
		Name: f,
		Mode: 0644,
		Size: int64(len(newMain)),
	}
	if err := tw.WriteHeader(hdr); err != nil {
		return fmt.Errorf("unable to write header for %v: %v", f, err)
	}
	if _, err := tw.Write([]byte(newMain)); err != nil {
		return fmt.Errorf("unable to write %v to tar file: %v", f, err)
	}
	return nil
}
func findInGopath(dir string, gopath []string) (string, error) {
	for _, v := range gopath {
		dst := filepath.Join(v, "src", dir)
		if _, err := os.Stat(dst); err == nil {
			return dst, nil
		}
	}
	return "", fmt.Errorf("unable to find package %v in gopath %v", dir, gopath)
}
func copyTree(tw *tar.Writer, dstDir, srcDir string) error {
	entries, err := ioutil.ReadDir(srcDir)
	if err != nil {
		return fmt.Errorf("unable to read dir %v: %v", srcDir, err)
	}
	for _, entry := range entries {
		n := entry.Name()
		if skipFiles[n] {
			continue
		}
		s := filepath.Join(srcDir, n)
		d := filepath.Join(dstDir, n)
		if entry.IsDir() {
			if err := copyTree(tw, d, s); err != nil {
				return fmt.Errorf("unable to copy dir %v to %v: %v", s, d, err)
			}
			continue
		}
		if err := copyFile(tw, d, s); err != nil {
			return fmt.Errorf("unable to copy dir %v to %v: %v", s, d, err)
		}
	}
	return nil
}
func copyFile(tw *tar.Writer, dst, src string) error {
	s, err := os.Open(src)
	if err != nil {
		return fmt.Errorf("unable to open %v: %v", src, err)
	}
	defer s.Close()
	fi, err := s.Stat()
	if err != nil {
		return fmt.Errorf("unable to stat %v: %v", src, err)
	}

	hdr, err := tar.FileInfoHeader(fi, dst)
	if err != nil {
		return fmt.Errorf("unable to create tar header for %v: %v", dst, err)
	}
	hdr.Name = dst
	if err := tw.WriteHeader(hdr); err != nil {
		return fmt.Errorf("unable to write header for %v: %v", dst, err)
	}
	_, err = io.Copy(tw, s)
	if err != nil {
		return fmt.Errorf("unable to copy %v to %v: %v", src, dst, err)
	}
	return nil
}
func checkMain(ctxt *build.Context) (bool, []string, error) {
	pkg, err := ctxt.ImportDir(*rootDir, 0)
	if err != nil {
		return false, nil, fmt.Errorf("unable to analyze source: %v", err)
	}
	if !pkg.IsCommand() {
		errorf("Your app's package needs to be changed from %q to \"main\".\n", pkg.Name)
	}
	// Search for a "func main"
	var hasMain bool
	var appFiles []string
	for _, f := range pkg.GoFiles {
		n := filepath.Join(*rootDir, f)
		appFiles = append(appFiles, n)
		if hasMain, err = readFile(n); err != nil {
			return false, nil, fmt.Errorf("error parsing %q: %v", n, err)
		}
	}
	return hasMain, appFiles, nil
}
func isMain(f *ast.FuncDecl) bool {
	ft := f.Type
	return f.Name.Name == "main" && f.Recv == nil && ft.Params.NumFields() == 0 && ft.Results.NumFields() == 0
}
func readFile(filename string) (hasMain bool, err error) {
	var src []byte
	src, err = ioutil.ReadFile(filename)
	if err != nil {
		return
	}
	fset := token.NewFileSet()
	file, err := parser.ParseFile(fset, filename, src, 0)
	for _, decl := range file.Decls {
		funcDecl, ok := decl.(*ast.FuncDecl)
		if !ok {
			continue
		}
		if !isMain(funcDecl) {
			continue
		}
		hasMain = true
		break
	}
	return
}
func initField(val reflect.Value, index []int) reflect.Value {
	for _, i := range index[:len(index)-1] {
		val = val.Field(i)
		if val.Kind() == reflect.Ptr {
			if val.IsNil() {
				val.Set(reflect.New(val.Type().Elem()))
			}
			val = val.Elem()
		}
	}
	return val.Field(index[len(index)-1])
}
func loadEntity(dst interface{}, src *pb.EntityProto) (err error) {
	ent, err := protoToEntity(src)
	if err != nil {
		return err
	}
	if e, ok := dst.(PropertyLoadSaver); ok {
		return e.Load(ent.Properties)
	}
	return LoadStruct(dst, ent.Properties)
}
func validIndexNameOrDocID(s string) bool {
	if strings.HasPrefix(s, "!") {
		return false
	}
	for _, c := range s {
		if c < 0x21 || 0x7f <= c {
			return false
		}
	}
	return true
}
func Open(name string) (*Index, error) {
	if !validIndexNameOrDocID(name) {
		return nil, fmt.Errorf("search: invalid index name %q", name)
	}
	return &Index{
		spec: pb.IndexSpec{
			Name: &name,
		},
	}, nil
}
func (x *Index) Put(c context.Context, id string, src interface{}) (string, error) {
	ids, err := x.PutMulti(c, []string{id}, []interface{}{src})
	if err != nil {
		return "", err
	}
	return ids[0], nil
}
func (x *Index) Get(c context.Context, id string, dst interface{}) error {
	if id == "" || !validIndexNameOrDocID(id) {
		return fmt.Errorf("search: invalid ID %q", id)
	}
	req := &pb.ListDocumentsRequest{
		Params: &pb.ListDocumentsParams{
			IndexSpec:  &x.spec,
			StartDocId: proto.String(id),
			Limit:      proto.Int32(1),
		},
	}
	res := &pb.ListDocumentsResponse{}
	if err := internal.Call(c, "search", "ListDocuments", req, res); err != nil {
		return err
	}
	if res.Status == nil || res.Status.GetCode() != pb.SearchServiceError_OK {
		return fmt.Errorf("search: %s: %s", res.Status.GetCode(), res.Status.GetErrorDetail())
	}
	if len(res.Document) != 1 || res.Document[0].GetId() != id {
		return ErrNoSuchDocument
	}
	return loadDoc(dst, res.Document[0], nil)
}
func (x *Index) Delete(c context.Context, id string) error {
	return x.DeleteMulti(c, []string{id})
}
func (x *Index) DeleteMulti(c context.Context, ids []string) error {
	if len(ids) > maxDocumentsPerPutDelete {
		return ErrTooManyDocuments
	}

	req := &pb.DeleteDocumentRequest{
		Params: &pb.DeleteDocumentParams{
			DocId:     ids,
			IndexSpec: &x.spec,
		},
	}
	res := &pb.DeleteDocumentResponse{}
	if err := internal.Call(c, "search", "DeleteDocument", req, res); err != nil {
		return err
	}
	if len(res.Status) != len(ids) {
		return fmt.Errorf("search: internal error: wrong number of results (%d, expected %d)",
			len(res.Status), len(ids))
	}
	multiErr, hasErr := make(appengine.MultiError, len(ids)), false
	for i, s := range res.Status {
		if s.GetCode() != pb.SearchServiceError_OK {
			multiErr[i] = fmt.Errorf("search: %s: %s", s.GetCode(), s.GetErrorDetail())
			hasErr = true
		}
	}
	if hasErr {
		return multiErr
	}
	return nil
}
func (x *Index) Search(c context.Context, query string, opts *SearchOptions) *Iterator {
	t := &Iterator{
		c:           c,
		index:       x,
		searchQuery: query,
		more:        moreSearch,
	}
	if opts != nil {
		if opts.Cursor != "" {
			if opts.Offset != 0 {
				return errIter("at most one of Cursor and Offset may be specified")
			}
			t.searchCursor = proto.String(string(opts.Cursor))
		}
		t.limit = opts.Limit
		t.fields = opts.Fields
		t.idsOnly = opts.IDsOnly
		t.sort = opts.Sort
		t.exprs = opts.Expressions
		t.refinements = opts.Refinements
		t.facetOpts = opts.Facets
		t.searchOffset = opts.Offset
		t.countAccuracy = opts.CountAccuracy
	}
	return t
}
func (t *Iterator) fetchMore() {
	if t.err == nil && len(t.listRes)+len(t.searchRes) == 0 && t.more != nil {
		t.err = t.more(t)
	}
}
func (t *Iterator) Next(dst interface{}) (string, error) {
	t.fetchMore()
	if t.err != nil {
		return "", t.err
	}

	var doc *pb.Document
	var exprs []*pb.Field
	switch {
	case len(t.listRes) != 0:
		doc = t.listRes[0]
		t.listRes = t.listRes[1:]
	case len(t.searchRes) != 0:
		doc = t.searchRes[0].Document
		exprs = t.searchRes[0].Expression
		t.searchCursor = t.searchRes[0].Cursor
		t.searchRes = t.searchRes[1:]
	default:
		return "", Done
	}
	if doc == nil {
		return "", errors.New("search: internal error: no document returned")
	}
	if !t.idsOnly && dst != nil {
		if err := loadDoc(dst, doc, exprs); err != nil {
			return "", err
		}
	}
	return doc.GetId(), nil
}
func (t *Iterator) Facets() ([][]FacetResult, error) {
	t.fetchMore()
	if t.err != nil && t.err != Done {
		return nil, t.err
	}

	var facets [][]FacetResult
	for _, f := range t.facetRes {
		fres := make([]FacetResult, 0, len(f.Value))
		for _, v := range f.Value {
			ref := v.Refinement
			facet := FacetResult{
				Facet: Facet{Name: ref.GetName()},
				Count: int(v.GetCount()),
			}
			if ref.Value != nil {
				facet.Value = Atom(*ref.Value)
			} else {
				facet.Value = protoToRange(ref.Range)
			}
			fres = append(fres, facet)
		}
		facets = append(facets, fres)
	}
	return facets, nil
}
func DefaultBucketName(c context.Context) (string, error) {
	req := &aipb.GetDefaultGcsBucketNameRequest{}
	res := &aipb.GetDefaultGcsBucketNameResponse{}

	err := internal.Call(c, "app_identity_service", "GetDefaultGcsBucketName", req, res)
	if err != nil {
		return "", fmt.Errorf("file: no default bucket name returned in RPC response: %v", res)
	}
	return res.GetDefaultGcsBucketName(), nil
}
func (k *Key) valid() bool {
	if k == nil {
		return false
	}
	for ; k != nil; k = k.parent {
		if k.kind == "" || k.appID == "" {
			return false
		}
		if k.stringID != "" && k.intID != 0 {
			return false
		}
		if k.parent != nil {
			if k.parent.Incomplete() {
				return false
			}
			if k.parent.appID != k.appID || k.parent.namespace != k.namespace {
				return false
			}
		}
	}
	return true
}
func (k *Key) Equal(o *Key) bool {
	for k != nil && o != nil {
		if k.kind != o.kind || k.stringID != o.stringID || k.intID != o.intID || k.appID != o.appID || k.namespace != o.namespace {
			return false
		}
		k, o = k.parent, o.parent
	}
	return k == o
}
func (k *Key) root() *Key {
	for k.parent != nil {
		k = k.parent
	}
	return k
}
func (k *Key) marshal(b *bytes.Buffer) {
	if k.parent != nil {
		k.parent.marshal(b)
	}
	b.WriteByte('/')
	b.WriteString(k.kind)
	b.WriteByte(',')
	if k.stringID != "" {
		b.WriteString(k.stringID)
	} else {
		b.WriteString(strconv.FormatInt(k.intID, 10))
	}
}
func (k *Key) String() string {
	if k == nil {
		return ""
	}
	b := bytes.NewBuffer(make([]byte, 0, 512))
	k.marshal(b)
	return b.String()
}
func (k *Key) Encode() string {
	ref := keyToProto("", k)

	b, err := proto.Marshal(ref)
	if err != nil {
		panic(err)
	}

	// Trailing padding is stripped.
	return strings.TrimRight(base64.URLEncoding.EncodeToString(b), "=")
}
func DecodeKey(encoded string) (*Key, error) {
	// Re-add padding.
	if m := len(encoded) % 4; m != 0 {
		encoded += strings.Repeat("=", 4-m)
	}

	b, err := base64.URLEncoding.DecodeString(encoded)
	if err != nil {
		return nil, err
	}

	ref := new(pb.Reference)
	if err := proto.Unmarshal(b, ref); err != nil {
		return nil, err
	}

	return protoToKey(ref)
}
func NewIncompleteKey(c context.Context, kind string, parent *Key) *Key {
	return NewKey(c, kind, "", 0, parent)
}
func NewKey(c context.Context, kind, stringID string, intID int64, parent *Key) *Key {
	// If there's a parent key, use its namespace.
	// Otherwise, use any namespace attached to the context.
	var namespace string
	if parent != nil {
		namespace = parent.namespace
	} else {
		namespace = internal.NamespaceFromContext(c)
	}

	return &Key{
		kind:      kind,
		stringID:  stringID,
		intID:     intID,
		parent:    parent,
		appID:     internal.FullyQualifiedAppID(c),
		namespace: namespace,
	}
}
func AllocateIDs(c context.Context, kind string, parent *Key, n int) (low, high int64, err error) {
	if kind == "" {
		return 0, 0, errors.New("datastore: AllocateIDs given an empty kind")
	}
	if n < 0 {
		return 0, 0, fmt.Errorf("datastore: AllocateIDs given a negative count: %d", n)
	}
	if n == 0 {
		return 0, 0, nil
	}
	req := &pb.AllocateIdsRequest{
		ModelKey: keyToProto("", NewIncompleteKey(c, kind, parent)),
		Size:     proto.Int64(int64(n)),
	}
	res := &pb.AllocateIdsResponse{}
	if err := internal.Call(c, "datastore_v3", "AllocateIds", req, res); err != nil {
		return 0, 0, err
	}
	// The protobuf is inclusive at both ends. Idiomatic Go (e.g. slices, for loops)
	// is inclusive at the low end and exclusive at the high end, so we add 1.
	low = res.GetStart()
	high = res.GetEnd() + 1
	if low+int64(n) != high {
		return 0, 0, fmt.Errorf("datastore: internal error: could not allocate %d IDs", n)
	}
	return low, high, nil
}
func IsOverQuota(err error) bool {
	callErr, ok := err.(*internal.CallError)
	return ok && callErr.Code == 4
}
func ClassicContextFromContext(ctx netcontext.Context) (appengine.Context, error) {
	c := fromContext(ctx)
	if c == nil {
		return nil, errNotAppEngineContext
	}
	return c, nil
}
func Send(c context.Context, msg *Message) error {
	return send(c, "Send", msg)
}
func SendToAdmins(c context.Context, msg *Message) error {
	return send(c, "SendToAdmins", msg)
}
func (r *Report) Failures() int {
	count := 0

	for _, p := range r.Packages {
		for _, t := range p.Tests {
			if t.Result == FAIL {
				count++
			}
		}
	}

	return count
}
func decodeFieldNumberAndTyp3(bz []byte) (num uint32, typ Typ3, n int, err error) {

	// Read uvarint value.
	var value64 = uint64(0)
	value64, n, err = DecodeUvarint(bz)
	if err != nil {
		return
	}

	// Decode first typ3 byte.
	typ = Typ3(value64 & 0x07)

	// Decode num.
	var num64 uint64
	num64 = value64 >> 3
	if num64 > (1<<29 - 1) {
		err = fmt.Errorf("invalid field num %v", num64)
		return
	}
	num = uint32(num64)
	return
}
func checkTyp3(rt reflect.Type, typ Typ3, fopts FieldOptions) (err error) {
	typWanted := typeToTyp3(rt, fopts)
	if typ != typWanted {
		err = fmt.Errorf("unexpected Typ3. want %v, got %v", typWanted, typ)
	}
	return
}
func decodeTyp3(bz []byte) (typ Typ3, n int, err error) {
	if len(bz) == 0 {
		err = fmt.Errorf("EOF while reading typ3 byte")
		return
	}
	if bz[0]&0xF8 != 0 {
		err = fmt.Errorf("invalid typ3 byte: %v", Typ3(bz[0]).String())
		return
	}
	typ = Typ3(bz[0])
	n = 1
	return
}
func NewPrefixBytes(prefixBytes []byte) PrefixBytes {
	pb := PrefixBytes{}
	copy(pb[:], prefixBytes)
	return pb
}
func getLengthStr(info *TypeInfo) string {
	switch info.Type.Kind() {
	case reflect.Array,
		reflect.Int8,
		reflect.Int16, reflect.Int32, reflect.Int64,
		reflect.Float32, reflect.Float64,
		reflect.Complex64, reflect.Complex128:
		s := info.Type.Size()
		return fmt.Sprintf("0x%X", s)
	default:
		return "variable"
	}
}
func (cdc *Codec) collectImplementers_nolock(info *TypeInfo) {
	for _, cinfo := range cdc.concreteInfos {
		if cinfo.PtrToType.Implements(info.Type) {
			info.Implementers[cinfo.Prefix] = append(
				info.Implementers[cinfo.Prefix], cinfo)
		}
	}
}
func (cdc *Codec) checkConflictsInPrio_nolock(iinfo *TypeInfo) error {

	for _, cinfos := range iinfo.Implementers {
		if len(cinfos) < 2 {
			continue
		}
		for _, cinfo := range cinfos {
			var inPrio = false
			for _, disfix := range iinfo.InterfaceInfo.Priority {
				if cinfo.GetDisfix() == disfix {
					inPrio = true
				}
			}
			if !inPrio {
				return fmt.Errorf("%v conflicts with %v other(s). Add it to the priority list for %v.",
					cinfo.Type, len(cinfos), iinfo.Type)
			}
		}
	}
	return nil
}
func constructConcreteType(cinfo *TypeInfo) (crv, irvSet reflect.Value) {
	// Construct new concrete type.
	if cinfo.PointerPreferred {
		cPtrRv := reflect.New(cinfo.Type)
		crv = cPtrRv.Elem()
		irvSet = cPtrRv
	} else {
		crv = reflect.New(cinfo.Type).Elem()
		irvSet = crv
	}
	return
}
func (cdc *Codec) MarshalBinaryLengthPrefixedWriter(w io.Writer, o interface{}) (n int64, err error) {
	var bz, _n = []byte(nil), int(0)
	bz, err = cdc.MarshalBinaryLengthPrefixed(o)
	if err != nil {
		return 0, err
	}
	_n, err = w.Write(bz) // TODO: handle overflow in 32-bit systems.
	n = int64(_n)
	return
}
func (cdc *Codec) MarshalBinaryBare(o interface{}) ([]byte, error) {

	// Dereference value if pointer.
	var rv, _, isNilPtr = derefPointers(reflect.ValueOf(o))
	if isNilPtr {
		// NOTE: You can still do so by calling
		// `.MarshalBinaryLengthPrefixed(struct{ *SomeType })` or so on.
		panic("MarshalBinaryBare cannot marshal a nil pointer directly. Try wrapping in a struct?")
	}

	// Encode Amino:binary bytes.
	var bz []byte
	buf := new(bytes.Buffer)
	rt := rv.Type()
	info, err := cdc.getTypeInfo_wlock(rt)
	if err != nil {
		return nil, err
	}
	err = cdc.encodeReflectBinary(buf, info, rv, FieldOptions{BinFieldNum: 1}, true)
	if err != nil {
		return nil, err
	}
	bz = buf.Bytes()

	// If registered concrete, prepend prefix bytes.
	if info.Registered {
		pb := info.Prefix.Bytes()
		bz = append(pb, bz...)
	}

	return bz, nil
}
func (cdc *Codec) UnmarshalBinaryLengthPrefixed(bz []byte, ptr interface{}) error {
	if len(bz) == 0 {
		return errors.New("UnmarshalBinaryLengthPrefixed cannot decode empty bytes")
	}

	// Read byte-length prefix.
	u64, n := binary.Uvarint(bz)
	if n < 0 {
		return fmt.Errorf("Error reading msg byte-length prefix: got code %v", n)
	}
	if u64 > uint64(len(bz)-n) {
		return fmt.Errorf("Not enough bytes to read in UnmarshalBinaryLengthPrefixed, want %v more bytes but only have %v",
			u64, len(bz)-n)
	} else if u64 < uint64(len(bz)-n) {
		return fmt.Errorf("Bytes left over in UnmarshalBinaryLengthPrefixed, should read %v more bytes but have %v",
			u64, len(bz)-n)
	}
	bz = bz[n:]

	// Decode.
	return cdc.UnmarshalBinaryBare(bz, ptr)
}
func (cdc *Codec) UnmarshalBinaryBare(bz []byte, ptr interface{}) error {

	rv := reflect.ValueOf(ptr)
	if rv.Kind() != reflect.Ptr {
		panic("Unmarshal expects a pointer")
	}
	rv = rv.Elem()
	rt := rv.Type()
	info, err := cdc.getTypeInfo_wlock(rt)
	if err != nil {
		return err
	}
	// If registered concrete, consume and verify prefix bytes.
	if info.Registered {
		pb := info.Prefix.Bytes()
		if len(bz) < 4 {
			return fmt.Errorf("UnmarshalBinaryBare expected to read prefix bytes %X (since it is registered concrete) but got %X", pb, bz)
		} else if !bytes.Equal(bz[:4], pb) {
			return fmt.Errorf("UnmarshalBinaryBare expected to read prefix bytes %X (since it is registered concrete) but got %X...", pb, bz[:4])
		}
		bz = bz[4:]
	}
	// Decode contents into rv.
	n, err := cdc.decodeReflectBinary(bz, info, rv, FieldOptions{BinFieldNum: 1}, true)
	if err != nil {
		return fmt.Errorf("unmarshal to %v failed after %d bytes (%v): %X", info.Type, n, err, bz)
	}
	if n != len(bz) {
		return fmt.Errorf("unmarshal to %v didn't read all bytes. Expected to read %v, only read %v: %X", info.Type, len(bz), n, bz)
	}
	return nil
}
func (cdc *Codec) MustMarshalJSON(o interface{}) []byte {
	bz, err := cdc.MarshalJSON(o)
	if err != nil {
		panic(err)
	}
	return bz
}
func (cdc *Codec) MustUnmarshalJSON(bz []byte, ptr interface{}) {
	if err := cdc.UnmarshalJSON(bz, ptr); err != nil {
		panic(err)
	}
}
func (cdc *Codec) MarshalJSONIndent(o interface{}, prefix, indent string) ([]byte, error) {
	bz, err := cdc.MarshalJSON(o)
	if err != nil {
		return nil, err
	}
	var out bytes.Buffer
	err = json.Indent(&out, bz, prefix, indent)
	if err != nil {
		return nil, err
	}
	return out.Bytes(), nil
}
func newDataReader(r io.Reader) *internalDataReader {
	buffered := bufio.NewReader(r)

	reader := internalDataReader{
		wrapped:r,
		buffered:buffered,
	}

	return &reader
}
func (r *internalDataReader) Read(data []byte) (n int, err error) {

	const IAC = 255

	const SB = 250
	const SE = 240

	const WILL = 251
	const WONT = 252
	const DO   = 253
	const DONT = 254

	p := data

	for len(p) > 0 {
		var b byte

		b, err = r.buffered.ReadByte()
		if nil != err {
			return n, err
		}

		if IAC == b {
			var peeked []byte

			peeked, err = r.buffered.Peek(1)
			if nil != err {
				return n, err
			}

			switch peeked[0] {
			case WILL, WONT, DO, DONT:
				_, err = r.buffered.Discard(2)
				if nil != err {
					return n, err
				}
			case IAC:
				p[0] = IAC
				n++
				p = p[1:]

				_, err = r.buffered.Discard(1)
				if nil != err {
					return n, err
				}
			case SB:
				for {
					var b2 byte
					b2, err = r.buffered.ReadByte()
					if nil != err {
						return n, err
					}

					if IAC == b2 {
						peeked, err = r.buffered.Peek(1)
						if nil != err {
							return n, err
						}

						if IAC == peeked[0] {
							_, err = r.buffered.Discard(1)
							if nil != err {
								return n, err
							}
						}

						if SE == peeked[0] {
							_, err = r.buffered.Discard(1)
							if nil != err {
								return n, err
							}
							break
						}
					}
				}
			case SE:
				_, err = r.buffered.Discard(1)
				if nil != err {
					return n, err
				}
			default:
				// If we get in here, this is not following the TELNET protocol.
//@TODO: Make a better error.
				err = errCorrupted
				return n, err
			}
		} else {

			p[0] = b
			n++
			p = p[1:]
		}
	}

	return n, nil
}
func (server *Server) ListenAndServeTLS(certFile string, keyFile string) error {

	addr := server.Addr
	if "" == addr {
		addr = ":telnets"
	}


	listener, err := net.Listen("tcp", addr)
	if nil != err {
		return err
	}


	// Apparently have to make a copy of the TLS config this way, rather than by
	// simple assignment, to prevent some unexported fields from being copied over.
	//
	// It would be nice if tls.Config had a method that would do this "safely".
	// (I.e., what happens if in the future more exported fields are added to
	// tls.Config?)
	var tlsConfig *tls.Config = nil
	if nil == server.TLSConfig {
		tlsConfig = &tls.Config{}
	} else {
		tlsConfig = &tls.Config{
			Rand:                     server.TLSConfig.Rand,
			Time:                     server.TLSConfig.Time,
			Certificates:             server.TLSConfig.Certificates,
			NameToCertificate:        server.TLSConfig.NameToCertificate,
			GetCertificate:           server.TLSConfig.GetCertificate,
			RootCAs:                  server.TLSConfig.RootCAs,
			NextProtos:               server.TLSConfig.NextProtos,
			ServerName:               server.TLSConfig.ServerName,
			ClientAuth:               server.TLSConfig.ClientAuth,
			ClientCAs:                server.TLSConfig.ClientCAs,
			InsecureSkipVerify:       server.TLSConfig.InsecureSkipVerify,
			CipherSuites:             server.TLSConfig.CipherSuites,
			PreferServerCipherSuites: server.TLSConfig.PreferServerCipherSuites,
			SessionTicketsDisabled:   server.TLSConfig.SessionTicketsDisabled,
			SessionTicketKey:         server.TLSConfig.SessionTicketKey,
			ClientSessionCache:       server.TLSConfig.ClientSessionCache,
			MinVersion:               server.TLSConfig.MinVersion,
			MaxVersion:               server.TLSConfig.MaxVersion,
			CurvePreferences:         server.TLSConfig.CurvePreferences,
		}
	}


	tlsConfigHasCertificate := len(tlsConfig.Certificates) > 0 || nil != tlsConfig.GetCertificate
	if "" == certFile || "" == keyFile || !tlsConfigHasCertificate {
		tlsConfig.Certificates = make([]tls.Certificate, 1)

		var err error
		tlsConfig.Certificates[0], err = tls.LoadX509KeyPair(certFile, keyFile)
		if nil != err {
			return err
		}
	}


	tlsListener := tls.NewListener(listener, tlsConfig)


	return server.Serve(tlsListener)
}
func (fn ProducerFunc) Produce(ctx telnet.Context, name string, args ...string) Handler {
	return fn(ctx, name, args...)
}
func PromoteHandlerFunc(fn HandlerFunc, args ...string) Handler {
	stdin,      stdinPipe := io.Pipe()
	stdoutPipe, stdout    := io.Pipe()
	stderrPipe, stderr    := io.Pipe()

	argsCopy := make([]string, len(args))
	for i, datum := range args {
		argsCopy[i] = datum
	}

	handler := internalPromotedHandlerFunc{
		err:nil,

		fn:fn,

		stdin:stdin,
		stdout:stdout,
		stderr:stderr,

		stdinPipe:stdinPipe,
		stdoutPipe:stdoutPipe,
		stderrPipe:stderrPipe,

		args:argsCopy,
	}

	return &handler
}
func Serve(listener net.Listener, handler Handler) error {

	server := &Server{Handler: handler}
	return server.Serve(listener)
}
func (server *Server) Serve(listener net.Listener) error {

	defer listener.Close()


	logger := server.logger()


	handler := server.Handler
	if nil == handler {
//@TODO: Should this be a "ShellHandler" instead, that gives a shell-like experience by default
//       If this is changd, then need to change the comment in the "type Server struct" definition.
		logger.Debug("Defaulted handler to EchoHandler.")
		handler = EchoHandler
	}


	for {
		// Wait for a new TELNET client connection.
		logger.Debugf("Listening at %q.", listener.Addr())
		conn, err := listener.Accept()
		if err != nil {
//@TODO: Could try to recover from certain kinds of errors. Maybe waiting a while before trying again.
			return err
		}
		logger.Debugf("Received new connection from %q.", conn.RemoteAddr())

		// Handle the new TELNET client connection by spawning
		// a new goroutine.
		go server.handle(conn, handler)
		logger.Debugf("Spawned handler to handle connection from %q.", conn.RemoteAddr())
	}
}
func (p *Parser) Fail(msg string) {
	p.WriteUsage(os.Stderr)
	fmt.Fprintln(os.Stderr, "error:", msg)
	os.Exit(-1)
}
func (p *Parser) WriteUsage(w io.Writer) {
	var positionals, options []*spec
	for _, spec := range p.specs {
		if spec.positional {
			positionals = append(positionals, spec)
		} else {
			options = append(options, spec)
		}
	}

	if p.version != "" {
		fmt.Fprintln(w, p.version)
	}

	fmt.Fprintf(w, "Usage: %s", p.config.Program)

	// write the option component of the usage message
	for _, spec := range options {
		// prefix with a space
		fmt.Fprint(w, " ")
		if !spec.required {
			fmt.Fprint(w, "[")
		}
		fmt.Fprint(w, synopsis(spec, "--"+spec.long))
		if !spec.required {
			fmt.Fprint(w, "]")
		}
	}

	// write the positional component of the usage message
	for _, spec := range positionals {
		// prefix with a space
		fmt.Fprint(w, " ")
		up := strings.ToUpper(spec.long)
		if spec.multiple {
			if !spec.required {
				fmt.Fprint(w, "[")
			}
			fmt.Fprintf(w, "%s [%s ...]", up, up)
			if !spec.required {
				fmt.Fprint(w, "]")
			}
		} else {
			fmt.Fprint(w, up)
		}
	}
	fmt.Fprint(w, "\n")
}
func (p *Parser) WriteHelp(w io.Writer) {
	var positionals, options []*spec
	for _, spec := range p.specs {
		if spec.positional {
			positionals = append(positionals, spec)
		} else {
			options = append(options, spec)
		}
	}

	if p.description != "" {
		fmt.Fprintln(w, p.description)
	}
	p.WriteUsage(w)

	// write the list of positionals
	if len(positionals) > 0 {
		fmt.Fprint(w, "\nPositional arguments:\n")
		for _, spec := range positionals {
			left := "  " + strings.ToUpper(spec.long)
			fmt.Fprint(w, left)
			if spec.help != "" {
				if len(left)+2 < colWidth {
					fmt.Fprint(w, strings.Repeat(" ", colWidth-len(left)))
				} else {
					fmt.Fprint(w, "\n"+strings.Repeat(" ", colWidth))
				}
				fmt.Fprint(w, spec.help)
			}
			fmt.Fprint(w, "\n")
		}
	}

	// write the list of options
	fmt.Fprint(w, "\nOptions:\n")
	for _, spec := range options {
		printOption(w, spec)
	}

	// write the list of built in options
	printOption(w, &spec{boolean: true, long: "help", short: "h", help: "display this help and exit"})
	if p.version != "" {
		printOption(w, &spec{boolean: true, long: "version", help: "display version and exit"})
	}
}
func MustParse(dest ...interface{}) *Parser {
	p, err := NewParser(Config{}, dest...)
	if err != nil {
		fmt.Println(err)
		os.Exit(-1)
	}
	err = p.Parse(flags())
	if err == ErrHelp {
		p.WriteHelp(os.Stdout)
		os.Exit(0)
	}
	if err == ErrVersion {
		fmt.Println(p.version)
		os.Exit(0)
	}
	if err != nil {
		p.Fail(err.Error())
	}
	return p
}
func Parse(dest ...interface{}) error {
	p, err := NewParser(Config{}, dest...)
	if err != nil {
		return err
	}
	return p.Parse(flags())
}
func walkFields(v reflect.Value, visit func(field reflect.StructField, val reflect.Value, owner reflect.Type) bool) {
	t := v.Type()
	for i := 0; i < t.NumField(); i++ {
		field := t.Field(i)
		val := v.Field(i)
		expand := visit(field, val, t)
		if expand && field.Type.Kind() == reflect.Struct {
			walkFields(val, visit)
		}
	}
}
func (p *Parser) Parse(args []string) error {
	// If -h or --help were specified then print usage
	for _, arg := range args {
		if arg == "-h" || arg == "--help" {
			return ErrHelp
		}
		if arg == "--version" {
			return ErrVersion
		}
		if arg == "--" {
			break
		}
	}

	// Process all command line arguments
	return process(p.specs, args)
}
func setSlice(dest reflect.Value, values []string, trunc bool) error {
	if !dest.CanSet() {
		return fmt.Errorf("field is not writable")
	}

	var ptr bool
	elem := dest.Type().Elem()
	if elem.Kind() == reflect.Ptr && !elem.Implements(textUnmarshalerType) {
		ptr = true
		elem = elem.Elem()
	}

	// Truncate the dest slice in case default values exist
	if trunc && !dest.IsNil() {
		dest.SetLen(0)
	}

	for _, s := range values {
		v := reflect.New(elem)
		if err := scalar.ParseValue(v.Elem(), s); err != nil {
			return err
		}
		if !ptr {
			v = v.Elem()
		}
		dest.Set(reflect.Append(dest, v))
	}
	return nil
}
func canParse(t reflect.Type) (parseable, boolean, multiple bool) {
	parseable = scalar.CanParse(t)
	boolean = isBoolean(t)
	if parseable {
		return
	}

	// Look inside pointer types
	if t.Kind() == reflect.Ptr {
		t = t.Elem()
	}
	// Look inside slice types
	if t.Kind() == reflect.Slice {
		multiple = true
		t = t.Elem()
	}

	parseable = scalar.CanParse(t)
	boolean = isBoolean(t)
	if parseable {
		return
	}

	// Look inside pointer types (again, in case of []*Type)
	if t.Kind() == reflect.Ptr {
		t = t.Elem()
	}

	parseable = scalar.CanParse(t)
	boolean = isBoolean(t)
	if parseable {
		return
	}

	return false, false, false
}
func isBoolean(t reflect.Type) bool {
	switch {
	case t.Implements(textUnmarshalerType):
		return false
	case t.Kind() == reflect.Bool:
		return true
	case t.Kind() == reflect.Ptr && t.Elem().Kind() == reflect.Bool:
		return true
	default:
		return false
	}
}
func NewFromMap(m map[string]interface{}) *Tree {
	t := &Tree{root: &node{}}
	for k, v := range m {
		t.Insert(k, v)
	}
	return t
}
func (t *Tree) Insert(s string, v interface{}) (interface{}, bool) {
	var parent *node
	n := t.root
	search := s
	for {
		// Handle key exhaution
		if len(search) == 0 {
			if n.isLeaf() {
				old := n.leaf.val
				n.leaf.val = v
				return old, true
			}

			n.leaf = &leafNode{
				key: s,
				val: v,
			}
			t.size++
			return nil, false
		}

		// Look for the edge
		parent = n
		n = n.getEdge(search[0])

		// No edge, create one
		if n == nil {
			e := edge{
				label: search[0],
				node: &node{
					leaf: &leafNode{
						key: s,
						val: v,
					},
					prefix: search,
				},
			}
			parent.addEdge(e)
			t.size++
			return nil, false
		}

		// Determine longest prefix of the search key on match
		commonPrefix := longestPrefix(search, n.prefix)
		if commonPrefix == len(n.prefix) {
			search = search[commonPrefix:]
			continue
		}

		// Split the node
		t.size++
		child := &node{
			prefix: search[:commonPrefix],
		}
		parent.updateEdge(search[0], child)

		// Restore the existing node
		child.addEdge(edge{
			label: n.prefix[commonPrefix],
			node:  n,
		})
		n.prefix = n.prefix[commonPrefix:]

		// Create a new leaf node
		leaf := &leafNode{
			key: s,
			val: v,
		}

		// If the new key is a subset, add to to this node
		search = search[commonPrefix:]
		if len(search) == 0 {
			child.leaf = leaf
			return nil, false
		}

		// Create a new edge for the node
		child.addEdge(edge{
			label: search[0],
			node: &node{
				leaf:   leaf,
				prefix: search,
			},
		})
		return nil, false
	}
}
func (t *Tree) Delete(s string) (interface{}, bool) {
	var parent *node
	var label byte
	n := t.root
	search := s
	for {
		// Check for key exhaution
		if len(search) == 0 {
			if !n.isLeaf() {
				break
			}
			goto DELETE
		}

		// Look for an edge
		parent = n
		label = search[0]
		n = n.getEdge(label)
		if n == nil {
			break
		}

		// Consume the search prefix
		if strings.HasPrefix(search, n.prefix) {
			search = search[len(n.prefix):]
		} else {
			break
		}
	}
	return nil, false

DELETE:
	// Delete the leaf
	leaf := n.leaf
	n.leaf = nil
	t.size--

	// Check if we should delete this node from the parent
	if parent != nil && len(n.edges) == 0 {
		parent.delEdge(label)
	}

	// Check if we should merge this node
	if n != t.root && len(n.edges) == 1 {
		n.mergeChild()
	}

	// Check if we should merge the parent's other child
	if parent != nil && parent != t.root && len(parent.edges) == 1 && !parent.isLeaf() {
		parent.mergeChild()
	}

	return leaf.val, true
}
func (t *Tree) DeletePrefix(s string) int {
	return t.deletePrefix(nil, t.root, s)
}
func (t *Tree) deletePrefix(parent, n *node, prefix string) int {
	// Check for key exhaustion
	if len(prefix) == 0 {
		// Remove the leaf node
		subTreeSize := 0
		//recursively walk from all edges of the node to be deleted
		recursiveWalk(n, func(s string, v interface{}) bool {
			subTreeSize++
			return false
		})
		if n.isLeaf() {
			n.leaf = nil
		}
		n.edges = nil // deletes the entire subtree

		// Check if we should merge the parent's other child
		if parent != nil && parent != t.root && len(parent.edges) == 1 && !parent.isLeaf() {
			parent.mergeChild()
		}
		t.size -= subTreeSize
		return subTreeSize
	}

	// Look for an edge
	label := prefix[0]
	child := n.getEdge(label)
	if child == nil || (!strings.HasPrefix(child.prefix, prefix) && !strings.HasPrefix(prefix, child.prefix)) {
		return 0
	}

	// Consume the search prefix
	if len(child.prefix) > len(prefix) {
		prefix = prefix[len(prefix):]
	} else {
		prefix = prefix[len(child.prefix):]
	}
	return t.deletePrefix(n, child, prefix)
}
func (t *Tree) Get(s string) (interface{}, bool) {
	n := t.root
	search := s
	for {
		// Check for key exhaution
		if len(search) == 0 {
			if n.isLeaf() {
				return n.leaf.val, true
			}
			break
		}

		// Look for an edge
		n = n.getEdge(search[0])
		if n == nil {
			break
		}

		// Consume the search prefix
		if strings.HasPrefix(search, n.prefix) {
			search = search[len(n.prefix):]
		} else {
			break
		}
	}
	return nil, false
}
func (t *Tree) LongestPrefix(s string) (string, interface{}, bool) {
	var last *leafNode
	n := t.root
	search := s
	for {
		// Look for a leaf node
		if n.isLeaf() {
			last = n.leaf
		}

		// Check for key exhaution
		if len(search) == 0 {
			break
		}

		// Look for an edge
		n = n.getEdge(search[0])
		if n == nil {
			break
		}

		// Consume the search prefix
		if strings.HasPrefix(search, n.prefix) {
			search = search[len(n.prefix):]
		} else {
			break
		}
	}
	if last != nil {
		return last.key, last.val, true
	}
	return "", nil, false
}
func (t *Tree) Minimum() (string, interface{}, bool) {
	n := t.root
	for {
		if n.isLeaf() {
			return n.leaf.key, n.leaf.val, true
		}
		if len(n.edges) > 0 {
			n = n.edges[0].node
		} else {
			break
		}
	}
	return "", nil, false
}
func (t *Tree) WalkPrefix(prefix string, fn WalkFn) {
	n := t.root
	search := prefix
	for {
		// Check for key exhaution
		if len(search) == 0 {
			recursiveWalk(n, fn)
			return
		}

		// Look for an edge
		n = n.getEdge(search[0])
		if n == nil {
			break
		}

		// Consume the search prefix
		if strings.HasPrefix(search, n.prefix) {
			search = search[len(n.prefix):]

		} else if strings.HasPrefix(n.prefix, search) {
			// Child may be under our search prefix
			recursiveWalk(n, fn)
			return
		} else {
			break
		}
	}

}
func recursiveWalk(n *node, fn WalkFn) bool {
	// Visit the leaf values if any
	if n.leaf != nil && fn(n.leaf.key, n.leaf.val) {
		return true
	}

	// Recurse on the children
	for _, e := range n.edges {
		if recursiveWalk(e.node, fn) {
			return true
		}
	}
	return false
}
func (t *Tree) ToMap() map[string]interface{} {
	out := make(map[string]interface{}, t.size)
	t.Walk(func(k string, v interface{}) bool {
		out[k] = v
		return false
	})
	return out
}
func checkip(ip string) (iptype uint32, ipnum *big.Int, ipindex uint32) {
	iptype = 0
	ipnum = big.NewInt(0)
	ipnumtmp := big.NewInt(0)
	ipindex = 0
	ipaddress := net.ParseIP(ip)
	
	if ipaddress != nil {
		v4 := ipaddress.To4()
		
		if v4 != nil {
			iptype = 4
			ipnum.SetBytes(v4)
		} else {
			v6 := ipaddress.To16()
			
			if v6 != nil {
				iptype = 6
				ipnum.SetBytes(v6)
			}
		}
	}
	if iptype == 4 {
		if meta.ipv4indexbaseaddr > 0 {
			ipnumtmp.Rsh(ipnum, 16)
			ipnumtmp.Lsh(ipnumtmp, 3)
			ipindex = uint32(ipnumtmp.Add(ipnumtmp, big.NewInt(int64(meta.ipv4indexbaseaddr))).Uint64())
		}
	} else if iptype == 6 {
		if meta.ipv6indexbaseaddr > 0 {
			ipnumtmp.Rsh(ipnum, 112)
			ipnumtmp.Lsh(ipnumtmp, 3)
			ipindex = uint32(ipnumtmp.Add(ipnumtmp, big.NewInt(int64(meta.ipv6indexbaseaddr))).Uint64())
		}
	}
	return 
}
func readuint32(pos uint32) uint32 {
	pos2 := int64(pos)
	var retval uint32
	data := make([]byte, 4)
	_, err := f.ReadAt(data, pos2 - 1)
	if err != nil {
		fmt.Println("File read failed:", err)
	}
	buf := bytes.NewReader(data)
	err = binary.Read(buf, binary.LittleEndian, &retval)
	if err != nil {
		fmt.Println("Binary read failed:", err)
	}
	return retval
}
func readuint128(pos uint32) *big.Int {
	pos2 := int64(pos)
	retval := big.NewInt(0)
	data := make([]byte, 16)
	_, err := f.ReadAt(data, pos2 - 1)
	if err != nil {
		fmt.Println("File read failed:", err)
	}
	
	// little endian to big endian
	for i, j := 0, len(data)-1; i < j; i, j = i+1, j-1 {
		data[i], data[j] = data[j], data[i]
	}
	retval.SetBytes(data)
	return retval
}
func loadmessage (mesg string) IP2Locationrecord {
	var x IP2Locationrecord
	
	x.Country_short = mesg
	x.Country_long = mesg
	x.Region = mesg
	x.City = mesg
	x.Isp = mesg
	x.Domain = mesg
	x.Zipcode = mesg
	x.Timezone = mesg
	x.Netspeed = mesg
	x.Iddcode = mesg
	x.Areacode = mesg
	x.Weatherstationcode = mesg
	x.Weatherstationname = mesg
	x.Mcc = mesg
	x.Mnc = mesg
	x.Mobilebrand = mesg
	x.Usagetype = mesg
	
	return x
}
func Printrecord(x IP2Locationrecord) {
	fmt.Printf("country_short: %s\n", x.Country_short)
	fmt.Printf("country_long: %s\n", x.Country_long)
	fmt.Printf("region: %s\n", x.Region)
	fmt.Printf("city: %s\n", x.City)
	fmt.Printf("isp: %s\n", x.Isp)
	fmt.Printf("latitude: %f\n", x.Latitude)
	fmt.Printf("longitude: %f\n", x.Longitude)
	fmt.Printf("domain: %s\n", x.Domain)
	fmt.Printf("zipcode: %s\n", x.Zipcode)
	fmt.Printf("timezone: %s\n", x.Timezone)
	fmt.Printf("netspeed: %s\n", x.Netspeed)
	fmt.Printf("iddcode: %s\n", x.Iddcode)
	fmt.Printf("areacode: %s\n", x.Areacode)
	fmt.Printf("weatherstationcode: %s\n", x.Weatherstationcode)
	fmt.Printf("weatherstationname: %s\n", x.Weatherstationname)
	fmt.Printf("mcc: %s\n", x.Mcc)
	fmt.Printf("mnc: %s\n", x.Mnc)
	fmt.Printf("mobilebrand: %s\n", x.Mobilebrand)
	fmt.Printf("elevation: %f\n", x.Elevation)
	fmt.Printf("usagetype: %s\n", x.Usagetype)
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	gc.Save()
	gc.Scale(0.5, 0.5)
	// Draw a (partial) gopher
	Draw(gc)
	gc.Restore()

	// Return the output filename
	return samples.Output("gopher", ext), nil
}
func SaveToPdfFile(filePath string, pdf *gofpdf.Fpdf) error {
	return pdf.OutputFileAndClose(filePath)
}
func (p *Path) CubicCurveTo(cx1, cy1, cx2, cy2, x, y float64) {
	if len(p.Components) == 0 { //special case when no move has been done
		p.MoveTo(x, y)
	} else {
		p.appendToPath(CubicCurveToCmp, cx1, cy1, cx2, cy2, x, y)
	}
	p.x = x
	p.y = y
}
func (p *Path) ArcTo(cx, cy, rx, ry, startAngle, angle float64) {
	endAngle := startAngle + angle
	clockWise := true
	if angle < 0 {
		clockWise = false
	}
	// normalize
	if clockWise {
		for endAngle < startAngle {
			endAngle += math.Pi * 2.0
		}
	} else {
		for startAngle < endAngle {
			startAngle += math.Pi * 2.0
		}
	}
	startX := cx + math.Cos(startAngle)*rx
	startY := cy + math.Sin(startAngle)*ry
	if len(p.Components) > 0 {
		p.LineTo(startX, startY)
	} else {
		p.MoveTo(startX, startY)
	}
	p.appendToPath(ArcToCmp, cx, cy, rx, ry, startAngle, angle)
	p.x = cx + math.Cos(endAngle)*rx
	p.y = cy + math.Sin(endAngle)*ry
}
func (p *Path) String() string {
	s := ""
	j := 0
	for _, cmd := range p.Components {
		switch cmd {
		case MoveToCmp:
			s += fmt.Sprintf("MoveTo: %f, %f\n", p.Points[j], p.Points[j+1])
			j = j + 2
		case LineToCmp:
			s += fmt.Sprintf("LineTo: %f, %f\n", p.Points[j], p.Points[j+1])
			j = j + 2
		case QuadCurveToCmp:
			s += fmt.Sprintf("QuadCurveTo: %f, %f, %f, %f\n", p.Points[j], p.Points[j+1], p.Points[j+2], p.Points[j+3])
			j = j + 4
		case CubicCurveToCmp:
			s += fmt.Sprintf("CubicCurveTo: %f, %f, %f, %f, %f, %f\n", p.Points[j], p.Points[j+1], p.Points[j+2], p.Points[j+3], p.Points[j+4], p.Points[j+5])
			j = j + 6
		case ArcToCmp:
			s += fmt.Sprintf("ArcTo: %f, %f, %f, %f, %f, %f\n", p.Points[j], p.Points[j+1], p.Points[j+2], p.Points[j+3], p.Points[j+4], p.Points[j+5])
			j = j + 6
		case CloseCmp:
			s += "Close\n"
		}
	}
	return s
}
func (path *Path) VerticalFlip() *Path {
	p := path.Copy()
	j := 0
	for _, cmd := range p.Components {
		switch cmd {
		case MoveToCmp, LineToCmp:
			p.Points[j+1] = -p.Points[j+1]
			j = j + 2
		case QuadCurveToCmp:
			p.Points[j+1] = -p.Points[j+1]
			p.Points[j+3] = -p.Points[j+3]
			j = j + 4
		case CubicCurveToCmp:
			p.Points[j+1] = -p.Points[j+1]
			p.Points[j+3] = -p.Points[j+3]
			p.Points[j+5] = -p.Points[j+5]
			j = j + 6
		case ArcToCmp:
			p.Points[j+1] = -p.Points[j+1]
			p.Points[j+3] = -p.Points[j+3]
			p.Points[j+4] = -p.Points[j+4] // start angle
			p.Points[j+5] = -p.Points[j+5] // angle
			j = j + 6
		case CloseCmp:
		}
	}
	p.y = -p.y
	return p
}
func NewGlyphCache() *GlyphCacheImp {
	glyphs := make(map[string]map[rune]*Glyph)
	return &GlyphCacheImp {
		glyphs: glyphs,
	}
}
func (glyphCache *GlyphCacheImp) Fetch(gc draw2d.GraphicContext, fontName string, chr rune) *Glyph {
	if glyphCache.glyphs[fontName] == nil {
		glyphCache.glyphs[fontName] = make(map[rune]*Glyph, 60)
	}
	if glyphCache.glyphs[fontName][chr] == nil {
		glyphCache.glyphs[fontName][chr] = renderGlyph(gc, fontName, chr)
	}
	return glyphCache.glyphs[fontName][chr].Copy()
}
func renderGlyph(gc draw2d.GraphicContext, fontName string, chr rune) *Glyph {
	gc.Save()
	defer gc.Restore()
	gc.BeginPath()
	width := gc.CreateStringPath(string(chr), 0, 0)
	path := gc.GetPath()
	return &Glyph{
		Path:  &path,
		Width: width,
	}
}
func (g *Glyph) Copy() *Glyph {
	return &Glyph{
		Path:  g.Path.Copy(),
		Width: g.Width,
	}
}
func (g *Glyph) Fill(gc draw2d.GraphicContext, x, y float64) float64 {
	gc.Save()
	gc.BeginPath()
	gc.Translate(x, y)
	gc.Fill(g.Path)
	gc.Restore()
	return g.Width
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	gc.SetFillRule(draw2d.FillRuleWinding)
	gc.Clear()
	// Draw the line
	for x := 5.0; x < 297; x += 10 {
		Draw(gc, x, 0, x, 210)
	}
	gc.ClearRect(100, 75, 197, 135)
	draw2dkit.Ellipse(gc, 148.5, 105, 35, 25)
	gc.SetFillColor(color.RGBA{0xff, 0xff, 0x44, 0xff})
	gc.FillStroke()

	// Return the output filename
	return samples.Output("line", ext), nil
}
func Draw(gc draw2d.GraphicContext, x0, y0, x1, y1 float64) {
	// Draw a line
	gc.MoveTo(x0, y0)
	gc.LineTo(x1, y1)
	gc.Stroke()
}
func (p *Painter) Paint(ss []raster.Span, done bool) {
	//gl.Begin(gl.LINES)
	sslen := len(ss)
	clenrequired := sslen * 8
	vlenrequired := sslen * 4
	if clenrequired >= (cap(p.colors) - len(p.colors)) {
		p.Flush()

		if clenrequired >= cap(p.colors) {
			p.vertices = make([]int32, 0, vlenrequired+(vlenrequired/2))
			p.colors = make([]uint8, 0, clenrequired+(clenrequired/2))
		}
	}
	vi := len(p.vertices)
	ci := len(p.colors)
	p.vertices = p.vertices[0 : vi+vlenrequired]
	p.colors = p.colors[0 : ci+clenrequired]
	var (
		colors   []uint8
		vertices []int32
	)
	for _, s := range ss {
		a := uint8((s.Alpha * p.ca / M16) >> 8)

		colors = p.colors[ci:]
		colors[0] = p.cr
		colors[1] = p.cg
		colors[2] = p.cb
		colors[3] = a
		colors[4] = p.cr
		colors[5] = p.cg
		colors[6] = p.cb
		colors[7] = a
		ci += 8
		vertices = p.vertices[vi:]
		vertices[0] = int32(s.X0)
		vertices[1] = int32(s.Y)
		vertices[2] = int32(s.X1)
		vertices[3] = int32(s.Y)
		vi += 4
	}
}
func (p *Painter) SetColor(c color.Color) {
	r, g, b, a := c.RGBA()
	if a == 0 {
		p.cr = 0
		p.cg = 0
		p.cb = 0
		p.ca = a
	} else {
		p.cr = uint8((r * M16 / a) >> 8)
		p.cg = uint8((g * M16 / a) >> 8)
		p.cb = uint8((b * M16 / a) >> 8)
		p.ca = a
	}
}
func NewPainter() *Painter {
	p := new(Painter)
	p.vertices = make([]int32, 0, 1024)
	p.colors = make([]uint8, 0, 1024)
	return p
}
func (gc *GraphicContext) GetStringBounds(s string) (left, top, right, bottom float64) {
	f, err := gc.loadCurrentFont()
	if err != nil {
		log.Println(err)
		return 0, 0, 0, 0
	}
	top, left, bottom, right = 10e6, 10e6, -10e6, -10e6
	cursor := 0.0
	prev, hasPrev := truetype.Index(0), false
	for _, rune := range s {
		index := f.Index(rune)
		if hasPrev {
			cursor += fUnitsToFloat64(f.Kern(fixed.Int26_6(gc.Current.Scale), prev, index))
		}
		if err := gc.glyphBuf.Load(gc.Current.Font, fixed.Int26_6(gc.Current.Scale), index, font.HintingNone); err != nil {
			log.Println(err)
			return 0, 0, 0, 0
		}
		e0 := 0
		for _, e1 := range gc.glyphBuf.Ends {
			ps := gc.glyphBuf.Points[e0:e1]
			for _, p := range ps {
				x, y := pointToF64Point(p)
				top = math.Min(top, y)
				bottom = math.Max(bottom, y)
				left = math.Min(left, x+cursor)
				right = math.Max(right, x+cursor)
			}
		}
		cursor += fUnitsToFloat64(f.HMetric(fixed.Int26_6(gc.Current.Scale), index).AdvanceWidth)
		prev, hasPrev = index, true
	}
	return left, top, right, bottom
}
func (gc *GraphicContext) recalc() {
	gc.Current.Scale = gc.Current.FontSize * float64(gc.DPI) * (64.0 / 72.0)
}
func (gc *GraphicContext) SetFont(font *truetype.Font) {
	gc.Current.Font = font
}
func (gc *GraphicContext) ClearRect(x1, y1, x2, y2 int) {
	mask := gc.newMask(x1, y1, x2-x1, y2-y1)

	newGroup := &Group{
		Groups: gc.svg.Groups,
		Mask:   "url(#" + mask.Id + ")",
	}

	// replace groups with new masked group
	gc.svg.Groups = []*Group{newGroup}
}
func (gc *GraphicContext) drawString(text string, drawType drawType, x, y float64) float64 {
	switch gc.svg.FontMode {
	case PathFontMode:
		w := gc.CreateStringPath(text, x, y)
		gc.drawPaths(drawType)
		gc.Current.Path.Clear()
		return w
	case SvgFontMode:
		gc.embedSvgFont(text)
	}

	// create elements
	svgText := Text{}
	group := gc.newGroup(drawType)

	// set attrs to text element
	svgText.Text = text
	svgText.FontSize = gc.Current.FontSize
	svgText.X = x
	svgText.Y = y
	svgText.FontFamily = gc.Current.FontData.Name

	// attach to group
	group.Texts = []*Text{&svgText}
	left, _, right, _ := gc.GetStringBounds(text)
	return right - left
}
func (gc *GraphicContext) newGroup(drawType drawType) *Group {
	group := Group{}
	// set attrs to group
	if drawType&stroked == stroked {
		group.Stroke = toSvgRGBA(gc.Current.StrokeColor)
		group.StrokeWidth = toSvgLength(gc.Current.LineWidth)
		group.StrokeLinecap = gc.Current.Cap.String()
		group.StrokeLinejoin = gc.Current.Join.String()
		if len(gc.Current.Dash) > 0 {
			group.StrokeDasharray = toSvgArray(gc.Current.Dash)
			group.StrokeDashoffset = toSvgLength(gc.Current.DashOffset)
		}
	}

	if drawType&filled == filled {
		group.Fill = toSvgRGBA(gc.Current.FillColor)
		group.FillRule = toSvgFillRule(gc.Current.FillRule)
	}

	group.Transform = toSvgTransform(gc.Current.Tr)

	// attach
	gc.svg.Groups = append(gc.svg.Groups, &group)

	return &group
}
func (gc *GraphicContext) newMask(x, y, width, height int) *Mask {
	mask := &Mask{}
	mask.X = float64(x)
	mask.Y = float64(y)
	mask.Width = toSvgLength(float64(width))
	mask.Height = toSvgLength(float64(height))

	// attach mask
	gc.svg.Masks = append(gc.svg.Masks, mask)
	mask.Id = "mask-" + strconv.Itoa(len(gc.svg.Masks))
	return mask
}
func (gc *GraphicContext) embedSvgFont(text string) *Font {
	fontName := gc.Current.FontData.Name
	gc.loadCurrentFont()

	// find or create font Element
	svgFont := (*Font)(nil)
	for _, font := range gc.svg.Fonts {
		if font.Name == fontName {
			svgFont = font
			break
		}
	}
	if svgFont == nil {
		// create new
		svgFont = &Font{}
		// and attach
		gc.svg.Fonts = append(gc.svg.Fonts, svgFont)
	}

	// fill with glyphs

	gc.Save()
	defer gc.Restore()
	gc.SetFontSize(2048)
	defer gc.SetDPI(gc.GetDPI())
	gc.SetDPI(92)
filling:
	for _, rune := range text {
		for _, g := range svgFont.Glyphs {
			if g.Rune == Rune(rune) {
				continue filling
			}
		}
		glyph := gc.glyphCache.Fetch(gc, gc.GetFontName(), rune)
		// glyphCache.Load indirectly calls CreateStringPath for single rune string

		glypPath := glyph.Path.VerticalFlip() // svg font glyphs have oposite y axe
		svgFont.Glyphs = append(svgFont.Glyphs, &Glyph{
			Rune:      Rune(rune),
			Desc:      toSvgPathDesc(glypPath),
			HorizAdvX: glyph.Width,
		})
	}

	// set attrs
	svgFont.Id = "font-" + strconv.Itoa(len(gc.svg.Fonts))
	svgFont.Name = fontName

	// TODO use css @font-face with id instead of this
	svgFont.Face = &Face{Family: fontName, Units: 2048, HorizAdvX: 2048}
	return svgFont
}
func TraceQuad(t Liner, quad []float64, flatteningThreshold float64) error {
	if len(quad) < 6 {
		return errors.New("quad length must be >= 6")
	}
	// Allocates curves stack
	var curves [CurveRecursionLimit * 6]float64
	copy(curves[0:6], quad[0:6])
	i := 0
	// current curve
	var c []float64
	var dx, dy, d float64

	for i >= 0 {
		c = curves[i:]
		dx = c[4] - c[0]
		dy = c[5] - c[1]

		d = math.Abs(((c[2]-c[4])*dy - (c[3]-c[5])*dx))

		// if it's flat then trace a line
		if (d*d) <= flatteningThreshold*(dx*dx+dy*dy) || i == len(curves)-6 {
			t.LineTo(c[4], c[5])
			i -= 6
		} else {
			// second half of bezier go lower onto the stack
			SubdivideQuad(c, curves[i+6:], curves[i:])
			i += 6
		}
	}
	return nil
}
func (cs *ContextStack) GetFontName() string {
	fontData := cs.FontData
	return fmt.Sprintf("%s:%d:%d:%9.2f", fontData.Name, fontData.Family, fontData.Style, cs.FontSize)
}
func NewStackGraphicContext() *StackGraphicContext {
	gc := &StackGraphicContext{}
	gc.Current = new(ContextStack)
	gc.Current.Tr = draw2d.NewIdentityMatrix()
	gc.Current.Path = new(draw2d.Path)
	gc.Current.LineWidth = 1.0
	gc.Current.StrokeColor = image.Black
	gc.Current.FillColor = image.White
	gc.Current.Cap = draw2d.RoundCap
	gc.Current.FillRule = draw2d.FillRuleEvenOdd
	gc.Current.Join = draw2d.RoundJoin
	gc.Current.FontSize = 10
	gc.Current.FontData = DefaultFontData
	return gc
}
func NewFolderFontCache(folder string) *FolderFontCache {
	return &FolderFontCache{
		fonts:  make(map[string]*truetype.Font),
		folder: folder,
		namer:  FontFileName,
	}
}
func NewSyncFolderFontCache(folder string) *SyncFolderFontCache {
	return &SyncFolderFontCache{
		fonts:  make(map[string]*truetype.Font),
		folder: folder,
		namer:  FontFileName,
	}
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	gc.SetStrokeColor(image.Black)
	gc.SetFillColor(image.White)
	gc.Save()
	// Draw a (partial) gopher
	gc.Translate(-60, 65)
	gc.Rotate(-30 * (math.Pi / 180.0))
	Draw(gc, 48, 48, 240, 72)
	gc.Restore()

	// Return the output filename
	return samples.Output("gopher2", ext), nil
}
func NewPdf(orientationStr, unitStr, sizeStr string) *gofpdf.Fpdf {
	pdf := gofpdf.New(orientationStr, unitStr, sizeStr, draw2d.GetFontFolder())
	// to be compatible with draw2d
	pdf.SetMargins(0, 0, 0)
	pdf.SetDrawColor(0, 0, 0)
	pdf.SetFillColor(255, 255, 255)
	pdf.SetLineCapStyle("round")
	pdf.SetLineJoinStyle("round")
	pdf.SetLineWidth(1)
	pdf.AddPage()
	return pdf
}
func clearRect(gc *GraphicContext, x1, y1, x2, y2 float64) {
	// save state
	f := gc.Current.FillColor
	x, y := gc.pdf.GetXY()
	// cover page with white rectangle
	gc.SetFillColor(white)
	draw2dkit.Rectangle(gc, x1, y1, x2, y2)
	gc.Fill()
	// restore state
	gc.SetFillColor(f)
	gc.pdf.MoveTo(x, y)
}
func NewGraphicContext(pdf *gofpdf.Fpdf) *GraphicContext {
	gc := &GraphicContext{draw2dbase.NewStackGraphicContext(), pdf, DPI}
	gc.SetDPI(DPI)
	return gc
}
func (gc *GraphicContext) Clear() {
	width, height := gc.pdf.GetPageSize()
	clearRect(gc, 0, 0, width, height)
}
func (gc *GraphicContext) GetStringBounds(s string) (left, top, right, bottom float64) {
	_, h := gc.pdf.GetFontSize()
	d := gc.pdf.GetFontDesc("", "")
	if d.Ascent == 0 {
		// not defined (standard font?), use average of 81%
		top = 0.81 * h
	} else {
		top = -float64(d.Ascent) * h / float64(d.Ascent-d.Descent)
	}
	return 0, top, gc.pdf.GetStringWidth(s), top + h
}
func (gc *GraphicContext) CreateStringPath(text string, x, y float64) (cursor float64) {
	//fpdf uses the top left corner
	left, top, right, bottom := gc.GetStringBounds(text)
	w := right - left
	h := bottom - top
	// gc.pdf.SetXY(x, y-h) do not use this as y-h might be negative
	margin := gc.pdf.GetCellMargin()
	gc.pdf.MoveTo(x-left-margin, y+top)
	gc.pdf.CellFormat(w, h, text, "", 0, "BL", false, 0, "")
	return w
}
func (gc *GraphicContext) FillStringAt(text string, x, y float64) (cursor float64) {
	return gc.CreateStringPath(text, x, y)
}
func (gc *GraphicContext) SetStrokeColor(c color.Color) {
	gc.StackGraphicContext.SetStrokeColor(c)
	gc.pdf.SetDrawColor(rgb(c))
}
func (gc *GraphicContext) SetFillColor(c color.Color) {
	gc.StackGraphicContext.SetFillColor(c)
	gc.pdf.SetFillColor(rgb(c))
	gc.pdf.SetTextColor(rgb(c))
}
func (gc *GraphicContext) SetLineDash(Dash []float64, DashOffset float64) {
	gc.StackGraphicContext.SetLineDash(Dash, DashOffset)
	gc.pdf.SetDashPattern(Dash, DashOffset)
}
func (gc *GraphicContext) SetLineWidth(LineWidth float64) {
	gc.StackGraphicContext.SetLineWidth(LineWidth)
	gc.pdf.SetLineWidth(LineWidth)
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	// Draw hello world
	Draw(gc, fmt.Sprintf("Hello World %d dpi", gc.GetDPI()))

	// Return the output filename
	return samples.Output("helloworld", ext), nil
}
func Draw(gc draw2d.GraphicContext, text string) {
	// Draw a rounded rectangle using default colors
	draw2dkit.RoundedRectangle(gc, 5, 5, 135, 95, 10, 10)
	gc.FillStroke()

	// Set the font luximbi.ttf
	gc.SetFontData(draw2d.FontData{Name: "luxi", Family: draw2d.FontFamilyMono, Style: draw2d.FontStyleBold | draw2d.FontStyleItalic})
	// Set the fill text color to black
	gc.SetFillColor(image.Black)
	gc.SetFontSize(14)
	// Display Hello World
	gc.FillStringAt("Hello World", 8, 52)
}
func SaveToPngFile(filePath string, m image.Image) error {
	// Create the file
	f, err := os.Create(filePath)
	if err != nil {
		return err
	}
	defer f.Close()
	// Create Writer from file
	b := bufio.NewWriter(f)
	// Write the image into the buffer
	err = png.Encode(b, m)
	if err != nil {
		return err
	}
	err = b.Flush()
	if err != nil {
		return err
	}
	return nil
}
func LoadFromPngFile(filePath string) (image.Image, error) {
	// Open file
	f, err := os.OpenFile(filePath, 0, 0)
	if err != nil {
		return nil, err
	}
	defer f.Close()
	b := bufio.NewReader(f)
	img, err := png.Decode(b)
	if err != nil {
		return nil, err
	}
	return img, nil
}
func Resource(folder, filename, ext string) string {
	var root string
	if ext == "pdf" || ext == "svg" {
		root = "../"
	}
	return fmt.Sprintf("%sresource/%s/%s", root, folder, filename)
}
func Output(name, ext string) string {
	var root string
	if ext == "pdf" || ext == "svg" {
		root = "../"
	}
	return fmt.Sprintf("%soutput/samples/%s.%s", root, name, ext)
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	gc.Save()

	// flip the image
	gc.Translate(0, 200)
	gc.Scale(0.35, -0.35)
	gc.Translate(70, -200)

	// Tiger postscript drawing
	tiger := samples.Resource("image", "tiger.ps", ext)

	// Draw tiger
	Draw(gc, tiger)
	gc.Restore()

	// Return the output filename
	return samples.Output("postscript", ext), nil
}
func Draw(gc draw2d.GraphicContext, filename string) {
	// Open the postscript
	src, err := os.OpenFile(filename, 0, 0)
	if err != nil {
		panic(err)
	}
	defer src.Close()
	bytes, err := ioutil.ReadAll(src)
	reader := strings.NewReader(string(bytes))

	// Initialize and interpret the postscript
	interpreter := ps.NewInterpreter(gc)
	interpreter.Execute(reader)
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	// Draw the droid
	Draw(gc, 297, 210)

	// Return the output filename
	return samples.Output("geometry", ext), nil
}
func Bubble(gc draw2d.GraphicContext, x, y, width, height float64) {
	sx, sy := width/100, height/100
	gc.MoveTo(x+sx*50, y)
	gc.QuadCurveTo(x, y, x, y+sy*37.5)
	gc.QuadCurveTo(x, y+sy*75, x+sx*25, y+sy*75)
	gc.QuadCurveTo(x+sx*25, y+sy*95, x+sx*5, y+sy*100)
	gc.QuadCurveTo(x+sx*35, y+sy*95, x+sx*40, y+sy*75)
	gc.QuadCurveTo(x+sx*100, y+sy*75, x+sx*100, y+sy*37.5)
	gc.QuadCurveTo(x+sx*100, y, x+sx*50, y)
	gc.Stroke()
}
func Dash(gc draw2d.GraphicContext, x, y, width, height float64) {
	sx, sy := width/162, height/205
	gc.SetStrokeColor(image.Black)
	gc.SetLineDash([]float64{height / 10, height / 50, height / 50, height / 50}, -50.0)
	gc.SetLineCap(draw2d.ButtCap)
	gc.SetLineJoin(draw2d.RoundJoin)
	gc.SetLineWidth(height / 50)

	gc.MoveTo(x+sx*60.0, y)
	gc.LineTo(x+sx*60.0, y)
	gc.LineTo(x+sx*162, y+sy*205)
	rLineTo(gc, sx*-102.4, 0)
	gc.CubicCurveTo(x+sx*-17, y+sy*205, x+sx*-17, y+sy*103, x+sx*60.0, y+sy*103.0)
	gc.Stroke()
	gc.SetLineDash(nil, 0.0)
}
func CubicCurve(gc draw2d.GraphicContext, x, y, width, height float64) {
	sx, sy := width/162, height/205
	x0, y0 := x, y+sy*100.0
	x1, y1 := x+sx*75, y+sy*205
	x2, y2 := x+sx*125, y
	x3, y3 := x+sx*205, y+sy*100

	gc.SetStrokeColor(image.Black)
	gc.SetFillColor(color.NRGBA{0xAA, 0xAA, 0xAA, 0xFF})
	gc.SetLineWidth(width / 10)
	gc.MoveTo(x0, y0)
	gc.CubicCurveTo(x1, y1, x2, y2, x3, y3)
	gc.Stroke()

	gc.SetStrokeColor(color.NRGBA{0xFF, 0x33, 0x33, 0x88})

	gc.SetLineWidth(width / 20)
	// draw segment of curve
	gc.MoveTo(x0, y0)
	gc.LineTo(x1, y1)
	gc.LineTo(x2, y2)
	gc.LineTo(x3, y3)
	gc.Stroke()
}
func FillStroke(gc draw2d.GraphicContext, x, y, width, height float64) {
	sx, sy := width/210, height/215
	gc.MoveTo(x+sx*113.0, y)
	gc.LineTo(x+sx*215.0, y+sy*215)
	rLineTo(gc, sx*-100, 0)
	gc.CubicCurveTo(x+sx*35, y+sy*215, x+sx*35, y+sy*113, x+sx*113.0, y+sy*113)
	gc.Close()

	gc.MoveTo(x+sx*50.0, y)
	rLineTo(gc, sx*51.2, sy*51.2)
	rLineTo(gc, sx*-51.2, sy*51.2)
	rLineTo(gc, sx*-51.2, sy*-51.2)
	gc.Close()

	gc.SetLineWidth(width / 20.0)
	gc.SetFillColor(color.NRGBA{0, 0, 0xFF, 0xFF})
	gc.SetStrokeColor(image.Black)
	gc.FillStroke()
}
func FillStyle(gc draw2d.GraphicContext, x, y, width, height float64) {
	sx, sy := width/232, height/220
	gc.SetLineWidth(width / 40)

	draw2dkit.Rectangle(gc, x+sx*0, y+sy*12, x+sx*232, y+sy*70)

	var wheel1, wheel2 draw2d.Path
	wheel1.ArcTo(x+sx*52, y+sy*70, sx*40, sy*40, 0, 2*math.Pi)
	wheel2.ArcTo(x+sx*180, y+sy*70, sx*40, sy*40, 0, -2*math.Pi)

	gc.SetFillRule(draw2d.FillRuleEvenOdd)
	gc.SetFillColor(color.NRGBA{0, 0xB2, 0, 0xFF})

	gc.SetStrokeColor(image.Black)
	gc.FillStroke(&wheel1, &wheel2)

	draw2dkit.Rectangle(gc, x, y+sy*140, x+sx*232, y+sy*198)
	wheel1.Clear()
	wheel1.ArcTo(x+sx*52, y+sy*198, sx*40, sy*40, 0, 2*math.Pi)
	wheel2.Clear()
	wheel2.ArcTo(x+sx*180, y+sy*198, sx*40, sy*40, 0, -2*math.Pi)

	gc.SetFillRule(draw2d.FillRuleWinding)
	gc.SetFillColor(color.NRGBA{0, 0, 0xE5, 0xFF})
	gc.FillStroke(&wheel1, &wheel2)
}
func PathTransform(gc draw2d.GraphicContext, x, y, width, height float64) {
	gc.Save()
	gc.SetLineWidth(width / 10)
	gc.Translate(x+width/2, y+height/2)
	gc.Scale(1, 4)
	gc.ArcTo(0, 0, width/8, height/8, 0, math.Pi*2)
	gc.Close()
	gc.Stroke()
	gc.Restore()
}
func Star(gc draw2d.GraphicContext, x, y, width, height float64) {
	gc.Save()
	gc.Translate(x+width/2, y+height/2)
	gc.SetLineWidth(width / 40)
	for i := 0.0; i < 360; i = i + 10 { // Go from 0 to 360 degrees in 10 degree steps
		gc.Save()                        // Keep rotations temporary
		gc.Rotate(i * (math.Pi / 180.0)) // Rotate by degrees on stack from 'for'
		gc.MoveTo(0, 0)
		gc.LineTo(width/2, 0)
		gc.Stroke()
		gc.Restore()
	}
	gc.Restore()
}
func Draw(gc draw2d.GraphicContext, width, height float64) {
	mx, my := width*0.025, height*0.025 // margin
	dx, dy := (width-2*mx)/4, (height-2*my)/3
	w, h := dx-2*mx, dy-2*my
	x0, y := 2*mx, 2*my
	x := x0
	Bubble(gc, x, y, w, h)
	x += dx
	CurveRectangle(gc, x, y, w, h, color.NRGBA{0x80, 0, 0, 0x80}, color.NRGBA{0x80, 0x80, 0xFF, 0xFF})
	x += dx
	Dash(gc, x, y, w, h)
	x += dx
	Arc(gc, x, y, w, h)
	x = x0
	y += dy
	ArcNegative(gc, x, y, w, h)
	x += dx
	CubicCurve(gc, x, y, w, h)
	x += dx
	FillString(gc, x, y, w, h)
	x += dx
	FillStroke(gc, x, y, w, h)
	x = x0
	y += dy
	FillStyle(gc, x, y, w, h)
	x += dx
	PathTransform(gc, x, y, w, h)
	x += dx
	Star(gc, x, y, w, h)
	x += dx
	gopher2.Draw(gc, x, y, w, h/2)
}
func ConvertPath(path *draw2d.Path, pdf Vectorizer) {
	var startX, startY float64 = 0, 0
	i := 0
	for _, cmp := range path.Components {
		switch cmp {
		case draw2d.MoveToCmp:
			startX, startY = path.Points[i], path.Points[i+1]
			pdf.MoveTo(startX, startY)
			i += 2
		case draw2d.LineToCmp:
			pdf.LineTo(path.Points[i], path.Points[i+1])
			i += 2
		case draw2d.QuadCurveToCmp:
			pdf.CurveTo(path.Points[i], path.Points[i+1], path.Points[i+2], path.Points[i+3])
			i += 4
		case draw2d.CubicCurveToCmp:
			pdf.CurveBezierCubicTo(path.Points[i], path.Points[i+1], path.Points[i+2], path.Points[i+3], path.Points[i+4], path.Points[i+5])
			i += 6
		case draw2d.ArcToCmp:
			pdf.ArcTo(path.Points[i], path.Points[i+1], path.Points[i+2], path.Points[i+3],
				0, // degRotate
				path.Points[i+4]*deg,                    // degStart = startAngle
				(path.Points[i+4]-path.Points[i+5])*deg) // degEnd = startAngle-angle
			i += 6
		case draw2d.CloseCmp:
			pdf.LineTo(startX, startY)
			pdf.ClosePath()
		}
	}
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	// Draw the line
	const offset = 75.0
	x := 35.0
	caps := []draw2d.LineCap{draw2d.ButtCap, draw2d.SquareCap, draw2d.RoundCap}
	joins := []draw2d.LineJoin{draw2d.BevelJoin, draw2d.MiterJoin, draw2d.RoundJoin}
	for i := range caps {
		Draw(gc, caps[i], joins[i], x, 50, x, 160, offset)
		x += offset
	}

	// Return the output filename
	return samples.Output("linecapjoin", ext), nil
}
func Draw(gc draw2d.GraphicContext, cap draw2d.LineCap, join draw2d.LineJoin,
	x0, y0, x1, y1, offset float64) {
	gc.SetLineCap(cap)
	gc.SetLineJoin(join)

	// Draw thick line
	gc.SetStrokeColor(color.NRGBA{0x33, 0x33, 0x33, 0xFF})
	gc.SetLineWidth(30.0)
	gc.MoveTo(x0, y0)
	gc.LineTo((x0+x1)/2+offset, (y0+y1)/2)
	gc.LineTo(x1, y1)
	gc.Stroke()

	// Draw thin helping line
	gc.SetStrokeColor(color.NRGBA{0xFF, 0x33, 0x33, 0xFF})
	gc.SetLineWidth(2.56)
	gc.MoveTo(x0, y0)
	gc.LineTo((x0+x1)/2+offset, (y0+y1)/2)
	gc.LineTo(x1, y1)
	gc.Stroke()
}
func DrawContour(path draw2d.PathBuilder, ps []truetype.Point, dx, dy float64) {
	if len(ps) == 0 {
		return
	}
	startX, startY := pointToF64Point(ps[0])
	path.MoveTo(startX+dx, startY+dy)
	q0X, q0Y, on0 := startX, startY, true
	for _, p := range ps[1:] {
		qX, qY := pointToF64Point(p)
		on := p.Flags&0x01 != 0
		if on {
			if on0 {
				path.LineTo(qX+dx, qY+dy)
			} else {
				path.QuadCurveTo(q0X+dx, q0Y+dy, qX+dx, qY+dy)
			}
		} else {
			if on0 {
				// No-op.
			} else {
				midX := (q0X + qX) / 2
				midY := (q0Y + qY) / 2
				path.QuadCurveTo(q0X+dx, q0Y+dy, midX+dx, midY+dy)
			}
		}
		q0X, q0Y, on0 = qX, qY, on
	}
	// Close the curve.
	if on0 {
		path.LineTo(startX+dx, startY+dy)
	} else {
		path.QuadCurveTo(q0X+dx, q0Y+dy, startX+dx, startY+dy)
	}
}
func Flatten(path *draw2d.Path, flattener Flattener, scale float64) {
	// First Point
	var startX, startY float64 = 0, 0
	// Current Point
	var x, y float64 = 0, 0
	i := 0
	for _, cmp := range path.Components {
		switch cmp {
		case draw2d.MoveToCmp:
			x, y = path.Points[i], path.Points[i+1]
			startX, startY = x, y
			if i != 0 {
				flattener.End()
			}
			flattener.MoveTo(x, y)
			i += 2
		case draw2d.LineToCmp:
			x, y = path.Points[i], path.Points[i+1]
			flattener.LineTo(x, y)
			flattener.LineJoin()
			i += 2
		case draw2d.QuadCurveToCmp:
			TraceQuad(flattener, path.Points[i-2:], 0.5)
			x, y = path.Points[i+2], path.Points[i+3]
			flattener.LineTo(x, y)
			i += 4
		case draw2d.CubicCurveToCmp:
			TraceCubic(flattener, path.Points[i-2:], 0.5)
			x, y = path.Points[i+4], path.Points[i+5]
			flattener.LineTo(x, y)
			i += 6
		case draw2d.ArcToCmp:
			x, y = TraceArc(flattener, path.Points[i], path.Points[i+1], path.Points[i+2], path.Points[i+3], path.Points[i+4], path.Points[i+5], scale)
			flattener.LineTo(x, y)
			i += 6
		case draw2d.CloseCmp:
			flattener.LineTo(startX, startY)
			flattener.Close()
		}
	}
	flattener.End()
}
func (gc *GraphicContext) Clear() {
	width, height := gc.img.Bounds().Dx(), gc.img.Bounds().Dy()
	gc.ClearRect(0, 0, width, height)
}
func (gc *GraphicContext) ClearRect(x1, y1, x2, y2 int) {
	imageColor := image.NewUniform(gc.Current.FillColor)
	draw.Draw(gc.img, image.Rect(x1, y1, x2, y2), imageColor, image.ZP, draw.Over)
}
func DrawImage(src image.Image, dest draw.Image, tr draw2d.Matrix, op draw.Op, filter ImageFilter) {
	var transformer draw.Transformer
	switch filter {
	case LinearFilter:
		transformer = draw.NearestNeighbor
	case BilinearFilter:
		transformer = draw.BiLinear
	case BicubicFilter:
		transformer = draw.CatmullRom
	}
	transformer.Transform(dest, f64.Aff3{tr[0], tr[1], tr[4], tr[2], tr[3], tr[5]}, src, src.Bounds(), op, nil)
}
func Main(gc draw2d.GraphicContext, ext string) (string, error) {
	// Margin between the image and the frame
	const margin = 30
	// Line width od the frame
	const lineWidth = 3

	// Gopher image
	gopher := samples.Resource("image", "gopher.png", ext)

	// Draw gopher
	err := Draw(gc, gopher, 297, 210, margin, lineWidth)

	// Return the output filename
	return samples.Output("frameimage", ext), err
}
func Draw(gc draw2d.GraphicContext, png string,
	dw, dh, margin, lineWidth float64) error {
	// Draw frame
	draw2dkit.RoundedRectangle(gc, lineWidth, lineWidth, dw-lineWidth, dh-lineWidth, 100, 100)
	gc.SetLineWidth(lineWidth)
	gc.FillStroke()

	// load the source image
	source, err := draw2dimg.LoadFromPngFile(png)
	if err != nil {
		return err
	}
	// Size of source image
	sw, sh := float64(source.Bounds().Dx()), float64(source.Bounds().Dy())
	// Draw image to fit in the frame
	// TODO Seems to have a transform bug here on draw image
	scale := math.Min((dw-margin*2)/sw, (dh-margin*2)/sh)
	gc.Save()
	gc.Translate((dw-sw*scale)/2, (dh-sh*scale)/2)
	gc.Scale(scale, scale)
	gc.Rotate(0.2)

	gc.DrawImage(source)
	gc.Restore()
	return nil
}
func Draw(gc draw2d.GraphicContext, x, y float64) {
	// set the fill and stroke color of the droid
	gc.SetFillColor(color.RGBA{0x44, 0xff, 0x44, 0xff})
	gc.SetStrokeColor(color.RGBA{0x44, 0x44, 0x44, 0xff})

	// set line properties
	gc.SetLineCap(draw2d.RoundCap)
	gc.SetLineWidth(5)

	// head
	gc.MoveTo(x+30, y+70)
	gc.ArcTo(x+80, y+70, 50, 50, 180*(math.Pi/180), 180*(math.Pi/180))
	gc.Close()
	gc.FillStroke()
	gc.MoveTo(x+60, y+25)
	gc.LineTo(x+50, y+10)
	gc.MoveTo(x+100, y+25)
	gc.LineTo(x+110, y+10)
	gc.Stroke()

	// left eye
	draw2dkit.Circle(gc, x+60, y+45, 5)
	gc.FillStroke()

	// right eye
	draw2dkit.Circle(gc, x+100, y+45, 5)
	gc.FillStroke()

	// body
	draw2dkit.RoundedRectangle(gc, x+30, y+75, x+30+100, y+75+90, 10, 10)
	gc.FillStroke()
	draw2dkit.Rectangle(gc, x+30, y+75, x+30+100, y+75+80)
	gc.FillStroke()

	// left arm
	draw2dkit.RoundedRectangle(gc, x+5, y+80, x+5+20, y+80+70, 10, 10)
	gc.FillStroke()

	// right arm
	draw2dkit.RoundedRectangle(gc, x+135, y+80, x+135+20, y+80+70, 10, 10)
	gc.FillStroke()

	// left leg
	draw2dkit.RoundedRectangle(gc, x+50, y+150, x+50+20, y+150+50, 10, 10)
	gc.FillStroke()

	// right leg
	draw2dkit.RoundedRectangle(gc, x+90, y+150, x+90+20, y+150+50, 10, 10)
	gc.FillStroke()
}
func ChecksumString32S(s string, seed uint32) uint32 {
	if len(s) == 0 {
		return Checksum32S(nil, seed)
	}
	ss := (*reflect.StringHeader)(unsafe.Pointer(&s))
	return Checksum32S((*[maxInt32]byte)(unsafe.Pointer(ss.Data))[:len(s):len(s)], seed)
}
func ChecksumString64S(s string, seed uint64) uint64 {
	if len(s) == 0 {
		return Checksum64S(nil, seed)
	}

	ss := (*reflect.StringHeader)(unsafe.Pointer(&s))
	return Checksum64S((*[maxInt32]byte)(unsafe.Pointer(ss.Data))[:len(s):len(s)], seed)
}
func NewS32(seed uint32) (xx *XXHash32) {
	xx = &XXHash32{
		seed: seed,
	}
	xx.Reset()
	return
}
func NewS64(seed uint64) (xx *XXHash64) {
	xx = &XXHash64{
		seed: seed,
	}
	xx.Reset()
	return
}
func round64(h, v uint64) uint64 {
	h += v * prime64x2
	h = rotl64_31(h)
	h *= prime64x1
	return h
}
func Checksum32S(in []byte, seed uint32) (h uint32) {
	var i int

	if len(in) > 15 {
		var (
			v1 = seed + prime32x1 + prime32x2
			v2 = seed + prime32x2
			v3 = seed + 0
			v4 = seed - prime32x1
		)
		for ; i < len(in)-15; i += 16 {
			in := in[i : i+16 : len(in)]
			v1 += u32(in[0:4:len(in)]) * prime32x2
			v1 = rotl32_13(v1) * prime32x1

			v2 += u32(in[4:8:len(in)]) * prime32x2
			v2 = rotl32_13(v2) * prime32x1

			v3 += u32(in[8:12:len(in)]) * prime32x2
			v3 = rotl32_13(v3) * prime32x1

			v4 += u32(in[12:16:len(in)]) * prime32x2
			v4 = rotl32_13(v4) * prime32x1
		}

		h = rotl32_1(v1) + rotl32_7(v2) + rotl32_12(v3) + rotl32_18(v4)

	} else {
		h = seed + prime32x5
	}

	h += uint32(len(in))
	for ; i <= len(in)-4; i += 4 {
		in := in[i : i+4 : len(in)]
		h += u32(in[0:4:len(in)]) * prime32x3
		h = rotl32_17(h) * prime32x4
	}

	for ; i < len(in); i++ {
		h += uint32(in[i]) * prime32x5
		h = rotl32_11(h) * prime32x1
	}

	h ^= h >> 15
	h *= prime32x2
	h ^= h >> 13
	h *= prime32x3
	h ^= h >> 16

	return
}
func Checksum64S(in []byte, seed uint64) uint64 {
	if len(in) == 0 && seed == 0 {
		return 0xef46db3751d8e999
	}

	if len(in) > 31 {
		return checksum64(in, seed)
	}

	return checksum64Short(in, seed)
}
func getStage() (stage int, advanceStage func() error, resetEnv func() error) {
	var origValue string
	stage = 0

	daemonStage := os.Getenv(stageVar)
	stageTag := strings.SplitN(daemonStage, ":", 2)
	stageInfo := strings.SplitN(stageTag[0], "/", 3)

	if len(stageInfo) == 3 {
		stageStr, tm, check := stageInfo[0], stageInfo[1], stageInfo[2]

		hash := sha1.New()
		hash.Write([]byte(stageStr + "/" + tm + "/"))

		if check != hex.EncodeToString(hash.Sum([]byte{})) {
			// This whole chunk is original data
			origValue = daemonStage
		} else {
			stage, _ = strconv.Atoi(stageStr)

			if len(stageTag) == 2 {
				origValue = stageTag[1]
			}
		}
	} else {
		origValue = daemonStage
	}

	advanceStage = func() error {
		base := fmt.Sprintf("%d/%09d/", stage+1, time.Now().Nanosecond())
		hash := sha1.New()
		hash.Write([]byte(base))
		tag := base + hex.EncodeToString(hash.Sum([]byte{}))

		if err := os.Setenv(stageVar, tag+":"+origValue); err != nil {
			return fmt.Errorf("can't set %s: %s", stageVar, err)
		}
		return nil
	}
	resetEnv = func() error {
		return os.Setenv(stageVar, origValue)
	}

	return stage, advanceStage, resetEnv
}
func New() *Glg {

	g := &Glg{
		levelCounter: new(uint32),
		buffer: sync.Pool{
			New: func() interface{} {
				return bytes.NewBuffer(make([]byte, 0, bufferSize))
			},
		},
	}

	atomic.StoreUint32(g.levelCounter, uint32(FATAL))

	for lev, log := range map[LEVEL]*logger{
		// standard out
		PRINT: {
			std:     os.Stdout,
			color:   Colorless,
			isColor: true,
			mode:    STD,
		},
		LOG: {
			std:     os.Stdout,
			color:   Colorless,
			isColor: true,
			mode:    STD,
		},
		INFO: {
			std:     os.Stdout,
			color:   Green,
			isColor: true,
			mode:    STD,
		},
		DEBG: {
			std:     os.Stdout,
			color:   Purple,
			isColor: true,
			mode:    STD,
		},
		OK: {
			std:     os.Stdout,
			color:   Cyan,
			isColor: true,
			mode:    STD,
		},
		WARN: {
			std:     os.Stdout,
			color:   Orange,
			isColor: true,
			mode:    STD,
		},
		// error out
		ERR: {
			std:     os.Stderr,
			color:   Red,
			isColor: true,
			mode:    STD,
		},
		FAIL: {
			std:     os.Stderr,
			color:   Red,
			isColor: true,
			mode:    STD,
		},
		FATAL: {
			std:     os.Stderr,
			color:   Red,
			isColor: true,
			mode:    STD,
		},
	} {
		log.tag = lev.String()
		log.updateMode()
		g.logger.Store(lev, log)
	}

	return g
}
func Get() *Glg {
	once.Do(func() {
		fastime.SetFormat(timeFormat)
		glg = New()
	})
	return glg
}
func (g *Glg) SetMode(mode MODE) *Glg {
	g.logger.Range(func(key, val interface{}) bool {
		l := val.(*logger)
		l.mode = mode
		l.updateMode()
		g.logger.Store(key.(LEVEL), l)
		return true
	})

	return g
}
func (g *Glg) SetPrefix(pref string) *Glg {
	v, ok := g.logger.Load(PRINT)
	if ok {
		value := v.(*logger)
		value.tag = pref
		g.logger.Store(PRINT, value)
	}
	return g
}
func (g *Glg) GetCurrentMode(level LEVEL) MODE {
	l, ok := g.logger.Load(level)
	if ok {
		return l.(*logger).mode
	}
	return NONE
}
func (g *Glg) InitWriter() *Glg {
	g.logger.Range(func(key, val interface{}) bool {
		l := val.(*logger)
		l.writer = nil
		l.updateMode()
		g.logger.Store(key.(LEVEL), l)
		return true
	})
	return g
}
func (g *Glg) SetWriter(writer io.Writer) *Glg {
	if writer == nil {
		return g
	}

	g.logger.Range(func(key, val interface{}) bool {
		l := val.(*logger)
		l.writer = writer
		l.updateMode()
		g.logger.Store(key.(LEVEL), l)
		return true
	})

	return g
}
func (g *Glg) SetLevelColor(level LEVEL, color func(string) string) *Glg {
	lev, ok := g.logger.Load(level)
	if ok {
		l := lev.(*logger)
		l.color = color
		g.logger.Store(level, l)
	}

	return g
}
func (g *Glg) SetLevelWriter(level LEVEL, writer io.Writer) *Glg {
	if writer == nil {
		return g
	}

	lev, ok := g.logger.Load(level)
	if ok {
		l := lev.(*logger)
		l.writer = writer
		l.updateMode()
		g.logger.Store(level, l)
	}

	return g
}
func (g *Glg) AddStdLevel(tag string, mode MODE, isColor bool) *Glg {
	atomic.AddUint32(g.levelCounter, 1)
	lev := LEVEL(atomic.LoadUint32(g.levelCounter))
	g.levelMap.Store(tag, lev)
	l := &logger{
		writer:  nil,
		std:     os.Stdout,
		color:   Colorless,
		isColor: isColor,
		mode:    mode,
		tag:     tag,
	}
	l.updateMode()
	g.logger.Store(lev, l)
	return g
}
func (g *Glg) EnableColor() *Glg {

	g.logger.Range(func(key, val interface{}) bool {
		l := val.(*logger)
		l.isColor = true
		l.updateMode()
		g.logger.Store(key.(LEVEL), l)
		return true
	})

	return g
}
func (g *Glg) EnableLevelColor(lv LEVEL) *Glg {
	ins, ok := g.logger.Load(lv)
	if ok {
		l := ins.(*logger)
		l.isColor = true
		l.updateMode()
		g.logger.Store(lv, l)
	}
	return g
}
func (g *Glg) DisableLevelColor(lv LEVEL) *Glg {
	ins, ok := g.logger.Load(lv)
	if ok {
		l := ins.(*logger)
		l.isColor = false
		l.updateMode()
		g.logger.Store(lv, l)
	}
	return g
}
func (g *Glg) RawString(data []byte) string {
	str := *(*string)(unsafe.Pointer(&data))
	return str[strings.Index(str, sep)+sepl : len(str)-rcl]
}
func (g *Glg) TagStringToLevel(tag string) LEVEL {
	l, ok := g.levelMap.Load(tag)
	if !ok {
		return 255
	}
	return l.(LEVEL)
}
func Println(val ...interface{}) error {
	return glg.out(PRINT, blankFormat(len(val)), val...)
}
func (g *Glg) Fatal(val ...interface{}) {
	err := g.out(FATAL, blankFormat(len(val)), val...)
	if err != nil {
		err = g.Error(err.Error())
		if err != nil {
			panic(err)
		}
	}
	exit(1)
}
func (g *Glg) Fatalf(format string, val ...interface{}) {
	err := g.out(FATAL, format, val...)
	if err != nil {
		err = g.Error(err.Error())
		if err != nil {
			panic(err)
		}
	}
	exit(1)
}
func (g *Glg) isModeEnable(l LEVEL) bool {
	return g.GetCurrentMode(l) != NONE
}
func CaptureMetrics(hnd http.Handler, w http.ResponseWriter, r *http.Request) Metrics {
	return CaptureMetricsFn(w, func(ww http.ResponseWriter) {
		hnd.ServeHTTP(ww, r)
	})
}
func (da *cedar) get(key []byte, from, pos int) *int {
	for ; pos < len(key); pos++ {
		if value := da.Array[from].Value; value >= 0 && value != ValueLimit {
			to := da.follow(from, 0)
			da.Array[to].Value = value
		}
		from = da.follow(from, key[pos])
	}
	to := from
	if da.Array[from].Value < 0 {
		to = da.follow(from, 0)
	}
	return &da.Array[to].Value
}
func (da *Cedar) Save(out io.Writer, dataType string) error {
	switch dataType {
	case "gob", "GOB":
		dataEecoder := gob.NewEncoder(out)
		return dataEecoder.Encode(da.cedar)
	case "json", "JSON":
		dataEecoder := json.NewEncoder(out)
		return dataEecoder.Encode(da.cedar)
	}
	return ErrInvalidDataType
}
func (da *Cedar) SaveToFile(fileName string, dataType string) error {
	file, err := os.OpenFile(fileName, os.O_CREATE|os.O_WRONLY, 0666)
	if err != nil {
		return err
	}
	defer file.Close()
	out := bufio.NewWriter(file)
	defer out.Flush()
	da.Save(out, dataType)
	return nil
}
func (da *Cedar) Load(in io.Reader, dataType string) error {
	switch dataType {
	case "gob", "GOB":
		dataDecoder := gob.NewDecoder(in)
		return dataDecoder.Decode(da.cedar)
	case "json", "JSON":
		dataDecoder := json.NewDecoder(in)
		return dataDecoder.Decode(da.cedar)
	}
	return ErrInvalidDataType
}
func (da *Cedar) LoadFromFile(fileName string, dataType string) error {
	file, err := os.OpenFile(fileName, os.O_RDONLY, 0600)
	defer file.Close()
	if err != nil {
		return err
	}
	in := bufio.NewReader(file)
	return da.Load(in, dataType)
}
func (da *Cedar) Key(id int) (key []byte, err error) {
	for id > 0 {
		from := da.Array[id].Check
		if from < 0 {
			return nil, ErrNoPath
		}
		if char := byte(da.Array[from].base() ^ id); char != 0 {
			key = append(key, char)
		}
		id = from
	}
	if id != 0 || len(key) == 0 {
		return nil, ErrInvalidKey
	}
	for i := 0; i < len(key)/2; i++ {
		key[i], key[len(key)-i-1] = key[len(key)-i-1], key[i]
	}
	return key, nil
}
func (da *Cedar) Value(id int) (value int, err error) {
	value = da.Array[id].Value
	if value >= 0 {
		return value, nil
	}
	to := da.Array[id].base()
	if da.Array[to].Check == id && da.Array[to].Value >= 0 {
		return da.Array[to].Value, nil
	}
	return 0, ErrNoValue
}
func (da *Cedar) Delete(key []byte) error {
	// if the path does not exist, or the end is not a leaf, nothing to delete
	to, err := da.Jump(key, 0)
	if err != nil {
		return ErrNoPath
	}

	if da.Array[to].Value < 0 {
		base := da.Array[to].base()
		if da.Array[base].Check == to {
			to = base
		}
	}

	for to > 0 {
		from := da.Array[to].Check
		base := da.Array[from].base()
		label := byte(to ^ base)

		// if `to` has sibling, remove `to` from the sibling list, then stop
		if da.Ninfos[to].Sibling != 0 || da.Ninfos[from].Child != label {
			// delete the label from the child ring first
			da.popSibling(from, base, label)
			// then release the current node `to` to the empty node ring
			da.pushEnode(to)
			break
		}
		// otherwise, just release the current node `to` to the empty node ring
		da.pushEnode(to)
		// then check its parent node
		to = from
	}
	return nil
}
func (v *Version) Set(version string) error {
	metadata := splitOff(&version, "+")
	preRelease := PreRelease(splitOff(&version, "-"))
	dotParts := strings.SplitN(version, ".", 3)

	if len(dotParts) != 3 {
		return fmt.Errorf("%s is not in dotted-tri format", version)
	}

	if err := validateIdentifier(string(preRelease)); err != nil {
		return fmt.Errorf("failed to validate pre-release: %v", err)
	}

	if err := validateIdentifier(metadata); err != nil {
		return fmt.Errorf("failed to validate metadata: %v", err)
	}

	parsed := make([]int64, 3, 3)

	for i, v := range dotParts[:3] {
		val, err := strconv.ParseInt(v, 10, 64)
		parsed[i] = val
		if err != nil {
			return err
		}
	}

	v.Metadata = metadata
	v.PreRelease = preRelease
	v.Major = parsed[0]
	v.Minor = parsed[1]
	v.Patch = parsed[2]
	return nil
}
func (v Version) Compare(versionB Version) int {
	if cmp := recursiveCompare(v.Slice(), versionB.Slice()); cmp != 0 {
		return cmp
	}
	return preReleaseCompare(v, versionB)
}
func (v Version) Slice() []int64 {
	return []int64{v.Major, v.Minor, v.Patch}
}
func (v *Version) BumpMajor() {
	v.Major += 1
	v.Minor = 0
	v.Patch = 0
	v.PreRelease = PreRelease("")
	v.Metadata = ""
}
func (v *Version) BumpMinor() {
	v.Minor += 1
	v.Patch = 0
	v.PreRelease = PreRelease("")
	v.Metadata = ""
}
func (v *Version) BumpPatch() {
	v.Patch += 1
	v.PreRelease = PreRelease("")
	v.Metadata = ""
}
func validateIdentifier(id string) error {
	if id != "" && !reIdentifier.MatchString(id) {
		return fmt.Errorf("%s is not a valid semver identifier", id)
	}
	return nil
}
func newStream(bufsize int, replay bool) *Stream {
	return &Stream{
		AutoReplay:  replay,
		subscribers: make([]*Subscriber, 0),
		register:    make(chan *Subscriber),
		deregister:  make(chan *Subscriber),
		event:       make(chan *Event, bufsize),
		quit:        make(chan bool),
		Eventlog:    make(EventLog, 0),
	}
}
func (str *Stream) addSubscriber(eventid string) *Subscriber {
	sub := &Subscriber{
		eventid:    eventid,
		quit:       str.deregister,
		connection: make(chan *Event, 64),
	}

	str.register <- sub
	return sub
}
func New() *Server {
	return &Server{
		BufferSize: DefaultBufferSize,
		AutoStream: false,
		AutoReplay: true,
		Streams:    make(map[string]*Stream),
	}
}
func (s *Server) Close() {
	s.mu.Lock()
	defer s.mu.Unlock()

	for id := range s.Streams {
		s.Streams[id].quit <- true
		delete(s.Streams, id)
	}
}
func (s *Server) CreateStream(id string) *Stream {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.Streams[id] != nil {
		return s.Streams[id]
	}

	str := newStream(s.BufferSize, s.AutoReplay)
	str.run()

	s.Streams[id] = str

	return str
}
func (s *Server) RemoveStream(id string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.Streams[id] != nil {
		s.Streams[id].close()
		delete(s.Streams, id)
	}
}
func (s *Server) StreamExists(id string) bool {
	s.mu.Lock()
	defer s.mu.Unlock()

	return s.Streams[id] != nil
}
func (s *Server) Publish(id string, event *Event) {
	s.mu.Lock()
	defer s.mu.Unlock()
	if s.Streams[id] != nil {
		s.Streams[id].event <- s.process(event)
	}
}
func NewClient(url string) *Client {
	return &Client{
		URL:        url,
		Connection: &http.Client{},
		Headers:    make(map[string]string),
		subscribed: make(map[chan *Event]chan bool),
	}
}
func (c *Client) Subscribe(stream string, handler func(msg *Event)) error {
	operation := func() error {
		resp, err := c.request(stream)
		if err != nil {
			return err
		}
		defer resp.Body.Close()

		reader := NewEventStreamReader(resp.Body)

		for {
			// Read each new line and process the type of event
			event, err := reader.ReadEvent()
			if err != nil {
				if err == io.EOF {
					return nil
				}

				// run user specified disconnect function
				if c.disconnectcb != nil {
					c.disconnectcb(c)
				}

				return err
			}

			// If we get an error, ignore it.
			if msg, err := c.processEvent(event); err == nil {
				if len(msg.ID) > 0 {
					c.EventID = string(msg.ID)
				} else {
					msg.ID = []byte(c.EventID)
				}

				handler(msg)
			}
		}
	}
	return backoff.Retry(operation, backoff.NewExponentialBackOff())
}
func (c *Client) SubscribeChan(stream string, ch chan *Event) error {
	var connected bool
	errch := make(chan error)
	c.mu.Lock()
	c.subscribed[ch] = make(chan bool)
	c.mu.Unlock()

	go func() {
		operation := func() error {
			resp, err := c.request(stream)
			if err != nil {
				c.cleanup(resp, ch)
				return err
			}

			if resp.StatusCode != 200 {
				c.cleanup(resp, ch)
				return errors.New("could not connect to stream")
			}

			if !connected {
				errch <- nil
				connected = true
			}

			reader := NewEventStreamReader(resp.Body)

			for {
				// Read each new line and process the type of event
				event, err := reader.ReadEvent()
				if err != nil {
					if err == io.EOF {
						c.cleanup(resp, ch)
						return nil
					}

					// run user specified disconnect function
					if c.disconnectcb != nil {
						c.disconnectcb(c)
					}

					return err
				}

				// If we get an error, ignore it.
				if msg, err := c.processEvent(event); err == nil {
					if len(msg.ID) > 0 {
						c.EventID = string(msg.ID)
					} else {
						msg.ID = []byte(c.EventID)
					}

					select {
					case <-c.subscribed[ch]:
						c.cleanup(resp, ch)
						return nil
					case ch <- msg:
						// message sent
					}
				}
			}
		}

		err := backoff.Retry(operation, backoff.NewExponentialBackOff())
		if err != nil && !connected {
			errch <- err
		}
	}()
	err := <-errch
	close(errch)

	return err
}
func (c *Client) SubscribeRaw(handler func(msg *Event)) error {
	return c.Subscribe("", handler)
}
func (c *Client) Unsubscribe(ch chan *Event) {
	c.mu.Lock()
	defer c.mu.Unlock()

	if c.subscribed[ch] != nil {
		c.subscribed[ch] <- true
	}
}
func NewEventStreamReader(eventStream io.Reader) *EventStreamReader {
	scanner := bufio.NewScanner(eventStream)
	split := func(data []byte, atEOF bool) (int, []byte, error) {
		if atEOF && len(data) == 0 {
			return 0, nil, nil
		}

		// We have a full event payload to parse.
		if i := bytes.Index(data, []byte("\r\n\r\n")); i >= 0 {
			return i + 1, data[0:i], nil
		}
		if i := bytes.Index(data, []byte("\r\r")); i >= 0 {
			return i + 1, data[0:i], nil
		}
		if i := bytes.Index(data, []byte("\n\n")); i >= 0 {
			return i + 1, data[0:i], nil
		}
		// If we're at EOF, we have all of the data.
		if atEOF {
			return len(data), data, nil
		}
		// Request more data.
		return 0, nil, nil
	}
	// Set the split function for the scanning operation.
	scanner.Split(split)

	return &EventStreamReader{
		scanner: scanner,
	}
}
func (e *EventStreamReader) ReadEvent() ([]byte, error) {
	if e.scanner.Scan() {
		event := e.scanner.Bytes()
		return event, nil
	}
	if err := e.scanner.Err(); err != nil {
		return nil, err
	}
	return nil, io.EOF
}
func (s *Server) HTTPHandler(w http.ResponseWriter, r *http.Request) {
	flusher, err := w.(http.Flusher)
	if !err {
		http.Error(w, "Streaming unsupported!", http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")
	w.Header().Set("Access-Control-Allow-Origin", "*")

	// Get the StreamID from the URL
	streamID := r.URL.Query().Get("stream")
	if streamID == "" {
		http.Error(w, "Please specify a stream!", http.StatusInternalServerError)
		return
	}

	stream := s.getStream(streamID)

	if stream == nil && !s.AutoStream {
		http.Error(w, "Stream not found!", http.StatusInternalServerError)
		return
	} else if stream == nil && s.AutoStream {
		stream = s.CreateStream(streamID)
	}

	eventid := r.Header.Get("Last-Event-ID")
	if eventid == "" {
		eventid = "0"
	}

	// Create the stream subscriber
	sub := stream.addSubscriber(eventid)
	defer sub.close()

	notify := w.(http.CloseNotifier).CloseNotify()
	go func() {
		<-notify
		sub.close()
	}()

	// Push events to client
	for {
		select {
		case ev, ok := <-sub.connection:
			if !ok {
				return
			}

			// If the data buffer is an empty string abort.
			if len(ev.Data) == 0 {
				break
			}

			// if the event has expired, dont send it
			if s.EventTTL != 0 && time.Now().After(ev.timestamp.Add(s.EventTTL)) {
				continue
			}

			fmt.Fprintf(w, "id: %s\n", ev.ID)
			fmt.Fprintf(w, "data: %s\n", ev.Data)
			if len(ev.Event) > 0 {
				fmt.Fprintf(w, "event: %s\n", ev.Event)
			}
			if len(ev.Retry) > 0 {
				fmt.Fprintf(w, "retry: %s\n", ev.Retry)
			}
			fmt.Fprint(w, "\n")
			flusher.Flush()
		}
	}
}
func (e *EventLog) Add(ev *Event) {
	ev.ID = []byte(e.currentindex())
	ev.timestamp = time.Now()
	(*e) = append((*e), ev)
}
func (e *EventLog) Replay(s *Subscriber) {
	for i := 0; i < len((*e)); i++ {
		if string((*e)[i].ID) >= s.eventid {
			s.connection <- (*e)[i]
		}
	}
}
func readKey(path string) (crypto.Signer, error) {
	b, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}
	d, _ := pem.Decode(b)
	if d == nil {
		return nil, fmt.Errorf("no block found in %q", path)
	}
	switch d.Type {
	case rsaPrivateKey:
		return x509.ParsePKCS1PrivateKey(d.Bytes)
	case ecPrivateKey:
		return x509.ParseECPrivateKey(d.Bytes)
	default:
		return nil, fmt.Errorf("%q is unsupported", d.Type)
	}
}
func writeKey(path string, k *ecdsa.PrivateKey) error {
	f, err := os.OpenFile(path, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)
	if err != nil {
		return err
	}
	bytes, err := x509.MarshalECPrivateKey(k)
	if err != nil {
		return err
	}
	b := &pem.Block{Type: ecPrivateKey, Bytes: bytes}
	if err := pem.Encode(f, b); err != nil {
		f.Close()
		return err
	}
	return f.Close()
}
func anyKey(filename string, gen bool) (crypto.Signer, error) {
	k, err := readKey(filename)
	if err == nil {
		return k, nil
	}
	if !os.IsNotExist(err) || !gen {
		return nil, err
	}
	ecKey, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)
	if err != nil {
		return nil, err
	}
	return ecKey, writeKey(filename, ecKey)
}
func sameDir(existing, filename string) string {
	return filepath.Join(filepath.Dir(existing), filename)
}
func printAccount(w io.Writer, a *acme.Account, kp string) {
	tw := tabwriter.NewWriter(w, 0, 8, 0, '\t', 0)
	fmt.Fprintln(tw, "URI:\t", a.URI)
	fmt.Fprintln(tw, "Key:\t", kp)
	fmt.Fprintln(tw, "Contact:\t", strings.Join(a.Contact, ", "))
	fmt.Fprintln(tw, "Terms:\t", a.CurrentTerms)
	agreed := a.AgreedTerms
	if a.AgreedTerms == "" {
		agreed = "no"
	} else if a.AgreedTerms == a.CurrentTerms {
		agreed = "yes"
	}
	fmt.Fprintln(tw, "Accepted:\t", agreed)
	// TODO: print authorization and certificates
	tw.Flush()
}
func tmpl(w io.Writer, text string, data interface{}) {
	t := template.New("top")
	t.Funcs(template.FuncMap{
		"trim":       strings.TrimSpace,
		"capitalize": capitalize,
	})
	template.Must(t.Parse(text))
	ew := &errWriter{w: w}
	err := t.Execute(ew, data)
	if ew.err != nil {
		// I/O error writing; ignore write on closed pipe
		if strings.Contains(ew.err.Error(), "pipe") {
			os.Exit(1)
		}
		fatalf("writing output: %v", ew.err)
	}
	if err != nil {
		panic(err)
	}
}
func printUsage(w io.Writer) {
	bw := bufio.NewWriter(w)
	tmpl(bw, usageTemplate, commands)
	bw.Flush()
}
func FromRequest(r *http.Request) string {
	// Fetch header value
	xRealIP := r.Header.Get("X-Real-Ip")
	xForwardedFor := r.Header.Get("X-Forwarded-For")

	// If both empty, return IP from remote address
	if xRealIP == "" && xForwardedFor == "" {
		var remoteIP string

		// If there are colon in remote address, remove the port number
		// otherwise, return remote address as is
		if strings.ContainsRune(r.RemoteAddr, ':') {
			remoteIP, _, _ = net.SplitHostPort(r.RemoteAddr)
		} else {
			remoteIP = r.RemoteAddr
		}

		return remoteIP
	}

	// Check list of IP in X-Forwarded-For and return the first global address
	for _, address := range strings.Split(xForwardedFor, ",") {
		address = strings.TrimSpace(address)
		isPrivate, err := isPrivateAddress(address)
		if !isPrivate && err == nil {
			return address
		}
	}

	// If nothing succeed, return X-Real-IP
	return xRealIP
}
func (p *ClearParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClear, p, nil)
}
func (p *DisableParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDisable, nil, nil)
}
func (p *RemoveDOMStorageItemParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveDOMStorageItem, p, nil)
}
func (p *SetDOMStorageItemParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDOMStorageItem, p, nil)
}
func (p *DeliverPushMessageParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDeliverPushMessage, p, nil)
}
func (p *DispatchSyncEventParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDispatchSyncEvent, p, nil)
}
func (p *InspectWorkerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandInspectWorker, p, nil)
}
func (p *SetForceUpdateOnPageLoadParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetForceUpdateOnPageLoad, p, nil)
}
func (p *SkipWaitingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSkipWaiting, p, nil)
}
func (p *StartWorkerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartWorker, p, nil)
}
func (p *StopAllWorkersParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopAllWorkers, nil, nil)
}
func (p *StopWorkerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopWorker, p, nil)
}
func (p *UnregisterParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandUnregister, p, nil)
}
func (p *UpdateRegistrationParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandUpdateRegistration, p, nil)
}
func (p *BindParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandBind, p, nil)
}
func (p *UnbindParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandUnbind, p, nil)
}
func (e *ExceptionDetails) Error() string {
	// TODO: watch script parsed events and match the ExceptionDetails.ScriptID
	// to the name/location of the actual code and display here
	return fmt.Sprintf("encountered exception '%s' (%d:%d)", e.Text, e.LineNumber, e.ColumnNumber)
}
func (p *ReleaseAnimationsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandReleaseAnimations, p, nil)
}
func (p *SeekAnimationsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSeekAnimations, p, nil)
}
func (p *SetPausedParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetPaused, p, nil)
}
func (p *SetPlaybackRateParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetPlaybackRate, p, nil)
}
func (p *SetTimingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetTiming, p, nil)
}
func (p *PrepareForLeakDetectionParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandPrepareForLeakDetection, nil, nil)
}
func (p *ForciblyPurgeJavaScriptMemoryParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandForciblyPurgeJavaScriptMemory, nil, nil)
}
func (p *SetPressureNotificationsSuppressedParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetPressureNotificationsSuppressed, p, nil)
}
func (p *SimulatePressureNotificationParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSimulatePressureNotification, p, nil)
}
func (p StartSamplingParams) WithSamplingInterval(samplingInterval int64) *StartSamplingParams {
	p.SamplingInterval = samplingInterval
	return &p
}
func (p StartSamplingParams) WithSuppressRandomness(suppressRandomness bool) *StartSamplingParams {
	p.SuppressRandomness = suppressRandomness
	return &p
}
func (p *StartSamplingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartSampling, p, nil)
}
func (p *StopSamplingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopSampling, nil, nil)
}
func (p *ClearDeviceOrientationOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearDeviceOrientationOverride, nil, nil)
}
func (p *SetDeviceOrientationOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDeviceOrientationOverride, p, nil)
}
func (p *StartViolationsReportParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartViolationsReport, p, nil)
}
func (p *StopViolationsReportParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopViolationsReport, nil, nil)
}
func (t Modifier) String() string {
	switch t {
	case ModifierNone:
		return "None"
	case ModifierAlt:
		return "Alt"
	case ModifierCtrl:
		return "Ctrl"
	case ModifierMeta:
		return "Meta"
	case ModifierShift:
		return "Shift"
	}

	return fmt.Sprintf("Modifier(%d)", t)
}
func (p GetPartialAXTreeParams) WithNodeID(nodeID cdp.NodeID) *GetPartialAXTreeParams {
	p.NodeID = nodeID
	return &p
}
func (p GetPartialAXTreeParams) WithBackendNodeID(backendNodeID cdp.BackendNodeID) *GetPartialAXTreeParams {
	p.BackendNodeID = backendNodeID
	return &p
}
func (p GetPartialAXTreeParams) WithObjectID(objectID runtime.RemoteObjectID) *GetPartialAXTreeParams {
	p.ObjectID = objectID
	return &p
}
func (p GetPartialAXTreeParams) WithFetchRelatives(fetchRelatives bool) *GetPartialAXTreeParams {
	p.FetchRelatives = fetchRelatives
	return &p
}
func (p *SetTimeDomainParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetTimeDomain, p, nil)
}
func (p ProfileSnapshotParams) WithClipRect(clipRect *dom.Rect) *ProfileSnapshotParams {
	p.ClipRect = clipRect
	return &p
}
func (p *ReleaseSnapshotParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandReleaseSnapshot, p, nil)
}
func (p *ClearObjectStoreParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearObjectStore, p, nil)
}
func (p *DeleteDatabaseParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDeleteDatabase, p, nil)
}
func (p *DeleteObjectStoreEntriesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDeleteObjectStoreEntries, p, nil)
}
func (p RequestDataParams) WithKeyRange(keyRange *KeyRange) *RequestDataParams {
	p.KeyRange = keyRange
	return &p
}
func (p *SetSamplingIntervalParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetSamplingInterval, p, nil)
}
func (p *StartParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStart, nil, nil)
}
func (p StartPreciseCoverageParams) WithCallCount(callCount bool) *StartPreciseCoverageParams {
	p.CallCount = callCount
	return &p
}
func (p StartPreciseCoverageParams) WithDetailed(detailed bool) *StartPreciseCoverageParams {
	p.Detailed = detailed
	return &p
}
func (p *StartPreciseCoverageParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartPreciseCoverage, p, nil)
}
func (p *StartTypeProfileParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartTypeProfile, nil, nil)
}
func (p *StopPreciseCoverageParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopPreciseCoverage, nil, nil)
}
func (p *StopTypeProfileParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopTypeProfile, nil, nil)
}
func (p *SetIgnoreCertificateErrorsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetIgnoreCertificateErrors, p, nil)
}
func (p *AddInspectedHeapObjectParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandAddInspectedHeapObject, p, nil)
}
func (p *CollectGarbageParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandCollectGarbage, nil, nil)
}
func (p StartSamplingParams) WithSamplingInterval(samplingInterval float64) *StartSamplingParams {
	p.SamplingInterval = samplingInterval
	return &p
}
func (p *StartTrackingHeapObjectsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartTrackingHeapObjects, p, nil)
}
func (p StopTrackingHeapObjectsParams) WithReportProgress(reportProgress bool) *StopTrackingHeapObjectsParams {
	p.ReportProgress = reportProgress
	return &p
}
func (p *StopTrackingHeapObjectsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopTrackingHeapObjects, p, nil)
}
func (p TakeHeapSnapshotParams) WithReportProgress(reportProgress bool) *TakeHeapSnapshotParams {
	p.ReportProgress = reportProgress
	return &p
}
func (p *TakeHeapSnapshotParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandTakeHeapSnapshot, p, nil)
}
func (p GrantPermissionsParams) WithBrowserContextID(browserContextID target.BrowserContextID) *GrantPermissionsParams {
	p.BrowserContextID = browserContextID
	return &p
}
func (p *GrantPermissionsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandGrantPermissions, p, nil)
}
func (p ResetPermissionsParams) WithBrowserContextID(browserContextID target.BrowserContextID) *ResetPermissionsParams {
	p.BrowserContextID = browserContextID
	return &p
}
func (p *ResetPermissionsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandResetPermissions, p, nil)
}
func (p *CrashParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandCrash, nil, nil)
}
func (p *CrashGpuProcessParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandCrashGpuProcess, nil, nil)
}
func (p GetHistogramsParams) WithQuery(query string) *GetHistogramsParams {
	p.Query = query
	return &p
}
func (p GetWindowForTargetParams) WithTargetID(targetID target.ID) *GetWindowForTargetParams {
	p.TargetID = targetID
	return &p
}
func (p *SetWindowBoundsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetWindowBounds, p, nil)
}
func (p SetDockTileParams) WithImage(image string) *SetDockTileParams {
	p.Image = image
	return &p
}
func (p *SetDockTileParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDockTile, p, nil)
}
func (p *DeleteCacheParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDeleteCache, p, nil)
}
func (p *DeleteEntryParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDeleteEntry, p, nil)
}
func (p RequestEntriesParams) WithPathFilter(pathFilter string) *RequestEntriesParams {
	p.PathFilter = pathFilter
	return &p
}
func (p *DiscardSearchResultsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDiscardSearchResults, p, nil)
}
func (p *FocusParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandFocus, p, nil)
}
func (p *MarkUndoableStateParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandMarkUndoableState, nil, nil)
}
func (p PerformSearchParams) WithIncludeUserAgentShadowDOM(includeUserAgentShadowDOM bool) *PerformSearchParams {
	p.IncludeUserAgentShadowDOM = includeUserAgentShadowDOM
	return &p
}
func (p *RedoParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRedo, nil, nil)
}
func (p *RemoveAttributeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveAttribute, p, nil)
}
func (p *RemoveNodeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveNode, p, nil)
}
func (p *RequestChildNodesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRequestChildNodes, p, nil)
}
func (p ResolveNodeParams) WithNodeID(nodeID cdp.NodeID) *ResolveNodeParams {
	p.NodeID = nodeID
	return &p
}
func (p ResolveNodeParams) WithBackendNodeID(backendNodeID cdp.BackendNodeID) *ResolveNodeParams {
	p.BackendNodeID = backendNodeID
	return &p
}
func (p ResolveNodeParams) WithExecutionContextID(executionContextID runtime.ExecutionContextID) *ResolveNodeParams {
	p.ExecutionContextID = executionContextID
	return &p
}
func (p *SetAttributeValueParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetAttributeValue, p, nil)
}
func (p SetAttributesAsTextParams) WithName(name string) *SetAttributesAsTextParams {
	p.Name = name
	return &p
}
func (p *SetAttributesAsTextParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetAttributesAsText, p, nil)
}
func (p *SetFileInputFilesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetFileInputFiles, p, nil)
}
func (p *SetInspectedNodeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetInspectedNode, p, nil)
}
func (p *SetNodeValueParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetNodeValue, p, nil)
}
func (p *SetOuterHTMLParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetOuterHTML, p, nil)
}
func (p *UndoParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandUndo, nil, nil)
}
func (p *ForcePseudoStateParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandForcePseudoState, p, nil)
}
func (p *SetEffectivePropertyValueForNodeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetEffectivePropertyValueForNode, p, nil)
}
func (p *StartRuleUsageTrackingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartRuleUsageTracking, nil, nil)
}
func (p *CloseParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClose, p, nil)
}
func (p *ClearDataForOriginParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearDataForOrigin, p, nil)
}
func (p *TrackCacheStorageForOriginParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandTrackCacheStorageForOrigin, p, nil)
}
func (p *TrackIndexedDBForOriginParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandTrackIndexedDBForOrigin, p, nil)
}
func (p *UntrackCacheStorageForOriginParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandUntrackCacheStorageForOrigin, p, nil)
}
func (p *UntrackIndexedDBForOriginParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandUntrackIndexedDBForOrigin, p, nil)
}
func (p *HideHighlightParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandHideHighlight, nil, nil)
}
func (p *HighlightFrameParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandHighlightFrame, p, nil)
}
func (p HighlightNodeParams) WithNodeID(nodeID cdp.NodeID) *HighlightNodeParams {
	p.NodeID = nodeID
	return &p
}
func (p HighlightNodeParams) WithBackendNodeID(backendNodeID cdp.BackendNodeID) *HighlightNodeParams {
	p.BackendNodeID = backendNodeID
	return &p
}
func (p HighlightNodeParams) WithObjectID(objectID runtime.RemoteObjectID) *HighlightNodeParams {
	p.ObjectID = objectID
	return &p
}
func (p HighlightNodeParams) WithSelector(selector string) *HighlightNodeParams {
	p.Selector = selector
	return &p
}
func (p *HighlightNodeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandHighlightNode, p, nil)
}
func (p *HighlightQuadParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandHighlightQuad, p, nil)
}
func (p *HighlightRectParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandHighlightRect, p, nil)
}
func (p SetInspectModeParams) WithHighlightConfig(highlightConfig *HighlightConfig) *SetInspectModeParams {
	p.HighlightConfig = highlightConfig
	return &p
}
func (p *SetInspectModeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetInspectMode, p, nil)
}
func (p *SetShowAdHighlightsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowAdHighlights, p, nil)
}
func (p SetPausedInDebuggerMessageParams) WithMessage(message string) *SetPausedInDebuggerMessageParams {
	p.Message = message
	return &p
}
func (p *SetPausedInDebuggerMessageParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetPausedInDebuggerMessage, p, nil)
}
func (p *SetShowDebugBordersParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowDebugBorders, p, nil)
}
func (p *SetShowFPSCounterParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowFPSCounter, p, nil)
}
func (p *SetShowPaintRectsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowPaintRects, p, nil)
}
func (p *SetShowScrollBottleneckRectsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowScrollBottleneckRects, p, nil)
}
func (p *SetShowHitTestBordersParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowHitTestBorders, p, nil)
}
func (p *SetShowViewportSizeOnResizeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetShowViewportSizeOnResize, p, nil)
}
func (p *ClearBrowserCacheParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearBrowserCache, nil, nil)
}
func (p *ClearBrowserCookiesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearBrowserCookies, nil, nil)
}
func (p ContinueInterceptedRequestParams) WithErrorReason(errorReason ErrorReason) *ContinueInterceptedRequestParams {
	p.ErrorReason = errorReason
	return &p
}
func (p ContinueInterceptedRequestParams) WithRawResponse(rawResponse string) *ContinueInterceptedRequestParams {
	p.RawResponse = rawResponse
	return &p
}
func (p ContinueInterceptedRequestParams) WithURL(url string) *ContinueInterceptedRequestParams {
	p.URL = url
	return &p
}
func (p ContinueInterceptedRequestParams) WithMethod(method string) *ContinueInterceptedRequestParams {
	p.Method = method
	return &p
}
func (p ContinueInterceptedRequestParams) WithPostData(postData string) *ContinueInterceptedRequestParams {
	p.PostData = postData
	return &p
}
func (p ContinueInterceptedRequestParams) WithHeaders(headers Headers) *ContinueInterceptedRequestParams {
	p.Headers = headers
	return &p
}
func (p ContinueInterceptedRequestParams) WithAuthChallengeResponse(authChallengeResponse *AuthChallengeResponse) *ContinueInterceptedRequestParams {
	p.AuthChallengeResponse = authChallengeResponse
	return &p
}
func (p *ContinueInterceptedRequestParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandContinueInterceptedRequest, p, nil)
}
func (p DeleteCookiesParams) WithURL(url string) *DeleteCookiesParams {
	p.URL = url
	return &p
}
func (p DeleteCookiesParams) WithDomain(domain string) *DeleteCookiesParams {
	p.Domain = domain
	return &p
}
func (p DeleteCookiesParams) WithPath(path string) *DeleteCookiesParams {
	p.Path = path
	return &p
}
func (p *DeleteCookiesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDeleteCookies, p, nil)
}
func (p EmulateNetworkConditionsParams) WithConnectionType(connectionType ConnectionType) *EmulateNetworkConditionsParams {
	p.ConnectionType = connectionType
	return &p
}
func (p *EmulateNetworkConditionsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandEmulateNetworkConditions, p, nil)
}
func (p *EnableParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandEnable, p, nil)
}
func (p GetCookiesParams) WithUrls(urls []string) *GetCookiesParams {
	p.Urls = urls
	return &p
}
func (p *ReplayXHRParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandReplayXHR, p, nil)
}
func (p *SetBlockedURLSParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetBlockedURLS, p, nil)
}
func (p *SetBypassServiceWorkerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetBypassServiceWorker, p, nil)
}
func (p *SetCacheDisabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetCacheDisabled, p, nil)
}
func (p SetCookieParams) WithURL(url string) *SetCookieParams {
	p.URL = url
	return &p
}
func (p SetCookieParams) WithDomain(domain string) *SetCookieParams {
	p.Domain = domain
	return &p
}
func (p SetCookieParams) WithPath(path string) *SetCookieParams {
	p.Path = path
	return &p
}
func (p SetCookieParams) WithSecure(secure bool) *SetCookieParams {
	p.Secure = secure
	return &p
}
func (p SetCookieParams) WithHTTPOnly(httpOnly bool) *SetCookieParams {
	p.HTTPOnly = httpOnly
	return &p
}
func (p SetCookieParams) WithSameSite(sameSite CookieSameSite) *SetCookieParams {
	p.SameSite = sameSite
	return &p
}
func (p SetCookieParams) WithExpires(expires *cdp.TimeSinceEpoch) *SetCookieParams {
	p.Expires = expires
	return &p
}
func (p *SetCookiesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetCookies, p, nil)
}
func (p *SetDataSizeLimitsForTestParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDataSizeLimitsForTest, p, nil)
}
func (p *SetExtraHTTPHeadersParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetExtraHTTPHeaders, p, nil)
}
func (p *SetRequestInterceptionParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetRequestInterception, p, nil)
}
func (p *ClearDeviceMetricsOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearDeviceMetricsOverride, nil, nil)
}
func (p *ClearGeolocationOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearGeolocationOverride, nil, nil)
}
func (p *ResetPageScaleFactorParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandResetPageScaleFactor, nil, nil)
}
func (p *SetFocusEmulationEnabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetFocusEmulationEnabled, p, nil)
}
func (p *SetCPUThrottlingRateParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetCPUThrottlingRate, p, nil)
}
func (p SetDefaultBackgroundColorOverrideParams) WithColor(color *cdp.RGBA) *SetDefaultBackgroundColorOverrideParams {
	p.Color = color
	return &p
}
func (p *SetDefaultBackgroundColorOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDefaultBackgroundColorOverride, p, nil)
}
func (p SetDeviceMetricsOverrideParams) WithScale(scale float64) *SetDeviceMetricsOverrideParams {
	p.Scale = scale
	return &p
}
func (p SetDeviceMetricsOverrideParams) WithDontSetVisibleSize(dontSetVisibleSize bool) *SetDeviceMetricsOverrideParams {
	p.DontSetVisibleSize = dontSetVisibleSize
	return &p
}
func (p SetDeviceMetricsOverrideParams) WithScreenOrientation(screenOrientation *ScreenOrientation) *SetDeviceMetricsOverrideParams {
	p.ScreenOrientation = screenOrientation
	return &p
}
func (p *SetDeviceMetricsOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDeviceMetricsOverride, p, nil)
}
func (p *SetScrollbarsHiddenParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetScrollbarsHidden, p, nil)
}
func (p *SetDocumentCookieDisabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDocumentCookieDisabled, p, nil)
}
func (p *SetEmitTouchEventsForMouseParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetEmitTouchEventsForMouse, p, nil)
}
func (p *SetEmulatedMediaParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetEmulatedMedia, p, nil)
}
func (p SetGeolocationOverrideParams) WithLatitude(latitude float64) *SetGeolocationOverrideParams {
	p.Latitude = latitude
	return &p
}
func (p SetGeolocationOverrideParams) WithLongitude(longitude float64) *SetGeolocationOverrideParams {
	p.Longitude = longitude
	return &p
}
func (p SetGeolocationOverrideParams) WithAccuracy(accuracy float64) *SetGeolocationOverrideParams {
	p.Accuracy = accuracy
	return &p
}
func (p *SetGeolocationOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetGeolocationOverride, p, nil)
}
func (p *SetPageScaleFactorParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetPageScaleFactor, p, nil)
}
func (p *SetScriptExecutionDisabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetScriptExecutionDisabled, p, nil)
}
func (p SetTouchEmulationEnabledParams) WithMaxTouchPoints(maxTouchPoints int64) *SetTouchEmulationEnabledParams {
	p.MaxTouchPoints = maxTouchPoints
	return &p
}
func (p *SetTouchEmulationEnabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetTouchEmulationEnabled, p, nil)
}
func (p SetVirtualTimePolicyParams) WithBudget(budget float64) *SetVirtualTimePolicyParams {
	p.Budget = budget
	return &p
}
func (p SetVirtualTimePolicyParams) WithMaxVirtualTimeTaskStarvationCount(maxVirtualTimeTaskStarvationCount int64) *SetVirtualTimePolicyParams {
	p.MaxVirtualTimeTaskStarvationCount = maxVirtualTimeTaskStarvationCount
	return &p
}
func (p SetVirtualTimePolicyParams) WithWaitForNavigation(waitForNavigation bool) *SetVirtualTimePolicyParams {
	p.WaitForNavigation = waitForNavigation
	return &p
}
func (p SetUserAgentOverrideParams) WithAcceptLanguage(acceptLanguage string) *SetUserAgentOverrideParams {
	p.AcceptLanguage = acceptLanguage
	return &p
}
func (p SetUserAgentOverrideParams) WithPlatform(platform string) *SetUserAgentOverrideParams {
	p.Platform = platform
	return &p
}
func (p *SetUserAgentOverrideParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetUserAgentOverride, p, nil)
}
func (p *DispatchKeyEventParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDispatchKeyEvent, p, nil)
}
func (p *InsertTextParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandInsertText, p, nil)
}
func (p DispatchMouseEventParams) WithButtons(buttons int64) *DispatchMouseEventParams {
	p.Buttons = buttons
	return &p
}
func (p *DispatchMouseEventParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDispatchMouseEvent, p, nil)
}
func (p *DispatchTouchEventParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDispatchTouchEvent, p, nil)
}
func (p *EmulateTouchFromMouseEventParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandEmulateTouchFromMouseEvent, p, nil)
}
func (p *SetIgnoreInputEventsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetIgnoreInputEvents, p, nil)
}
func (p *SynthesizePinchGestureParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSynthesizePinchGesture, p, nil)
}
func (p SynthesizeScrollGestureParams) WithXOverscroll(xOverscroll float64) *SynthesizeScrollGestureParams {
	p.XOverscroll = xOverscroll
	return &p
}
func (p SynthesizeScrollGestureParams) WithYOverscroll(yOverscroll float64) *SynthesizeScrollGestureParams {
	p.YOverscroll = yOverscroll
	return &p
}
func (p *SynthesizeScrollGestureParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSynthesizeScrollGesture, p, nil)
}
func (p *SynthesizeTapGestureParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSynthesizeTapGesture, p, nil)
}
func (t MethodType) Domain() string {
	return string(t[:strings.IndexByte(string(t), '.')])
}
func (p GetEventListenersParams) WithDepth(depth int64) *GetEventListenersParams {
	p.Depth = depth
	return &p
}
func (p *RemoveDOMBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveDOMBreakpoint, p, nil)
}
func (p RemoveEventListenerBreakpointParams) WithTargetName(targetName string) *RemoveEventListenerBreakpointParams {
	p.TargetName = targetName
	return &p
}
func (p *RemoveEventListenerBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveEventListenerBreakpoint, p, nil)
}
func (p *RemoveInstrumentationBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveInstrumentationBreakpoint, p, nil)
}
func (p *RemoveXHRBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveXHRBreakpoint, p, nil)
}
func (p *SetDOMBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDOMBreakpoint, p, nil)
}
func (p *SetEventListenerBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetEventListenerBreakpoint, p, nil)
}
func (p *SetInstrumentationBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetInstrumentationBreakpoint, p, nil)
}
func (p *SetXHRBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetXHRBreakpoint, p, nil)
}
func WithExecutor(parent context.Context, executor Executor) context.Context {
	return context.WithValue(parent, executorKey, executor)
}
func Execute(ctx context.Context, method string, params easyjson.Marshaler, res easyjson.Unmarshaler) error {
	if executor := ctx.Value(executorKey); executor != nil {
		return executor.(Executor).Execute(ctx, method, params, res)
	}
	return ErrInvalidContext
}
func (n *Node) AttributeValue(name string) string {
	n.RLock()
	defer n.RUnlock()

	for i := 0; i < len(n.Attributes); i += 2 {
		if n.Attributes[i] == name {
			return n.Attributes[i+1]
		}
	}

	return ""
}
func (n *Node) xpath(stopAtDocument, stopAtID bool) string {
	n.RLock()
	defer n.RUnlock()

	p := ""
	pos := ""
	id := n.AttributeValue("id")
	switch {
	case n.Parent == nil:
		return n.LocalName

	case stopAtDocument && n.NodeType == NodeTypeDocument:
		return ""

	case stopAtID && id != "":
		p = "/"
		pos = `[@id='` + id + `']`

	case n.Parent != nil:
		var i int
		var found bool

		n.Parent.RLock()
		for j := 0; j < len(n.Parent.Children); j++ {
			if n.Parent.Children[j].LocalName == n.LocalName {
				i++
			}
			if n.Parent.Children[j].NodeID == n.NodeID {
				found = true
				break
			}
		}
		n.Parent.RUnlock()

		if found {
			pos = "[" + strconv.Itoa(i) + "]"
		}

		p = n.Parent.xpath(stopAtDocument, stopAtID)
	}

	return p + "/" + n.LocalName + pos
}
func (t NodeType) String() string {
	switch t {
	case NodeTypeElement:
		return "Element"
	case NodeTypeAttribute:
		return "Attribute"
	case NodeTypeText:
		return "Text"
	case NodeTypeCDATA:
		return "CDATA"
	case NodeTypeEntityReference:
		return "EntityReference"
	case NodeTypeEntity:
		return "Entity"
	case NodeTypeProcessingInstruction:
		return "ProcessingInstruction"
	case NodeTypeComment:
		return "Comment"
	case NodeTypeDocument:
		return "Document"
	case NodeTypeDocumentType:
		return "DocumentType"
	case NodeTypeDocumentFragment:
		return "DocumentFragment"
	case NodeTypeNotation:
		return "Notation"
	}

	return fmt.Sprintf("NodeType(%d)", t)
}
func (p *SetSinkToUseParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetSinkToUse, p, nil)
}
func (p *StartTabMirroringParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartTabMirroring, p, nil)
}
func (p *StopCastingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopCasting, p, nil)
}
func (p *StartObservingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartObserving, p, nil)
}
func (p *StopObservingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopObserving, p, nil)
}
func (p *SetRecordingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetRecording, p, nil)
}
func (p *ClearEventsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearEvents, p, nil)
}
func (p CallFunctionOnParams) WithObjectID(objectID RemoteObjectID) *CallFunctionOnParams {
	p.ObjectID = objectID
	return &p
}
func (p CallFunctionOnParams) WithArguments(arguments []*CallArgument) *CallFunctionOnParams {
	p.Arguments = arguments
	return &p
}
func (p CallFunctionOnParams) WithExecutionContextID(executionContextID ExecutionContextID) *CallFunctionOnParams {
	p.ExecutionContextID = executionContextID
	return &p
}
func (p CallFunctionOnParams) WithObjectGroup(objectGroup string) *CallFunctionOnParams {
	p.ObjectGroup = objectGroup
	return &p
}
func (p *DiscardConsoleEntriesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDiscardConsoleEntries, nil, nil)
}
func (p EvaluateParams) WithContextID(contextID ExecutionContextID) *EvaluateParams {
	p.ContextID = contextID
	return &p
}
func (p GetPropertiesParams) WithOwnProperties(ownProperties bool) *GetPropertiesParams {
	p.OwnProperties = ownProperties
	return &p
}
func (p GetPropertiesParams) WithGeneratePreview(generatePreview bool) *GetPropertiesParams {
	p.GeneratePreview = generatePreview
	return &p
}
func (p GlobalLexicalScopeNamesParams) WithExecutionContextID(executionContextID ExecutionContextID) *GlobalLexicalScopeNamesParams {
	p.ExecutionContextID = executionContextID
	return &p
}
func (p QueryObjectsParams) WithObjectGroup(objectGroup string) *QueryObjectsParams {
	p.ObjectGroup = objectGroup
	return &p
}
func (p *ReleaseObjectParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandReleaseObject, p, nil)
}
func (p *ReleaseObjectGroupParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandReleaseObjectGroup, p, nil)
}
func (p *RunIfWaitingForDebuggerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRunIfWaitingForDebugger, nil, nil)
}
func (p *SetCustomObjectFormatterEnabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetCustomObjectFormatterEnabled, p, nil)
}
func (p *SetMaxCallStackSizeToCaptureParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetMaxCallStackSizeToCapture, p, nil)
}
func (p *TerminateExecutionParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandTerminateExecution, nil, nil)
}
func (p *AddBindingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandAddBinding, p, nil)
}
func (p *RemoveBindingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveBinding, p, nil)
}
func (p *EndParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandEnd, nil, nil)
}
func (p *RecordClockSyncMarkerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRecordClockSyncMarker, p, nil)
}
func (p StartParams) WithBufferUsageReportingInterval(bufferUsageReportingInterval float64) *StartParams {
	p.BufferUsageReportingInterval = bufferUsageReportingInterval
	return &p
}
func (p BeginFrameParams) WithNoDisplayUpdates(noDisplayUpdates bool) *BeginFrameParams {
	p.NoDisplayUpdates = noDisplayUpdates
	return &p
}
func (p BeginFrameParams) WithScreenshot(screenshot *ScreenshotParams) *BeginFrameParams {
	p.Screenshot = screenshot
	return &p
}
func (p *BringToFrontParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandBringToFront, nil, nil)
}
func (p CaptureScreenshotParams) WithClip(clip *Viewport) *CaptureScreenshotParams {
	p.Clip = clip
	return &p
}
func (p CaptureScreenshotParams) WithFromSurface(fromSurface bool) *CaptureScreenshotParams {
	p.FromSurface = fromSurface
	return &p
}
func (p CreateIsolatedWorldParams) WithWorldName(worldName string) *CreateIsolatedWorldParams {
	p.WorldName = worldName
	return &p
}
func (p CreateIsolatedWorldParams) WithGrantUniveralAccess(grantUniveralAccess bool) *CreateIsolatedWorldParams {
	p.GrantUniveralAccess = grantUniveralAccess
	return &p
}
func (p *ResetNavigationHistoryParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandResetNavigationHistory, nil, nil)
}
func (p HandleJavaScriptDialogParams) WithPromptText(promptText string) *HandleJavaScriptDialogParams {
	p.PromptText = promptText
	return &p
}
func (p *HandleJavaScriptDialogParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandHandleJavaScriptDialog, p, nil)
}
func (p NavigateParams) WithReferrer(referrer string) *NavigateParams {
	p.Referrer = referrer
	return &p
}
func (p NavigateParams) WithTransitionType(transitionType TransitionType) *NavigateParams {
	p.TransitionType = transitionType
	return &p
}
func (p NavigateParams) WithFrameID(frameID cdp.FrameID) *NavigateParams {
	p.FrameID = frameID
	return &p
}
func (p *NavigateToHistoryEntryParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandNavigateToHistoryEntry, p, nil)
}
func (p PrintToPDFParams) WithLandscape(landscape bool) *PrintToPDFParams {
	p.Landscape = landscape
	return &p
}
func (p PrintToPDFParams) WithDisplayHeaderFooter(displayHeaderFooter bool) *PrintToPDFParams {
	p.DisplayHeaderFooter = displayHeaderFooter
	return &p
}
func (p PrintToPDFParams) WithPrintBackground(printBackground bool) *PrintToPDFParams {
	p.PrintBackground = printBackground
	return &p
}
func (p PrintToPDFParams) WithScale(scale float64) *PrintToPDFParams {
	p.Scale = scale
	return &p
}
func (p PrintToPDFParams) WithPaperWidth(paperWidth float64) *PrintToPDFParams {
	p.PaperWidth = paperWidth
	return &p
}
func (p PrintToPDFParams) WithPaperHeight(paperHeight float64) *PrintToPDFParams {
	p.PaperHeight = paperHeight
	return &p
}
func (p PrintToPDFParams) WithIgnoreInvalidPageRanges(ignoreInvalidPageRanges bool) *PrintToPDFParams {
	p.IgnoreInvalidPageRanges = ignoreInvalidPageRanges
	return &p
}
func (p PrintToPDFParams) WithFooterTemplate(footerTemplate string) *PrintToPDFParams {
	p.FooterTemplate = footerTemplate
	return &p
}
func (p PrintToPDFParams) WithPreferCSSPageSize(preferCSSPageSize bool) *PrintToPDFParams {
	p.PreferCSSPageSize = preferCSSPageSize
	return &p
}
func (p ReloadParams) WithScriptToEvaluateOnLoad(scriptToEvaluateOnLoad string) *ReloadParams {
	p.ScriptToEvaluateOnLoad = scriptToEvaluateOnLoad
	return &p
}
func (p *ReloadParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandReload, p, nil)
}
func (p *RemoveScriptToEvaluateOnNewDocumentParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveScriptToEvaluateOnNewDocument, p, nil)
}
func (p *ScreencastFrameAckParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandScreencastFrameAck, p, nil)
}
func (p *SetAdBlockingEnabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetAdBlockingEnabled, p, nil)
}
func (p *SetBypassCSPParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetBypassCSP, p, nil)
}
func (p *SetFontFamiliesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetFontFamilies, p, nil)
}
func (p *SetFontSizesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetFontSizes, p, nil)
}
func (p *SetDocumentContentParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDocumentContent, p, nil)
}
func (p SetDownloadBehaviorParams) WithDownloadPath(downloadPath string) *SetDownloadBehaviorParams {
	p.DownloadPath = downloadPath
	return &p
}
func (p *SetDownloadBehaviorParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDownloadBehavior, p, nil)
}
func (p *SetLifecycleEventsEnabledParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetLifecycleEventsEnabled, p, nil)
}
func (p StartScreencastParams) WithFormat(format ScreencastFormat) *StartScreencastParams {
	p.Format = format
	return &p
}
func (p StartScreencastParams) WithMaxWidth(maxWidth int64) *StartScreencastParams {
	p.MaxWidth = maxWidth
	return &p
}
func (p StartScreencastParams) WithMaxHeight(maxHeight int64) *StartScreencastParams {
	p.MaxHeight = maxHeight
	return &p
}
func (p StartScreencastParams) WithEveryNthFrame(everyNthFrame int64) *StartScreencastParams {
	p.EveryNthFrame = everyNthFrame
	return &p
}
func (p *StartScreencastParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStartScreencast, p, nil)
}
func (p *StopLoadingParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopLoading, nil, nil)
}
func (p *SetWebLifecycleStateParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetWebLifecycleState, p, nil)
}
func (p *StopScreencastParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStopScreencast, nil, nil)
}
func (p *SetProduceCompilationCacheParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetProduceCompilationCache, p, nil)
}
func (p *AddCompilationCacheParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandAddCompilationCache, p, nil)
}
func (p *ClearCompilationCacheParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandClearCompilationCache, nil, nil)
}
func (p GenerateTestReportParams) WithGroup(group string) *GenerateTestReportParams {
	p.Group = group
	return &p
}
func (p *WaitForDebuggerParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandWaitForDebugger, nil, nil)
}
func (p *ActivateTargetParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandActivateTarget, p, nil)
}
func (p ExposeDevToolsProtocolParams) WithBindingName(bindingName string) *ExposeDevToolsProtocolParams {
	p.BindingName = bindingName
	return &p
}
func (p *ExposeDevToolsProtocolParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandExposeDevToolsProtocol, p, nil)
}
func (p CreateTargetParams) WithBrowserContextID(browserContextID BrowserContextID) *CreateTargetParams {
	p.BrowserContextID = browserContextID
	return &p
}
func (p DetachFromTargetParams) WithSessionID(sessionID SessionID) *DetachFromTargetParams {
	p.SessionID = sessionID
	return &p
}
func (p *DetachFromTargetParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDetachFromTarget, p, nil)
}
func (p *DisposeBrowserContextParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandDisposeBrowserContext, p, nil)
}
func (p SendMessageToTargetParams) WithSessionID(sessionID SessionID) *SendMessageToTargetParams {
	p.SessionID = sessionID
	return &p
}
func (p *SendMessageToTargetParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSendMessageToTarget, p, nil)
}
func (p *SetAutoAttachParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetAutoAttach, p, nil)
}
func (p *SetDiscoverTargetsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetDiscoverTargets, p, nil)
}
func (p *SetRemoteLocationsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetRemoteLocations, p, nil)
}
func (p EnableParams) WithPatterns(patterns []*RequestPattern) *EnableParams {
	p.Patterns = patterns
	return &p
}
func (p EnableParams) WithHandleAuthRequests(handleAuthRequests bool) *EnableParams {
	p.HandleAuthRequests = handleAuthRequests
	return &p
}
func (p *FailRequestParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandFailRequest, p, nil)
}
func (p FulfillRequestParams) WithBody(body string) *FulfillRequestParams {
	p.Body = body
	return &p
}
func (p FulfillRequestParams) WithResponsePhrase(responsePhrase string) *FulfillRequestParams {
	p.ResponsePhrase = responsePhrase
	return &p
}
func (p *FulfillRequestParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandFulfillRequest, p, nil)
}
func (p ContinueRequestParams) WithURL(url string) *ContinueRequestParams {
	p.URL = url
	return &p
}
func (p ContinueRequestParams) WithMethod(method string) *ContinueRequestParams {
	p.Method = method
	return &p
}
func (p ContinueRequestParams) WithPostData(postData string) *ContinueRequestParams {
	p.PostData = postData
	return &p
}
func (p ContinueRequestParams) WithHeaders(headers []*HeaderEntry) *ContinueRequestParams {
	p.Headers = headers
	return &p
}
func (p *ContinueRequestParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandContinueRequest, p, nil)
}
func (p *ContinueWithAuthParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandContinueWithAuth, p, nil)
}
func (p *ContinueToLocationParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandContinueToLocation, p, nil)
}
func (p EvaluateOnCallFrameParams) WithIncludeCommandLineAPI(includeCommandLineAPI bool) *EvaluateOnCallFrameParams {
	p.IncludeCommandLineAPI = includeCommandLineAPI
	return &p
}
func (p *PauseParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandPause, nil, nil)
}
func (p *PauseOnAsyncCallParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandPauseOnAsyncCall, p, nil)
}
func (p *RemoveBreakpointParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandRemoveBreakpoint, p, nil)
}
func (p *ResumeParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandResume, nil, nil)
}
func (p *SetAsyncCallStackDepthParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetAsyncCallStackDepth, p, nil)
}
func (p *SetBlackboxPatternsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetBlackboxPatterns, p, nil)
}
func (p *SetBlackboxedRangesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetBlackboxedRanges, p, nil)
}
func (p SetBreakpointByURLParams) WithURL(url string) *SetBreakpointByURLParams {
	p.URL = url
	return &p
}
func (p SetBreakpointByURLParams) WithURLRegex(urlRegex string) *SetBreakpointByURLParams {
	p.URLRegex = urlRegex
	return &p
}
func (p SetBreakpointByURLParams) WithScriptHash(scriptHash string) *SetBreakpointByURLParams {
	p.ScriptHash = scriptHash
	return &p
}
func (p SetBreakpointByURLParams) WithColumnNumber(columnNumber int64) *SetBreakpointByURLParams {
	p.ColumnNumber = columnNumber
	return &p
}
func (p SetBreakpointOnFunctionCallParams) WithCondition(condition string) *SetBreakpointOnFunctionCallParams {
	p.Condition = condition
	return &p
}
func (p *SetBreakpointsActiveParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetBreakpointsActive, p, nil)
}
func (p *SetPauseOnExceptionsParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetPauseOnExceptions, p, nil)
}
func (p *SetReturnValueParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetReturnValue, p, nil)
}
func (p SetScriptSourceParams) WithDryRun(dryRun bool) *SetScriptSourceParams {
	p.DryRun = dryRun
	return &p
}
func (p *SetSkipAllPausesParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetSkipAllPauses, p, nil)
}
func (p *SetVariableValueParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandSetVariableValue, p, nil)
}
func (p StepIntoParams) WithBreakOnAsyncCall(breakOnAsyncCall bool) *StepIntoParams {
	p.BreakOnAsyncCall = breakOnAsyncCall
	return &p
}
func (p *StepIntoParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStepInto, p, nil)
}
func (p *StepOutParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStepOut, nil, nil)
}
func (p *StepOverParams) Do(ctx context.Context) (err error) {
	return cdp.Execute(ctx, CommandStepOver, nil, nil)
}
func (nb *NotifyBundle) Listen(n Network, a ma.Multiaddr) {
	if nb.ListenF != nil {
		nb.ListenF(n, a)
	}
}
func (nb *NotifyBundle) ListenClose(n Network, a ma.Multiaddr) {
	if nb.ListenCloseF != nil {
		nb.ListenCloseF(n, a)
	}
}
func (nb *NotifyBundle) Connected(n Network, c Conn) {
	if nb.ConnectedF != nil {
		nb.ConnectedF(n, c)
	}
}
func (nb *NotifyBundle) Disconnected(n Network, c Conn) {
	if nb.DisconnectedF != nil {
		nb.DisconnectedF(n, c)
	}
}
func (nb *NotifyBundle) OpenedStream(n Network, s Stream) {
	if nb.OpenedStreamF != nil {
		nb.OpenedStreamF(n, s)
	}
}
func (nb *NotifyBundle) ClosedStream(n Network, s Stream) {
	if nb.ClosedStreamF != nil {
		nb.ClosedStreamF(n, s)
	}
}
func WithNoDial(ctx context.Context, reason string) context.Context {
	return context.WithValue(ctx, noDial, reason)
}
func GetNoDial(ctx context.Context) (nodial bool, reason string) {
	v := ctx.Value(noDial)
	if v != nil {
		return true, v.(string)
	}

	return false, ""
}
func WithDialPeerTimeout(ctx context.Context, timeout time.Duration) context.Context {
	return context.WithValue(ctx, dialPeerTimeoutCtxKey{}, timeout)
}
func (drv *Driver) Open(name string) (driver.Conn, error) {
	conn, err := drv.Driver.Open(name)
	if err != nil {
		return conn, err
	}

	wrapped := &Conn{conn, drv.hooks}
	if isExecer(conn) && isQueryer(conn) && isSessionResetter(conn) {
		return &ExecerQueryerContextWithSessionResetter{wrapped,
			&ExecerContext{wrapped}, &QueryerContext{wrapped},
			&SessionResetter{wrapped}}, nil
	} else if isExecer(conn) && isQueryer(conn) {
		return &ExecerQueryerContext{wrapped, &ExecerContext{wrapped},
			&QueryerContext{wrapped}}, nil
	} else if isExecer(conn) {
		// If conn implements an Execer interface, return a driver.Conn which
		// also implements Execer
		return &ExecerContext{wrapped}, nil
	} else if isQueryer(conn) {
		// If conn implements an Queryer interface, return a driver.Conn which
		// also implements Queryer
		return &QueryerContext{wrapped}, nil
	}
	return wrapped, nil
}
func (f *FixedSizeRingBuf) Prevpos(from int) int {
	if from >= f.N || from < 0 {
		return -2
	}
	if f.Readable == 0 {
		return -1
	}
	if from == f.Beg {
		return -1
	}
	a0, a1, b0, b1 := f.LegalPos()
	switch {
	case from == a0:
		return -1
	case from > a0 && from <= a1:
		return from - 1
	case from == b0:
		return a1
	case from > b0 && from <= b1:
		return from - 1
	}
	return -1
}
func (f *FixedSizeRingBuf) Last() int {
	if f.Readable == 0 {
		return -1
	}

	last := f.Beg + f.Readable - 1
	if last < f.N {
		// we fit without wrapping
		return last
	}

	return last % f.N
}
func (f *FixedSizeRingBuf) DeleteMostRecentBytes(n int) {
	if n <= 0 {
		return
	}
	if n >= f.Readable {
		f.Readable = 0
		return
	}
	f.Readable -= n
}
func NewFloat64RingBuf(maxViewItems int) *Float64RingBuf {
	n := maxViewItems
	r := &Float64RingBuf{
		N:        n,
		Beg:      0,
		Readable: 0,
	}
	r.A = make([]float64, n, n)

	return r
}
func (b *Float64RingBuf) TwoContig(makeCopy bool) (first []float64, second []float64) {

	extent := b.Beg + b.Readable
	if extent <= b.N {
		// we fit contiguously in this buffer without wrapping to the other.
		// Let second stay an empty slice.
		return b.A[b.Beg:(b.Beg + b.Readable)], second
	}

	return b.A[b.Beg:b.N], b.A[0:(extent % b.N)]
}
func (b *Float64RingBuf) Earliest() (v float64, ok bool) {
	if b.Readable == 0 {
		return
	}

	return b.A[b.Beg], true
}
func (b *Float64RingBuf) Values() []float64 {
	first, second := b.TwoContig(false)

	if len(first) == 0 {
		return second
	}

	if len(second) == 0 {
		return first
	}

	out := make([]float64, len(first) + len(second))

	copy(out, first)
	copy(out[len(first):], second)

	return out
}
func NewAtomicFixedSizeRingBuf(maxViewInBytes int) *AtomicFixedSizeRingBuf {
	n := maxViewInBytes
	r := &AtomicFixedSizeRingBuf{
		Use: 0, // 0 or 1, whichever is actually in use at the moment.
		// If we are asked for Bytes() and we wrap, linearize into the other.

		N:        n,
		Beg:      0,
		readable: 0,
	}
	r.A[0] = make([]byte, n, n)
	r.A[1] = make([]byte, n, n)

	return r
}
func (b *AtomicFixedSizeRingBuf) Reset() {
	b.tex.Lock()
	defer b.tex.Unlock()

	b.Beg = 0
	b.readable = 0
	b.Use = 0
}
func NewPointerRingBuf(sliceN int) *PointerRingBuf {
	n := sliceN
	r := &PointerRingBuf{
		N:        n,
		Beg:      0,
		Readable: 0,
	}
	r.A = make([]interface{}, n, n)

	return r
}
func (b *PointerRingBuf) TwoContig() (first []interface{}, second []interface{}) {

	extent := b.Beg + b.Readable
	if extent <= b.N {
		// we fit contiguously in this buffer without wrapping to the other.
		// Let second stay an empty slice.
		return b.A[b.Beg:(b.Beg + b.Readable)], second
	}

	return b.A[b.Beg:b.N], b.A[0:(extent % b.N)]
}
func (r *Reader) NumPage() int {
	return int(r.Trailer().Key("Root").Key("Pages").Key("Count").Int64())
}
func (p Page) Font(name string) Font {
	return Font{p.Resources().Key("Font").Key(name)}
}
func (f Font) Width(code int) float64 {
	first := f.FirstChar()
	last := f.LastChar()
	if code < first || last < code {
		return 0
	}
	return f.V.Key("Widths").Index(code - first).Float64()
}
func (f Font) Encoder() TextEncoding {
	enc := f.V.Key("Encoding")
	switch enc.Kind() {
	case Name:
		switch enc.Name() {
		case "WinAnsiEncoding":
			return &byteEncoder{&winAnsiEncoding}
		case "MacRomanEncoding":
			return &byteEncoder{&macRomanEncoding}
		case "Identity-H":
			// TODO: Should be big-endian UCS-2 decoder
			return &nopEncoder{}
		default:
			println("unknown encoding", enc.Name())
			return &nopEncoder{}
		}
	case Dict:
		return &dictEncoder{enc.Key("Differences")}
	case Null:
		// ok, try ToUnicode
	default:
		println("unexpected encoding", enc.String())
		return &nopEncoder{}
	}

	toUnicode := f.V.Key("ToUnicode")
	if toUnicode.Kind() == Dict {
		m := readCmap(toUnicode)
		if m == nil {
			return &nopEncoder{}
		}
		return m
	}

	return &byteEncoder{&pdfDocEncoding}
}
func Interpret(strm Value, do func(stk *Stack, op string)) {
	rd := strm.Reader()
	b := newBuffer(rd, 0)
	b.allowEOF = true
	b.allowObjptr = false
	b.allowStream = false
	var stk Stack
	var dicts []dict
Reading:
	for {
		tok := b.readToken()
		if tok == io.EOF {
			break
		}
		if kw, ok := tok.(keyword); ok {
			switch kw {
			case "null", "[", "]", "<<", ">>":
				break
			default:
				for i := len(dicts) - 1; i >= 0; i-- {
					if v, ok := dicts[i][name(kw)]; ok {
						stk.Push(Value{nil, objptr{}, v})
						continue Reading
					}
				}
				do(&stk, string(kw))
				continue
			case "dict":
				stk.Pop()
				stk.Push(Value{nil, objptr{}, make(dict)})
				continue
			case "currentdict":
				if len(dicts) == 0 {
					panic("no current dictionary")
				}
				stk.Push(Value{nil, objptr{}, dicts[len(dicts)-1]})
				continue
			case "begin":
				d := stk.Pop()
				if d.Kind() != Dict {
					panic("cannot begin non-dict")
				}
				dicts = append(dicts, d.data.(dict))
				continue
			case "end":
				if len(dicts) <= 0 {
					panic("mismatched begin/end")
				}
				dicts = dicts[:len(dicts)-1]
				continue
			case "def":
				if len(dicts) <= 0 {
					panic("def without open dict")
				}
				val := stk.Pop()
				key, ok := stk.Pop().data.(name)
				if !ok {
					panic("def of non-name")
				}
				dicts[len(dicts)-1][key] = val.data
				continue
			case "pop":
				stk.Pop()
				continue
			}
		}
		b.unreadToken(tok)
		obj := b.readObject()
		stk.Push(Value{nil, objptr{}, obj})
	}
}
func Open(file string) (*Reader, error) {
	// TODO: Deal with closing file.
	f, err := os.Open(file)
	if err != nil {
		return nil, err
	}
	fi, err := f.Stat()
	if err != nil {
		f.Close()
		return nil, err
	}
	return NewReader(f, fi.Size())
}
func NewReader(f io.ReaderAt, size int64) (*Reader, error) {
	return NewReaderEncrypted(f, size, nil)
}
func NewReaderEncrypted(f io.ReaderAt, size int64, pw func() string) (*Reader, error) {
	buf := make([]byte, 10)
	f.ReadAt(buf, 0)
	if !bytes.HasPrefix(buf, []byte("%PDF-1.")) || buf[7] < '0' || buf[7] > '7' || buf[8] != '\r' && buf[8] != '\n' {
		return nil, fmt.Errorf("not a PDF file: invalid header")
	}
	end := size
	const endChunk = 100
	buf = make([]byte, endChunk)
	f.ReadAt(buf, end-endChunk)
	for len(buf) > 0 && buf[len(buf)-1] == '\n' || buf[len(buf)-1] == '\r' {
		buf = buf[:len(buf)-1]
	}
	buf = bytes.TrimRight(buf, "\r\n\t ")
	if !bytes.HasSuffix(buf, []byte("%%EOF")) {
		return nil, fmt.Errorf("not a PDF file: missing %%%%EOF")
	}
	i := findLastLine(buf, "startxref")
	if i < 0 {
		return nil, fmt.Errorf("malformed PDF file: missing final startxref")
	}

	r := &Reader{
		f:   f,
		end: end,
	}
	pos := end - endChunk + int64(i)
	b := newBuffer(io.NewSectionReader(f, pos, end-pos), pos)
	if b.readToken() != keyword("startxref") {
		return nil, fmt.Errorf("malformed PDF file: missing startxref")
	}
	startxref, ok := b.readToken().(int64)
	if !ok {
		return nil, fmt.Errorf("malformed PDF file: startxref not followed by integer")
	}
	b = newBuffer(io.NewSectionReader(r.f, startxref, r.end-startxref), startxref)
	xref, trailerptr, trailer, err := readXref(r, b)
	if err != nil {
		return nil, err
	}
	r.xref = xref
	r.trailer = trailer
	r.trailerptr = trailerptr
	if trailer["Encrypt"] == nil {
		return r, nil
	}
	err = r.initEncrypt("")
	if err == nil {
		return r, nil
	}
	if pw == nil || err != ErrInvalidPassword {
		return nil, err
	}
	for {
		next := pw()
		if next == "" {
			break
		}
		if r.initEncrypt(next) == nil {
			return r, nil
		}
	}
	return nil, err
}
func (r *Reader) Trailer() Value {
	return Value{r, r.trailerptr, r.trailer}
}
func (v Value) Kind() ValueKind {
	switch v.data.(type) {
	default:
		return Null
	case bool:
		return Bool
	case int64:
		return Integer
	case float64:
		return Real
	case string:
		return String
	case name:
		return Name
	case dict:
		return Dict
	case array:
		return Array
	case stream:
		return Stream
	}
}
func newBuffer(r io.Reader, offset int64) *buffer {
	return &buffer{
		r:           r,
		offset:      offset,
		buf:         make([]byte, 0, 4096),
		allowObjptr: true,
		allowStream: true,
	}
}
func (r *ResultSet) Paginate(perPage, page int) (*PaginationInfo, error) {

	info := new(PaginationInfo)

	// Get count on a different session to avoid blocking
	sess := r.Collection.Connection.Session.Copy()

	count, err := sess.DB(r.Collection.Database).C(r.Collection.Name).Find(r.Params).Count()
	sess.Close()

	if err != nil {
		return info, err
	}

	// Calculate how many pages
	totalPages := int(math.Ceil(float64(count) / float64(perPage)))

	if page < 1 {
		page = 1
	} else if page > totalPages {
		page = totalPages
	}

	skip := (page - 1) * perPage

	r.Query.Skip(skip).Limit(perPage)

	info.TotalPages = totalPages
	info.PerPage = perPage
	info.Current = page
	info.TotalRecords = count

	if info.Current < info.TotalPages {
		info.RecordsOnPage = info.PerPage
	} else {

		info.RecordsOnPage = int(math.Mod(float64(count), float64(perPage)))

		if info.RecordsOnPage == 0 && count > 0 {
			info.RecordsOnPage = perPage
		}

	}

	return info, nil
}
func CascadeDelete(collection *Collection, doc interface{}) {
	// Find out which properties to cascade
	if conv, ok := doc.(interface {
		GetCascade(*Collection) []*CascadeConfig
	}); ok {
		toCascade := conv.GetCascade(collection)

		// Get the ID

		for _, conf := range toCascade {
			if len(conf.ReferenceQuery) == 0 {
				id, err := reflections.GetField(doc, "Id")
				if err != nil {
					panic(err)
				}
				conf.ReferenceQuery = []*ReferenceField{&ReferenceField{"_id", id}}
			}

			cascadeDeleteWithConfig(conf)

		}

	}
}
func cascadeDeleteWithConfig(conf *CascadeConfig) (*mgo.ChangeInfo, error) {

	switch conf.RelType {
	case REL_ONE:
		update := map[string]map[string]interface{}{
			"$set": map[string]interface{}{},
		}

		if len(conf.ThroughProp) > 0 {
			update["$set"][conf.ThroughProp] = nil
		} else {
			for _, p := range conf.Properties {
				update["$set"][p] = nil
			}
		}

		return conf.Collection.Collection().UpdateAll(conf.Query, update)
	case REL_MANY:
		update := map[string]map[string]interface{}{
			"$pull": map[string]interface{}{},
		}

		q := bson.M{}
		for _, f := range conf.ReferenceQuery {
			q[f.BsonName] = f.Value
		}
		update["$pull"][conf.ThroughProp] = q
		return conf.Collection.Collection().UpdateAll(conf.Query, update)
	}

	return &mgo.ChangeInfo{}, errors.New("Invalid relation type")
}
func cascadeSaveWithConfig(conf *CascadeConfig, doc Document) (*mgo.ChangeInfo, error) {
	// Create a new map with just the props to cascade

	data := conf.Data

	switch conf.RelType {
	case REL_ONE:
		if len(conf.OldQuery) > 0 {

			update1 := map[string]map[string]interface{}{
				"$set": map[string]interface{}{},
			}

			if len(conf.ThroughProp) > 0 {
				update1["$set"][conf.ThroughProp] = nil
			} else {
				for _, p := range conf.Properties {
					update1["$set"][p] = nil
				}
			}

			ret, err := conf.Collection.Collection().UpdateAll(conf.OldQuery, update1)

			if conf.RemoveOnly {
				return ret, err
			}
		}

		update := make(map[string]interface{})

		if len(conf.ThroughProp) > 0 {
			m := bson.M{}
			m[conf.ThroughProp] = data
			update["$set"] = m
		} else {
			update["$set"] = data
		}

		// Just update
		return conf.Collection.Collection().UpdateAll(conf.Query, update)
	case REL_MANY:

		update1 := map[string]map[string]interface{}{
			"$pull": map[string]interface{}{},
		}

		q := bson.M{}
		for _, f := range conf.ReferenceQuery {
			q[f.BsonName] = f.Value
		}
		update1["$pull"][conf.ThroughProp] = q

		if len(conf.OldQuery) > 0 {
			ret, err := conf.Collection.Collection().UpdateAll(conf.OldQuery, update1)
			if conf.RemoveOnly {
				return ret, err
			}
		}

		// Remove self from current relations, so we can replace it
		conf.Collection.Collection().UpdateAll(conf.Query, update1)

		update2 := map[string]map[string]interface{}{
			"$push": map[string]interface{}{},
		}

		update2["$push"][conf.ThroughProp] = data
		return conf.Collection.Collection().UpdateAll(conf.Query, update2)

	}

	return &mgo.ChangeInfo{}, errors.New("Invalid relation type")

}
func MapFromCascadeProperties(properties []string, doc Document) map[string]interface{} {
	data := make(map[string]interface{})

	for _, prop := range properties {
		split := strings.Split(prop, ".")

		if len(split) == 1 {
			data[prop], _ = dotaccess.Get(doc, prop)
		} else {
			actualProp := split[len(split)-1]
			split := append([]string{}, split[:len(split)-1]...)
			curData := data

			for _, s := range split {
				if _, ok := curData[s]; ok {
					if mapped, ok := curData[s].(map[string]interface{}); ok {
						curData = mapped
					} else {
						panic("Cannot access non-map property via dot notation")
					}

				} else {
					curData[s] = make(map[string]interface{})
					if mapped, ok := curData[s].(map[string]interface{}); ok {
						curData = mapped
					} else {
						panic("Cannot access non-map property via dot notation")
					}
				}
			}

			val, _ := dotaccess.Get(doc, prop)
			// if bsonId, ok := val.(bson.ObjectId); ok {
			// 	if !bsonId.Valid() {
			// 		curData[actualProp] = ""
			// 		continue
			// 	}
			// }
			curData[actualProp] = val
		}
	}

	return data
}
func (m *Connection) Connect() (err error) {
	defer func() {
		if r := recover(); r != nil {
			// panic(r)
			// return
			if e, ok := r.(error); ok {
				err = e
			} else if e, ok := r.(string); ok {
				err = errors.New(e)
			} else {
				err = errors.New(fmt.Sprint(r))
			}

		}
	}()

	if m.Config.DialInfo == nil {
		if m.Config.DialInfo, err = mgo.ParseURL(m.Config.ConnectionString); err != nil {
			panic(fmt.Sprintf("cannot parse given URI %s due to error: %s", m.Config.ConnectionString, err.Error()))
		}
	}

	session, err := mgo.DialWithInfo(m.Config.DialInfo)
	if err != nil {
		return err
	}

	m.Session = session

	m.Session.SetMode(mgo.Monotonic, true)

	return nil
}
func lowerInitial(str string) string {
	for i, v := range str {
		return string(unicode.ToLower(v)) + str[i+1:]
	}
	return ""
}
func (c *Collection) Find(query interface{}) *ResultSet {
	col := c.Collection()

	// Count for testing
	q := col.Find(query)

	resultset := new(ResultSet)

	resultset.Query = q
	resultset.Params = query
	resultset.Collection = c

	return resultset
}
func (c *Client) RawInformationQuery(from, to, id, iqType, requestNamespace, body string) (string, error) {
	const xmlIQ = "<iq from='%s' to='%s' id='%s' type='%s'><query xmlns='%s'>%s</query></iq>"
	_, err := fmt.Fprintf(c.conn, xmlIQ, xmlEscape(from), xmlEscape(to), id, iqType, requestNamespace, body)
	return id, err
}
func (o Options) NewClient() (*Client, error) {
	host := o.Host
	c, err := connect(host, o.User, o.Password)
	if err != nil {
		return nil, err
	}

	if strings.LastIndex(o.Host, ":") > 0 {
		host = host[:strings.LastIndex(o.Host, ":")]
	}

	client := new(Client)
	if o.NoTLS {
		client.conn = c
	} else {
		var tlsconn *tls.Conn
		if o.TLSConfig != nil {
			tlsconn = tls.Client(c, o.TLSConfig)
		} else {
			DefaultConfig.ServerName = host
			newconfig := DefaultConfig
			newconfig.ServerName = host
			tlsconn = tls.Client(c, &newconfig)
		}
		if err = tlsconn.Handshake(); err != nil {
			return nil, err
		}
		insecureSkipVerify := DefaultConfig.InsecureSkipVerify
		if o.TLSConfig != nil {
			insecureSkipVerify = o.TLSConfig.InsecureSkipVerify
		}
		if !insecureSkipVerify {
			if err = tlsconn.VerifyHostname(host); err != nil {
				return nil, err
			}
		}
		client.conn = tlsconn
	}

	if err := client.init(&o); err != nil {
		client.Close()
		return nil, err
	}

	return client, nil
}
func (c *Client) Close() error {
	if c.conn != (*tls.Conn)(nil) {
		return c.conn.Close()
	}
	return nil
}
func (c *Client) startTLSIfRequired(f *streamFeatures, o *Options, domain string) (*streamFeatures, error) {
	// whether we start tls is a matter of opinion: the server's and the user's.
	switch {
	case f.StartTLS == nil:
		// the server does not support STARTTLS
		return f, nil
	case !o.StartTLS && f.StartTLS.Required == nil:
		return f, nil
	case f.StartTLS.Required != nil:
		// the server requires STARTTLS.
	case !o.StartTLS:
		// the user wants STARTTLS and the server supports it.
	}
	var err error

	fmt.Fprintf(c.conn, "<starttls xmlns='urn:ietf:params:xml:ns:xmpp-tls'/>\n")
	var k tlsProceed
	if err = c.p.DecodeElement(&k, nil); err != nil {
		return f, errors.New("unmarshal <proceed>: " + err.Error())
	}

	tc := o.TLSConfig
	if tc == nil {
		tc = new(tls.Config)
		*tc = DefaultConfig
		//TODO(scott): we should consider using the server's address or reverse lookup
		tc.ServerName = domain
	}
	t := tls.Client(c.conn, tc)

	if err = t.Handshake(); err != nil {
		return f, errors.New("starttls handshake: " + err.Error())
	}
	c.conn = t

	// restart our declaration of XMPP stream intentions.
	tf, err := c.startStream(o, domain)
	if err != nil {
		return f, err
	}
	return tf, nil
}
func (c *Client) startStream(o *Options, domain string) (*streamFeatures, error) {
	if o.Debug {
		c.p = xml.NewDecoder(tee{c.conn, DebugWriter})
	} else {
		c.p = xml.NewDecoder(c.conn)
	}

	_, err := fmt.Fprintf(c.conn, "<?xml version='1.0'?>\n"+
		"<stream:stream to='%s' xmlns='%s'\n"+
		" xmlns:stream='%s' version='1.0'>\n",
		xmlEscape(domain), nsClient, nsStream)
	if err != nil {
		return nil, err
	}

	// We expect the server to start a <stream>.
	se, err := nextStart(c.p)
	if err != nil {
		return nil, err
	}
	if se.Name.Space != nsStream || se.Name.Local != "stream" {
		return nil, fmt.Errorf("expected <stream> but got <%v> in %v", se.Name.Local, se.Name.Space)
	}

	// Now we're in the stream and can use Unmarshal.
	// Next message should be <features> to tell us authentication options.
	// See section 4.6 in RFC 3920.
	f := new(streamFeatures)
	if err = c.p.DecodeElement(f, nil); err != nil {
		return f, errors.New("unmarshal <features>: " + err.Error())
	}
	return f, nil
}
func (c *Client) IsEncrypted() bool {
	_, ok := c.conn.(*tls.Conn)
	return ok
}
func (c *Client) Recv() (stanza interface{}, err error) {
	for {
		_, val, err := next(c.p)
		if err != nil {
			return Chat{}, err
		}
		switch v := val.(type) {
		case *clientMessage:
			stamp, _ := time.Parse(
				"2006-01-02T15:04:05Z",
				v.Delay.Stamp,
			)
			chat := Chat{
				Remote:    v.From,
				Type:      v.Type,
				Text:      v.Body,
				Subject:   v.Subject,
				Thread:    v.Thread,
				Other:     v.OtherStrings(),
				OtherElem: v.Other,
				Stamp:     stamp,
			}
			return chat, nil
		case *clientQuery:
			var r Roster
			for _, item := range v.Item {
				r = append(r, Contact{item.Jid, item.Name, item.Group})
			}
			return Chat{Type: "roster", Roster: r}, nil
		case *clientPresence:
			return Presence{v.From, v.To, v.Type, v.Show, v.Status}, nil
		case *clientIQ:
			// TODO check more strictly
			if bytes.Equal(bytes.TrimSpace(v.Query), []byte(`<ping xmlns='urn:xmpp:ping'/>`)) || bytes.Equal(bytes.TrimSpace(v.Query), []byte(`<ping xmlns="urn:xmpp:ping"/>`)) {
				err := c.SendResultPing(v.ID, v.From)
				if err != nil {
					return Chat{}, err
				}
			}
			return IQ{ID: v.ID, From: v.From, To: v.To, Type: v.Type, Query: v.Query}, nil
		}
	}
}
func (c *Client) Send(chat Chat) (n int, err error) {
	var subtext = ``
	var thdtext = ``
	if chat.Subject != `` {
		subtext = `<subject>` + xmlEscape(chat.Subject) + `</subject>`
	}
	if chat.Thread != `` {
		thdtext = `<thread>` + xmlEscape(chat.Thread) + `</thread>`
	}

	stanza := "<message to='%s' type='%s' id='%s' xml:lang='en'>" + subtext + "<body>%s</body>" + thdtext + "</message>"

	return fmt.Fprintf(c.conn, stanza,
		xmlEscape(chat.Remote), xmlEscape(chat.Type), cnonce(), xmlEscape(chat.Text))
}
func (c *Client) SendOrg(org string) (n int, err error) {
	return fmt.Fprint(c.conn, org)
}
func (c *Client) SendKeepAlive() (n int, err error) {
	return fmt.Fprintf(c.conn, " ")
}
func (c *Client) SendHtml(chat Chat) (n int, err error) {
	return fmt.Fprintf(c.conn, "<message to='%s' type='%s' xml:lang='en'>"+
		"<body>%s</body>"+
		"<html xmlns='http://jabber.org/protocol/xhtml-im'><body xmlns='http://www.w3.org/1999/xhtml'>%s</body></html></message>",
		xmlEscape(chat.Remote), xmlEscape(chat.Type), xmlEscape(chat.Text), chat.Text)
}
func nextStart(p *xml.Decoder) (xml.StartElement, error) {
	for {
		t, err := p.Token()
		if err != nil || t == nil {
			return xml.StartElement{}, err
		}
		switch t := t.(type) {
		case xml.StartElement:
			return t, nil
		}
	}
}
func (c *Client) JoinProtectedMUC(jid, nick string, password string, history_type, history int, history_date *time.Time) (n int, err error) {
	if nick == "" {
		nick = c.jid
	}
	switch history_type {
	case NoHistory:
		return fmt.Fprintf(c.conn, "<presence to='%s/%s'>\n"+
			"<x xmlns='%s'>\n"+
			"<password>%s</password>"+
			"</x>\n"+
			"</presence>",
			xmlEscape(jid), xmlEscape(nick), nsMUC, xmlEscape(password))
	case CharHistory:
		return fmt.Fprintf(c.conn, "<presence to='%s/%s'>\n"+
			"<x xmlns='%s'>\n"+
			"<password>%s</password>\n"+
			"<history maxchars='%d'/></x>\n"+
			"</presence>",
			xmlEscape(jid), xmlEscape(nick), nsMUC, xmlEscape(password), history)
	case StanzaHistory:
		return fmt.Fprintf(c.conn, "<presence to='%s/%s'>\n"+
			"<x xmlns='%s'>\n"+
			"<password>%s</password>\n"+
			"<history maxstanzas='%d'/></x>\n"+
			"</presence>",
			xmlEscape(jid), xmlEscape(nick), nsMUC, xmlEscape(password), history)
	case SecondsHistory:
		return fmt.Fprintf(c.conn, "<presence to='%s/%s'>\n"+
			"<x xmlns='%s'>\n"+
			"<password>%s</password>\n"+
			"<history seconds='%d'/></x>\n"+
			"</presence>",
			xmlEscape(jid), xmlEscape(nick), nsMUC, xmlEscape(password), history)
	case SinceHistory:
		if history_date != nil {
			return fmt.Fprintf(c.conn, "<presence to='%s/%s'>\n"+
				"<x xmlns='%s'>\n"+
				"<password>%s</password>\n"+
				"<history since='%s'/></x>\n"+
				"</presence>",
				xmlEscape(jid), xmlEscape(nick), nsMUC, xmlEscape(password), history_date.Format(time.RFC3339))
		}
	}
	return 0, errors.New("Unknown history option")
}
func (c *Client) LeaveMUC(jid string) (n int, err error) {
	return fmt.Fprintf(c.conn, "<presence from='%s' to='%s' type='unavailable' />",
		c.jid, xmlEscape(jid))
}
func (m *Message) AttachBuffer(filename string, buf []byte, inline bool) error {
	m.Attachments[filename] = &Attachment{
		Filename: filename,
		Data:     buf,
		Inline:   inline,
	}
	return nil
}
func (m *Message) Attach(file string) error {
	return m.attach(file, false)
}
func (m *Message) Inline(file string) error {
	return m.attach(file, true)
}
func (m *Message) AddHeader(key string, value string) Header {
	newHeader := Header{Key: key, Value: value}
	m.Headers = append(m.Headers, newHeader)
	return newHeader
}
func (m *Message) Tolist() []string {
	tolist := m.To

	for _, cc := range m.Cc {
		tolist = append(tolist, cc)
	}

	for _, bcc := range m.Bcc {
		tolist = append(tolist, bcc)
	}

	return tolist
}
func (m *Message) Bytes() []byte {
	buf := bytes.NewBuffer(nil)

	buf.WriteString("From: " + m.From.String() + "\r\n")

	t := time.Now()
	buf.WriteString("Date: " + t.Format(time.RFC1123Z) + "\r\n")

	buf.WriteString("To: " + strings.Join(m.To, ",") + "\r\n")
	if len(m.Cc) > 0 {
		buf.WriteString("Cc: " + strings.Join(m.Cc, ",") + "\r\n")
	}

	//fix  Encode
	var coder = base64.StdEncoding
	var subject = "=?UTF-8?B?" + coder.EncodeToString([]byte(m.Subject)) + "?="
	buf.WriteString("Subject: " + subject + "\r\n")

	if len(m.ReplyTo) > 0 {
		buf.WriteString("Reply-To: " + m.ReplyTo + "\r\n")
	}

	buf.WriteString("MIME-Version: 1.0\r\n")

	// Add custom headers
	if len(m.Headers) > 0 {
		for _, header := range m.Headers {
			buf.WriteString(fmt.Sprintf("%s: %s\r\n", header.Key, header.Value))
		}
	}

	boundary := "f46d043c813270fc6b04c2d223da"

	if len(m.Attachments) > 0 {
		buf.WriteString("Content-Type: multipart/mixed; boundary=" + boundary + "\r\n")
		buf.WriteString("\r\n--" + boundary + "\r\n")
	}

	buf.WriteString(fmt.Sprintf("Content-Type: %s; charset=utf-8\r\n\r\n", m.BodyContentType))
	buf.WriteString(m.Body)
	buf.WriteString("\r\n")

	if len(m.Attachments) > 0 {
		for _, attachment := range m.Attachments {
			buf.WriteString("\r\n\r\n--" + boundary + "\r\n")

			if attachment.Inline {
				buf.WriteString("Content-Type: message/rfc822\r\n")
				buf.WriteString("Content-Disposition: inline; filename=\"" + attachment.Filename + "\"\r\n\r\n")

				buf.Write(attachment.Data)
			} else {
				ext := filepath.Ext(attachment.Filename)
				mimetype := mime.TypeByExtension(ext)
				if mimetype != "" {
					mime := fmt.Sprintf("Content-Type: %s\r\n", mimetype)
					buf.WriteString(mime)
				} else {
					buf.WriteString("Content-Type: application/octet-stream\r\n")
				}
				buf.WriteString("Content-Transfer-Encoding: base64\r\n")

				buf.WriteString("Content-Disposition: attachment; filename=\"=?UTF-8?B?")
				buf.WriteString(coder.EncodeToString([]byte(attachment.Filename)))
				buf.WriteString("?=\"\r\n\r\n")

				b := make([]byte, base64.StdEncoding.EncodedLen(len(attachment.Data)))
				base64.StdEncoding.Encode(b, attachment.Data)

				// write base64 content in lines of up to 76 chars
				for i, l := 0, len(b); i < l; i++ {
					buf.WriteByte(b[i])
					if (i+1)%76 == 0 {
						buf.WriteString("\r\n")
					}
				}
			}

			buf.WriteString("\r\n--" + boundary)
		}

		buf.WriteString("--")
	}

	return buf.Bytes()
}
func Send(addr string, auth smtp.Auth, m *Message) error {
	return smtp.SendMail(addr, auth, m.From.Address, m.Tolist(), m.Bytes())
}
func (e *Envelope) GetHeader(name string) string {
	if e.header == nil {
		return ""
	}
	return decodeHeader(e.header.Get(name))
}
func (e *Envelope) GetHeaderValues(name string) []string {
	if e.header == nil {
		return []string{}
	}

	rawValues := (*e.header)[textproto.CanonicalMIMEHeaderKey(name)]
	var values []string
	for _, v := range rawValues {
		values = append(values, decodeHeader(v))
	}
	return values
}
func (e *Envelope) SetHeader(name string, value []string) error {
	if name == "" {
		return fmt.Errorf("Provide non-empty header name")
	}

	for i, v := range value {
		if i == 0 {
			e.header.Set(name, mime.BEncoding.Encode("utf-8", v))
			continue
		}
		e.header.Add(name, mime.BEncoding.Encode("utf-8", v))
	}
	return nil
}
func (e *Envelope) AddHeader(name string, value string) error {
	if name == "" {
		return fmt.Errorf("Provide non-empty header name")
	}

	e.header.Add(name, mime.BEncoding.Encode("utf-8", value))
	return nil
}
func (e *Envelope) DeleteHeader(name string) error {
	if name == "" {
		return fmt.Errorf("Provide non-empty header name")
	}

	e.header.Del(name)
	return nil
}
func (e *Envelope) AddressList(key string) ([]*mail.Address, error) {
	if e.header == nil {
		return nil, fmt.Errorf("No headers available")
	}
	if !AddressHeaders[strings.ToLower(key)] {
		return nil, fmt.Errorf("%s is not an address header", key)
	}

	str := decodeToUTF8Base64Header(e.header.Get(key))
	if str == "" {
		return nil, mail.ErrHeaderNotPresent
	}

	// These statements are handy for debugging ParseAddressList errors
	// fmt.Println("in:  ", m.header.Get(key))
	// fmt.Println("out: ", str)
	ret, err := mail.ParseAddressList(str)
	switch {
	case err == nil:
		// carry on
	case err.Error() == "mail: expected comma":
		ret, err = mail.ParseAddressList(ensureCommaDelimitedAddresses(str))
		if err != nil {
			return nil, err
		}
	default:
		return nil, err
	}
	return ret, nil
}
func (e *Envelope) Clone() *Envelope {
	if e == nil {
		return nil
	}

	newEnvelope := &Envelope{
		e.Text,
		e.HTML,
		e.Root.Clone(nil),
		e.Attachments,
		e.Inlines,
		e.OtherParts,
		e.Errors,
		e.header,
	}
	return newEnvelope
}
func ReadEnvelope(r io.Reader) (*Envelope, error) {
	// Read MIME parts from reader
	root, err := ReadParts(r)
	if err != nil {
		return nil, errors.WithMessage(err, "Failed to ReadParts")
	}
	return EnvelopeFromPart(root)
}
func EnvelopeFromPart(root *Part) (*Envelope, error) {
	e := &Envelope{
		Root:   root,
		header: &root.Header,
	}

	if detectMultipartMessage(root) {
		// Multi-part message (message with attachments, etc)
		if err := parseMultiPartBody(root, e); err != nil {
			return nil, err
		}
	} else {
		if detectBinaryBody(root) {
			// Attachment only, no text
			if root.Disposition == cdInline {
				e.Inlines = append(e.Inlines, root)
			} else {
				e.Attachments = append(e.Attachments, root)
			}
		} else {
			// Only text, no attachments
			if err := parseTextOnlyBody(root, e); err != nil {
				return nil, err
			}
		}
	}

	// Down-convert HTML to text if necessary
	if e.Text == "" && e.HTML != "" {
		// We always warn when this happens
		e.Root.addWarning(
			ErrorPlainTextFromHTML,
			"Message did not contain a text/plain part")
		var err error
		if e.Text, err = html2text.FromString(e.HTML); err != nil {
			// Downcoversion shouldn't fail
			e.Text = ""
			p := e.Root.BreadthMatchFirst(matchHTMLBodyPart)
			p.addError(
				ErrorPlainTextFromHTML,
				"Failed to downconvert HTML: %v",
				err)
		}
	}

	// Copy part errors into Envelope.
	if e.Root != nil {
		_ = e.Root.DepthMatchAll(func(part *Part) bool {
			// Using DepthMatchAll to traverse all parts, don't care about result.
			for i := range part.Errors {
				// Range index is needed to get the correct address, because range value points to
				// a locally scoped variable.
				e.Errors = append(e.Errors, part.Errors[i])
			}
			return false
		})
	}

	return e, nil
}
func parseTextOnlyBody(root *Part, e *Envelope) error {
	// Determine character set
	var charset string
	var isHTML bool
	if ctype := root.Header.Get(hnContentType); ctype != "" {
		if mediatype, mparams, _, err := parseMediaType(ctype); err == nil {
			isHTML = (mediatype == ctTextHTML)
			if mparams[hpCharset] != "" {
				charset = mparams[hpCharset]
			}
		}
	}

	// Read transcoded text
	if isHTML {
		rawHTML := string(root.Content)
		// Note: Empty e.Text will trigger html2text conversion
		e.HTML = rawHTML
		if charset == "" {
			// Search for charset in HTML metadata
			if charset = coding.FindCharsetInHTML(rawHTML); charset != "" {
				// Found charset in HTML
				if convHTML, err := coding.ConvertToUTF8String(charset, root.Content); err == nil {
					// Successful conversion
					e.HTML = convHTML
				} else {
					// Conversion failed
					root.addWarning(ErrorCharsetConversion, err.Error())
				}
			}
			// Converted from charset in HTML
			return nil
		}
	} else {
		e.Text = string(root.Content)
	}

	return nil
}
func parseMultiPartBody(root *Part, e *Envelope) error {
	// Parse top-level multipart
	ctype := root.Header.Get(hnContentType)
	mediatype, params, _, err := parseMediaType(ctype)
	if err != nil {
		return fmt.Errorf("Unable to parse media type: %v", err)
	}
	if !strings.HasPrefix(mediatype, ctMultipartPrefix) {
		return fmt.Errorf("Unknown mediatype: %v", mediatype)
	}
	boundary := params[hpBoundary]
	if boundary == "" {
		return fmt.Errorf("Unable to locate boundary param in Content-Type header")
	}

	// Locate text body
	if mediatype == ctMultipartAltern {
		p := root.BreadthMatchFirst(func(p *Part) bool {
			return p.ContentType == ctTextPlain && p.Disposition != cdAttachment
		})
		if p != nil {
			e.Text = string(p.Content)
		}
	} else {
		// multipart is of a mixed type
		parts := root.DepthMatchAll(func(p *Part) bool {
			return p.ContentType == ctTextPlain && p.Disposition != cdAttachment
		})
		for i, p := range parts {
			if i > 0 {
				e.Text += "\n--\n"
			}
			e.Text += string(p.Content)
		}
	}

	// Locate HTML body
	p := root.BreadthMatchFirst(matchHTMLBodyPart)
	if p != nil {
		e.HTML += string(p.Content)
	}

	// Locate attachments
	e.Attachments = root.BreadthMatchAll(func(p *Part) bool {
		return p.Disposition == cdAttachment || p.ContentType == ctAppOctetStream
	})

	// Locate inlines
	e.Inlines = root.BreadthMatchAll(func(p *Part) bool {
		return p.Disposition == cdInline && !strings.HasPrefix(p.ContentType, ctMultipartPrefix)
	})

	// Locate others parts not considered in attachments or inlines
	e.OtherParts = root.BreadthMatchAll(func(p *Part) bool {
		if strings.HasPrefix(p.ContentType, ctMultipartPrefix) {
			return false
		}
		if p.Disposition != "" {
			return false
		}
		if p.ContentType == ctAppOctetStream {
			return false
		}
		return p.ContentType != ctTextPlain && p.ContentType != ctTextHTML
	})

	return nil
}
func matchHTMLBodyPart(p *Part) bool {
	return p.ContentType == ctTextHTML && p.Disposition != cdAttachment
}
func ensureCommaDelimitedAddresses(s string) string {
	// This normalizes the whitespace, but may interfere with CFWS (comments with folding whitespace)
	// RFC-5322 3.4.0:
	//      because some legacy implementations interpret the comment,
	//      comments generally SHOULD NOT be used in address fields
	//      to avoid confusing such implementations.
	s = strings.Join(strings.Fields(s), " ")

	inQuotes := false
	inDomain := false
	escapeSequence := false
	sb := strings.Builder{}
	for _, r := range s {
		if escapeSequence {
			escapeSequence = false
			sb.WriteRune(r)
			continue
		}
		if r == '"' {
			inQuotes = !inQuotes
			sb.WriteRune(r)
			continue
		}
		if inQuotes {
			if r == '\\' {
				escapeSequence = true
				sb.WriteRune(r)
				continue
			}
		} else {
			if r == '@' {
				inDomain = true
				sb.WriteRune(r)
				continue
			}
			if inDomain {
				if r == ';' {
					sb.WriteRune(r)
					break
				}
				if r == ',' {
					inDomain = false
					sb.WriteRune(r)
					continue
				}
				if r == ' ' {
					inDomain = false
					sb.WriteRune(',')
					sb.WriteRune(r)
					continue
				}
			}
		}
		sb.WriteRune(r)
	}
	return sb.String()
}
func (p MailBuilder) Date(date time.Time) MailBuilder {
	p.date = date
	return p
}
func (p MailBuilder) From(name, addr string) MailBuilder {
	p.from = mail.Address{Name: name, Address: addr}
	return p
}
func (p MailBuilder) Subject(subject string) MailBuilder {
	p.subject = subject
	return p
}
func (p MailBuilder) To(name, addr string) MailBuilder {
	p.to = append(p.to, mail.Address{Name: name, Address: addr})
	return p
}
func (p MailBuilder) ToAddrs(to []mail.Address) MailBuilder {
	p.to = to
	return p
}
func (p MailBuilder) CC(name, addr string) MailBuilder {
	p.cc = append(p.cc, mail.Address{Name: name, Address: addr})
	return p
}
func (p MailBuilder) CCAddrs(cc []mail.Address) MailBuilder {
	p.cc = cc
	return p
}
func (p MailBuilder) ReplyTo(name, addr string) MailBuilder {
	p.replyTo = mail.Address{Name: name, Address: addr}
	return p
}
func (p MailBuilder) Header(name, value string) MailBuilder {
	// Copy existing header map
	h := textproto.MIMEHeader{}
	for k, v := range p.header {
		h[k] = v
	}
	h.Add(name, value)
	p.header = h
	return p
}
func (p MailBuilder) AddAttachment(b []byte, contentType string, fileName string) MailBuilder {
	part := NewPart(contentType)
	part.Content = b
	part.FileName = fileName
	part.Disposition = cdAttachment
	p.attachments = append(p.attachments, part)
	return p
}
func (p MailBuilder) AddFileAttachment(path string) MailBuilder {
	// Only allow first p.err value
	if p.err != nil {
		return p
	}
	f, err := os.Open(path)
	if err != nil {
		p.err = err
		return p
	}
	b, err := ioutil.ReadAll(f)
	if err != nil {
		p.err = err
		return p
	}
	name := filepath.Base(path)
	ctype := mime.TypeByExtension(filepath.Ext(name))
	return p.AddAttachment(b, ctype, name)
}
func (p MailBuilder) AddInline(
	b []byte,
	contentType string,
	fileName string,
	contentID string,
) MailBuilder {
	part := NewPart(contentType)
	part.Content = b
	part.FileName = fileName
	part.Disposition = cdInline
	part.ContentID = contentID
	p.inlines = append(p.inlines, part)
	return p
}
func (p MailBuilder) Equals(o MailBuilder) bool {
	return reflect.DeepEqual(p, o)
}
func (p *Part) Encode(writer io.Writer) error {
	if p.Header == nil {
		p.Header = make(textproto.MIMEHeader)
	}
	cte := p.setupMIMEHeaders()
	// Encode this part.
	b := bufio.NewWriter(writer)
	p.encodeHeader(b)
	if len(p.Content) > 0 {
		b.Write(crnl)
		if err := p.encodeContent(b, cte); err != nil {
			return err
		}
	}
	if p.FirstChild == nil {
		return b.Flush()
	}
	// Encode children.
	endMarker := []byte("\r\n--" + p.Boundary + "--")
	marker := endMarker[:len(endMarker)-2]
	c := p.FirstChild
	for c != nil {
		b.Write(marker)
		b.Write(crnl)
		if err := c.Encode(b); err != nil {
			return err
		}
		c = c.NextSibling
	}
	b.Write(endMarker)
	b.Write(crnl)
	return b.Flush()
}
func (p *Part) encodeHeader(b *bufio.Writer) {
	keys := make([]string, 0, len(p.Header))
	for k := range p.Header {
		keys = append(keys, k)
	}
	sort.Strings(keys)
	for _, k := range keys {
		for _, v := range p.Header[k] {
			encv := v
			switch selectTransferEncoding([]byte(v), true) {
			case teBase64:
				encv = mime.BEncoding.Encode(utf8, v)
			case teQuoted:
				encv = mime.QEncoding.Encode(utf8, v)
			}
			// _ used to prevent early wrapping
			wb := stringutil.Wrap(76, k, ":_", encv, "\r\n")
			wb[len(k)+1] = ' '
			b.Write(wb)
		}
	}
}
func (p *Part) encodeContent(b *bufio.Writer, cte transferEncoding) (err error) {
	switch cte {
	case teBase64:
		enc := base64.StdEncoding
		text := make([]byte, enc.EncodedLen(len(p.Content)))
		base64.StdEncoding.Encode(text, p.Content)
		// Wrap lines.
		lineLen := 76
		for len(text) > 0 {
			if lineLen > len(text) {
				lineLen = len(text)
			}
			if _, err = b.Write(text[:lineLen]); err != nil {
				return err
			}
			b.Write(crnl)
			text = text[lineLen:]
		}
	case teQuoted:
		qp := quotedprintable.NewWriter(b)
		if _, err = qp.Write(p.Content); err != nil {
			return err
		}
		err = qp.Close()
	default:
		_, err = b.Write(p.Content)
	}
	return err
}
func selectTransferEncoding(content []byte, quoteLineBreaks bool) transferEncoding {
	if len(content) == 0 {
		return te7Bit
	}
	// Binary chars remaining before we choose b64 encoding.
	threshold := b64Percent * len(content) / 100
	bincount := 0
	for _, b := range content {
		if (b < ' ' || '~' < b) && b != '\t' {
			if !quoteLineBreaks && (b == '\r' || b == '\n') {
				continue
			}
			bincount++
			if bincount >= threshold {
				return teBase64
			}
		}
	}
	if bincount == 0 {
		return te7Bit
	}
	return teQuoted
}
func setParamValue(p map[string]string, k, v string) {
	if v != "" {
		p[k] = v
	}
}
func NewBase64Cleaner(r io.Reader) *Base64Cleaner {
	return &Base64Cleaner{
		Errors: make([]error, 0),
		r:      r,
	}
}
func decodeToUTF8Base64Header(input string) string {
	if !strings.Contains(input, "=?") {
		// Don't scan if there is nothing to do here
		return input
	}

	tokens := strings.FieldsFunc(input, whiteSpaceRune)
	output := make([]string, len(tokens))
	for i, token := range tokens {
		if len(token) > 4 && strings.Contains(token, "=?") {
			// Stash parenthesis, they should not be encoded
			prefix := ""
			suffix := ""
			if token[0] == '(' {
				prefix = "("
				token = token[1:]
			}
			if token[len(token)-1] == ')' {
				suffix = ")"
				token = token[:len(token)-1]
			}
			// Base64 encode token
			output[i] = prefix + mime.BEncoding.Encode("UTF-8", decodeHeader(token)) + suffix
		} else {
			output[i] = token
		}
	}

	// Return space separated tokens
	return strings.Join(output, " ")
}
func parseMediaType(ctype string) (mtype string, params map[string]string, invalidParams []string, err error) {
	mtype, params, err = mime.ParseMediaType(ctype)
	if err != nil {
		// Small hack to remove harmless charset duplicate params.
		mctype := fixMangledMediaType(ctype, ";")
		mtype, params, err = mime.ParseMediaType(mctype)
		if err != nil {
			// Some badly formed media types forget to send ; between fields.
			mctype := fixMangledMediaType(ctype, " ")
			if strings.Contains(mctype, `name=""`) {
				mctype = strings.Replace(mctype, `name=""`, `name=" "`, -1)
			}
			mtype, params, err = mime.ParseMediaType(mctype)
			if err != nil {
				// If the media parameter has special characters, ensure that it is quoted.
				mtype, params, err = mime.ParseMediaType(fixUnquotedSpecials(mctype))
				if err != nil {
					return "", nil, nil, errors.WithStack(err)
				}
			}
		}
	}
	if mtype == ctPlaceholder {
		mtype = ""
	}
	for name, value := range params {
		if value != pvPlaceholder {
			continue
		}
		invalidParams = append(invalidParams, name)
		delete(params, name)
	}
	return mtype, params, invalidParams, err
}
func fixMangledMediaType(mtype, sep string) string {
	if mtype == "" {
		return ""
	}
	parts := strings.Split(mtype, sep)
	mtype = ""
	for i, p := range parts {
		switch i {
		case 0:
			if p == "" {
				// The content type is completely missing. Put in a placeholder.
				p = ctPlaceholder
			}
		default:
			if !strings.Contains(p, "=") {
				p = p + "=" + pvPlaceholder
			}

			// RFC-2047 encoded attribute name
			p = rfc2047AttributeName(p)

			pair := strings.Split(p, "=")
			if strings.Contains(mtype, pair[0]+"=") {
				// Ignore repeated parameters.
				continue
			}

			if strings.ContainsAny(pair[0], "()<>@,;:\"\\/[]?") {
				// attribute is a strict token and cannot be a quoted-string
				// if any of the above characters are present in a token it
				// must be quoted and is therefor an invalid attribute.
				// Discard the pair.
				continue
			}
		}
		mtype += p
		// Only terminate with semicolon if not the last parameter and if it doesn't already have a
		// semicolon.
		if i != len(parts)-1 && !strings.HasSuffix(mtype, ";") {
			mtype += ";"
		}
	}
	if strings.HasSuffix(mtype, ";") {
		mtype = mtype[:len(mtype)-1]
	}
	return mtype
}
func detectMultipartMessage(root *Part) bool {
	// Parse top-level multipart
	ctype := root.Header.Get(hnContentType)
	mediatype, _, _, err := parseMediaType(ctype)
	if err != nil {
		return false
	}
	// According to rfc2046#section-5.1.7 all other multipart should
	// be treated as multipart/mixed
	return strings.HasPrefix(mediatype, ctMultipartPrefix)
}
func detectBinaryBody(root *Part) bool {
	if detectTextHeader(root.Header, true) {
		return false
	}

	isBin := detectAttachmentHeader(root.Header)
	if !isBin {
		// This must be an attachment, if the Content-Type is not
		// 'text/plain' or 'text/html'.
		// Example:
		// Content-Type: application/pdf; name="doc.pdf"
		mediatype, _, _, _ := parseMediaType(root.Header.Get(hnContentType))
		mediatype = strings.ToLower(mediatype)
		if mediatype != ctTextPlain && mediatype != ctTextHTML {
			return true
		}
	}

	return isBin
}
func (p *Part) BreadthMatchFirst(matcher PartMatcher) *Part {
	q := list.New()
	q.PushBack(p)

	// Push children onto queue and attempt to match in that order
	for q.Len() > 0 {
		e := q.Front()
		p := e.Value.(*Part)
		if matcher(p) {
			return p
		}
		q.Remove(e)
		c := p.FirstChild
		for c != nil {
			q.PushBack(c)
			c = c.NextSibling
		}
	}

	return nil
}
func (p *Part) BreadthMatchAll(matcher PartMatcher) []*Part {
	q := list.New()
	q.PushBack(p)

	matches := make([]*Part, 0, 10)

	// Push children onto queue and attempt to match in that order
	for q.Len() > 0 {
		e := q.Front()
		p := e.Value.(*Part)
		if matcher(p) {
			matches = append(matches, p)
		}
		q.Remove(e)
		c := p.FirstChild
		for c != nil {
			q.PushBack(c)
			c = c.NextSibling
		}
	}

	return matches
}
func (p *Part) DepthMatchFirst(matcher PartMatcher) *Part {
	root := p
	for {
		if matcher(p) {
			return p
		}
		c := p.FirstChild
		if c != nil {
			p = c
		} else {
			for p.NextSibling == nil {
				if p == root {
					return nil
				}
				p = p.Parent
			}
			p = p.NextSibling
		}
	}
}
func (p *Part) DepthMatchAll(matcher PartMatcher) []*Part {
	root := p
	matches := make([]*Part, 0, 10)
	for {
		if matcher(p) {
			matches = append(matches, p)
		}
		c := p.FirstChild
		if c != nil {
			p = c
		} else {
			for p.NextSibling == nil {
				if p == root {
					return matches
				}
				p = p.Parent
			}
			p = p.NextSibling
		}
	}
}
func ToASCII(s string) string {
	// unicode.Mn: nonspacing marks
	tr := transform.Chain(norm.NFD, runes.Remove(runes.In(unicode.Mn)), runes.Map(mapLatinSpecial),
		norm.NFC)
	r, _, _ := transform.String(tr, s)
	return r
}
func NewPart(contentType string) *Part {
	return &Part{
		Header:      make(textproto.MIMEHeader),
		ContentType: contentType,
	}
}
func (p *Part) AddChild(child *Part) {
	if p == child {
		// Prevent paradox.
		return
	}
	if p != nil {
		if p.FirstChild == nil {
			// Make it the first child.
			p.FirstChild = child
		} else {
			// Append to sibling chain.
			current := p.FirstChild
			for current.NextSibling != nil {
				current = current.NextSibling
			}
			if current == child {
				// Prevent infinite loop.
				return
			}
			current.NextSibling = child
		}
	}
	// Update all new first-level children Parent pointers.
	for c := child; c != nil; c = c.NextSibling {
		if c == c.NextSibling {
			// Prevent infinite loop.
			return
		}
		c.Parent = p
	}
}
func (p *Part) TextContent() bool {
	if p.ContentType == "" {
		// RFC 2045: no CT is equivalent to "text/plain; charset=us-ascii"
		return true
	}
	return strings.HasPrefix(p.ContentType, "text/") ||
		strings.HasPrefix(p.ContentType, ctMultipartPrefix)
}
func (p *Part) setupHeaders(r *bufio.Reader, defaultContentType string) error {
	header, err := readHeader(r, p)
	if err != nil {
		return err
	}
	p.Header = header
	ctype := header.Get(hnContentType)
	if ctype == "" {
		if defaultContentType == "" {
			p.addWarning(ErrorMissingContentType, "MIME parts should have a Content-Type header")
			return nil
		}
		ctype = defaultContentType
	}
	// Parse Content-Type header.
	mtype, mparams, minvalidParams, err := parseMediaType(ctype)
	if err != nil {
		return err
	}
	if mtype == "" && len(mparams) > 0 {
		p.addWarning(
			ErrorMissingContentType,
			"Content-Type header has parameters but no content type")
	}
	for i := range minvalidParams {
		p.addWarning(
			ErrorMalformedHeader,
			"Content-Type header has malformed parameter %q",
			minvalidParams[i])
	}
	p.ContentType = mtype
	// Set disposition, filename, charset if available.
	p.setupContentHeaders(mparams)
	p.Boundary = mparams[hpBoundary]
	p.ContentID = coding.FromIDHeader(header.Get(hnContentID))
	return nil
}
func (p *Part) setupContentHeaders(mediaParams map[string]string) {
	// Determine content disposition, filename, character set.
	disposition, dparams, _, err := parseMediaType(p.Header.Get(hnContentDisposition))
	if err == nil {
		// Disposition is optional
		p.Disposition = disposition
		p.FileName = decodeHeader(dparams[hpFilename])
	}
	if p.FileName == "" && mediaParams[hpName] != "" {
		p.FileName = decodeHeader(mediaParams[hpName])
	}
	if p.FileName == "" && mediaParams[hpFile] != "" {
		p.FileName = decodeHeader(mediaParams[hpFile])
	}
	if p.Charset == "" {
		p.Charset = mediaParams[hpCharset]
	}
	if p.FileModDate.IsZero() {
		p.FileModDate, _ = time.Parse(time.RFC822, mediaParams[hpModDate])
	}
}
func (p *Part) convertFromDetectedCharset(r io.Reader) (io.Reader, error) {
	// Attempt to detect character set from part content.
	var cd *chardet.Detector
	switch p.ContentType {
	case "text/html":
		cd = chardet.NewHtmlDetector()
	default:
		cd = chardet.NewTextDetector()
	}

	buf, err := ioutil.ReadAll(r)
	if err != nil {
		return nil, errors.WithStack(err)
	}

	cs, err := cd.DetectBest(buf)
	switch err {
	case nil:
		// Carry on
	case chardet.NotDetectedError:
		p.addWarning(ErrorCharsetDeclaration, "charset could not be detected: %v", err)
	default:
		return nil, errors.WithStack(err)
	}

	// Restore r.
	r = bytes.NewReader(buf)

	if cs == nil || cs.Confidence < minCharsetConfidence {
		// Low confidence, use declared character set.
		return p.convertFromStatedCharset(r), nil
	}

	// Confidence exceeded our threshold, use detected character set.
	if p.Charset != "" && !strings.EqualFold(cs.Charset, p.Charset) {
		p.addWarning(ErrorCharsetDeclaration,
			"declared charset %q, detected %q, confidence %d",
			p.Charset, cs.Charset, cs.Confidence)
	}

	reader, err := coding.NewCharsetReader(cs.Charset, r)
	if err != nil {
		// Failed to get a conversion reader.
		p.addWarning(ErrorCharsetConversion, err.Error())
	} else {
		r = reader
		p.OrigCharset = p.Charset
		p.Charset = cs.Charset
	}

	return r, nil
}
func (p *Part) Clone(parent *Part) *Part {
	if p == nil {
		return nil
	}

	newPart := &Part{
		PartID:      p.PartID,
		Header:      p.Header,
		Parent:      parent,
		Boundary:    p.Boundary,
		ContentID:   p.ContentID,
		ContentType: p.ContentType,
		Disposition: p.Disposition,
		FileName:    p.FileName,
		Charset:     p.Charset,
		Errors:      p.Errors,
		Content:     p.Content,
		Epilogue:    p.Epilogue,
	}
	newPart.FirstChild = p.FirstChild.Clone(newPart)
	newPart.NextSibling = p.NextSibling.Clone(parent)

	return newPart
}
func ReadParts(r io.Reader) (*Part, error) {
	br := bufio.NewReader(r)
	root := &Part{PartID: "0"}
	// Read header; top-level default CT is text/plain us-ascii according to RFC 822.
	err := root.setupHeaders(br, `text/plain; charset="us-ascii"`)
	if err != nil {
		return nil, err
	}
	if strings.HasPrefix(root.ContentType, ctMultipartPrefix) {
		// Content is multipart, parse it.
		err = parseParts(root, br)
		if err != nil {
			return nil, err
		}
	} else {
		// Content is text or data, decode it.
		if err := root.decodeContent(br); err != nil {
			return nil, err
		}
	}
	return root, nil
}
func parseParts(parent *Part, reader *bufio.Reader) error {
	firstRecursion := parent.Parent == nil
	// Loop over MIME boundaries.
	br := newBoundaryReader(reader, parent.Boundary)
	for indexPartID := 1; true; indexPartID++ {
		next, err := br.Next()
		if err != nil && errors.Cause(err) != io.EOF {
			return err
		}
		if !next {
			break
		}
		p := &Part{}
		// Set this Part's PartID, indicating its position within the MIME Part tree.
		if firstRecursion {
			p.PartID = strconv.Itoa(indexPartID)
		} else {
			p.PartID = parent.PartID + "." + strconv.Itoa(indexPartID)
		}
		// Look for part header.
		bbr := bufio.NewReader(br)
		err = p.setupHeaders(bbr, "")
		if errors.Cause(err) == errEmptyHeaderBlock {
			// Empty header probably means the part didn't use the correct trailing "--" syntax to
			// close its boundary.
			if _, err = br.Next(); err != nil {
				if errors.Cause(err) == io.EOF || strings.HasSuffix(err.Error(), "EOF") {
					// There are no more Parts. The error must belong to the parent, because this
					// part doesn't exist.
					parent.addWarning(ErrorMissingBoundary, "Boundary %q was not closed correctly",
						parent.Boundary)
					break
				}
				// The error is already wrapped with a stack, so only adding a message here.
				// TODO: Once `errors` releases a version > v0.8.0, change to use errors.WithMessagef()
				return errors.WithMessage(err, fmt.Sprintf("error at boundary %v", parent.Boundary))
			}
		} else if err != nil {
			return err
		}
		// Insert this Part into the MIME tree.
		parent.AddChild(p)
		if p.Boundary == "" {
			// Content is text or data, decode it.
			if err := p.decodeContent(bbr); err != nil {
				return err
			}
		} else {
			// Content is another multipart.
			err = parseParts(p, bbr)
			if err != nil {
				return err
			}
		}
	}
	// Store any content following the closing boundary marker into the epilogue.
	epilogue, err := ioutil.ReadAll(reader)
	if err != nil {
		return errors.WithStack(err)
	}
	parent.Epilogue = epilogue
	// If a Part is "multipart/" Content-Type, it will have .0 appended to its PartID
	// i.e. it is the root of its MIME Part subtree.
	if !firstRecursion {
		parent.PartID += ".0"
	}
	return nil
}
func UUID() string {
	uuid := make([]byte, 16)
	uuidMutex.Lock()
	_, _ = uuidRand.Read(uuid)
	uuidMutex.Unlock()
	// variant bits; see section 4.1.1
	uuid[8] = uuid[8]&^0xc0 | 0x80
	// version 4 (pseudo-random); see section 4.1.3
	uuid[6] = uuid[6]&^0xf0 | 0x40
	return fmt.Sprintf("%x-%x-%x-%x-%x", uuid[0:4], uuid[4:6], uuid[6:8], uuid[8:10], uuid[10:])
}
func NewQPCleaner(r io.Reader) *QPCleaner {
	return &QPCleaner{
		in: bufio.NewReader(r),
	}
}
func (e *Error) Error() string {
	sev := "W"
	if e.Severe {
		sev = "E"
	}
	return fmt.Sprintf("[%s] %s: %s", sev, e.Name, e.Detail)
}
func (p *Part) addError(name string, detailFmt string, args ...interface{}) {
	p.Errors = append(
		p.Errors,
		&Error{
			name,
			fmt.Sprintf(detailFmt, args...),
			true,
		})
}
func (p *Part) addWarning(name string, detailFmt string, args ...interface{}) {
	p.Errors = append(
		p.Errors,
		&Error{
			name,
			fmt.Sprintf(detailFmt, args...),
			false,
		})
}
func Wrap(max int, strs ...string) []byte {
	input := make([]byte, 0)
	output := make([]byte, 0)
	for _, s := range strs {
		input = append(input, []byte(s)...)
	}
	if len(input) < max {
		// Doesn't need to be wrapped
		return input
	}
	ls := -1 // Last seen space index
	lw := -1 // Last written byte index
	ll := 0  // Length of current line
	for i := 0; i < len(input); i++ {
		ll++
		switch input[i] {
		case ' ', '\t':
			ls = i
		}
		if ll >= max {
			if ls >= 0 {
				output = append(output, input[lw+1:ls]...)
				output = append(output, '\r', '\n', ' ')
				lw = ls // Jump over the space we broke on
				ll = 1  // Count leading space above
				// Rewind
				i = lw + 1
				ls = -1
			}
		}
	}
	return append(output, input[lw+1:]...)
}
func ConvertToUTF8String(charset string, textBytes []byte) (string, error) {
	if strings.ToLower(charset) == utf8 {
		return string(textBytes), nil
	}
	csentry, ok := encodings[strings.ToLower(charset)]
	if !ok {
		return "", fmt.Errorf("Unsupported charset %q", charset)
	}
	input := bytes.NewReader(textBytes)
	reader := transform.NewReader(input, csentry.e.NewDecoder())
	output, err := ioutil.ReadAll(reader)
	if err != nil {
		return "", err
	}
	return string(output), nil
}
func JoinAddress(addrs []mail.Address) string {
	if len(addrs) == 0 {
		return ""
	}
	buf := &bytes.Buffer{}
	for i, a := range addrs {
		if i > 0 {
			_, _ = buf.WriteString(", ")
		}
		_, _ = buf.WriteString(a.String())
	}
	return buf.String()
}
func (md *markdown) Printf(format string, args ...interface{}) {
	fmt.Fprintf(md, format, args...)
}
func EnvelopeToMarkdown(w io.Writer, e *enmime.Envelope, name string) error {
	md := &markdown{bufio.NewWriter(w)}

	md.H1(name)

	// Output a sorted list of headers, minus the ones displayed later
	md.H2("Header")
	if e.Root != nil && e.Root.Header != nil {
		keys := make([]string, 0, len(e.Root.Header))
		for k := range e.Root.Header {
			switch strings.ToLower(k) {
			case "from", "to", "cc", "bcc", "reply-to", "subject":
				continue
			}
			keys = append(keys, k)
		}
		sort.Strings(keys)
		for _, k := range keys {
			md.Printf("    %v: %v\n", k, e.GetHeader(k))
		}
	}
	md.Println()

	md.H2("Envelope")
	for _, hkey := range addressHeaders {
		addrlist, err := e.AddressList(hkey)
		if err != nil {
			if err == mail.ErrHeaderNotPresent {
				continue
			}
			return err
		}
		md.H3(hkey)
		for _, addr := range addrlist {
			md.Printf("- %v `<%v>`\n", addr.Name, addr.Address)
		}
		md.Println()
	}
	md.H3("Subject")
	md.Println(e.GetHeader("Subject"))
	md.Println()

	md.H2("Body Text")
	md.Println(e.Text)
	md.Println()

	md.H2("Body HTML")
	md.Println(e.HTML)
	md.Println()

	md.H2("Attachment List")
	for _, a := range e.Attachments {
		md.Printf("- %v (%v)\n", a.FileName, a.ContentType)
		if a.ContentID != "" {
			md.Printf("  Content-ID: %s\n", a.ContentID)
		}
	}
	md.Println()

	md.H2("Inline List")
	for _, a := range e.Inlines {
		md.Printf("- %v (%v)\n", a.FileName, a.ContentType)
		if a.ContentID != "" {
			md.Printf("  Content-ID: %s\n", a.ContentID)
		}
	}
	md.Println()

	md.H2("Other Part List")
	for _, a := range e.OtherParts {
		md.Printf("- %v (%v)\n", a.FileName, a.ContentType)
		if a.ContentID != "" {
			md.Printf("  Content-ID: %s\n", a.ContentID)
		}
	}
	md.Println()

	md.H2("MIME Part Tree")
	if e.Root == nil {
		md.Println("Message was not MIME encoded")
	} else {
		FormatPart(md, e.Root, "    ")
	}

	if len(e.Errors) > 0 {
		md.Println()
		md.H2("Errors")
		for _, perr := range e.Errors {
			md.Println("-", perr)
		}
	}

	return md.Flush()
}
func FormatPart(w io.Writer, p *enmime.Part, indent string) {
	if p == nil {
		return
	}

	sibling := p.NextSibling
	child := p.FirstChild

	// Compute indent strings
	myindent := indent + "`-- "
	childindent := indent + "    "
	if sibling != nil {
		myindent = indent + "|-- "
		childindent = indent + "|   "
	}
	if p.Parent == nil {
		// Root shouldn't be decorated, has no siblings
		myindent = indent
		childindent = indent
	}

	// Format and print this node
	ctype := "MISSING TYPE"
	if p.ContentType != "" {
		ctype = p.ContentType
	}
	disposition := ""
	if p.Disposition != "" {
		disposition = fmt.Sprintf(", disposition: %s", p.Disposition)
	}
	filename := ""
	if p.FileName != "" {
		filename = fmt.Sprintf(", filename: %q", p.FileName)
	}
	errors := ""
	if len(p.Errors) > 0 {
		errors = fmt.Sprintf(" (errors: %v)", len(p.Errors))
	}
	fmt.Fprintf(w, "%s%s%s%s%s\n", myindent, ctype, disposition, filename, errors)

	// Recurse
	FormatPart(w, child, childindent)
	FormatPart(w, sibling, indent)
}
func newBoundaryReader(reader *bufio.Reader, boundary string) *boundaryReader {
	fullBoundary := []byte("\n--" + boundary + "--")
	return &boundaryReader{
		r:        reader,
		nlPrefix: fullBoundary[:len(fullBoundary)-2],
		prefix:   fullBoundary[1 : len(fullBoundary)-2],
		final:    fullBoundary[1:],
		buffer:   new(bytes.Buffer),
	}
}
func (b *boundaryReader) Read(dest []byte) (n int, err error) {
	if b.buffer.Len() >= len(dest) {
		// This read request can be satisfied entirely by the buffer
		return b.buffer.Read(dest)
	}

	peek, err := b.r.Peek(peekBufferSize)
	peekEOF := (err == io.EOF)
	if err != nil && !peekEOF && err != bufio.ErrBufferFull {
		// Unexpected error
		return 0, errors.WithStack(err)
	}
	var nCopy int
	idx, complete := locateBoundary(peek, b.nlPrefix)
	if idx != -1 {
		// Peeked boundary prefix, read until that point
		nCopy = idx
		if !complete && nCopy == 0 {
			// Incomplete boundary, move past it
			nCopy = 1
		}
	} else {
		// No boundary found, move forward a safe distance
		if nCopy = len(peek) - len(b.nlPrefix) - 1; nCopy <= 0 {
			nCopy = 0
			if peekEOF {
				// No more peek space remaining and no boundary found
				return 0, errors.WithStack(io.ErrUnexpectedEOF)
			}
		}
	}
	if nCopy > 0 {
		if _, err = io.CopyN(b.buffer, b.r, int64(nCopy)); err != nil {
			return 0, errors.WithStack(err)
		}
	}

	n, err = b.buffer.Read(dest)
	if err == io.EOF && !complete {
		// Only the buffer is empty, not the boundaryReader
		return n, nil
	}
	return n, err
}
func (b *boundaryReader) Next() (bool, error) {
	if b.finished {
		return false, nil
	}
	if b.partsRead > 0 {
		// Exhaust the current part to prevent errors when moving to the next part
		_, _ = io.Copy(ioutil.Discard, b)
	}
	for {
		line, err := b.r.ReadSlice('\n')
		if err != nil && err != io.EOF {
			return false, errors.WithStack(err)
		}
		if len(line) > 0 && (line[0] == '\r' || line[0] == '\n') {
			// Blank line
			continue
		}
		if b.isTerminator(line) {
			b.finished = true
			return false, nil
		}
		if err != io.EOF && b.isDelimiter(line) {
			// Start of a new part
			b.partsRead++
			return true, nil
		}
		if err == io.EOF {
			// Intentionally not wrapping with stack
			return false, io.EOF
		}
		if b.partsRead == 0 {
			// The first part didn't find the starting delimiter, burn off any preamble in front of
			// the boundary
			continue
		}
		b.finished = true
		return false, errors.Errorf("expecting boundary %q, got %q", string(b.prefix), string(line))
	}
}
func Parse(buf []byte, offset int) (interface{}, error) {
	obj, _, err := parseReturningOffset(buf, offset)
	return obj, err
}
func Assign(symbol string, value interface{}) ([]byte, error) {
	switch value.(type) {
	case []float64:
		return assignDoubleArray(symbol, value.([]float64))
	case []int32:
		return assignIntArray(symbol, value.([]int32))
	case []string:
		return assignStrArray(symbol, value.([]string))
	case []byte:
		return assignByteArray(symbol, value.([]byte))
	case string:
		return assignStr(symbol, value.(string))
	case int32:
		return assignInt(symbol, value.(int32))
	case float64:
		return assignDouble(symbol, value.(float64))
	default:
		return nil, errors.New("session assign: type is not supported")
	}
}
func NewRClient(host string, port int64) (RClient, error) {
	return NewRClientWithAuth(host, port, "", "")
}
func NewRClientWithAuth(host string, port int64, user, password string) (RClient, error) {
	addr, err := net.ResolveTCPAddr("tcp", host+":"+strconv.FormatInt(port, 10))
	if err != nil {
		return nil, err
	}

	rClient := &roger{
		address:  addr,
		user:     user,
		password: password,
	}

	if _, err = rClient.Eval("'Test session connection'"); err != nil {
		return nil, err
	}
	return rClient, nil
}
func Register(identifier string, generator func() string) {
	fakeType := inflect.Camelize(identifier)
	customGenerators[fakeType] = generator
}
func Fuzz(e interface{}) {
	ty := reflect.TypeOf(e)

	if ty.Kind() == reflect.Ptr {
		ty = ty.Elem()
	}

	if ty.Kind() == reflect.Struct {
		value := reflect.ValueOf(e).Elem()
		for i := 0; i < ty.NumField(); i++ {
			field := value.Field(i)

			if field.CanSet() {
				field.Set(fuzzValueFor(field.Kind()))
			}
		}

	}
}
func findFakeFunctionFor(fako string) func() string {
	result := func() string { return "" }

	for kind, function := range allGenerators() {
		if fako == kind {
			result = function
			break
		}
	}

	return result
}
func (opts *Options) Apply(options ...Option) error {
	for _, o := range options {
		if err := o(opts); err != nil {
			return err
		}
	}
	return nil
}
func (opts *Options) ToOption() Option {
	return func(nopts *Options) error {
		*nopts = *opts
		if opts.Other != nil {
			nopts.Other = make(map[interface{}]interface{}, len(opts.Other))
			for k, v := range opts.Other {
				nopts.Other[k] = v
			}
		}
		return nil
	}
}
func (e *eventChannel) waitThenClose() {
	<-e.ctx.Done()
	e.mu.Lock()
	close(e.ch)
	// 1. Signals that we're done.
	// 2. Frees memory (in case we end up hanging on to this for a while).
	e.ch = nil
	e.mu.Unlock()
}
func (e *eventChannel) send(ctx context.Context, ev *QueryEvent) {
	e.mu.Lock()
	// Closed.
	if e.ch == nil {
		e.mu.Unlock()
		return
	}
	// in case the passed context is unrelated, wait on both.
	select {
	case e.ch <- ev:
	case <-e.ctx.Done():
	case <-ctx.Done():
	}
	e.mu.Unlock()
}
func NewMovingAverage(age ...float64) MovingAverage {
	if len(age) == 0 || age[0] == AVG_METRIC_AGE {
		return new(SimpleEWMA)
	}
	return &VariableEWMA{
		decay: 2 / (age[0] + 1),
	}
}
func (e *VariableEWMA) Set(value float64) {
	e.value = value
	if e.count <= WARMUP_SAMPLES {
		e.count = WARMUP_SAMPLES + 1
	}
}
func calcKeys50(pass, salt []byte, kdfCount int) [][]byte {
	if len(salt) > maxPbkdf2Salt {
		salt = salt[:maxPbkdf2Salt]
	}
	keys := make([][]byte, 3)
	if len(keys) == 0 {
		return keys
	}

	prf := hmac.New(sha256.New, pass)
	prf.Write(salt)
	prf.Write([]byte{0, 0, 0, 1})

	t := prf.Sum(nil)
	u := append([]byte(nil), t...)

	kdfCount--

	for i, iter := range []int{kdfCount, 16, 16} {
		for iter > 0 {
			prf.Reset()
			prf.Write(u)
			u = prf.Sum(u[:0])
			for j := range u {
				t[j] ^= u[j]
			}
			iter--
		}
		keys[i] = append([]byte(nil), t...)
	}

	pwcheck := keys[2]
	for i, v := range pwcheck[pwCheckSize:] {
		pwcheck[i&(pwCheckSize-1)] ^= v
	}
	keys[2] = pwcheck[:pwCheckSize]

	return keys
}
func (a *archive50) getKeys(b *readBuf) (keys [][]byte, err error) {
	if len(*b) < 17 {
		return nil, errCorruptEncrypt
	}
	// read kdf count and salt
	kdfCount := int(b.byte())
	if kdfCount > maxKdfCount {
		return nil, errCorruptEncrypt
	}
	kdfCount = 1 << uint(kdfCount)
	salt := b.bytes(16)

	// check cache of keys for match
	for _, v := range a.keyCache {
		if kdfCount == v.kdfCount && bytes.Equal(salt, v.salt) {
			return v.keys, nil
		}
	}
	// not found, calculate keys
	keys = calcKeys50(a.pass, salt, kdfCount)

	// store in cache
	copy(a.keyCache[1:], a.keyCache[:])
	a.keyCache[0].kdfCount = kdfCount
	a.keyCache[0].salt = append([]byte(nil), salt...)
	a.keyCache[0].keys = keys

	return keys, nil
}
func checkPassword(b *readBuf, keys [][]byte) error {
	if len(*b) < 12 {
		return nil // not enough bytes, ignore for the moment
	}
	pwcheck := b.bytes(8)
	sum := b.bytes(4)
	csum := sha256.Sum256(pwcheck)
	if bytes.Equal(sum, csum[:len(sum)]) && !bytes.Equal(pwcheck, keys[2]) {
		return errBadPassword
	}
	return nil
}
func (a *archive50) parseFileEncryptionRecord(b readBuf, f *fileBlockHeader) error {
	if ver := b.uvarint(); ver != 0 {
		return errUnknownEncMethod
	}
	flags := b.uvarint()

	keys, err := a.getKeys(&b)
	if err != nil {
		return err
	}

	f.key = keys[0]
	if len(b) < 16 {
		return errCorruptEncrypt
	}
	f.iv = b.bytes(16)

	if flags&file5EncCheckPresent > 0 {
		if err := checkPassword(&b, keys); err != nil {
			return err
		}
	}
	if flags&file5EncUseMac > 0 {
		a.checksum.key = keys[1]
	}
	return nil
}
func (a *archive50) parseEncryptionBlock(b readBuf) error {
	if ver := b.uvarint(); ver != 0 {
		return errUnknownEncMethod
	}
	flags := b.uvarint()
	keys, err := a.getKeys(&b)
	if err != nil {
		return err
	}
	if flags&enc5CheckPresent > 0 {
		if err := checkPassword(&b, keys); err != nil {
			return err
		}
	}
	a.blockKey = keys[0]
	return nil
}
func newArchive50(r *bufio.Reader, password string) fileBlockReader {
	a := new(archive50)
	a.v = r
	a.pass = []byte(password)
	a.buf = make([]byte, 100)
	return a
}
func (cr *cipherBlockReader) Read(p []byte) (n int, err error) {
	for {
		if cr.n < len(cr.outbuf) {
			// return buffered output
			n = copy(p, cr.outbuf[cr.n:])
			cr.n += n
			return n, nil
		}
		if cr.err != nil {
			err = cr.err
			cr.err = nil
			return 0, err
		}
		if len(p) >= cap(cr.outbuf) {
			break
		}
		// p is not large enough to process a block, use outbuf instead
		n, cr.err = cr.read(cr.outbuf[:cap(cr.outbuf)])
		cr.outbuf = cr.outbuf[:n]
		cr.n = 0
	}
	// read blocks into p
	return cr.read(p)
}
func (cr *cipherBlockReader) ReadByte() (byte, error) {
	for {
		if cr.n < len(cr.outbuf) {
			c := cr.outbuf[cr.n]
			cr.n++
			return c, nil
		}
		if cr.err != nil {
			err := cr.err
			cr.err = nil
			return 0, err
		}
		// refill outbuf
		var n int
		n, cr.err = cr.read(cr.outbuf[:cap(cr.outbuf)])
		cr.outbuf = cr.outbuf[:n]
		cr.n = 0
	}
}
func newCipherBlockReader(r io.Reader, mode cipher.BlockMode) *cipherBlockReader {
	cr := &cipherBlockReader{r: r, mode: mode}
	cr.outbuf = make([]byte, 0, mode.BlockSize())
	cr.inbuf = make([]byte, 0, mode.BlockSize())
	return cr
}
func newAesDecryptReader(r io.Reader, key, iv []byte) *cipherBlockReader {
	block, err := aes.NewCipher(key)
	if err != nil {
		panic(err)
	}
	mode := cipher.NewCBCDecrypter(block, iv)

	return newCipherBlockReader(r, mode)
}
func limitByteReader(r byteReader, n int64) *limitedByteReader {
	return &limitedByteReader{limitedReader{r, n, io.ErrUnexpectedEOF}, r}
}
func (f *FileHeader) Mode() os.FileMode {
	var m os.FileMode

	if f.IsDir {
		m = os.ModeDir
	}
	if f.HostOS == HostOSWindows {
		if f.IsDir {
			m |= 0777
		} else if f.Attributes&1 > 0 {
			m |= 0444 // readonly
		} else {
			m |= 0666
		}
		return m
	}
	// assume unix perms for all remaining os types
	m |= os.FileMode(f.Attributes) & os.ModePerm

	// only check other bits on unix host created archives
	if f.HostOS != HostOSUnix {
		return m
	}

	if f.Attributes&0x200 != 0 {
		m |= os.ModeSticky
	}
	if f.Attributes&0x400 != 0 {
		m |= os.ModeSetgid
	}
	if f.Attributes&0x800 != 0 {
		m |= os.ModeSetuid
	}

	// Check for additional file types.
	if f.Attributes&0xF000 == 0xA000 {
		m |= os.ModeSymlink
	}
	return m
}
func (f *packedFileReader) nextBlockInFile() error {
	h, err := f.r.next()
	if err != nil {
		if err == io.EOF {
			// archive ended, but file hasn't
			return errUnexpectedArcEnd
		}
		return err
	}
	if h.first || h.Name != f.h.Name {
		return errInvalidFileBlock
	}
	f.h = h
	return nil
}
func (f *packedFileReader) next() (*fileBlockHeader, error) {
	if f.h != nil {
		// skip to last block in current file
		for !f.h.last {
			// discard remaining block data
			if _, err := io.Copy(ioutil.Discard, f.r); err != nil {
				return nil, err
			}
			if err := f.nextBlockInFile(); err != nil {
				return nil, err
			}
		}
		// discard last block data
		if _, err := io.Copy(ioutil.Discard, f.r); err != nil {
			return nil, err
		}
	}
	var err error
	f.h, err = f.r.next() // get next file block
	if err != nil {
		if err == errArchiveEnd {
			return nil, io.EOF
		}
		return nil, err
	}
	if !f.h.first {
		return nil, errInvalidFileBlock
	}
	return f.h, nil
}
func (f *packedFileReader) Read(p []byte) (int, error) {
	n, err := f.r.Read(p) // read current block data
	for err == io.EOF {   // current block empty
		if n > 0 {
			return n, nil
		}
		if f.h == nil || f.h.last {
			return 0, io.EOF // last block so end of file
		}
		if err := f.nextBlockInFile(); err != nil {
			return 0, err
		}
		n, err = f.r.Read(p) // read new block data
	}
	return n, err
}
func (r *Reader) Read(p []byte) (int, error) {
	n, err := r.r.Read(p)
	if err == io.EOF && r.cksum != nil && !r.cksum.valid() {
		return n, errBadFileChecksum
	}
	return n, err
}
func (r *Reader) Next() (*FileHeader, error) {
	if r.solidr != nil {
		// solid files must be read fully to update decoder information
		if _, err := io.Copy(ioutil.Discard, r.solidr); err != nil {
			return nil, err
		}
	}

	h, err := r.pr.next() // skip to next file
	if err != nil {
		return nil, err
	}
	r.solidr = nil

	br := byteReader(&r.pr) // start with packed file reader

	// check for encryption
	if len(h.key) > 0 && len(h.iv) > 0 {
		br = newAesDecryptReader(br, h.key, h.iv) // decrypt
	}
	r.r = br
	// check for compression
	if h.decoder != nil {
		err = r.dr.init(br, h.decoder, h.winSize, !h.solid)
		if err != nil {
			return nil, err
		}
		r.r = &r.dr
		if r.pr.r.isSolid() {
			r.solidr = r.r
		}
	}
	if h.UnPackedSize >= 0 && !h.UnKnownSize {
		// Limit reading to UnPackedSize as there may be padding
		r.r = &limitedReader{r.r, h.UnPackedSize, errShortFile}
	}
	r.cksum = h.cksum
	if r.cksum != nil {
		r.r = io.TeeReader(r.r, h.cksum) // write file data to checksum as it is read
	}
	fh := new(FileHeader)
	*fh = h.FileHeader
	return fh, nil
}
func NewReader(r io.Reader, password string) (*Reader, error) {
	br, ok := r.(*bufio.Reader)
	if !ok {
		br = bufio.NewReader(r)
	}
	fbr, err := newFileBlockReader(br, password)
	if err != nil {
		return nil, err
	}
	rr := new(Reader)
	rr.init(fbr)
	return rr, nil
}
func OpenReader(name, password string) (*ReadCloser, error) {
	v, err := openVolume(name, password)
	if err != nil {
		return nil, err
	}
	rc := new(ReadCloser)
	rc.v = v
	rc.Reader.init(v)
	return rc, nil
}
func getV3Filter(code []byte) (v3Filter, error) {
	// check if filter is a known standard filter
	c := crc32.ChecksumIEEE(code)
	for _, f := range standardV3Filters {
		if f.crc == c && f.len == len(code) {
			return f.f, nil
		}
	}

	// create new vm filter
	f := new(vmFilter)
	r := newRarBitReader(bytes.NewReader(code[1:])) // skip first xor byte check

	// read static data
	n, err := r.readBits(1)
	if err != nil {
		return nil, err
	}
	if n > 0 {
		m, err := r.readUint32()
		if err != nil {
			return nil, err
		}
		f.static = make([]byte, m+1)
		err = r.readFull(f.static)
		if err != nil {
			return nil, err
		}
	}

	f.code, err = readCommands(r)
	if err == io.EOF {
		err = nil
	}

	return f.execute, err
}
func (d *decoder29) init(r io.ByteReader, reset bool) error {
	if d.br == nil {
		d.br = newRarBitReader(r)
	} else {
		d.br.reset(r)
	}
	d.eof = false
	if reset {
		d.initFilters()
		d.lz.reset()
		d.ppm.reset()
		d.decode = nil
	}
	if d.decode == nil {
		return d.readBlockHeader()
	}
	return nil
}
func (d *decoder29) readBlockHeader() error {
	d.br.alignByte()
	n, err := d.br.readBits(1)
	if err == nil {
		if n > 0 {
			d.decode = d.ppm.decode
			err = d.ppm.init(d.br)
		} else {
			d.decode = d.lz.decode
			err = d.lz.init(d.br)
		}
	}
	if err == io.EOF {
		err = errDecoderOutOfData
	}
	return err

}
func readCodeLengthTable(br bitReader, codeLength []byte, addOld bool) error {
	var bitlength [20]byte
	for i := 0; i < len(bitlength); i++ {
		n, err := br.readBits(4)
		if err != nil {
			return err
		}
		if n == 0xf {
			cnt, err := br.readBits(4)
			if err != nil {
				return err
			}
			if cnt > 0 {
				// array already zero'd dont need to explicitly set
				i += cnt + 1
				continue
			}
		}
		bitlength[i] = byte(n)
	}

	var bl huffmanDecoder
	bl.init(bitlength[:])

	for i := 0; i < len(codeLength); i++ {
		l, err := bl.readSym(br)
		if err != nil {
			return err
		}

		if l < 16 {
			if addOld {
				codeLength[i] = (codeLength[i] + byte(l)) & 0xf
			} else {
				codeLength[i] = byte(l)
			}
			continue
		}

		var count int
		var value byte

		switch l {
		case 16, 18:
			count, err = br.readBits(3)
			count += 3
		default:
			count, err = br.readBits(7)
			count += 11
		}
		if err != nil {
			return err
		}
		if l < 18 {
			if i == 0 {
				return errInvalidLengthTable
			}
			value = codeLength[i-1]
		}
		for ; count > 0 && i < len(codeLength); i++ {
			codeLength[i] = value
			count--
		}
		i--
	}
	return nil
}
func (c *context) shrinkStates(states []state, size int) []state {
	i1 := units2Index[(len(states)+1)>>1]
	i2 := units2Index[(size+1)>>1]

	if size == 1 {
		// store state in context, and free states block
		n := c.statesIndex()
		c.s[1] = states[0]
		states = c.s[1:]
		c.a.addFreeBlock(n, i1)
	} else if i1 != i2 {
		if n := c.a.removeFreeBlock(i2); n > 0 {
			// allocate new block and copy
			copy(c.a.states[n:], states[:size])
			states = c.a.states[n:]
			// free old block
			c.a.addFreeBlock(c.statesIndex(), i1)
			c.setStatesIndex(n)
		} else {
			// split current block, and free units not needed
			n = c.statesIndex() + index2Units[i2]<<1
			u := index2Units[i1] - index2Units[i2]
			c.a.freeUnits(n, u)
		}
	}
	c.setNumStates(size)
	return states[:size]
}
func (c *context) expandStates() []state {
	states := c.states()
	ns := len(states)
	if ns == 1 {
		s := states[0]
		n := c.a.allocUnits(1)
		if n == 0 {
			return nil
		}
		c.setStatesIndex(n)
		states = c.a.states[n:]
		states[0] = s
	} else if ns&0x1 == 0 {
		u := ns >> 1
		i1 := units2Index[u]
		i2 := units2Index[u+1]
		if i1 != i2 {
			n := c.a.allocUnits(i2)
			if n == 0 {
				return nil
			}
			copy(c.a.states[n:], states)
			c.a.addFreeBlock(c.statesIndex(), i1)
			c.setStatesIndex(n)
			states = c.a.states[n:]
		}
	}
	c.setNumStates(ns + 1)
	return states[:ns+1]
}
func (a *subAllocator) pushByte(c byte) int32 {
	si := a.heap1Lo / 6 // state index
	oi := a.heap1Lo % 6 // byte position in state
	switch oi {
	case 0:
		a.states[si].sym = c
	case 1:
		a.states[si].freq = c
	default:
		n := (uint(oi) - 2) * 8
		mask := ^(uint32(0xFF) << n)
		succ := uint32(a.states[si].succ) & mask
		succ |= uint32(c) << n
		a.states[si].succ = int32(succ)
	}
	a.heap1Lo++
	if a.heap1Lo >= a.heap1Hi {
		return 0
	}
	return -a.heap1Lo
}
func (a *subAllocator) succByte(i int32) byte {
	i = -i
	si := i / 6
	oi := i % 6
	switch oi {
	case 0:
		return a.states[si].sym
	case 1:
		return a.states[si].freq
	default:
		n := (uint(oi) - 2) * 8
		succ := uint32(a.states[si].succ) >> n
		return byte(succ & 0xff)
	}
}
func (a *subAllocator) succContext(i int32) *context {
	if i <= 0 {
		return nil
	}
	return &context{i: i, s: a.states[i : i+2 : i+2], a: a}
}
func calcAes30Params(pass []uint16, salt []byte) (key, iv []byte) {
	p := make([]byte, 0, len(pass)*2+len(salt))
	for _, v := range pass {
		p = append(p, byte(v), byte(v>>8))
	}
	p = append(p, salt...)

	hash := sha1.New()
	iv = make([]byte, 16)
	s := make([]byte, 0, hash.Size())
	for i := 0; i < hashRounds; i++ {
		hash.Write(p)
		hash.Write([]byte{byte(i), byte(i >> 8), byte(i >> 16)})
		if i%(hashRounds/16) == 0 {
			s = hash.Sum(s[:0])
			iv[i/(hashRounds/16)] = s[4*4+3]
		}
	}
	key = hash.Sum(s[:0])
	key = key[:16]

	for k := key; len(k) >= 4; k = k[4:] {
		k[0], k[1], k[2], k[3] = k[3], k[2], k[1], k[0]
	}
	return key, iv
}
func parseDosTime(t uint32) time.Time {
	n := int(t)
	sec := n & 0x1f << 1
	min := n >> 5 & 0x3f
	hr := n >> 11 & 0x1f
	day := n >> 16 & 0x1f
	mon := time.Month(n >> 21 & 0x0f)
	yr := n>>25&0x7f + 1980
	return time.Date(yr, mon, day, hr, min, sec, 0, time.Local)
}
func decodeName(buf []byte) string {
	i := bytes.IndexByte(buf, 0)
	if i < 0 {
		return string(buf) // filename is UTF-8
	}

	name := buf[:i]
	encName := readBuf(buf[i+1:])
	if len(encName) < 2 {
		return "" // invalid encoding
	}
	highByte := uint16(encName.byte()) << 8
	flags := encName.byte()
	flagBits := 8
	var wchars []uint16 // decoded characters are UTF-16
	for len(wchars) < len(name) && len(encName) > 0 {
		if flagBits == 0 {
			flags = encName.byte()
			flagBits = 8
			if len(encName) == 0 {
				break
			}
		}
		switch flags >> 6 {
		case 0:
			wchars = append(wchars, uint16(encName.byte()))
		case 1:
			wchars = append(wchars, uint16(encName.byte())|highByte)
		case 2:
			if len(encName) < 2 {
				break
			}
			wchars = append(wchars, encName.uint16())
		case 3:
			n := encName.byte()
			b := name[len(wchars):]
			if l := int(n&0x7f) + 2; l < len(b) {
				b = b[:l]
			}
			if n&0x80 > 0 {
				if len(encName) < 1 {
					break
				}
				ec := encName.byte()
				for _, c := range b {
					wchars = append(wchars, uint16(c+ec)|highByte)
				}
			} else {
				for _, c := range b {
					wchars = append(wchars, uint16(c))
				}
			}
		}
		flags <<= 2
		flagBits -= 2
	}
	return string(utf16.Decode(wchars))
}
func readExtTimes(f *fileBlockHeader, b *readBuf) {
	if len(*b) < 2 {
		return // invalid, not enough data
	}
	flags := b.uint16()

	ts := []*time.Time{&f.ModificationTime, &f.CreationTime, &f.AccessTime}

	for i, t := range ts {
		n := flags >> uint((3-i)*4)
		if n&0x8 == 0 {
			continue
		}
		if i != 0 { // ModificationTime already read so skip
			if len(*b) < 4 {
				return // invalid, not enough data
			}
			*t = parseDosTime(b.uint32())
		}
		if n&0x4 > 0 {
			*t = t.Add(time.Second)
		}
		n &= 0x3
		if n == 0 {
			continue
		}
		if len(*b) < int(n) {
			return // invalid, not enough data
		}
		// add extra time data in 100's of nanoseconds
		d := time.Duration(0)
		for j := 3 - n; j < n; j++ {
			d |= time.Duration(b.byte()) << (j * 8)
		}
		d *= 100
		*t = t.Add(d)
	}
}
func (a *archive15) readBlockHeader() (*blockHeader15, error) {
	var err error
	b := a.buf[:7]
	r := io.Reader(a.v)
	if a.encrypted {
		salt := a.buf[:saltSize]
		_, err = io.ReadFull(r, salt)
		if err != nil {
			return nil, err
		}
		key, iv := a.getKeys(salt)
		r = newAesDecryptReader(r, key, iv)
		err = readFull(r, b)
	} else {
		_, err = io.ReadFull(r, b)
	}
	if err != nil {
		return nil, err
	}

	crc := b.uint16()
	hash := crc32.NewIEEE()
	hash.Write(b)
	h := new(blockHeader15)
	h.htype = b.byte()
	h.flags = b.uint16()
	size := b.uint16()
	if size < 7 {
		return nil, errCorruptHeader
	}
	size -= 7
	if int(size) > cap(a.buf) {
		a.buf = readBuf(make([]byte, size))
	}
	h.data = a.buf[:size]
	if err := readFull(r, h.data); err != nil {
		return nil, err
	}
	hash.Write(h.data)
	if crc != uint16(hash.Sum32()) {
		return nil, errBadHeaderCrc
	}
	if h.flags&blockHasData > 0 {
		if len(h.data) < 4 {
			return nil, errCorruptHeader
		}
		h.dataSize = int64(h.data.uint32())
	}
	if (h.htype == blockService || h.htype == blockFile) && h.flags&fileLargeData > 0 {
		if len(h.data) < 25 {
			return nil, errCorruptHeader
		}
		b := h.data[21:25]
		h.dataSize |= int64(b.uint32()) << 32
	}
	return h, nil
}
func newArchive15(r *bufio.Reader, password string) fileBlockReader {
	a := new(archive15)
	a.v = r
	a.pass = utf16.Encode([]rune(password)) // convert to UTF-16
	a.checksum.Hash32 = crc32.NewIEEE()
	a.buf = readBuf(make([]byte, 100))
	return a
}
func readFilter5Data(br bitReader) (int, error) {
	// TODO: should data really be uint? (for 32bit ints).
	// It will be masked later anyway by decode window mask.
	bytes, err := br.readBits(2)
	if err != nil {
		return 0, err
	}
	bytes++

	var data int
	for i := 0; i < bytes; i++ {
		n, err := br.readBits(8)
		if err != nil {
			return 0, err
		}
		data |= n << (uint(i) * 8)
	}
	return data, nil
}
func (w *window) writeByte(c byte) {
	w.buf[w.w] = c
	w.w = (w.w + 1) & w.mask
}
func (w *window) copyBytes(len, off int) {
	len &= w.mask

	n := w.available()
	if len > n {
		// if there is not enough space availaible we copy
		// as much as we can and save the offset and length
		// of the remaining data to be copied later.
		w.l = len - n
		w.o = off
		len = n
	}

	i := (w.w - off) & w.mask
	for ; len > 0; len-- {
		w.buf[w.w] = w.buf[i]
		w.w = (w.w + 1) & w.mask
		i = (i + 1) & w.mask
	}
}
func (w *window) read(p []byte) (n int) {
	if w.r > w.w {
		n = copy(p, w.buf[w.r:])
		w.r = (w.r + n) & w.mask
		p = p[n:]
	}
	if w.r < w.w {
		l := copy(p, w.buf[w.r:w.w])
		w.r += l
		n += l
	}
	if w.l > 0 && n > 0 {
		// if we have successfully read data, copy any
		// leftover data from a previous copyBytes.
		l := w.l
		w.l = 0
		w.copyBytes(l, w.o)
	}
	return n
}
func (d *decodeReader) queueFilter(f *filterBlock) error {
	if f.reset {
		d.filters = nil
	}
	if len(d.filters) >= maxQueuedFilters {
		return errTooManyFilters
	}
	// offset & length must be < window size
	f.offset &= d.win.mask
	f.length &= d.win.mask
	// make offset relative to previous filter in list
	for _, fb := range d.filters {
		if f.offset < fb.offset {
			// filter block must not start before previous filter
			return errInvalidFilter
		}
		f.offset -= fb.offset
	}
	d.filters = append(d.filters, f)
	return nil
}
func (d *decodeReader) processFilters() (err error) {
	f := d.filters[0]
	if f.offset > 0 {
		return nil
	}
	d.filters = d.filters[1:]
	if d.win.buffered() < f.length {
		// fill() didn't return enough bytes
		err = d.readErr()
		if err == nil || err == io.EOF {
			return errInvalidFilter
		}
		return err
	}

	if cap(d.buf) < f.length {
		d.buf = make([]byte, f.length)
	}
	d.outbuf = d.buf[:f.length]
	n := d.win.read(d.outbuf)
	for {
		// run filter passing buffer and total bytes read so far
		d.outbuf, err = f.filter(d.outbuf, d.tot)
		if err != nil {
			return err
		}
		if cap(d.outbuf) > cap(d.buf) {
			// Filter returned a bigger buffer, save it for future filters.
			d.buf = d.outbuf
		}
		if len(d.filters) == 0 {
			return nil
		}
		f = d.filters[0]

		if f.offset != 0 {
			// next filter not at current offset
			f.offset -= n
			return nil
		}
		if f.length != len(d.outbuf) {
			return errInvalidFilter
		}
		d.filters = d.filters[1:]

		if cap(d.outbuf) < cap(d.buf) {
			// Filter returned a smaller buffer. Copy it back to the saved buffer
			// so the next filter can make use of the larger buffer if needed.
			d.outbuf = append(d.buf[:0], d.outbuf...)
		}
	}
}
func (d *decodeReader) fill() {
	if d.err != nil {
		return
	}
	var fl []*filterBlock
	fl, d.err = d.dec.fill(&d.win) // fill window using decoder
	for _, f := range fl {
		err := d.queueFilter(f)
		if err != nil {
			d.err = err
			return
		}
	}
}
func (d *decodeReader) Read(p []byte) (n int, err error) {
	if len(d.outbuf) == 0 {
		// no filter output, see if we need to create more
		if d.win.buffered() == 0 {
			// fill empty window
			d.fill()
			if d.win.buffered() == 0 {
				return 0, d.readErr()
			}
		} else if len(d.filters) > 0 {
			f := d.filters[0]
			if f.offset == 0 && f.length > d.win.buffered() {
				d.fill() // filter at current offset needs more data
			}
		}
		if len(d.filters) > 0 {
			if err := d.processFilters(); err != nil {
				return 0, err
			}
		}
	}
	if len(d.outbuf) > 0 {
		// copy filter output into p
		n = copy(p, d.outbuf)
		d.outbuf = d.outbuf[n:]
	} else if len(d.filters) > 0 {
		f := d.filters[0]
		if f.offset < len(p) {
			// only read data up to beginning of next filter
			p = p[:f.offset]
		}
		n = d.win.read(p) // read directly from window
		f.offset -= n     // adjust first filter offset by bytes just read
	} else {
		n = d.win.read(p) // read directly from window
	}
	d.tot += int64(n)
	return n, nil
}
func readFull(r io.Reader, buf []byte) error {
	_, err := io.ReadFull(r, buf)
	if err == io.EOF {
		return io.ErrUnexpectedEOF
	}
	return err
}
func findSig(br *bufio.Reader) (int, error) {
	for n := 0; n <= maxSfxSize; {
		b, err := br.ReadSlice(sigPrefix[0])
		n += len(b)
		if err == bufio.ErrBufferFull {
			continue
		} else if err != nil {
			if err == io.EOF {
				err = errNoSig
			}
			return 0, err
		}

		b, err = br.Peek(len(sigPrefix[1:]) + 2)
		if err != nil {
			if err == io.EOF {
				err = errNoSig
			}
			return 0, err
		}
		if !bytes.HasPrefix(b, []byte(sigPrefix[1:])) {
			continue
		}
		b = b[len(sigPrefix)-1:]

		var ver int
		switch {
		case b[0] == 0:
			ver = fileFmt15
		case b[0] == 1 && b[1] == 0:
			ver = fileFmt50
		default:
			continue
		}
		_, _ = br.ReadSlice('\x00')

		return ver, nil
	}
	return 0, errNoSig
}
func (v *vm) execute(cmd []command) {
	v.ip = 0 // reset instruction pointer
	for n := 0; n < maxCommands; n++ {
		ip := v.ip
		if ip >= uint32(len(cmd)) {
			return
		}
		ins := cmd[ip]
		ins.f(v, ins.bm, ins.op) // run cpu instruction
		if v.ipMod {
			// command modified ip, don't increment
			v.ipMod = false
		} else {
			v.ip++ // increment ip for next command
		}
	}
}
func newVM(mem []byte) *vm {
	v := new(vm)

	if cap(mem) < vmSize+4 {
		v.m = make([]byte, vmSize+4)
		copy(v.m, mem)
	} else {
		v.m = mem[:vmSize+4]
		for i := len(mem); i < len(v.m); i++ {
			v.m[i] = 0
		}
	}
	v.r[7] = vmSize
	return v
}
func limitBitReader(br bitReader, n int, err error) bitReader {
	return &limitedBitReader{br, n, err}
}
func (r *rarBitReader) readUint32() (uint32, error) {
	n, err := r.readBits(2)
	if err != nil {
		return 0, err
	}
	if n != 1 {
		n, err = r.readBits(4 << uint(n))
		return uint32(n), err
	}
	n, err = r.readBits(4)
	if err != nil {
		return 0, err
	}
	if n == 0 {
		n, err = r.readBits(8)
		n |= -1 << 8
		return uint32(n), err
	}
	nlow, err := r.readBits(4)
	n = n<<4 | nlow
	return uint32(n), err
}
func step3(word *snowballword.SnowballWord) bool {

	// Search for a DERIVATIONAL ending in R2 (i.e. the entire
	// ending must lie in R2), and if one is found, remove it.

	suffix, _ := word.RemoveFirstSuffixIn(word.R2start, "ост", "ость")
	if suffix != "" {
		return true
	}
	return false
}
func Stem(word string, stemStopwWords bool) string {

	word = strings.ToLower(strings.TrimSpace(word))

	// Return small words and stop words
	if len(word) <= 2 || (stemStopwWords == false && isStopWord(word)) {
		return word
	}

	// Return special words immediately
	if specialVersion := stemSpecialWord(word); specialVersion != "" {
		word = specialVersion
		return word
	}

	w := snowballword.New(word)

	// Stem the word.  Note, each of these
	// steps will alter `w` in place.
	//
	preprocess(w)
	step0(w)
	step1a(w)
	step1b(w)
	step1c(w)
	step2(w)
	step3(w)
	step4(w)
	step5(w)
	postprocess(w)

	return w.String()

}
func step6(word *snowballword.SnowballWord) bool {

	// If the words ends é or è (unicode code points 233 and 232)
	// followed by at least one non-vowel, remove the accent from the e.

	// Note, this step is oddly articulated on Porter's Snowball website:
	// http://snowball.tartarus.org/algorithms/french/stemmer.html
	// More clearly stated, we should replace é or è with e in the
	// case where the suffix of the word is é or è followed by
	// one-or-more non-vowels.

	numNonVowels := 0
	for i := len(word.RS) - 1; i >= 0; i-- {
		r := word.RS[i]

		if isLowerVowel(r) == false {
			numNonVowels += 1
		} else {

			// `r` is a vowel

			if (r == 233 || r == 232) && numNonVowels > 0 {

				// Replace with "e", or unicode code point 101
				word.RS[i] = 101
				return true

			}
			return false
		}

	}
	return false
}
func step5(word *snowballword.SnowballWord) bool {

	suffix, _ := word.FirstSuffix("enn", "onn", "ett", "ell", "eill")
	if suffix != "" {
		word.RemoveLastNRunes(1)
	}
	return false
}
func step2a(word *snowballword.SnowballWord) bool {
	suffix, suffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS), "ya", "ye", "yan", "yen", "yeron", "yendo", "yo", "yó", "yas", "yes", "yais", "yamos")
	if suffix != "" {
		idx := len(word.RS) - len(suffixRunes) - 1
		if idx >= 0 && word.RS[idx] == 117 {
			word.RemoveLastNRunes(len(suffixRunes))
			return true
		}
	}
	return false
}
func step4(word *snowballword.SnowballWord) bool {

	// (1) Undouble "н", or, 2) if the word ends with a SUPERLATIVE ending,
	// (remove it and undouble н n), or 3) if the word ends ь (') (soft sign)
	// remove it.

	// Undouble "н"
	if word.HasSuffixRunes([]rune("нн")) {
		word.RemoveLastNRunes(1)
		return true
	}

	// Remove superlative endings
	suffix, _ := word.RemoveFirstSuffix("ейше", "ейш")
	if suffix != "" {
		// Undouble "н"
		if word.HasSuffixRunes([]rune("нн")) {
			word.RemoveLastNRunes(1)
		}
		return true
	}

	// Remove soft sign
	if rsLen := len(word.RS); rsLen > 0 && word.RS[rsLen-1] == 'ь' {
		word.RemoveLastNRunes(1)
		return true
	}
	return false
}
func Stem(word, language string, stemStopWords bool) (stemmed string, err error) {

	var f func(string, bool) string
	switch language {
	case "english":
		f = english.Stem
	case "spanish":
		f = spanish.Stem
	case "french":
		f = french.Stem
	case "russian":
		f = russian.Stem
	case "swedish":
		f = swedish.Stem
	case "norwegian":
		f = norwegian.Stem
	default:
		err = fmt.Errorf("Unknown language: %s", language)
		return
	}
	stemmed = f(word, stemStopWords)
	return

}
func step1c(w *snowballword.SnowballWord) bool {

	rsLen := len(w.RS)

	// Replace suffix y or Y by i if preceded by a non-vowel which is not
	// the first letter of the word (so cry -> cri, by -> by, say -> say)
	//
	// Note: the unicode code points for
	// y, Y, & i are 121, 89, & 105 respectively.
	//
	if len(w.RS) > 2 && (w.RS[rsLen-1] == 121 || w.RS[rsLen-1] == 89) && !isLowerVowel(w.RS[rsLen-2]) {
		w.RS[rsLen-1] = 105
		return true
	}
	return false
}
func step3(w *snowballword.SnowballWord) bool {

	suffix, suffixRunes := w.FirstSuffix(
		"ational", "tional", "alize", "icate", "ative",
		"iciti", "ical", "ful", "ness",
	)

	// If it is not in R1, do nothing
	if suffix == "" || len(suffixRunes) > len(w.RS)-w.R1start {
		return false
	}

	// Handle special cases where we're not just going to
	// replace the suffix with another suffix: there are
	// other things we need to do.
	//
	if suffix == "ative" {

		// If in R2, delete.
		//
		if len(w.RS)-w.R2start >= 5 {
			w.RemoveLastNRunes(len(suffixRunes))
			return true
		}
		return false
	}

	// Handle a suffix that was found, which is going
	// to be replaced with a different suffix.
	//
	var repl string
	switch suffix {
	case "ational":
		repl = "ate"
	case "tional":
		repl = "tion"
	case "alize":
		repl = "al"
	case "icate", "iciti", "ical":
		repl = "ic"
	case "ful", "ness":
		repl = ""
	}
	w.ReplaceSuffixRunes(suffixRunes, []rune(repl), true)
	return true

}
func isStopWord(word string) bool {
	switch word {
	case "au", "aux", "avec", "ce", "ces", "dans", "de", "des", "du",
		"elle", "en", "et", "eux", "il", "je", "la", "le", "leur",
		"lui", "ma", "mais", "me", "même", "mes", "moi", "mon", "ne",
		"nos", "notre", "nous", "on", "ou", "par", "pas", "pour", "qu",
		"que", "qui", "sa", "se", "ses", "son", "sur", "ta", "te",
		"tes", "toi", "ton", "tu", "un", "une", "vos", "votre", "vous",
		"c", "d", "j", "l", "à", "m", "n", "s", "t", "y", "été",
		"étée", "étées", "étés", "étant", "étante", "étants", "étantes",
		"suis", "es", "est", "sommes", "êtes", "sont", "serai",
		"seras", "sera", "serons", "serez", "seront", "serais",
		"serait", "serions", "seriez", "seraient", "étais", "était",
		"étions", "étiez", "étaient", "fus", "fut", "fûmes", "fûtes",
		"furent", "sois", "soit", "soyons", "soyez", "soient", "fusse",
		"fusses", "fût", "fussions", "fussiez", "fussent", "ayant",
		"ayante", "ayantes", "ayants", "eu", "eue", "eues", "eus",
		"ai", "as", "avons", "avez", "ont", "aurai", "auras", "aura",
		"aurons", "aurez", "auront", "aurais", "aurait", "aurions",
		"auriez", "auraient", "avais", "avait", "avions", "aviez",
		"avaient", "eut", "eûmes", "eûtes", "eurent", "aie", "aies",
		"ait", "ayons", "ayez", "aient", "eusse", "eusses", "eût",
		"eussions", "eussiez", "eussent":
		return true
	}
	return false
}
func capitalizeYUI(word *snowballword.SnowballWord) {

	// Keep track of vowels that we see
	vowelPreviously := false

	// Peak ahead to see if the next rune is a vowel
	vowelNext := func(j int) bool {
		return (j+1 < len(word.RS) && isLowerVowel(word.RS[j+1]))
	}

	// Look at all runes
	for i := 0; i < len(word.RS); i++ {

		// Nothing to do for non-vowels
		if isLowerVowel(word.RS[i]) == false {
			vowelPreviously = false
			continue
		}

		vowelHere := true

		switch word.RS[i] {
		case 121: // y

			// Is this "y" preceded OR followed by a vowel?
			if vowelPreviously || vowelNext(i) {
				word.RS[i] = 89 // Y
				vowelHere = false
			}

		case 117: // u

			// Is this "u" is flanked by vowels OR preceded by a "q"?
			if (vowelPreviously && vowelNext(i)) || (i >= 1 && word.RS[i-1] == 113) {
				word.RS[i] = 85 // U
				vowelHere = false
			}

		case 105: // i

			// Is this "i" is flanked by vowels?
			if vowelPreviously && vowelNext(i) {
				word.RS[i] = 73 // I
				vowelHere = false
			}
		}
		vowelPreviously = vowelHere
	}
}
func step2(w *snowballword.SnowballWord) bool {

	// Possible sufficies for this step, longest first.
	suffix, suffixRunes := w.FirstSuffix(
		"ational", "fulness", "iveness", "ization", "ousness",
		"biliti", "lessli", "tional", "alism", "aliti", "ation",
		"entli", "fulli", "iviti", "ousli", "anci", "abli",
		"alli", "ator", "enci", "izer", "bli", "ogi", "li",
	)

	// If it is not in R1, do nothing
	if suffix == "" || len(suffixRunes) > len(w.RS)-w.R1start {
		return false
	}

	// Handle special cases where we're not just going to
	// replace the suffix with another suffix: there are
	// other things we need to do.
	//
	switch suffix {

	case "li":

		// Delete if preceded by a valid li-ending. Valid li-endings inlude the
		// following charaters: cdeghkmnrt. (Note, the unicode code points for
		// these characters are, respectively, as follows:
		// 99 100 101 103 104 107 109 110 114 116)
		//
		rsLen := len(w.RS)
		if rsLen >= 3 {
			switch w.RS[rsLen-3] {
			case 99, 100, 101, 103, 104, 107, 109, 110, 114, 116:
				w.RemoveLastNRunes(len(suffixRunes))
				return true
			}
		}
		return false

	case "ogi":

		// Replace by og if preceded by l.
		// (Note, the unicode code point for l is 108)
		//
		rsLen := len(w.RS)
		if rsLen >= 4 && w.RS[rsLen-4] == 108 {
			w.ReplaceSuffixRunes(suffixRunes, []rune("og"), true)
		}
		return true
	}

	// Handle a suffix that was found, which is going
	// to be replaced with a different suffix.
	//
	var repl string
	switch suffix {
	case "tional":
		repl = "tion"
	case "enci":
		repl = "ence"
	case "anci":
		repl = "ance"
	case "abli":
		repl = "able"
	case "entli":
		repl = "ent"
	case "izer", "ization":
		repl = "ize"
	case "ational", "ation", "ator":
		repl = "ate"
	case "alism", "aliti", "alli":
		repl = "al"
	case "fulness":
		repl = "ful"
	case "ousli", "ousness":
		repl = "ous"
	case "iveness", "iviti":
		repl = "ive"
	case "biliti", "bli":
		repl = "ble"
	case "fulli":
		repl = "ful"
	case "lessli":
		repl = "less"
	}
	w.ReplaceSuffixRunes(suffixRunes, []rune(repl), true)
	return true

}
func step3(word *snowballword.SnowballWord) bool {
	suffix, suffixRunes := word.FirstSuffixIfIn(word.RVstart, len(word.RS),
		"os", "a", "o", "á", "í", "ó", "e", "é",
	)

	// No suffix found, nothing to do.
	//
	if suffix == "" {
		return false
	}

	// Remove all these suffixes
	word.RemoveLastNRunes(len(suffixRunes))

	if suffix == "e" || suffix == "é" {

		// If preceded by gu with the u in RV delete the u
		//
		guSuffix, _ := word.FirstSuffix("gu")
		if guSuffix != "" {
			word.RemoveLastNRunes(1)
		}
	}
	return true
}
func step0(w *snowballword.SnowballWord) bool {
	suffix, suffixRunes := w.FirstSuffix("'s'", "'s", "'")
	if suffix == "" {
		return false
	}
	w.RemoveLastNRunes(len(suffixRunes))
	return true
}
func VnvSuffix(word *snowballword.SnowballWord, f isVowelFunc, start int) int {
	for i := 1; i < len(word.RS[start:]); i++ {
		j := start + i
		if f(word.RS[j-1]) && !f(word.RS[j]) {
			return j + 1
		}
	}
	return len(word.RS)
}
func step1(w *snowballword.SnowballWord) bool {

	// Possible sufficies for this step, longest first.
	suffixes := []string{
		"heterna", "hetens", "anden", "heten", "heter", "arnas",
		"ernas", "ornas", "andes", "arens", "andet", "arna", "erna",
		"orna", "ande", "arne", "aste", "aren", "ades", "erns", "ade",
		"are", "ern", "ens", "het", "ast", "ad", "en", "ar", "er",
		"or", "as", "es", "at", "a", "e", "s",
	}

	// Using FirstSuffixIn since there are overlapping suffixes, where some might not be in the R1,
	// while another might. For example: "ärade"
	suffix, suffixRunes := w.FirstSuffixIn(w.R1start, len(w.RS), suffixes...)

	// If it is not in R1, do nothing
	if suffix == "" || len(suffixRunes) > len(w.RS)-w.R1start {
		return false
	}

	if suffix == "s" {
		// Delete if preceded by a valid s-ending. Valid s-endings inlude the
		// following charaters: bcdfghjklmnoprtvy.
		//
		rsLen := len(w.RS)
		if rsLen >= 2 {
			switch w.RS[rsLen-2] {
			case 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k',
				'l', 'm', 'n', 'o', 'p', 'r', 't', 'v', 'y':
				w.RemoveLastNRunes(len(suffixRunes))
				return true
			}
		}
		return false
	}
	// Remove the suffix
	w.RemoveLastNRunes(len(suffixRunes))
	return true
}
func step2a(word *snowballword.SnowballWord) bool {

	// Search for the longest among the following suffixes
	// in RV and if found, delete if preceded by a non-vowel.

	suffix, suffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS),
		"issantes", "issaIent", "issions", "issants", "issante",
		"iraIent", "issons", "issiez", "issent", "issant", "issait",
		"issais", "irions", "issez", "isses", "iront", "irons", "iriez",
		"irent", "irait", "irais", "îtes", "îmes", "isse", "irez",
		"iras", "irai", "ira", "ies", "ît", "it", "is", "ir", "ie", "i",
	)
	if suffix != "" {
		sLen := len(suffixRunes)
		idx := len(word.RS) - sLen - 1
		if idx >= 0 && word.FitsInRV(sLen+1) && isLowerVowel(word.RS[idx]) == false {
			word.RemoveLastNRunes(len(suffixRunes))
			return true
		}
	}
	return false
}
func removePerfectiveGerundEnding(word *snowballword.SnowballWord) bool {
	suffix, suffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS),
		"ившись", "ывшись", "вшись", "ивши", "ывши", "вши", "ив", "ыв", "в",
	)
	switch suffix {
	case "в", "вши", "вшись":

		// These are "Group 1" perfective gerund endings.
		// Group 1 endings must follow а (a) or я (ia) in RV.
		if precededByARinRV(word, len(suffixRunes)) == false {
			suffix = ""
		}

	}

	if suffix != "" {
		word.RemoveLastNRunes(len(suffixRunes))
		return true
	}
	return false
}
func removeAdjectivalEnding(word *snowballword.SnowballWord) bool {

	// Remove adjectival endings.  Start by looking for
	// an adjective ending.
	//
	suffix, _ := word.RemoveFirstSuffixIn(word.RVstart,
		"ими", "ыми", "его", "ого", "ему", "ому", "ее", "ие",
		"ые", "ое", "ей", "ий", "ый", "ой", "ем", "им", "ым",
		"ом", "их", "ых", "ую", "юю", "ая", "яя", "ою", "ею",
	)
	if suffix != "" {

		// We found an adjective ending.  Remove optional participle endings.
		//
		newSuffix, newSuffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS),
			"ивш", "ывш", "ующ",
			"ем", "нн", "вш", "ющ", "щ",
		)
		switch newSuffix {
		case "ем", "нн", "вш", "ющ", "щ":

			// These are "Group 1" participle endings.
			// Group 1 endings must follow а (a) or я (ia) in RV.
			if precededByARinRV(word, len(newSuffixRunes)) == false {
				newSuffix = ""
			}
		}

		if newSuffix != "" {
			word.RemoveLastNRunes(len(newSuffixRunes))
		}
		return true
	}
	return false
}
func step2b(word *snowballword.SnowballWord) bool {
	suffix, suffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS),
		"iésemos", "iéramos", "iríamos", "eríamos", "aríamos", "ásemos",
		"áramos", "ábamos", "isteis", "iríais", "iremos", "ieseis",
		"ierais", "eríais", "eremos", "asteis", "aríais", "aremos",
		"íamos", "irías", "irían", "iréis", "ieses", "iesen", "ieron",
		"ieras", "ieran", "iendo", "erías", "erían", "eréis", "aseis",
		"arías", "arían", "aréis", "arais", "abais", "íais", "iste",
		"iría", "irás", "irán", "imos", "iese", "iera", "idos", "idas",
		"ería", "erás", "erán", "aste", "ases", "asen", "aría", "arás",
		"arán", "aron", "aras", "aran", "ando", "amos", "ados", "adas",
		"abas", "aban", "ías", "ían", "éis", "áis", "iré", "irá", "ido",
		"ida", "eré", "erá", "emos", "ase", "aré", "ará", "ara", "ado",
		"ada", "aba", "ís", "ía", "ió", "ir", "id", "es", "er", "en",
		"ed", "as", "ar", "an", "ad",
	)
	switch suffix {
	case "":
		return false

	case "en", "es", "éis", "emos":

		// Delete, and if preceded by gu delete the u (the gu need not be in RV)
		word.RemoveLastNRunes(len(suffixRunes))
		guSuffix, _ := word.FirstSuffix("gu")
		if guSuffix != "" {
			word.RemoveLastNRunes(1)
		}

	default:

		// Delete
		word.RemoveLastNRunes(len(suffixRunes))
	}
	return true
}
func step4(word *snowballword.SnowballWord) bool {

	hadChange := false

	if word.String() == "voudrion" {
		log.Println("...", word)
	}

	// If the word ends s (unicode code point 115),
	// not preceded by a, i, o, u, è or s, delete it.
	//
	if idx := len(word.RS) - 1; idx >= 1 && word.RS[idx] == 115 {
		switch word.RS[idx-1] {

		case 97, 105, 111, 117, 232, 115:

			// Do nothing, preceded by a, i, o, u, è or s
			return false

		default:
			word.RemoveLastNRunes(1)
			hadChange = true

		}
	}

	// Note: all the following are restricted to the RV region.

	// Search for the longest among the following suffixes in RV.
	//
	suffix, suffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS),
		"Ière", "ière", "Ier", "ier", "ion", "e", "ë",
	)

	switch suffix {
	case "":
		return hadChange
	case "ion":

		// Delete if in R2 and preceded by s or t in RV

		const sLen int = 3 // equivalently, len(suffixRunes)
		idx := len(word.RS) - sLen - 1
		if word.FitsInR2(sLen) && idx >= 0 && word.FitsInRV(sLen+1) {
			if word.RS[idx] == 115 || word.RS[idx] == 116 {
				word.RemoveLastNRunes(sLen)
				return true
			}
		}
		return hadChange

	case "ier", "ière", "Ier", "Ière":
		// Replace with i
		word.ReplaceSuffixRunes(suffixRunes, []rune("i"), true)
		return true

	case "e":
		word.RemoveLastNRunes(1)
		return true

	case "ë":

		// If preceded by gu (unicode code point 103 & 117), delete
		idx := len(word.RS) - 1
		if idx >= 2 && word.RS[idx-2] == 103 && word.RS[idx-1] == 117 {
			word.RemoveLastNRunes(1)
			return true
		}
		return hadChange
	}

	return true
}
func step5(w *snowballword.SnowballWord) bool {

	// Last rune index = `lri`
	lri := len(w.RS) - 1

	// If R1 is emtpy, R2 is also empty, and we
	// need not do anything in step 5.
	//
	if w.R1start > lri {
		return false
	}

	if w.RS[lri] == 101 {

		// The word ends with "e", which is unicode code point 101.

		// Delete "e" suffix if in R2, or in R1 and not preceded
		// by a short syllable.
		if w.R2start <= lri || !endsShortSyllable(w, lri) {
			w.ReplaceSuffix("e", "", true)
			return true
		}
		return false

	} else if w.R2start <= lri && w.RS[lri] == 108 && lri-1 >= 0 && w.RS[lri-1] == 108 {

		// The word ends in double "l", and the final "l" is
		// in R2. (Note, the unicode code point for "l" is 108.)

		// Delete the second "l".
		w.ReplaceSuffix("l", "", true)
		return true

	}
	return false
}
func Stem(word string, stemStopwWords bool) string {

	word = strings.ToLower(strings.TrimSpace(word))

	// Return small words and stop words
	if len(word) <= 2 || (stemStopwWords == false && isStopWord(word)) {
		return word
	}

	w := snowballword.New(word)

	// Stem the word.  Note, each of these
	// steps will alter `w` in place.
	//

	preprocess(w)
	step0(w)
	changeInStep1 := step1(w)
	if changeInStep1 == false {
		changeInStep2a := step2a(w)
		if changeInStep2a == false {
			step2b(w)
		}
	}
	step3(w)
	postprocess(w)

	return w.String()

}
func Stem(word string, stemStopwWords bool) string {

	word = strings.ToLower(strings.TrimSpace(word))
	w := snowballword.New(word)

	// Return small words and stop words
	if len(w.RS) <= 2 || (stemStopwWords == false && isStopWord(word)) {
		return word
	}

	preprocess(w)
	step1(w)
	step2(w)
	step3(w)
	step4(w)
	return w.String()

}
func isStopWord(word string) bool {
	switch word {
	case "ut", "få", "hadde", "hva", "tilbake", "vil", "han", "meget", "men", "vi", "en", "før",
		"samme", "stille", "inn", "er", "kan", "makt", "ved", "forsøke", "hvis", "part", "rett",
		"måte", "denne", "mer", "i", "lang", "ny", "hans", "hvilken", "tid", "vite", "her", "opp",
		"var", "navn", "mye", "om", "sant", "tilstand", "der", "ikke", "mest", "punkt", "hvem",
		"skulle", "mange", "over", "vårt", "alle", "arbeid", "lik", "like", "gå", "når", "siden",
		"å", "begge", "bruke", "eller", "og", "til", "da", "et", "hvorfor", "nå", "sist", "slutt",
		"deres", "det", "hennes", "så", "mens", "bra", "din", "fordi", "gjøre", "god", "ha", "start",
		"andre", "må", "med", "under", "meg", "oss", "innen", "på", "verdi", "ville", "kunne", "uten",
		"vår", "slik", "ene", "folk", "min", "riktig", "enhver", "bort", "enn", "nei", "som", "våre", "disse",
		"gjorde", "lage", "si", "du", "fra", "også", "hvordan", "av", "eneste", "for", "hvor", "først", "hver":
		return true
	}
	return false
}
func isStopWord(word string) bool {
	switch word {
	case "och", "det", "att", "i", "en", "jag", "hon", "som", "han",
		"på", "den", "med", "var", "sig", "för", "så", "till", "är", "men",
		"ett", "om", "hade", "de", "av", "icke", "mig", "du", "henne", "då",
		"sin", "nu", "har", "inte", "hans", "honom", "skulle", "hennes",
		"där", "min", "man", "ej", "vid", "kunde", "något", "från", "ut",
		"när", "efter", "upp", "vi", "dem", "vara", "vad", "över", "än",
		"dig", "kan", "sina", "här", "ha", "mot", "alla", "under", "någon",
		"eller", "allt", "mycket", "sedan", "ju", "denna", "själv", "detta",
		"åt", "utan", "varit", "hur", "ingen", "mitt", "ni", "bli", "blev",
		"oss", "din", "dessa", "några", "deras", "blir", "mina", "samma",
		"vilken", "er", "sådan", "vår", "blivit", "dess", "inom", "mellan",
		"sådant", "varför", "varje", "vilka", "ditt", "vem", "vilket",
		"sitta", "sådana", "vart", "dina", "vars", "vårt", "våra",
		"ert", "era", "vilkas":
		return true
	}
	return false
}
func New(in string) (word *SnowballWord) {
	word = &SnowballWord{RS: []rune(in)}
	word.R1start = len(word.RS)
	word.R2start = len(word.RS)
	word.RVstart = len(word.RS)
	return
}
func (w *SnowballWord) RemoveLastNRunes(n int) {
	w.RS = w.RS[:len(w.RS)-n]
	w.resetR1R2()
}
func (w *SnowballWord) resetR1R2() {
	rsLen := len(w.RS)
	if w.R1start > rsLen {
		w.R1start = rsLen
	}
	if w.R2start > rsLen {
		w.R2start = rsLen
	}
	if w.RVstart > rsLen {
		w.RVstart = rsLen
	}
}
func (w *SnowballWord) slice(start, stop int) []rune {
	startMin := 0
	if start < startMin {
		start = startMin
	}
	max := len(w.RS) - 1
	if start > max {
		start = max
	}
	if stop > max {
		stop = max
	}
	return w.RS[start:stop]
}
func (w *SnowballWord) FitsInR1(x int) bool {
	return w.R1start <= len(w.RS)-x
}
func (w *SnowballWord) FitsInR2(x int) bool {
	return w.R2start <= len(w.RS)-x
}
func (w *SnowballWord) FitsInRV(x int) bool {
	return w.RVstart <= len(w.RS)-x
}
func (w *SnowballWord) FirstPrefix(prefixes ...string) (foundPrefix string, foundPrefixRunes []rune) {
	found := false
	rsLen := len(w.RS)

	for _, prefix := range prefixes {
		prefixRunes := []rune(prefix)
		if len(prefixRunes) > rsLen {
			continue
		}

		found = true
		for i, r := range prefixRunes {
			if i > rsLen-1 || (w.RS)[i] != r {
				found = false
				break
			}
		}
		if found {
			foundPrefix = prefix
			foundPrefixRunes = prefixRunes
			break
		}
	}
	return
}
func (w *SnowballWord) HasSuffixRunes(suffixRunes []rune) bool {
	return w.HasSuffixRunesIn(0, len(w.RS), suffixRunes)
}
func (w *SnowballWord) FirstSuffixIfIn(startPos, endPos int, suffixes ...string) (suffix string, suffixRunes []rune) {
	for _, suffix := range suffixes {
		suffixRunes := []rune(suffix)
		if w.HasSuffixRunesIn(0, endPos, suffixRunes) {
			if endPos-len(suffixRunes) >= startPos {
				return suffix, suffixRunes
			} else {
				// Empty out suffixRunes
				suffixRunes = suffixRunes[:0]
				return "", suffixRunes
			}
		}
	}

	// Empty out suffixRunes
	suffixRunes = suffixRunes[:0]
	return "", suffixRunes
}
func (w *SnowballWord) RemoveFirstSuffixIfIn(startPos int, suffixes ...string) (suffix string, suffixRunes []rune) {
	suffix, suffixRunes = w.FirstSuffixIfIn(startPos, len(w.RS), suffixes...)
	if suffix != "" {
		w.RemoveLastNRunes(len(suffixRunes))
	}
	return
}
func (w *SnowballWord) RemoveFirstSuffix(suffixes ...string) (suffix string, suffixRunes []rune) {
	return w.RemoveFirstSuffixIn(0, suffixes...)
}
func (w *SnowballWord) FirstSuffix(suffixes ...string) (suffix string, suffixRunes []rune) {
	return w.FirstSuffixIfIn(0, len(w.RS), suffixes...)
}
func preprocess(word *snowballword.SnowballWord) {

	// Clean up apostrophes
	normalizeApostrophes(word)
	trimLeftApostrophes(word)

	// Capitalize Y's that are not behaving
	// as vowels.
	capitalizeYs(word)

	// Find the two regions, R1 & R2
	r1start, r2start := r1r2(word)
	word.R1start = r1start
	word.R2start = r2start
}
func step0(word *snowballword.SnowballWord) bool {

	// Search for the longest among the following suffixes
	suffix1, suffix1Runes := word.FirstSuffixIn(word.RVstart, len(word.RS),
		"selas", "selos", "sela", "selo", "las", "les",
		"los", "nos", "me", "se", "la", "le", "lo",
	)

	// If the suffix empty or not in RV, we have nothing to do.
	if suffix1 == "" {
		return false
	}

	// We'll remove suffix1, if comes after one of the following
	suffix2, suffix2Runes := word.FirstSuffixIn(word.RVstart, len(word.RS)-len(suffix1),
		"iéndo", "iendo", "yendo", "ando", "ándo",
		"ár", "ér", "ír", "ar", "er", "ir",
	)
	switch suffix2 {
	case "":

		// Nothing to do
		return false

	case "iéndo", "ándo", "ár", "ér", "ír":

		// In these cases, deletion is followed by removing
		// the acute accent (e.g., haciéndola -> haciendo).

		var suffix2repl string
		switch suffix2 {
		case "":
			return false
		case "iéndo":
			suffix2repl = "iendo"
		case "ándo":
			suffix2repl = "ando"
		case "ár":
			suffix2repl = "ar"
		case "ír":
			suffix2repl = "ir"
		}
		word.RemoveLastNRunes(len(suffix1Runes))
		word.ReplaceSuffixRunes(suffix2Runes, []rune(suffix2repl), true)
		return true

	case "ando", "iendo", "ar", "er", "ir":
		word.RemoveLastNRunes(len(suffix1Runes))
		return true

	case "yendo":

		// In the case of "yendo", the "yendo" must lie in RV,
		// and be preceded by a "u" somewhere in the word.

		for i := 0; i < len(word.RS)-(len(suffix1)+len(suffix2)); i++ {

			// Note, the unicode code point for "u" is 117.
			if word.RS[i] == 117 {
				word.RemoveLastNRunes(len(suffix1Runes))
				return true
			}
		}
	}
	return false
}
func step1b(w *snowballword.SnowballWord) bool {

	suffix, suffixRunes := w.FirstSuffix("eedly", "ingly", "edly", "ing", "eed", "ed")

	switch suffix {

	case "":
		// No suffix found
		return false

	case "eed", "eedly":

		// Replace by ee if in R1
		if len(suffixRunes) <= len(w.RS)-w.R1start {
			w.ReplaceSuffixRunes(suffixRunes, []rune("ee"), true)
		}
		return true

	case "ed", "edly", "ing", "ingly":
		hasLowerVowel := false
		for i := 0; i < len(w.RS)-len(suffixRunes); i++ {
			if isLowerVowel(w.RS[i]) {
				hasLowerVowel = true
				break
			}
		}
		if hasLowerVowel {

			// This case requires a two-step transformation and, due
			// to the way we've implemented the `ReplaceSuffix` method
			// here, information about R1 and R2 would be lost between
			// the two.  Therefore, we need to keep track of the
			// original R1 & R2, so that we may set them below, at the
			// end of this case.
			//
			originalR1start := w.R1start
			originalR2start := w.R2start

			// Delete if the preceding word part contains a vowel
			w.RemoveLastNRunes(len(suffixRunes))

			// ...and after the deletion...

			newSuffix, newSuffixRunes := w.FirstSuffix("at", "bl", "iz", "bb", "dd", "ff", "gg", "mm", "nn", "pp", "rr", "tt")
			switch newSuffix {

			case "":

				// If the word is short, add "e"
				if isShortWord(w) {

					// By definition, r1 and r2 are the empty string for
					// short words.
					w.RS = append(w.RS, []rune("e")...)
					w.R1start = len(w.RS)
					w.R2start = len(w.RS)
					return true
				}

			case "at", "bl", "iz":

				// If the word ends "at", "bl" or "iz" add "e"
				w.ReplaceSuffixRunes(newSuffixRunes, []rune(newSuffix+"e"), true)

			case "bb", "dd", "ff", "gg", "mm", "nn", "pp", "rr", "tt":

				// If the word ends with a double remove the last letter.
				// Note that, "double" does not include all possible doubles,
				// just those shown above.
				//
				w.RemoveLastNRunes(1)
			}

			// Because we did a double replacement, we need to fix
			// R1 and R2 manually. This is just becase of how we've
			// implemented the `ReplaceSuffix` method.
			//
			rsLen := len(w.RS)
			if originalR1start < rsLen {
				w.R1start = originalR1start
			} else {
				w.R1start = rsLen
			}
			if originalR2start < rsLen {
				w.R2start = originalR2start
			} else {
				w.R2start = rsLen
			}

			return true
		}

	}

	return false
}
func step2b(word *snowballword.SnowballWord) bool {

	// Search for the longest among the following suffixes in RV.
	//
	suffix, suffixRunes := word.FirstSuffixIn(word.RVstart, len(word.RS),
		"eraIent", "assions", "erions", "assiez", "assent",
		"èrent", "eront", "erons", "eriez", "erait", "erais",
		"asses", "antes", "aIent", "âtes", "âmes", "ions",
		"erez", "eras", "erai", "asse", "ants", "ante", "ées",
		"iez", "era", "ant", "ait", "ais", "és", "ée", "ât",
		"ez", "er", "as", "ai", "é", "a",
	)

	switch suffix {
	case "ions":

		// Delete if in R2
		suffixLen := len(suffixRunes)
		if word.FitsInR2(suffixLen) {
			word.RemoveLastNRunes(suffixLen)
			return true
		}
		return false

	case "é", "ée", "ées", "és", "èrent", "er", "era",
		"erai", "eraIent", "erais", "erait", "eras", "erez",
		"eriez", "erions", "erons", "eront", "ez", "iez":

		// Delete
		word.RemoveLastNRunes(len(suffixRunes))
		return true

	case "âmes", "ât", "âtes", "a", "ai", "aIent",
		"ais", "ait", "ant", "ante", "antes", "ants", "as",
		"asse", "assent", "asses", "assiez", "assions":

		// Delete
		word.RemoveLastNRunes(len(suffixRunes))

		// If preceded by e (unicode code point 101), delete
		//
		idx := len(word.RS) - 1
		if idx >= 0 && word.RS[idx] == 101 && word.FitsInRV(1) {
			word.RemoveLastNRunes(1)
		}
		return true

	}
	return false
}
func capitalizeYs(word *snowballword.SnowballWord) (numCapitalizations int) {
	for i, r := range word.RS {

		// (Note: Y & y unicode code points = 89 & 121)

		if r == 121 && (i == 0 || isLowerVowel(word.RS[i-1])) {
			word.RS[i] = 89
			numCapitalizations += 1
		}
	}
	return
}
func uncapitalizeYs(word *snowballword.SnowballWord) {
	for i, r := range word.RS {

		// (Note: Y & y unicode code points = 89 & 121)

		if r == 89 {
			word.RS[i] = 121
		}
	}
	return
}
func stemSpecialWord(word string) (stemmed string) {
	switch word {
	case "skis":
		stemmed = "ski"
	case "skies":
		stemmed = "sky"
	case "dying":
		stemmed = "die"
	case "lying":
		stemmed = "lie"
	case "tying":
		stemmed = "tie"
	case "idly":
		stemmed = "idl"
	case "gently":
		stemmed = "gentl"
	case "ugly":
		stemmed = "ugli"
	case "early":
		stemmed = "earli"
	case "only":
		stemmed = "onli"
	case "singly":
		stemmed = "singl"
	case "sky":
		stemmed = "sky"
	case "news":
		stemmed = "news"
	case "howe":
		stemmed = "howe"
	case "atlas":
		stemmed = "atlas"
	case "cosmos":
		stemmed = "cosmos"
	case "bias":
		stemmed = "bias"
	case "andes":
		stemmed = "andes"
	case "inning":
		stemmed = "inning"
	case "innings":
		stemmed = "inning"
	case "outing":
		stemmed = "outing"
	case "outings":
		stemmed = "outing"
	case "canning":
		stemmed = "canning"
	case "cannings":
		stemmed = "canning"
	case "herring":
		stemmed = "herring"
	case "herrings":
		stemmed = "herring"
	case "earring":
		stemmed = "earring"
	case "earrings":
		stemmed = "earring"
	case "proceed":
		stemmed = "proceed"
	case "proceeds":
		stemmed = "proceed"
	case "proceeded":
		stemmed = "proceed"
	case "proceeding":
		stemmed = "proceed"
	case "exceed":
		stemmed = "exceed"
	case "exceeds":
		stemmed = "exceed"
	case "exceeded":
		stemmed = "exceed"
	case "exceeding":
		stemmed = "exceed"
	case "succeed":
		stemmed = "succeed"
	case "succeeds":
		stemmed = "succeed"
	case "succeeded":
		stemmed = "succeed"
	case "succeeding":
		stemmed = "succeed"
	}
	return
}
func isShortWord(w *snowballword.SnowballWord) (isShort bool) {

	// If r1 is not empty, the word is not short
	if w.R1start < len(w.RS) {
		return
	}

	// Otherwise it must end in a short syllable
	return endsShortSyllable(w, len(w.RS))
}
func step1a(w *snowballword.SnowballWord) bool {

	suffix, suffixRunes := w.FirstSuffix("sses", "ied", "ies", "us", "ss", "s")
	switch suffix {

	case "sses":

		// Replace by ss
		w.ReplaceSuffixRunes(suffixRunes, []rune("ss"), true)
		return true

	case "ies", "ied":

		// Replace by i if preceded by more than one letter,
		// otherwise by ie (so ties -> tie, cries -> cri).

		var repl string
		if len(w.RS) > 4 {
			repl = "i"
		} else {
			repl = "ie"
		}
		w.ReplaceSuffixRunes(suffixRunes, []rune(repl), true)
		return true

	case "us", "ss":

		// Do nothing
		return false

	case "s":

		// Delete if the preceding word part contains a vowel
		// not immediately before the s (so gas and this retain
		// the s, gaps and kiwis lose it)
		//
		for i := 0; i < len(w.RS)-2; i++ {
			if isLowerVowel(w.RS[i]) {
				w.RemoveLastNRunes(len(suffixRunes))
				return true
			}
		}
	}
	return false
}
func Set(key string, value interface{}) {
	gid := curGoroutineID()
	dataLock.Lock()
	if data[gid] == nil {
		data[gid] = Values{}
	}
	data[gid][key] = value
	dataLock.Unlock()
}
func Get(key string) interface{} {
	gid := curGoroutineID()
	dataLock.RLock()
	if data[gid] == nil {
		dataLock.RUnlock()
		return nil
	}
	value := data[gid][key]
	dataLock.RUnlock()
	return value
}
func Cleanup() {
	gid := curGoroutineID()
	dataLock.Lock()
	delete(data, gid)
	dataLock.Unlock()
}
func getValues() Values {
	gid := curGoroutineID()
	dataLock.Lock()
	values := data[gid]
	dataLock.Unlock()
	return values
}
func linkGRs(parentData Values) {
	childID := curGoroutineID()
	dataLock.Lock()
	data[childID] = parentData
	dataLock.Unlock()
}
func unlinkGRs() {
	childID := curGoroutineID()
	dataLock.Lock()
	delete(data, childID)
	dataLock.Unlock()
}
func AppUri(appName, path string, config helpersinternal.CurlConfig) string {
	uriCreator := &helpersinternal.AppUriCreator{CurlConfig: config}

	return uriCreator.AppUri(appName, path)
}
func CurlAppWithTimeout(cfg helpersinternal.CurlConfig, appName, path string, timeout time.Duration, args ...string) string {
	appCurler := helpersinternal.NewAppCurler(Curl, cfg)
	return appCurler.CurlAndWait(cfg, appName, path, timeout, args...)
}
func CurlApp(cfg helpersinternal.CurlConfig, appName, path string, args ...string) string {
	appCurler := helpersinternal.NewAppCurler(Curl, cfg)
	return appCurler.CurlAndWait(cfg, appName, path, CURL_TIMEOUT, args...)
}
func CurlAppRoot(cfg helpersinternal.CurlConfig, appName string) string {
	appCurler := helpersinternal.NewAppCurler(Curl, cfg)
	return appCurler.CurlAndWait(cfg, appName, "/", CURL_TIMEOUT)
}
func GetTags(prefix rune, str string, terminator ...rune) (tags []Tag) {
	// If we have no terminators given, default to only whitespace
	if len(terminator) == 0 {
		terminator = []rune(" ")
	}
	// get list of indexes in our str that is a terminator
	// Always include the beginning of our str a terminator. This is so we can
	// detect the first character as a prefix
	termIndexes := []int{-1}
	for i, char := range str {
		if isTerminator(char, terminator...) {
			termIndexes = append(termIndexes, i)
		}
	}
	// Always include last character as a terminator
	termIndexes = append(termIndexes, len(str))

	// check if the character AFTER our term index is our prefix
	for i, t := range termIndexes {
		// ensure term index is not the last character in str
		if t >= (len(str) - 1) {
			break
		}
		if str[t+1] == byte(prefix) {
			tagText := strings.TrimLeft(str[t+2:termIndexes[i+1]], string(prefix))
			if tagText == "" {
				continue
			}
			index := t + 1
			tags = append(tags, Tag{prefix, tagText, index})
		}
	}

	return
}
func GetTagsAsUniqueStrings(prefix rune, str string, terminator ...rune) (strs []string) {
	tags := GetTags(prefix, str, terminator...)
	for _, tag := range tags {
		strs = append(strs, tag.Tag)
	}
	return uniquify(strs)
}
func isTerminator(r rune, terminator ...rune) bool {
	for _, t := range terminator {
		if r == t {
			return true
		}
	}
	return unicode.IsSpace(r) || !unicode.IsPrint(r)
}
func uniquify(in []string) (out []string) {
	for _, i := range in {
		if i == "" {
			continue
		}
		for _, o := range out {
			if i == o {
				continue
			}
		}
		out = append(out, i)
	}
	return
}
func New(config Config) gin.HandlerFunc {
	location := newLocation(config)

	return func(c *gin.Context) {
		location.applyToContext(c)
	}
}
func Get(c *gin.Context) *url.URL {
	v, ok := c.Get(key)

	if !ok {
		return nil
	}

	vv, ok := v.(*url.URL)

	if !ok {
		return nil
	}

	return vv
}
func GenerateRSAKeyPair(bits int, src io.Reader) (PrivKey, PubKey, error) {
	if bits < 512 {
		return nil, nil, ErrRsaKeyTooSmall
	}
	priv, err := rsa.GenerateKey(src, bits)
	if err != nil {
		return nil, nil, err
	}
	pk := &priv.PublicKey
	return &RsaPrivateKey{sk: priv}, &RsaPublicKey{pk}, nil
}
func (pk *RsaPublicKey) Verify(data, sig []byte) (bool, error) {
	hashed := sha256.Sum256(data)
	err := rsa.VerifyPKCS1v15(pk.k, crypto.SHA256, hashed[:], sig)
	if err != nil {
		return false, err
	}
	return true, nil
}
func (pk *RsaPublicKey) Encrypt(b []byte) ([]byte, error) {
	return rsa.EncryptPKCS1v15(rand.Reader, pk.k, b)
}
func (sk *RsaPrivateKey) Sign(message []byte) ([]byte, error) {
	hashed := sha256.Sum256(message)
	return rsa.SignPKCS1v15(rand.Reader, sk.sk, crypto.SHA256, hashed[:])
}
func (sk *RsaPrivateKey) GetPublic() PubKey {
	if sk.pk == nil {
		sk.pk = &sk.sk.PublicKey
	}
	return &RsaPublicKey{sk.pk}
}
func (sk *RsaPrivateKey) Decrypt(b []byte) ([]byte, error) {
	return rsa.DecryptPKCS1v15(rand.Reader, sk.sk, b)
}
func UnmarshalRsaPrivateKey(b []byte) (PrivKey, error) {
	sk, err := x509.ParsePKCS1PrivateKey(b)
	if err != nil {
		return nil, err
	}
	if sk.N.BitLen() < 512 {
		return nil, ErrRsaKeyTooSmall
	}
	return &RsaPrivateKey{sk: sk}, nil
}
func GenerateKeyPair(typ, bits int) (PrivKey, PubKey, error) {
	return GenerateKeyPairWithReader(typ, bits, rand.Reader)
}
func GenerateKeyPairWithReader(typ, bits int, src io.Reader) (PrivKey, PubKey, error) {
	switch typ {
	case RSA:
		return GenerateRSAKeyPair(bits, src)
	case Ed25519:
		return GenerateEd25519Key(src)
	case Secp256k1:
		return GenerateSecp256k1Key(src)
	case ECDSA:
		return GenerateECDSAKeyPair(src)
	default:
		return nil, nil, ErrBadKeyType
	}
}
func GenerateEKeyPair(curveName string) ([]byte, GenSharedKey, error) {
	var curve elliptic.Curve

	switch curveName {
	case "P-256":
		curve = elliptic.P256()
	case "P-384":
		curve = elliptic.P384()
	case "P-521":
		curve = elliptic.P521()
	}

	priv, x, y, err := elliptic.GenerateKey(curve, rand.Reader)
	if err != nil {
		return nil, nil, err
	}

	pubKey := elliptic.Marshal(curve, x, y)

	done := func(theirPub []byte) ([]byte, error) {
		// Verify and unpack node's public key.
		x, y := elliptic.Unmarshal(curve, theirPub)
		if x == nil {
			return nil, fmt.Errorf("malformed public key: %d %v", len(theirPub), theirPub)
		}

		if !curve.IsOnCurve(x, y) {
			return nil, errors.New("invalid public key")
		}

		// Generate shared secret.
		secret, _ := curve.ScalarMult(x, y, priv)

		return secret.Bytes(), nil
	}

	return pubKey, done, nil
}
func UnmarshalPublicKey(data []byte) (PubKey, error) {
	pmes := new(pb.PublicKey)
	err := proto.Unmarshal(data, pmes)
	if err != nil {
		return nil, err
	}

	um, ok := PubKeyUnmarshallers[pmes.GetType()]
	if !ok {
		return nil, ErrBadKeyType
	}

	return um(pmes.GetData())
}
func MarshalPublicKey(k PubKey) ([]byte, error) {
	pbmes := new(pb.PublicKey)
	pbmes.Type = k.Type()
	data, err := k.Raw()
	if err != nil {
		return nil, err
	}
	pbmes.Data = data

	return proto.Marshal(pbmes)
}
func UnmarshalPrivateKey(data []byte) (PrivKey, error) {
	pmes := new(pb.PrivateKey)
	err := proto.Unmarshal(data, pmes)
	if err != nil {
		return nil, err
	}

	um, ok := PrivKeyUnmarshallers[pmes.GetType()]
	if !ok {
		return nil, ErrBadKeyType
	}

	return um(pmes.GetData())
}
func MarshalPrivateKey(k PrivKey) ([]byte, error) {
	pbmes := new(pb.PrivateKey)
	pbmes.Type = k.Type()
	data, err := k.Raw()
	if err != nil {
		return nil, err
	}

	pbmes.Data = data
	return proto.Marshal(pbmes)
}
func KeyEqual(k1, k2 Key) bool {
	if k1 == k2 {
		return true
	}

	b1, err1 := k1.Bytes()
	b2, err2 := k2.Bytes()
	return bytes.Equal(b1, b2) && err1 == err2
}
func GenerateECDSAKeyPair(src io.Reader) (PrivKey, PubKey, error) {
	return GenerateECDSAKeyPairWithCurve(ECDSACurve, src)
}
func GenerateECDSAKeyPairWithCurve(curve elliptic.Curve, src io.Reader) (PrivKey, PubKey, error) {
	priv, err := ecdsa.GenerateKey(curve, src)
	if err != nil {
		return nil, nil, err
	}

	return &ECDSAPrivateKey{priv}, &ECDSAPublicKey{&priv.PublicKey}, nil
}
func ECDSAKeyPairFromKey(priv *ecdsa.PrivateKey) (PrivKey, PubKey, error) {
	if priv == nil {
		return nil, nil, ErrNilPrivateKey
	}

	return &ECDSAPrivateKey{priv}, &ECDSAPublicKey{&priv.PublicKey}, nil
}
func UnmarshalECDSAPrivateKey(data []byte) (PrivKey, error) {
	priv, err := x509.ParseECPrivateKey(data)
	if err != nil {
		return nil, err
	}

	return &ECDSAPrivateKey{priv}, nil
}
func UnmarshalECDSAPublicKey(data []byte) (PubKey, error) {
	pubIfc, err := x509.ParsePKIXPublicKey(data)
	if err != nil {
		return nil, err
	}

	pub, ok := pubIfc.(*ecdsa.PublicKey)
	if !ok {
		return nil, ErrNotECDSAPubKey
	}

	return &ECDSAPublicKey{pub}, nil
}
func (ePriv *ECDSAPrivateKey) Equals(o Key) bool {
	oPriv, ok := o.(*ECDSAPrivateKey)
	if !ok {
		return false
	}

	return ePriv.priv.D.Cmp(oPriv.priv.D) == 0
}
func (ePriv *ECDSAPrivateKey) Sign(data []byte) ([]byte, error) {
	hash := sha256.Sum256(data)
	r, s, err := ecdsa.Sign(rand.Reader, ePriv.priv, hash[:])
	if err != nil {
		return nil, err
	}

	return asn1.Marshal(ECDSASig{
		R: r,
		S: s,
	})
}
func (ePub *ECDSAPublicKey) Equals(o Key) bool {
	oPub, ok := o.(*ECDSAPublicKey)
	if !ok {
		return false
	}

	return ePub.pub.X != nil && ePub.pub.Y != nil && oPub.pub.X != nil && oPub.pub.Y != nil &&
		0 == ePub.pub.X.Cmp(oPub.pub.X) && 0 == ePub.pub.Y.Cmp(oPub.pub.Y)
}
func (ePub *ECDSAPublicKey) Verify(data, sigBytes []byte) (bool, error) {
	sig := new(ECDSASig)
	if _, err := asn1.Unmarshal(sigBytes, sig); err != nil {
		return false, err
	}
	if sig == nil {
		return false, ErrNilSig
	}

	hash := sha256.Sum256(data)

	return ecdsa.Verify(ePub.pub, hash[:], sig.R, sig.S), nil
}
func GenerateSecp256k1Key(src io.Reader) (PrivKey, PubKey, error) {
	privk, err := btcec.NewPrivateKey(btcec.S256())
	if err != nil {
		return nil, nil, err
	}

	k := (*Secp256k1PrivateKey)(privk)
	return k, k.GetPublic(), nil
}
func UnmarshalSecp256k1PrivateKey(data []byte) (PrivKey, error) {
	if len(data) != btcec.PrivKeyBytesLen {
		return nil, fmt.Errorf("expected secp256k1 data size to be %d", btcec.PrivKeyBytesLen)
	}

	privk, _ := btcec.PrivKeyFromBytes(btcec.S256(), data)
	return (*Secp256k1PrivateKey)(privk), nil
}
func UnmarshalSecp256k1PublicKey(data []byte) (PubKey, error) {
	k, err := btcec.ParsePubKey(data, btcec.S256())
	if err != nil {
		return nil, err
	}

	return (*Secp256k1PublicKey)(k), nil
}
func (k *Secp256k1PrivateKey) Equals(o Key) bool {
	sk, ok := o.(*Secp256k1PrivateKey)
	if !ok {
		return false
	}

	return k.D.Cmp(sk.D) == 0
}
func (k *Secp256k1PrivateKey) Sign(data []byte) ([]byte, error) {
	hash := sha256.Sum256(data)
	sig, err := (*btcec.PrivateKey)(k).Sign(hash[:])
	if err != nil {
		return nil, err
	}

	return sig.Serialize(), nil
}
func (k *Secp256k1PublicKey) Equals(o Key) bool {
	sk, ok := o.(*Secp256k1PublicKey)
	if !ok {
		return false
	}

	return (*btcec.PublicKey)(k).IsEqual((*btcec.PublicKey)(sk))
}
func (k *Secp256k1PublicKey) Verify(data []byte, sigStr []byte) (bool, error) {
	sig, err := btcec.ParseDERSignature(sigStr, btcec.S256())
	if err != nil {
		return false, err
	}

	hash := sha256.Sum256(data)
	return sig.Verify(hash[:], (*btcec.PublicKey)(k)), nil
}
func (k *Ed25519PrivateKey) Raw() ([]byte, error) {
	// The Ed25519 private key contains two 32-bytes curve points, the private
	// key and the public key.
	// It makes it more efficient to get the public key without re-computing an
	// elliptic curve multiplication.
	buf := make([]byte, len(k.k))
	copy(buf, k.k)

	return buf, nil
}
func (k *Ed25519PrivateKey) Sign(msg []byte) ([]byte, error) {
	return ed25519.Sign(k.k, msg), nil
}
func (k *Ed25519PublicKey) Equals(o Key) bool {
	edk, ok := o.(*Ed25519PublicKey)
	if !ok {
		return false
	}

	return bytes.Equal(k.k, edk.k)
}
func (k *Ed25519PublicKey) Verify(data []byte, sig []byte) (bool, error) {
	return ed25519.Verify(k.k, data, sig), nil
}
func UnmarshalEd25519PublicKey(data []byte) (PubKey, error) {
	if len(data) != 32 {
		return nil, errors.New("expect ed25519 public key data size to be 32")
	}

	return &Ed25519PublicKey{
		k: ed25519.PublicKey(data),
	}, nil
}
func UnmarshalEd25519PrivateKey(data []byte) (PrivKey, error) {
	switch len(data) {
	case ed25519.PrivateKeySize + ed25519.PublicKeySize:
		// Remove the redundant public key. See issue #36.
		redundantPk := data[ed25519.PrivateKeySize:]
		pk := data[ed25519.PrivateKeySize-ed25519.PublicKeySize : ed25519.PrivateKeySize]
		if !bytes.Equal(pk, redundantPk) {
			return nil, errors.New("expected redundant ed25519 public key to be redundant")
		}

		// No point in storing the extra data.
		newKey := make([]byte, ed25519.PrivateKeySize)
		copy(newKey, data[:ed25519.PrivateKeySize])
		data = newKey
	case ed25519.PrivateKeySize:
	default:
		return nil, fmt.Errorf(
			"expected ed25519 data size to be %d or %d, got %d",
			ed25519.PrivateKeySize,
			ed25519.PrivateKeySize+ed25519.PublicKeySize,
			len(data),
		)
	}

	return &Ed25519PrivateKey{
		k: ed25519.PrivateKey(data),
	}, nil
}
func EditScriptForStrings(source []rune, target []rune, op Options) EditScript {
	return backtrace(len(source), len(target),
		MatrixForStrings(source, target, op), op)
}
func EditScriptForMatrix(matrix [][]int, op Options) EditScript {
	return backtrace(len(matrix)-1, len(matrix[0])-1, matrix, op)
}
func WriteMatrix(source []rune, target []rune, matrix [][]int, writer io.Writer) {
	fmt.Fprintf(writer, "    ")
	for _, targetRune := range target {
		fmt.Fprintf(writer, "  %c", targetRune)
	}
	fmt.Fprintf(writer, "\n")
	fmt.Fprintf(writer, "  %2d", matrix[0][0])
	for j, _ := range target {
		fmt.Fprintf(writer, " %2d", matrix[0][j+1])
	}
	fmt.Fprintf(writer, "\n")
	for i, sourceRune := range source {
		fmt.Fprintf(writer, "%c %2d", sourceRune, matrix[i+1][0])
		for j, _ := range target {
			fmt.Fprintf(writer, " %2d", matrix[i+1][j+1])
		}
		fmt.Fprintf(writer, "\n")
	}
}
func New(path string) (Lockfile, error) {
	if !filepath.IsAbs(path) {
		return Lockfile(""), ErrNeedAbsPath
	}
	return Lockfile(path), nil
}
func (l Lockfile) GetOwner() (*os.Process, error) {
	name := string(l)

	// Ok, see, if we have a stale lockfile here
	content, err := ioutil.ReadFile(name)
	if err != nil {
		return nil, err
	}

	// try hard for pids. If no pid, the lockfile is junk anyway and we delete it.
	pid, err := scanPidLine(content)
	if err != nil {
		return nil, err
	}
	running, err := isRunning(pid)
	if err != nil {
		return nil, err
	}

	if running {
		proc, err := os.FindProcess(pid)
		if err != nil {
			return nil, err
		}
		return proc, nil
	}
	return nil, ErrDeadOwner

}
func (l Lockfile) TryLock() error {
	name := string(l)

	// This has been checked by New already. If we trigger here,
	// the caller didn't use New and re-implemented it's functionality badly.
	// So panic, that he might find this easily during testing.
	if !filepath.IsAbs(name) {
		panic(ErrNeedAbsPath)
	}

	tmplock, err := ioutil.TempFile(filepath.Dir(name), filepath.Base(name)+".")
	if err != nil {
		return err
	}

	cleanup := func() {
		_ = tmplock.Close()
		_ = os.Remove(tmplock.Name())
	}
	defer cleanup()

	if err := writePidLine(tmplock, os.Getpid()); err != nil {
		return err
	}

	// EEXIST and similiar error codes, caught by os.IsExist, are intentionally ignored,
	// as it means that someone was faster creating this link
	// and ignoring this kind of error is part of the algorithm.
	// The we will probably fail the pid owner check later, if this process is still alive.
	// We cannot ignore ALL errors, since failure to support hard links, disk full
	// as well as many other errors can happen to a filesystem operation
	// and we really want to abort on those.
	if err := os.Link(tmplock.Name(), name); err != nil {
		if !os.IsExist(err) {
			return err
		}
	}

	fiTmp, err := os.Lstat(tmplock.Name())
	if err != nil {
		return err
	}
	fiLock, err := os.Lstat(name)
	if err != nil {
		// tell user that a retry would be a good idea
		if os.IsNotExist(err) {
			return ErrNotExist
		}
		return err
	}

	// Success
	if os.SameFile(fiTmp, fiLock) {
		return nil
	}

	proc, err := l.GetOwner()
	switch err {
	default:
		// Other errors -> defensively fail and let caller handle this
		return err
	case nil:
		if proc.Pid != os.Getpid() {
			return ErrBusy
		}
	case ErrDeadOwner, ErrInvalidPid:
		// cases we can fix below
	}

	// clean stale/invalid lockfile
	err = os.Remove(name)
	if err != nil {
		// If it doesn't exist, then it doesn't matter who removed it.
		if !os.IsNotExist(err) {
			return err
		}
	}

	// now that the stale lockfile is gone, let's recurse
	return l.TryLock()
}
func (l Lockfile) Unlock() error {
	proc, err := l.GetOwner()
	switch err {
	case ErrInvalidPid, ErrDeadOwner:
		return ErrRogueDeletion
	case nil:
		if proc.Pid == os.Getpid() {
			// we really own it, so let's remove it.
			return os.Remove(string(l))
		}
		// Not owned by me, so don't delete it.
		return ErrRogueDeletion
	default:
		// This is an application error or system error.
		// So give a better error for logging here.
		if os.IsNotExist(err) {
			return ErrRogueDeletion
		}
		// Other errors -> defensively fail and let caller handle this
		return err
	}
}
func NewBase(configs ...baseConfigFunc) *Base {
	b := &Base{
		clock: glock.NewRealClock(),

		config:    NewConfig(),
		logLevel:  LevelDebug,
		sequence:  0,
		BaseAttrs: NewAttrs(),

		loggers:      make([]Logger, 0),
		hookPreQueue: make([]HookPreQueue, 0),
	}

	for _, f := range configs {
		f(b)
	}

	return b
}
func (b *Base) SetFallbackLogger(logger Logger) error {
	if logger == nil {
		if b.fallbackLogger != nil && b.fallbackLogger.IsInitialized() {
			b.fallbackLogger.ShutdownLogger()
		}
		b.fallbackLogger = nil
		return nil
	}

	if !logger.IsInitialized() {
		err := logger.InitLogger()
		if err != nil {
			return err
		}
	}

	// Shut down any old logger we might already have a reference to
	if b.fallbackLogger != nil && b.fallbackLogger.IsInitialized() {
		b.fallbackLogger.ShutdownLogger()
	}

	b.fallbackLogger = logger

	return nil
}
func (b *Base) AddLogger(logger Logger) error {
	if b.IsInitialized() && !logger.IsInitialized() {
		err := logger.InitLogger()
		if err != nil {
			return err
		}
	} else if !b.IsInitialized() && logger.IsInitialized() {
		err := logger.ShutdownLogger()
		if err != nil {
			return err
		}
	}
	b.loggers = append(b.loggers, logger)

	if hook, ok := logger.(HookPreQueue); ok {
		b.hookPreQueue = append(b.hookPreQueue, hook)
	}

	logger.SetBase(b)
	return nil
}
func (b *Base) LogWithTime(level LogLevel, ts time.Time, m *Attrs, msg string, a ...interface{}) error {
	if !b.shouldLog(level) {
		return nil
	}

	if !b.isInitialized {
		return ErrNotInitialized
	}

	if len(b.config.FilenameAttr) > 0 || len(b.config.LineNumberAttr) > 0 {
		file, line := getCallerInfo()
		if m == nil {
			m = NewAttrs()
		}
		if len(b.config.FilenameAttr) > 0 {
			m.SetAttr(b.config.FilenameAttr, file)
		}
		if len(b.config.LineNumberAttr) > 0 {
			m.SetAttr(b.config.LineNumberAttr, line)
		}
	}

	if len(b.config.SequenceAttr) > 0 {
		if m == nil {
			m = NewAttrs()
		}
		seq := atomic.AddUint64(&b.sequence, 1)
		m.SetAttr(b.config.SequenceAttr, seq)
	}

	nm := newMessage(ts, b, level, m, msg, a...)

	for _, hook := range b.hookPreQueue {
		err := hook.PreQueue(nm)
		if err != nil {
			return err
		}
	}

	return b.queue.queueMessage(nm)
}
func (b *Base) Log(level LogLevel, m *Attrs, msg string, a ...interface{}) error {
	return b.LogWithTime(level, b.clock.Now(), m, msg, a...)
}
func (b *Base) Warnm(m *Attrs, msg string, a ...interface{}) error {
	return b.Warningm(m, msg, a...)
}
func NewTemplateWithFuncMap(tpl string, funcMap template.FuncMap) (*Template, error) {
	var levels = []LogLevel{LevelNone, LevelDebug, LevelInfo, LevelWarning, LevelError, LevelFatal}
	tpls := make(map[LogLevel]*template.Template, 0)
	for _, level := range levels {
		// If color is overridden, we need to ensure that {{reset}} resets for all levels.
		_, forceReset := funcMap["color"]
		fMap := getFuncMap(level, forceReset)
		for name, f := range funcMap {
			fMap[name] = f
		}

		parsedTpl, err := template.New(getLevelName(level)).
			Funcs(fMap).
			Parse(tpl)
		if err != nil {
			return nil, err
		}
		tpls[level] = parsedTpl
	}

	newTpl := &Template{
		tpls: tpls,
	}

	return newTpl, nil
}
func (t *Template) Execute(msg *TemplateMsg, colorize bool) (string, error) {
	tplLevel := msg.Level
	if !colorize {
		tplLevel = LevelNone
	}
	var buf bytes.Buffer
	execTpl := t.tpls[tplLevel]
	if execTpl == nil {
		return "", ErrUnknownLevel
	}
	err := execTpl.Execute(&buf, msg)
	if err != nil {
		return "", err
	}

	return buf.String(), nil
}
func NewTemplateMsg(timestamp time.Time, level LogLevel, m map[string]interface{}, msg string) *TemplateMsg {
	msgAttrs := m
	if msgAttrs == nil {
		msgAttrs = make(map[string]interface{})
	}
	tplMsg := &TemplateMsg{
		Timestamp: timestamp,
		Message:   msg,
		Level:     level,
		LevelName: level.String(),
		Attrs:     msgAttrs,
	}
	return tplMsg
}
func NewLogAdapterFor(base WrappableLogger, attrs *Attrs) *LogAdapter {
	if attrs == nil {
		attrs = NewAttrs()
	}

	return &LogAdapter{
		base:  base,
		attrs: attrs,
	}
}
func (la *LogAdapter) SetAttr(key string, value interface{}) {
	la.attrs.SetAttr(key, value)
}
func (la *LogAdapter) LogWithTime(level LogLevel, ts time.Time, attrs *Attrs, msg string, a ...interface{}) error {
	if la.logLevel != nil && level > *la.logLevel {
		return nil
	}

	mergedAttrs := la.attrs.clone()
	mergedAttrs.MergeAttrs(attrs)
	return la.base.LogWithTime(level, ts, mergedAttrs, msg, a...)
}
func (la *LogAdapter) Log(level LogLevel, attrs *Attrs, msg string, a ...interface{}) error {
	if la.logLevel != nil && level > *la.logLevel {
		return nil
	}

	mergedAttrs := la.attrs.clone()
	mergedAttrs.MergeAttrs(attrs)
	return la.base.Log(level, mergedAttrs, msg, a...)
}
func (la *LogAdapter) Dbgm(m *Attrs, msg string, a ...interface{}) error {
	return la.Debugm(m, msg, a...)
}
func NewAttrsFromMap(attrs map[string]interface{}) *Attrs {
	newAttrs := NewAttrs()
	for attrKey, attrVal := range attrs {
		newAttrs.SetAttr(attrKey, attrVal)
	}
	return newAttrs
}
func NewAttrsFromAttrs(attrs ...*Attrs) *Attrs {
	newAttrs := NewAttrs()
	for _, attr := range attrs {
		newAttrs.MergeAttrs(attr)
	}
	return newAttrs
}
func (a *Attrs) MergeAttrs(attrs *Attrs) {
	if attrs == nil {
		return
	}
	a.attrsLock.Lock()
	defer a.attrsLock.Unlock()
	for hash, val := range attrs.attrs {
		a.attrs[hash] = val
	}
}
func (a *Attrs) SetAttr(key string, value interface{}) *Attrs {
	a.attrsLock.Lock()
	defer a.attrsLock.Unlock()

	valVal := reflect.ValueOf(value)
	switch valVal.Kind() {
	case reflect.Func:
		value = valVal.Type().String()
	}

	hash := getAttrHash(key)
	a.attrs[hash] = value
	return a
}
func (a *Attrs) GetAttr(key string) interface{} {
	a.attrsLock.RLock()
	defer a.attrsLock.RUnlock()

	return a.attrs[getAttrHash(key)]
}
func (a *Attrs) RemoveAttr(key string) {
	a.attrsLock.Lock()
	defer a.attrsLock.Unlock()

	delete(a.attrs, getAttrHash(key))
}
func (a *Attrs) Attrs() map[string]interface{} {
	a.attrsLock.RLock()
	defer a.attrsLock.RUnlock()

	attrs := make(map[string]interface{})
	for hash, val := range a.attrs {
		key, _ := getHashAttr(hash)
		attrs[key] = val
	}
	return attrs
}
func Debugm(m *Attrs, msg string, a ...interface{}) error {
	return curDefault.Debugm(m, msg, a...)
}
func Infom(m *Attrs, msg string, a ...interface{}) error {
	return curDefault.Infom(m, msg, a...)
}
func Warningm(m *Attrs, msg string, a ...interface{}) error {
	return curDefault.Warningm(m, msg, a...)
}
func Errm(m *Attrs, msg string, a ...interface{}) error {
	return Errorm(m, msg, a...)
}
func Errorm(m *Attrs, msg string, a ...interface{}) error {
	return curDefault.Errorm(m, msg, a...)
}
func Fatalm(m *Attrs, msg string, a ...interface{}) error {
	return curDefault.Fatalm(m, msg, a...)
}
func Dief(exitCode int, msg string, a ...interface{}) {
	curDefault.Dief(exitCode, msg, a...)
}
func Diem(exitCode int, m *Attrs, msg string, a ...interface{}) {
	curDefault.Diem(exitCode, m, msg, a...)
}
func ToLogLevel(level string) (LogLevel, error) {
	lowLevel := strings.ToLower(level)

	switch lowLevel {
	case "dbg":
		fallthrough
	case "debug":
		return LevelDebug, nil
	case "info":
		return LevelInfo, nil
	case "warn":
		fallthrough
	case "warning":
		return LevelWarning, nil
	case "err":
		fallthrough
	case "error":
		return LevelError, nil
	case "fatal":
		return LevelFatal, nil
	case "none":
		return LevelNone, nil
	}

	return 0, ErrUnknownLevel
}
func CallErr(f func() error) error {
	checkRun()
	errChan := make(chan error)
	callQueue <- func() {
		errChan <- f()
	}
	return <-errChan
}
func New(opts ...Option) (*StackdriverHook, error) {
	var err error

	sh := &StackdriverHook{
		levels: logrus.AllLevels,
	}

	// apply opts
	for _, o := range opts {
		err = o(sh)
		if err != nil {
			return nil, err
		}
	}

	// check service, resource, logName set
	if sh.service == nil && sh.agentClient == nil {
		return nil, errors.New("no stackdriver service was provided")
	}
	if sh.resource == nil && sh.agentClient == nil {
		return nil, errors.New("the monitored resource was not provided")
	}
	if sh.projectID == "" && sh.agentClient == nil {
		return nil, errors.New("the project id was not provided")
	}

	// set default project name
	if sh.logName == "" {
		err = LogName(DefaultName)(sh)
		if err != nil {
			return nil, err
		}
	}

	// If error reporting log name not set, set it to log name
	// plus string suffix
	if sh.errorReportingLogName == "" {
		sh.errorReportingLogName = sh.logName + "_errors"
	}

	return sh, nil
}
func (sh *StackdriverHook) Fire(entry *logrus.Entry) error {
	sh.waitGroup.Add(1)
	go func(entry *logrus.Entry) {
		defer sh.waitGroup.Done()
		var httpReq *logging.HttpRequest

		// convert entry data to labels
		labels := make(map[string]string, len(entry.Data))
		for k, v := range entry.Data {
			switch x := v.(type) {
			case string:
				labels[k] = x

			case *http.Request:
				httpReq = &logging.HttpRequest{
					Referer:       x.Referer(),
					RemoteIp:      x.RemoteAddr,
					RequestMethod: x.Method,
					RequestUrl:    x.URL.String(),
					UserAgent:     x.UserAgent(),
				}

			case *logging.HttpRequest:
				httpReq = x

			default:
				labels[k] = fmt.Sprintf("%v", v)
			}
		}

		// write log entry
		if sh.agentClient != nil {
			sh.sendLogMessageViaAgent(entry, labels, httpReq)
		} else {
			sh.sendLogMessageViaAPI(entry, labels, httpReq)
		}
	}(sh.copyEntry(entry))

	return nil
}
func Levels(levels ...logrus.Level) Option {
	return func(sh *StackdriverHook) error {
		sh.levels = levels
		return nil
	}
}
func ProjectID(projectID string) Option {
	return func(sh *StackdriverHook) error {
		sh.projectID = projectID
		return nil
	}
}
func EntriesService(service *logging.EntriesService) Option {
	return func(sh *StackdriverHook) error {
		sh.service = service
		return nil
	}
}
func LoggingService(service *logging.Service) Option {
	return func(sh *StackdriverHook) error {
		sh.service = service.Entries
		return nil
	}
}
func ErrorService(errorService *errorReporting.Service) Option {
	return func(sh *StackdriverHook) error {
		sh.errorService = errorService
		return nil
	}
}
func HTTPClient(client *http.Client) Option {
	return func(sh *StackdriverHook) error {
		// create logging service
		l, err := logging.New(client)
		if err != nil {
			return err
		}
		// create error reporting service
		e, err := errorReporting.New(client)
		if err != nil {
			return err
		} else {
			ErrorService(e)
		}

		return LoggingService(l)(sh)
	}
}
func MonitoredResource(resource *logging.MonitoredResource) Option {
	return func(sh *StackdriverHook) error {
		sh.resource = resource
		return nil
	}
}
func ErrorReportingLogName(name string) Option {
	return func(sh *StackdriverHook) error {
		sh.errorReportingLogName = name
		return nil
	}
}
func Labels(labels map[string]string) Option {
	return func(sh *StackdriverHook) error {
		sh.labels = labels
		return nil
	}
}
func PartialSuccess(enabled bool) Option {
	return func(sh *StackdriverHook) error {
		sh.partialSuccess = enabled
		return nil
	}
}
func GoogleComputeCredentials(serviceAccount string) Option {
	return func(sh *StackdriverHook) error {
		var err error

		// get compute metadata scopes associated with the service account
		scopes, err := metadata.Scopes(serviceAccount)
		if err != nil {
			return err
		}

		// check if all the necessary scopes are provided
		for _, s := range requiredScopes {
			if !sliceContains(scopes, s) {
				// NOTE: if you are seeing this error, you probably need to
				// recreate your compute instance with the correct scope
				//
				// as of August 2016, there is not a way to add a scope to an
				// existing compute instance
				return fmt.Errorf("missing required scope %s in compute metadata", s)
			}
		}

		return HTTPClient(&http.Client{
			Transport: &oauth2.Transport{
				Source: google.ComputeTokenSource(serviceAccount),
			},
		})(sh)
	}
}
func (c Codec) NewEncoder(w io.Writer) *Encoder {
	return NewEncoder(c.NewEmitter(w))
}
func (c Codec) NewDecoder(r io.Reader) *Decoder {
	return NewDecoder(c.NewParser(r))
}
func (c Codec) NewStreamEncoder(w io.Writer) *StreamEncoder {
	return NewStreamEncoder(c.NewEmitter(w))
}
func (c Codec) NewStreamDecoder(r io.Reader) *StreamDecoder {
	return NewStreamDecoder(c.NewParser(r))
}
func (reg *Registry) Register(mimetype string, codec Codec) {
	defer reg.mutex.Unlock()
	reg.mutex.Lock()

	if reg.codecs == nil {
		reg.codecs = make(map[string]Codec)
	}

	reg.codecs[mimetype] = codec
}
func (reg *Registry) Unregister(mimetype string) {
	defer reg.mutex.Unlock()
	reg.mutex.Lock()

	delete(reg.codecs, mimetype)
}
func (reg *Registry) Lookup(mimetype string) (codec Codec, ok bool) {
	reg.mutex.RLock()
	codec, ok = reg.codecs[mimetype]
	reg.mutex.RUnlock()
	return
}
func (reg *Registry) Codecs() (codecs map[string]Codec) {
	codecs = make(map[string]Codec)
	reg.mutex.RLock()
	for mimetype, codec := range reg.codecs {
		codecs[mimetype] = codec
	}
	reg.mutex.RUnlock()
	return
}
func (e *Error) Type() string {
	s := e.Error()

	if i := strings.IndexByte(s, ' '); i < 0 {
		s = ""
	} else {
		s = s[:i]

		for _, c := range s {
			if !unicode.IsUpper(c) {
				s = ""
				break
			}
		}
	}

	return s
}
func Install(typ reflect.Type, adapter Adapter) {
	if adapter.Encode == nil {
		panic("objconv: the encoder function of an adapter cannot be nil")
	}

	if adapter.Decode == nil {
		panic("objconv: the decoder function of an adapter cannot be nil")
	}

	adapterMutex.Lock()
	adapterStore[typ] = adapter
	adapterMutex.Unlock()

	// We have to clear the struct cache because it may now have become invalid.
	// Because installing adapters is done in the package initialization phase
	// it's unlikely that any encoding or decoding operations are taking place
	// at this time so there should be no performance impact of clearing the
	// cache.
	structCache.clear()
}
func AdapterOf(typ reflect.Type) (a Adapter, ok bool) {
	adapterMutex.RLock()
	a, ok = adapterStore[typ]
	adapterMutex.RUnlock()
	return
}
func AppendDuration(b []byte, d time.Duration) []byte {
	// Largest time is 2540400h10m10.000000000s
	var buf [32]byte
	w := len(buf)

	u := uint64(d)
	neg := d < 0
	if neg {
		u = -u
	}

	if u < uint64(time.Second) {
		// Special case: if duration is smaller than a second,
		// use smaller units, like 1.2ms
		var prec int
		w--
		buf[w] = 's'
		w--
		switch {
		case u == 0:
			return append(b, '0', 's')
		case u < uint64(time.Microsecond):
			// print nanoseconds
			prec = 0
			buf[w] = 'n'
		case u < uint64(time.Millisecond):
			// print microseconds
			prec = 3
			// U+00B5 'µ' micro sign == 0xC2 0xB5
			w-- // Need room for two bytes.
			copy(buf[w:], "µ")
		default:
			// print milliseconds
			prec = 6
			buf[w] = 'm'
		}
		w, u = fmtFrac(buf[:w], u, prec)
		w = fmtInt(buf[:w], u)
	} else {
		w--
		buf[w] = 's'

		w, u = fmtFrac(buf[:w], u, 9)

		// u is now integer seconds
		w = fmtInt(buf[:w], u%60)
		u /= 60

		// u is now integer minutes
		if u > 0 {
			w--
			buf[w] = 'm'
			w = fmtInt(buf[:w], u%60)
			u /= 60

			// u is now integer hours
			// Stop at hours because days can be different lengths.
			if u > 0 {
				w--
				buf[w] = 'h'
				w = fmtInt(buf[:w], u)
			}
		}
	}

	if neg {
		w--
		buf[w] = '-'
	}

	return append(b, buf[w:]...)
}
func fmtInt(buf []byte, v uint64) int {
	w := len(buf)
	if v == 0 {
		w--
		buf[w] = '0'
	} else {
		for v > 0 {
			w--
			buf[w] = byte(v%10) + '0'
			v /= 10
		}
	}
	return w
}
func NewDecoder(r io.Reader) *objconv.Decoder {
	return objconv.NewDecoder(NewParser(r))
}
func NewStreamDecoder(r io.Reader) *objconv.StreamDecoder {
	return objconv.NewStreamDecoder(NewParser(r))
}
func Unmarshal(b []byte, v interface{}) error {
	u := unmarshalerPool.Get().(*unmarshaler)
	u.reset(b)

	err := (objconv.Decoder{Parser: u}).Decode(v)

	u.reset(nil)
	unmarshalerPool.Put(u)
	return err
}
func (t Type) String() string {
	switch t {
	case Nil:
		return "nil"
	case Bool:
		return "bool"
	case Int:
		return "int"
	case Uint:
		return "uint"
	case Float:
		return "float"
	case String:
		return "string"
	case Bytes:
		return "bytes"
	case Time:
		return "time"
	case Duration:
		return "duration"
	case Error:
		return "error"
	case Array:
		return "array"
	case Map:
		return "map"
	default:
		return "<type>"
	}
}
func zeroValueOf(t reflect.Type) reflect.Value {
	zeroMutex.RLock()
	v, ok := zeroCache[t]
	zeroMutex.RUnlock()

	if !ok {
		v = reflect.Zero(t)
		zeroMutex.Lock()
		zeroCache[t] = v
		zeroMutex.Unlock()
	}

	return v
}
func NewValueParser(v interface{}) *ValueParser {
	return &ValueParser{
		stack: []reflect.Value{reflect.ValueOf(v)},
	}
}
func ParseTag(s string) Tag {
	var name string
	var omitzero bool
	var omitempty bool

	name, s = parseNextTagToken(s)

	for len(s) != 0 {
		var token string
		switch token, s = parseNextTagToken(s); token {
		case "omitempty":
			omitempty = true
		case "omitzero":
			omitzero = true
		}
	}

	return Tag{
		Name:      name,
		Omitempty: omitempty,
		Omitzero:  omitzero,
	}
}
func NewEncoder(w io.Writer) *objconv.Encoder {
	return objconv.NewEncoder(NewEmitter(w))
}
func NewStreamEncoder(w io.Writer) *objconv.StreamEncoder {
	return objconv.NewStreamEncoder(NewEmitter(w))
}
func NewPrettyEncoder(w io.Writer) *objconv.Encoder {
	return objconv.NewEncoder(NewPrettyEmitter(w))
}
func NewPrettyStreamEncoder(w io.Writer) *objconv.StreamEncoder {
	return objconv.NewStreamEncoder(NewPrettyEmitter(w))
}
func Marshal(v interface{}) (b []byte, err error) {
	m := marshalerPool.Get().(*marshaler)
	m.b.Truncate(0)

	if err = (objconv.Encoder{Emitter: m}).Encode(v); err == nil {
		b = make([]byte, m.b.Len())
		copy(b, m.b.Bytes())
	}

	marshalerPool.Put(m)
	return
}
func NewEncoder(e Emitter) *Encoder {
	if e == nil {
		panic("objconv: the emitter is nil")
	}
	return &Encoder{Emitter: e}
}
func (e Encoder) EncodeArray(n int, f func(Encoder) error) (err error) {
	if e.key {
		if e.key, err = false, e.Emitter.EmitMapValue(); err != nil {
			return
		}
	}

	if err = e.Emitter.EmitArrayBegin(n); err != nil {
		return
	}

encodeArray:
	for i := 0; n < 0 || i < n; i++ {
		if i != 0 {
			if e.Emitter.EmitArrayNext(); err != nil {
				return
			}
		}
		switch err = f(e); err {
		case nil:
		case End:
			break encodeArray
		default:
			return
		}
	}

	return e.Emitter.EmitArrayEnd()
}
func (e Encoder) EncodeMap(n int, f func(Encoder, Encoder) error) (err error) {
	if e.key {
		if e.key, err = false, e.Emitter.EmitMapValue(); err != nil {
			return
		}
	}

	if err = e.Emitter.EmitMapBegin(n); err != nil {
		return
	}

encodeMap:
	for i := 0; n < 0 || i < n; i++ {
		if i != 0 {
			if err = e.Emitter.EmitMapNext(); err != nil {
				return
			}
		}
		e.key = true
		err = f(
			Encoder{Emitter: e.Emitter, SortMapKeys: e.SortMapKeys},
			Encoder{Emitter: e.Emitter, SortMapKeys: e.SortMapKeys, key: true},
		)
		// Because internal calls don't use the exported methods they may not
		// reset this flag to false when expected, forcing the value here.
		e.key = false

		switch err {
		case nil:
		case End:
			break encodeMap
		default:
			return
		}
	}

	return e.Emitter.EmitMapEnd()
}
func NewStreamEncoder(e Emitter) *StreamEncoder {
	if e == nil {
		panic("objconv.NewStreamEncoder: the emitter is nil")
	}
	return &StreamEncoder{Emitter: e}
}
func (e *StreamEncoder) Open(n int) error {
	if err := e.err; err != nil {
		return err
	}

	if e.closed {
		return io.ErrClosedPipe
	}

	if !e.opened {
		e.max = n
		e.opened = true

		if !e.oneshot {
			e.err = e.Emitter.EmitArrayBegin(n)
		}
	}

	return e.err
}
func (e *StreamEncoder) Close() error {
	if !e.closed {
		if err := e.Open(-1); err != nil {
			return err
		}

		e.closed = true

		if !e.oneshot {
			e.err = e.Emitter.EmitArrayEnd()
		}
	}

	return e.err
}
func (e *StreamEncoder) Encode(v interface{}) error {
	if err := e.Open(-1); err != nil {
		return err
	}

	if e.max >= 0 && e.cnt >= e.max {
		return fmt.Errorf("objconv: too many values sent to a stream encoder exceed the configured limit of %d", e.max)
	}

	if !e.oneshot && e.cnt != 0 {
		e.err = e.Emitter.EmitArrayNext()
	}

	if e.err == nil {
		e.err = (Encoder{
			Emitter:     e.Emitter,
			SortMapKeys: e.SortMapKeys,
		}).Encode(v)

		if e.cnt++; e.max >= 0 && e.cnt >= e.max {
			e.Close()
		}
	}

	return e.err
}
func newStructType(t reflect.Type, c map[reflect.Type]*structType) *structType {
	if s := c[t]; s != nil {
		return s
	}

	n := t.NumField()
	s := &structType{
		fields:       make([]structField, 0, n),
		fieldsByName: make(map[string]*structField),
	}
	c[t] = s

	for i := 0; i != n; i++ {
		ft := t.Field(i)

		if ft.Anonymous || len(ft.PkgPath) != 0 { // anonymous or non-exported
			continue
		}

		sf := makeStructField(ft, c)

		if sf.name == "-" { // skip
			continue
		}

		s.fields = append(s.fields, sf)
		s.fieldsByName[sf.name] = &s.fields[len(s.fields)-1]
	}

	return s
}
func (cache *structTypeCache) lookup(t reflect.Type) (s *structType) {
	cache.mutex.RLock()
	s = cache.store[t]
	cache.mutex.RUnlock()

	if s == nil {
		// There's a race confition here where this value may be generated
		// multiple times.
		// The impact in practice is really small as it's unlikely to happen
		// often, we take the approach of keeping the logic simple and avoid
		// a more complex synchronization logic required to solve this edge
		// case.
		s = newStructType(t, map[reflect.Type]*structType{})
		cache.mutex.Lock()
		cache.store[t] = s
		cache.mutex.Unlock()
	}

	return
}
func (cache *structTypeCache) clear() {
	cache.mutex.Lock()
	for typ := range cache.store {
		delete(cache.store, typ)
	}
	cache.mutex.Unlock()
}
func CheckUint64Bounds(v uint64, max uint64, t reflect.Type) (err error) {
	if v > max {
		err = fmt.Errorf("objconv: %d overflows the maximum value of %d for %s", v, max, t)
	}
	return
}
func CheckInt64Bounds(v int64, min int64, max uint64, t reflect.Type) (err error) {
	if v < min {
		err = fmt.Errorf("objconv: %d overflows the minimum value of %d for %s", v, min, t)
	}
	if v > 0 && uint64(v) > max {
		err = fmt.Errorf("objconv: %d overflows the maximum value of %d for %s", v, max, t)
	}
	return
}
func NewDecoder(p Parser) *Decoder {
	if p == nil {
		panic("objconv: the parser is nil")
	}
	return &Decoder{Parser: p}
}
func (d Decoder) Decode(v interface{}) error {
	to := reflect.ValueOf(v)

	if d.off != 0 {
		var err error
		if d.off, err = 0, d.Parser.ParseMapValue(d.off-1); err != nil {
			return err
		}
	}

	if !to.IsValid() {
		// This special case for a nil value is used to make it possible to
		// discard decoded values.
		_, err := d.decodeInterface(to)
		return err
	}

	// Optimization for ValueDecoder, in practice tho it's also handled in the
	// methods that are based on reflection.
	switch x := v.(type) {
	case ValueDecoder:
		return x.DecodeValue(d)
	}

	if to.Kind() == reflect.Ptr {
		// In most cases the method receives a pointer, but we may also have to
		// support types that aren't pointers but implement ValueDecoder, or
		// types that have got adapters set.
		// If we're not in either of those cases the code will likely panic when
		// the value is set because it won't be addressable.
		to = to.Elem()
	}

	_, err := d.decode(to)
	return err
}
func (d Decoder) DecodeArray(f func(Decoder) error) (err error) {
	var typ Type

	if d.off != 0 {
		if d.off, err = 0, d.Parser.ParseMapValue(d.off-1); err != nil {
			return
		}
	}

	if typ, err = d.Parser.ParseType(); err != nil {
		return
	}

	err = d.decodeArrayImpl(typ, f)
	return
}
func (d Decoder) DecodeMap(f func(Decoder, Decoder) error) (err error) {
	var typ Type

	if d.off != 0 {
		if d.off, err = 0, d.Parser.ParseMapValue(d.off-1); err != nil {
			return
		}
	}

	if typ, err = d.Parser.ParseType(); err != nil {
		return
	}

	err = d.decodeMapImpl(typ, f)
	return
}
func NewStreamDecoder(p Parser) *StreamDecoder {
	if p == nil {
		panic("objconv: the parser is nil")
	}
	return &StreamDecoder{Parser: p}
}
func (d *StreamDecoder) Len() int {
	if d.err != nil {
		return 0
	}

	if d.typ == Unknown {
		if d.init() != nil {
			return 0
		}
	}

	return d.max - d.cnt
}
func (d *StreamDecoder) Err() error {
	if d.err == End {
		return nil
	}
	return d.err
}
func (d *StreamDecoder) Decode(v interface{}) error {
	if d.err != nil {
		return d.err
	}

	err := error(nil)
	cnt := d.cnt
	max := d.max
	dec := Decoder{
		Parser:  d.Parser,
		MapType: d.MapType,
	}

	switch d.typ {
	case Unknown:
		err = d.init()
		max = d.max
	case Array:
		if cnt == max {
			err = dec.Parser.ParseArrayEnd(cnt)
		} else if cnt != 0 {
			err = dec.Parser.ParseArrayNext(cnt)
		}
	}

	if err == nil {
		if cnt == max {
			err = End
		} else {
			switch err = dec.Decode(v); err {
			case nil:
				cnt++
			case End:
				cnt++
				max = cnt
			default:
				if max < 0 && dec.Parser.ParseArrayEnd(cnt) == nil {
					err = End
				}
			}
		}
	}

	d.err = err
	d.cnt = cnt
	d.max = max
	return err
}
func (d *StreamDecoder) Encoder(e Emitter) (enc *StreamEncoder, err error) {
	var typ Type

	if typ, err = d.Parser.ParseType(); err == nil {
		enc = NewStreamEncoder(e)
		enc.oneshot = typ != Array
	}

	return
}
func init() {
	for _, f := range strings.Split(os.Getenv("LOGFLAGS"), ",") {
		switch f {
		case "longfile":
			defaultFlags |= Llongfile
		case "shortfile":
			defaultFlags |= Lshortfile
		}
	}
}
func LevelFromString(s string) (l Level, ok bool) {
	switch strings.ToLower(s) {
	case "trace", "trc":
		return LevelTrace, true
	case "debug", "dbg":
		return LevelDebug, true
	case "info", "inf":
		return LevelInfo, true
	case "warn", "wrn":
		return LevelWarn, true
	case "error", "err":
		return LevelError, true
	case "critical", "crt":
		return LevelCritical, true
	case "off":
		return LevelOff, true
	default:
		return LevelInfo, false
	}
}
func NewBackend(w io.Writer, opts ...BackendOption) *Backend {
	b := &Backend{w: w, flag: defaultFlags}
	for _, o := range opts {
		o(b)
	}
	return b
}
func callsite(flag uint32) (string, int) {
	_, file, line, ok := runtime.Caller(calldepth)
	if !ok {
		return "???", 0
	}
	if flag&Lshortfile != 0 {
		short := file
		for i := len(file) - 1; i > 0; i-- {
			if os.IsPathSeparator(file[i]) {
				short = file[i+1:]
				break
			}
		}
		file = short
	}
	return file, line
}
func (b *Backend) print(lvl, tag string, args ...interface{}) {
	t := time.Now() // get as early as possible

	bytebuf := buffer()

	var file string
	var line int
	if b.flag&(Lshortfile|Llongfile) != 0 {
		file, line = callsite(b.flag)
	}

	formatHeader(bytebuf, t, lvl, tag, file, line)
	buf := bytes.NewBuffer(*bytebuf)
	fmt.Fprintln(buf, args...)
	*bytebuf = buf.Bytes()

	b.mu.Lock()
	b.w.Write(*bytebuf)
	b.mu.Unlock()

	recycleBuffer(bytebuf)
}
func (b *Backend) Logger(subsystemTag string) Logger {
	return &slog{LevelInfo, subsystemTag, b}
}
func (l *slog) Trace(args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelTrace {
		l.b.print("TRC", l.tag, args...)
	}
}
func (l *slog) Tracef(format string, args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelTrace {
		l.b.printf("TRC", l.tag, format, args...)
	}
}
func (l *slog) Debug(args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelDebug {
		l.b.print("DBG", l.tag, args...)
	}
}
func (l *slog) Debugf(format string, args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelDebug {
		l.b.printf("DBG", l.tag, format, args...)
	}
}
func (l *slog) Info(args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelInfo {
		l.b.print("INF", l.tag, args...)
	}
}
func (l *slog) Infof(format string, args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelInfo {
		l.b.printf("INF", l.tag, format, args...)
	}
}
func (l *slog) Warn(args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelWarn {
		l.b.print("WRN", l.tag, args...)
	}
}
func (l *slog) Warnf(format string, args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelWarn {
		l.b.printf("WRN", l.tag, format, args...)
	}
}
func (l *slog) Error(args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelError {
		l.b.print("ERR", l.tag, args...)
	}
}
func (l *slog) Errorf(format string, args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelError {
		l.b.printf("ERR", l.tag, format, args...)
	}
}
func (l *slog) Critical(args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelCritical {
		l.b.print("CRT", l.tag, args...)
	}
}
func (l *slog) Criticalf(format string, args ...interface{}) {
	lvl := l.Level()
	if lvl <= LevelCritical {
		l.b.printf("CRT", l.tag, format, args...)
	}
}
func (l *slog) Level() Level {
	return Level(atomic.LoadUint32((*uint32)(&l.lvl)))
}
func (l *slog) SetLevel(level Level) {
	atomic.StoreUint32((*uint32)(&l.lvl), uint32(level))
}
func (permission *Permission) Concat(newPermission *Permission) *Permission {
	var result = Permission{
		Role:         Global,
		AllowedRoles: map[PermissionMode][]string{},
		DeniedRoles:  map[PermissionMode][]string{},
	}

	var appendRoles = func(p *Permission) {
		if p != nil {
			result.Role = p.Role

			for mode, roles := range p.DeniedRoles {
				result.DeniedRoles[mode] = append(result.DeniedRoles[mode], roles...)
			}

			for mode, roles := range p.AllowedRoles {
				result.AllowedRoles[mode] = append(result.AllowedRoles[mode], roles...)
			}
		}
	}

	appendRoles(newPermission)
	appendRoles(permission)
	return &result
}
func (permission Permission) HasPermission(mode PermissionMode, roles ...interface{}) bool {
	var roleNames []string
	for _, role := range roles {
		if r, ok := role.(string); ok {
			roleNames = append(roleNames, r)
		} else if roler, ok := role.(Roler); ok {
			roleNames = append(roleNames, roler.GetRoles()...)
		} else {
			fmt.Printf("invalid role %#v\n", role)
			return false
		}
	}

	if len(permission.DeniedRoles) != 0 {
		if DeniedRoles := permission.DeniedRoles[mode]; DeniedRoles != nil {
			if includeRoles(DeniedRoles, roleNames) {
				return false
			}
		}
	}

	// return true if haven't define allowed roles
	if len(permission.AllowedRoles) == 0 {
		return true
	}

	if AllowedRoles := permission.AllowedRoles[mode]; AllowedRoles != nil {
		if includeRoles(AllowedRoles, roleNames) {
			return true
		}
	}

	return false
}
func ConcatPermissioner(ps ...Permissioner) Permissioner {
	var newPS []Permissioner
	for _, p := range ps {
		if p != nil {
			newPS = append(newPS, p)
		}
	}
	return permissioners(newPS)
}
func (ps permissioners) HasPermission(mode PermissionMode, roles ...interface{}) bool {
	for _, p := range ps {
		if p != nil && !p.HasPermission(mode, roles) {
			return false
		}
	}

	return true
}
func (role *Role) Register(name string, fc Checker) {
	if role.definitions == nil {
		role.definitions = map[string]Checker{}
	}

	definition := role.definitions[name]
	if definition != nil {
		fmt.Printf("Role `%v` already defined, overwrited it!\n", name)
	}
	role.definitions[name] = fc
}
func (role *Role) NewPermission() *Permission {
	return &Permission{
		Role:         role,
		AllowedRoles: map[PermissionMode][]string{},
		DeniedRoles:  map[PermissionMode][]string{},
	}
}
func (role *Role) Get(name string) (Checker, bool) {
	fc, ok := role.definitions[name]
	return fc, ok
}
func (p *Process) isPtrFromHeap(a core.Address) bool {
	return p.findHeapInfo(a).IsPtr(a, p.proc.PtrSize())
}
func (p *Process) IsPtr(a core.Address) bool {
	h := p.findHeapInfo(a)
	if h != nil {
		return h.IsPtr(a, p.proc.PtrSize())
	}
	for _, m := range p.modules {
		for _, s := range [2]string{"data", "bss"} {
			min := core.Address(m.r.Field(s).Uintptr())
			max := core.Address(m.r.Field("e" + s).Uintptr())
			if a < min || a >= max {
				continue
			}
			gc := m.r.Field("gc" + s + "mask").Field("bytedata").Address()
			i := a.Sub(min)
			return p.proc.ReadUint8(gc.Add(i/8))>>uint(i%8) != 0
		}
	}
	// Everywhere else can't be a pointer. At least, not a pointer into the Go heap.
	// TODO: stacks?
	// TODO: finalizers?
	return false
}
func (p *Process) FindObject(a core.Address) (Object, int64) {
	// Round down to the start of an object.
	h := p.findHeapInfo(a)
	if h == nil {
		// Not in Go heap, or in a span
		// that doesn't hold Go objects (freed, stacks, ...)
		return 0, 0
	}
	x := h.base.Add(a.Sub(h.base) / h.size * h.size)
	// Check if object is marked.
	h = p.findHeapInfo(x)
	if h.mark>>(uint64(x)%heapInfoSize/8)&1 == 0 { // free or garbage
		return 0, 0
	}
	return Object(x), a.Sub(x)
}
func (p *Process) ForEachObject(fn func(x Object) bool) {
	for _, k := range p.pages {
		pt := p.pageTable[k]
		for i := range pt {
			h := &pt[i]
			m := h.mark
			for m != 0 {
				j := bits.TrailingZeros64(m)
				m &= m - 1
				x := Object(k)*pageTableSize*heapInfoSize + Object(i)*heapInfoSize + Object(j)*8
				if !fn(x) {
					return
				}
			}
		}
	}
}
func (p *Process) ForEachRoot(fn func(r *Root) bool) {
	for _, r := range p.globals {
		if !fn(r) {
			return
		}
	}
	for _, g := range p.goroutines {
		for _, f := range g.frames {
			for _, r := range f.roots {
				if !fn(r) {
					return
				}
			}
		}
	}
}
func (p *Process) Addr(x Object) core.Address {
	return core.Address(x)
}
func (p *Process) Size(x Object) int64 {
	return p.findHeapInfo(core.Address(x)).size
}
func (p *Process) Type(x Object) (*Type, int64) {
	p.typeHeap()

	i, _ := p.findObjectIndex(core.Address(x))
	return p.types[i].t, p.types[i].r
}
func (p *Process) ForEachRootPtr(r *Root, fn func(int64, Object, int64) bool) {
	edges1(p, r, 0, r.Type, fn)
}
func edges1(p *Process, r *Root, off int64, t *Type, fn func(int64, Object, int64) bool) bool {
	switch t.Kind {
	case KindBool, KindInt, KindUint, KindFloat, KindComplex:
		// no edges here
	case KindIface, KindEface:
		// The first word is a type or itab.
		// Itabs are never in the heap.
		// Types might be, though.
		a := r.Addr.Add(off)
		if r.Frame == nil || r.Frame.Live[a] {
			dst, off2 := p.FindObject(p.proc.ReadPtr(a))
			if dst != 0 {
				if !fn(off, dst, off2) {
					return false
				}
			}
		}
		// Treat second word like a pointer.
		off += p.proc.PtrSize()
		fallthrough
	case KindPtr, KindString, KindSlice, KindFunc:
		a := r.Addr.Add(off)
		if r.Frame == nil || r.Frame.Live[a] {
			dst, off2 := p.FindObject(p.proc.ReadPtr(a))
			if dst != 0 {
				if !fn(off, dst, off2) {
					return false
				}
			}
		}
	case KindArray:
		s := t.Elem.Size
		for i := int64(0); i < t.Count; i++ {
			if !edges1(p, r, off+i*s, t.Elem, fn) {
				return false
			}
		}
	case KindStruct:
		for _, f := range t.Fields {
			if !edges1(p, r, off+f.Off, f.Type, fn) {
				return false
			}
		}
	}
	return true
}
func (p *Process) setHeapPtr(a core.Address) {
	h := p.allocHeapInfo(a)
	if p.proc.PtrSize() == 8 {
		i := uint(a%heapInfoSize) / 8
		h.ptr[0] |= uint64(1) << i
		return
	}
	i := a % heapInfoSize / 4
	h.ptr[i/64] |= uint64(1) << (i % 64)
}
func (p *Process) findHeapInfo(a core.Address) *heapInfo {
	k := a / heapInfoSize / pageTableSize
	i := a / heapInfoSize % pageTableSize
	t := p.pageTable[k]
	if t == nil {
		return nil
	}
	h := &t[i]
	if h.base == 0 {
		return nil
	}
	return h
}
func (p *Process) allocHeapInfo(a core.Address) *heapInfo {
	k := a / heapInfoSize / pageTableSize
	i := a / heapInfoSize % pageTableSize
	t := p.pageTable[k]
	if t == nil {
		t = new(pageTableEntry)
		for j := 0; j < pageTableSize; j++ {
			t[j].firstIdx = -1
		}
		p.pageTable[k] = t
		p.pages = append(p.pages, k)
	}
	return &t[i]
}
func runtimeName(dt dwarf.Type) string {
	switch x := dt.(type) {
	case *dwarf.PtrType:
		if _, ok := x.Type.(*dwarf.VoidType); ok {
			return "unsafe.Pointer"
		}
		return "*" + runtimeName(x.Type)
	case *dwarf.ArrayType:
		return fmt.Sprintf("[%d]%s", x.Count, runtimeName(x.Type))
	case *dwarf.StructType:
		if !strings.HasPrefix(x.StructName, "struct {") {
			// This is a named type, return that name.
			return stripPackagePath(x.StructName)
		}
		// Figure out which fields have anonymous names.
		var anon []bool
		for _, f := range strings.Split(x.StructName[8:len(x.StructName)-1], ";") {
			f = strings.TrimSpace(f)
			anon = append(anon, !strings.Contains(f, " "))
			// TODO: this isn't perfect. If the field type has a space in it,
			// then this logic doesn't work. Need to search for keyword for
			// field type, like "interface", "struct", ...
		}
		// Make sure anon is long enough. This probably never triggers.
		for len(anon) < len(x.Field) {
			anon = append(anon, false)
		}

		// Build runtime name from the DWARF fields.
		s := "struct {"
		first := true
		for _, f := range x.Field {
			if !first {
				s += ";"
			}
			name := f.Name
			if i := strings.Index(name, "."); i >= 0 {
				name = name[i+1:]
			}
			if anon[0] {
				s += fmt.Sprintf(" %s", runtimeName(f.Type))
			} else {
				s += fmt.Sprintf(" %s %s", name, runtimeName(f.Type))
			}
			first = false
			anon = anon[1:]
		}
		s += " }"
		return s
	default:
		return stripPackagePath(dt.String())
	}
}
func (p *Process) readRuntimeConstants() {
	p.rtConstants = map[string]int64{}

	// Hardcoded values for Go 1.9.
	// (Go did not have constants in DWARF before 1.10.)
	m := p.rtConstants
	m["_MSpanDead"] = 0
	m["_MSpanInUse"] = 1
	m["_MSpanManual"] = 2
	m["_MSpanFree"] = 3
	m["_Gidle"] = 0
	m["_Grunnable"] = 1
	m["_Grunning"] = 2
	m["_Gsyscall"] = 3
	m["_Gwaiting"] = 4
	m["_Gdead"] = 6
	m["_Gscan"] = 0x1000
	m["_PCDATA_StackMapIndex"] = 0
	m["_FUNCDATA_LocalsPointerMaps"] = 1
	m["_FUNCDATA_ArgsPointerMaps"] = 0
	m["tflagExtraStar"] = 1 << 1
	m["kindGCProg"] = 1 << 6
	m["kindDirectIface"] = 1 << 5
	m["_PageSize"] = 1 << 13
	m["_KindSpecialFinalizer"] = 1

	// From 1.10, these constants are recorded in DWARF records.
	d, _ := p.proc.DWARF()
	r := d.Reader()
	for e, err := r.Next(); e != nil && err == nil; e, err = r.Next() {
		if e.Tag != dwarf.TagConstant {
			continue
		}
		f := e.AttrField(dwarf.AttrName)
		if f == nil {
			continue
		}
		name := f.Val.(string)
		if !strings.HasPrefix(name, "runtime.") {
			continue
		}
		name = name[8:]
		c := e.AttrField(dwarf.AttrConstValue)
		if c == nil {
			continue
		}
		p.rtConstants[name] = c.Val.(int64)
	}
}
func (t *funcTab) add(min, max core.Address, f *Func) {
	t.entries = append(t.entries, funcTabEntry{min: min, max: max, f: f})
}
func (t *funcTab) sort() {
	sort.Slice(t.entries, func(i, j int) bool {
		return t.entries[i].min < t.entries[j].min
	})
}
func (t *funcTab) find(pc core.Address) *Func {
	n := sort.Search(len(t.entries), func(i int) bool {
		return t.entries[i].max > pc
	})
	if n == len(t.entries) || pc < t.entries[n].min || pc >= t.entries[n].max {
		return nil
	}
	return t.entries[n].f
}
func (t *pcTab) read(core *core.Process, data core.Address) {
	var pcQuantum int64
	switch core.Arch() {
	case "386", "amd64", "amd64p32":
		pcQuantum = 1
	case "s390x":
		pcQuantum = 2
	case "arm", "arm64", "mips", "mipsle", "mips64", "mips64le", "ppc64", "ppc64le":
		pcQuantum = 4
	default:
		panic("unknown architecture " + core.Arch())
	}
	val := int64(-1)
	first := true
	for {
		// Advance value.
		v, n := readVarint(core, data)
		if v == 0 && !first {
			return
		}
		data = data.Add(n)
		if v&1 != 0 {
			val += ^(v >> 1)
		} else {
			val += v >> 1
		}

		// Advance pc.
		v, n = readVarint(core, data)
		data = data.Add(n)
		t.entries = append(t.entries, pcTabEntry{bytes: v * pcQuantum, val: val})
		first = false
	}
}
func readVarint(core *core.Process, a core.Address) (val, n int64) {
	for {
		b := core.ReadUint8(a)
		val |= int64(b&0x7f) << uint(n*7)
		n++
		a++
		if b&0x80 == 0 {
			return
		}
	}
}
func useLine(c *cobra.Command) string {
	var useline string
	if c.HasParent() {
		useline = commandPath(c.Parent()) + " " + c.Use
	} else {
		useline = c.Use
	}
	if c.DisableFlagsInUseLine {
		return useline
	}
	if c.HasAvailableFlags() && !strings.Contains(useline, "[flags]") {
		useline += " [flags]"
	}
	return useline
}
func commandPath(c *cobra.Command) string {
	if c.HasParent() {
		return commandPath(c) + " " + c.Name()
	}
	return c.Use
}
func readCore() (*core.Process, *gocore.Process, error) {
	cc := coreCache
	if cc.cfg == cfg {
		return cc.coreP, cc.gocoreP, cc.err
	}
	c, err := core.Core(cfg.corefile, cfg.base, cfg.exePath)
	if err != nil {
		return nil, nil, err
	}
	p, err := gocore.Core(c)
	if os.IsNotExist(err) && cfg.exePath == "" {
		return nil, nil, fmt.Errorf("%v; consider specifying the --exe flag", err)
	}
	if err != nil {
		return nil, nil, err
	}
	for _, w := range c.Warnings() {
		fmt.Fprintf(os.Stderr, "WARNING: %s\n", w)
	}
	cc.cfg = cfg
	cc.coreP = c
	cc.gocoreP = p
	cc.err = nil
	return c, p, nil
}
func typeName(c *gocore.Process, x gocore.Object) string {
	size := c.Size(x)
	typ, repeat := c.Type(x)
	if typ == nil {
		return fmt.Sprintf("unk%d", size)
	}
	name := typ.String()
	n := size / typ.Size
	if n > 1 {
		if repeat < n {
			name = fmt.Sprintf("[%d+%d?]%s", repeat, n-repeat, name)
		} else {
			name = fmt.Sprintf("[%d]%s", repeat, name)
		}
	}
	return name
}
func fieldName(c *gocore.Process, x gocore.Object, off int64) string {
	size := c.Size(x)
	typ, repeat := c.Type(x)
	if typ == nil {
		return fmt.Sprintf("f%d", off)
	}
	n := size / typ.Size
	i := off / typ.Size
	if i == 0 && repeat == 1 {
		// Probably a singleton object, no need for array notation.
		return typeFieldName(typ, off)
	}
	if i >= n {
		// Partial space at the end of the object - the type can't be complete.
		return fmt.Sprintf("f%d", off)
	}
	q := ""
	if i >= repeat {
		// Past the known repeat section, add a ? because we're not sure about the type.
		q = "?"
	}
	return fmt.Sprintf("[%d]%s%s", i, typeFieldName(typ, off-i*typ.Size), q)
}
func typeFieldName(t *gocore.Type, off int64) string {
	switch t.Kind {
	case gocore.KindBool, gocore.KindInt, gocore.KindUint, gocore.KindFloat:
		return ""
	case gocore.KindComplex:
		if off == 0 {
			return ".real"
		}
		return ".imag"
	case gocore.KindIface, gocore.KindEface:
		if off == 0 {
			return ".type"
		}
		return ".data"
	case gocore.KindPtr, gocore.KindFunc:
		return ""
	case gocore.KindString:
		if off == 0 {
			return ".ptr"
		}
		return ".len"
	case gocore.KindSlice:
		if off == 0 {
			return ".ptr"
		}
		if off <= t.Size/2 {
			return ".len"
		}
		return ".cap"
	case gocore.KindArray:
		s := t.Elem.Size
		i := off / s
		return fmt.Sprintf("[%d]%s", i, typeFieldName(t.Elem, off-i*s))
	case gocore.KindStruct:
		for _, f := range t.Fields {
			if f.Off <= off && off < f.Off+f.Type.Size {
				return "." + f.Name + typeFieldName(f.Type, off-f.Off)
			}
		}
	}
	return ".???"
}
func (p *Process) FindFunc(pc core.Address) *Func {
	return p.funcTab.find(pc)
}
func Core(proc *core.Process) (p *Process, err error) {
	// Make sure we have DWARF info.
	if _, err := proc.DWARF(); err != nil {
		return nil, err
	}

	// Guard against failures of proc.Read* routines.
	/*
		defer func() {
			e := recover()
			if e == nil {
				return
			}
			p = nil
			if x, ok := e.(error); ok {
				err = x
				return
			}
			panic(e) // Not an error, re-panic it.
		}()
	*/

	p = &Process{
		proc:       proc,
		runtimeMap: map[core.Address]*Type{},
		dwarfMap:   map[dwarf.Type]*Type{},
	}

	// Initialize everything that just depends on DWARF.
	p.readDWARFTypes()
	p.readRuntimeConstants()
	p.readGlobals()

	// Find runtime globals we care about. Initialize regions for them.
	p.rtGlobals = map[string]region{}
	for _, g := range p.globals {
		if strings.HasPrefix(g.Name, "runtime.") {
			p.rtGlobals[g.Name[8:]] = region{p: p, a: g.Addr, typ: g.Type}
		}
	}

	// Read all the data that depend on runtime globals.
	p.buildVersion = p.rtGlobals["buildVersion"].String()
	p.readModules()
	p.readHeap()
	p.readGs()
	p.readStackVars() // needs to be after readGs.
	p.markObjects()   // needs to be after readGlobals, readStackVars.

	return p, nil
}
func (r region) Address() core.Address {
	if r.typ.Kind != KindPtr {
		panic("can't ask for the Address of a non-pointer " + r.typ.Name)
	}
	return r.p.proc.ReadPtr(r.a)
}
func (r region) Int() int64 {
	if r.typ.Kind != KindInt || r.typ.Size != r.p.proc.PtrSize() {
		panic("not an int: " + r.typ.Name)
	}
	return r.p.proc.ReadInt(r.a)
}
func (r region) Uintptr() uint64 {
	if r.typ.Kind != KindUint || r.typ.Size != r.p.proc.PtrSize() {
		panic("not a uintptr: " + r.typ.Name)
	}
	return r.p.proc.ReadUintptr(r.a)
}
func (r region) Cast(typ string) region {
	return region{p: r.p, a: r.a, typ: r.p.findType(typ)}
}
func (r region) Deref() region {
	if r.typ.Kind != KindPtr {
		panic("can't deref on non-pointer: " + r.typ.Name)
	}
	if r.typ.Elem == nil {
		panic("can't deref unsafe.Pointer")
	}
	p := r.p.proc.ReadPtr(r.a)
	return region{p: r.p, a: p, typ: r.typ.Elem}
}
func (r region) Uint64() uint64 {
	if r.typ.Kind != KindUint || r.typ.Size != 8 {
		panic("bad uint64 type " + r.typ.Name)
	}
	return r.p.proc.ReadUint64(r.a)
}
func (r region) Uint32() uint32 {
	if r.typ.Kind != KindUint || r.typ.Size != 4 {
		panic("bad uint32 type " + r.typ.Name)
	}
	return r.p.proc.ReadUint32(r.a)
}
func (r region) Int32() int32 {
	if r.typ.Kind != KindInt || r.typ.Size != 4 {
		panic("bad int32 type " + r.typ.Name)
	}
	return r.p.proc.ReadInt32(r.a)
}
func (r region) Uint16() uint16 {
	if r.typ.Kind != KindUint || r.typ.Size != 2 {
		panic("bad uint16 type " + r.typ.Name)
	}
	return r.p.proc.ReadUint16(r.a)
}
func (r region) Uint8() uint8 {
	if r.typ.Kind != KindUint || r.typ.Size != 1 {
		panic("bad uint8 type " + r.typ.Name)
	}
	return r.p.proc.ReadUint8(r.a)
}
func (r region) String() string {
	if r.typ.Kind != KindString {
		panic("bad string type " + r.typ.Name)
	}
	p := r.p.proc.ReadPtr(r.a)
	n := r.p.proc.ReadUintptr(r.a.Add(r.p.proc.PtrSize()))
	b := make([]byte, n)
	r.p.proc.ReadAt(b, p)
	return string(b)
}
func (r region) SlicePtr() region {
	if r.typ.Kind != KindSlice {
		panic("can't Ptr a non-slice")
	}
	return region{p: r.p, a: r.a, typ: &Type{Name: "*" + r.typ.Name[2:], Size: r.p.proc.PtrSize(), Kind: KindPtr, Elem: r.typ.Elem}}
}
func (r region) SliceLen() int64 {
	if r.typ.Kind != KindSlice {
		panic("can't len a non-slice")
	}
	return r.p.proc.ReadInt(r.a.Add(r.p.proc.PtrSize()))
}
func (r region) Field(f string) region {
	finfo := r.typ.field(f)
	if finfo == nil {
		panic("can't find field " + r.typ.Name + "." + f)
	}
	return region{p: r.p, a: r.a.Add(finfo.Off), typ: finfo.Type}
}
func (p *Process) ReadUint8(a Address) uint8 {
	m := p.findMapping(a)
	if m == nil {
		panic(fmt.Errorf("address %x is not mapped in the core file", a))
	}
	return m.contents[a.Sub(m.min)]
}
func (p *Process) ReadUint16(a Address) uint16 {
	m := p.findMapping(a)
	if m == nil {
		panic(fmt.Errorf("address %x is not mapped in the core file", a))
	}
	b := m.contents[a.Sub(m.min):]
	if len(b) < 2 {
		var buf [2]byte
		b = buf[:]
		p.ReadAt(b, a)
	}
	if p.littleEndian {
		return binary.LittleEndian.Uint16(b)
	}
	return binary.BigEndian.Uint16(b)
}
func (p *Process) ReadUint32(a Address) uint32 {
	m := p.findMapping(a)
	if m == nil {
		panic(fmt.Errorf("address %x is not mapped in the core file", a))
	}
	b := m.contents[a.Sub(m.min):]
	if len(b) < 4 {
		var buf [4]byte
		b = buf[:]
		p.ReadAt(b, a)
	}
	if p.littleEndian {
		return binary.LittleEndian.Uint32(b)
	}
	return binary.BigEndian.Uint32(b)
}
func (p *Process) ReadUint64(a Address) uint64 {
	m := p.findMapping(a)
	if m == nil {
		panic(fmt.Errorf("address %x is not mapped in the core file", a))
	}
	b := m.contents[a.Sub(m.min):]
	if len(b) < 8 {
		var buf [8]byte
		b = buf[:]
		p.ReadAt(b, a)
	}
	if p.littleEndian {
		return binary.LittleEndian.Uint64(b)
	}
	return binary.BigEndian.Uint64(b)
}
func (p *Process) ReadInt8(a Address) int8 {
	return int8(p.ReadUint8(a))
}
func (p *Process) ReadInt16(a Address) int16 {
	return int16(p.ReadUint16(a))
}
func (p *Process) ReadInt32(a Address) int32 {
	return int32(p.ReadUint32(a))
}
func (p *Process) ReadInt64(a Address) int64 {
	return int64(p.ReadUint64(a))
}
func (p *Process) ReadUintptr(a Address) uint64 {
	if p.ptrSize == 4 {
		return uint64(p.ReadUint32(a))
	}
	return p.ReadUint64(a)
}
func (p *Process) ReadPtr(a Address) Address {
	return Address(p.ReadUintptr(a))
}
func (p *Process) ReadCString(a Address) string {
	for n := int64(0); ; n++ {
		if p.ReadUint8(a.Add(n)) == 0 {
			b := make([]byte, n)
			p.ReadAt(b, a)
			return string(b)
		}
	}
}
func (m *Mapping) Source() (string, int64) {
	if m.f == nil {
		return "", 0
	}
	return m.f.Name(), m.off
}
func (p *Process) findMapping(a Address) *Mapping {
	t3 := p.pageTable[a>>52]
	if t3 == nil {
		return nil
	}
	t2 := t3[a>>42%(1<<10)]
	if t2 == nil {
		return nil
	}
	t1 := t2[a>>32%(1<<10)]
	if t1 == nil {
		return nil
	}
	t0 := t1[a>>22%(1<<10)]
	if t0 == nil {
		return nil
	}
	return t0[a>>12%(1<<10)]
}
func (a Address) Max(b Address) Address {
	if a > b {
		return a
	}
	return b
}
func (a Address) Min(b Address) Address {
	if a < b {
		return a
	}
	return b
}
func (a Address) Align(x int64) Address {
	return (a + Address(x) - 1) & ^(Address(x) - 1)
}
func (d *ltDom) initialize() {
	type workItem struct {
		name       vName
		parentName vName
	}

	// Initialize objs for mapping from object index back to Object.
	i := 0
	d.p.ForEachObject(func(x Object) bool {
		d.objs[i] = x
		i++
		return true
	})

	// Add roots to the work stack, essentially pretending to visit
	// the pseudo-root, numbering it 0.
	d.semis[pseudoRoot] = 0
	d.parents[pseudoRoot] = -1
	d.vertices[0] = pseudoRoot
	var work []workItem
	for i := 1; i < 1+d.nRoots; i++ {
		work = append(work, workItem{name: vName(i), parentName: 0})
	}

	n := vNumber(1) // 0 was the pseudo-root.

	// Build the spanning tree, assigning vertex numbers to each object
	// and initializing semi and parent.
	for len(work) != 0 {
		item := work[len(work)-1]
		work = work[:len(work)-1]

		if d.semis[item.name] != -1 {
			continue
		}

		d.semis[item.name] = n
		d.parents[item.name] = item.parentName
		d.vertices[n] = item.name
		n++

		visitChild := func(_ int64, child Object, _ int64) bool {
			childIdx, _ := d.p.findObjectIndex(d.p.Addr(child))
			work = append(work, workItem{name: vName(childIdx + d.nRoots + 1), parentName: item.name})
			return true
		}

		root, object := d.findVertexByName(item.name)
		if root != nil {
			d.p.ForEachRootPtr(root, visitChild)
		} else {
			d.p.ForEachPtr(object, visitChild)
		}

	}
}
func (d *ltDom) calculate() {
	// name -> bucket (a name), per Georgiadis.
	buckets := make([]vName, d.nVertices)
	for i := range buckets {
		buckets[i] = vName(i)
	}

	for i := vNumber(len(d.vertices)) - 1; i > 0; i-- {
		w := d.vertices[i]

		// Step 3. Implicitly define the immediate dominator of each node.
		for v := buckets[w]; v != w; v = buckets[v] {
			u := d.eval(v)
			if d.semis[u] < d.semis[v] {
				d.idom[v] = u
			} else {
				d.idom[v] = w
			}
		}

		// Step 2. Compute the semidominators of all nodes.
		root, obj := d.findVertexByName(w)
		// This loop never visits the pseudo-root.
		if root != nil {
			u := d.eval(pseudoRoot)
			if d.semis[u] < d.semis[w] {
				d.semis[w] = d.semis[u]
			}
		} else {
			d.p.ForEachReversePtr(obj, func(x Object, r *Root, _, _ int64) bool {
				var v int
				if r != nil {
					v = d.p.findRootIndex(r) + 1
				} else {
					v, _ = d.p.findObjectIndex(d.p.Addr(x))
					v += d.nRoots + 1
				}
				u := d.eval(vName(v))
				if d.semis[u] < d.semis[w] {
					d.semis[w] = d.semis[u]
				}
				return true
			})
		}

		d.link(d.parents[w], w)

		if d.parents[w] == d.vertices[d.semis[w]] {
			d.idom[w] = d.parents[w]
		} else {
			buckets[w] = buckets[d.vertices[d.semis[w]]]
			buckets[d.vertices[d.semis[w]]] = w
		}
	}

	// The final 'Step 3' is now outside the loop.
	for v := buckets[pseudoRoot]; v != pseudoRoot; v = buckets[v] {
		d.idom[v] = pseudoRoot
	}

	// Step 4. Explicitly define the immediate dominator of each
	// node, in preorder.
	for _, w := range d.vertices[1:] {
		if d.idom[w] != d.vertices[d.semis[w]] {
			d.idom[w] = d.idom[d.idom[w]]
		}
	}
}
func (d *ltDom) eval(v vName) vName {
	if d.ancestor[v] == -1 {
		return v
	}
	d.compress(v)
	return d.labels[v]
}
func (d *ltDom) compress(v vName) {
	var stackBuf [20]vName
	stack := stackBuf[:0]
	for d.ancestor[d.ancestor[v]] != -1 {
		stack = append(stack, v)
		v = d.ancestor[v]
	}

	for len(stack) != 0 {
		v := stack[len(stack)-1]
		stack = stack[:len(stack)-1]

		if d.semis[d.labels[d.ancestor[v]]] < d.semis[d.labels[v]] {
			d.labels[v] = d.labels[d.ancestor[v]]
		}
		d.ancestor[v] = d.ancestor[d.ancestor[v]]
	}
}
func (d *ltDom) link(v, w vName) {
	d.ancestor[w] = v
}
func (d *dominators) reverse() {
	// One inbound edge per vertex. Then we need an extra so that you can
	// always look at ridx[i+1], and another for working storage while
	// populating redge.
	cnt := make([]int, len(d.idom)+2)

	// Fill cnt[2:] with the number of outbound edges for each vertex.
	tmp := cnt[2:]
	for _, idom := range d.idom {
		tmp[idom]++
	}

	// Make tmp cumulative. After this step, cnt[1:] is what we want for
	// ridx, but the next step messes it up.
	var n int
	for idx, c := range tmp {
		n += c
		tmp[idx] = n
	}

	// Store outbound edges in redge, using cnt[1:] as the index to store
	// the next edge for each vertex. After we're done, everything's been
	// shifted over one, and cnt is ridx.
	redge := make([]vName, len(d.idom))
	tmp = cnt[1:]
	for i, idom := range d.idom {
		redge[tmp[idom]] = vName(i)
		tmp[idom]++
	}
	d.redge, d.ridx = redge, cnt[:len(cnt)-1]
}
func (d *dominators) calcSize(p *Process) {
	d.size = make([]int64, len(d.idom))
	type workItem struct {
		v    vName
		mode dfsMode
	}
	work := []workItem{{pseudoRoot, down}}

	for len(work) > 0 {
		item := &work[len(work)-1]

		kids := d.redge[d.ridx[item.v]:d.ridx[item.v+1]]
		if item.mode == down && len(kids) != 0 {
			item.mode = up
			for _, w := range kids {
				if w == 0 {
					// bogus self-edge. Ignore.
					continue
				}
				work = append(work, workItem{w, down})
			}
			continue
		}

		work = work[:len(work)-1]

		root, obj := d.findVertexByName(item.v)
		var size int64
		switch {
		case item.v == pseudoRoot:
			break
		case root != nil:
			size += root.Type.Size
		default:
			size += p.Size(obj)
		}
		for _, w := range kids {
			size += d.size[w]
		}
		d.size[item.v] = size
	}
}
func objField(c *gocore.Process, x gocore.Object, off int64) string {
	t, r := c.Type(x)
	if t == nil {
		return fmt.Sprintf("f%d", off)
	}
	s := ""
	if r > 1 {
		s = fmt.Sprintf("[%d]", off/t.Size)
		off %= t.Size
	}
	return s + typeFieldName(t, off)
}
func (p *Process) Readable(a Address) bool {
	return p.findMapping(a) != nil
}
func (p *Process) ReadableN(a Address, n int64) bool {
	for {
		m := p.findMapping(a)
		if m == nil || m.perm&Read == 0 {
			return false
		}
		c := m.max.Sub(a)
		if n <= c {
			return true
		}
		n -= c
		a = a.Add(c)
	}
}
func (p *Process) splitMappingsAt(a Address) {
	for _, m := range p.memory.mappings {
		if a < m.min || a > m.max {
			continue
		}
		if a == m.min || a == m.max {
			return
		}
		// Split this mapping at a.
		m2 := new(Mapping)
		*m2 = *m
		m.max = a
		m2.min = a
		if m2.f != nil {
			m2.off += m.Size()
		}
		if m2.origF != nil {
			m2.origOff += m.Size()
		}
		p.memory.mappings = append(p.memory.mappings, m2)
		return
	}
}
func (p *Process) DynamicType(t *Type, a core.Address) *Type {
	switch t.Kind {
	default:
		panic("asking for the dynamic type of a non-interface")
	case KindEface:
		x := p.proc.ReadPtr(a)
		if x == 0 {
			return nil
		}
		return p.runtimeType2Type(x)
	case KindIface:
		x := p.proc.ReadPtr(a)
		if x == 0 {
			return nil
		}
		// Read type out of itab.
		x = p.proc.ReadPtr(x.Add(p.proc.PtrSize()))
		return p.runtimeType2Type(x)
	}
}
func (fs *BtrfsFilesystem) Create(bytes uint64) error {

	// significantly
	idempotent := exec.Command("bash", "-e", "-x", "-c", `
		if [ ! -e $IMAGE_PATH ] || [ "$(stat --printf="%s" $IMAGE_PATH)" != "$SIZE_IN_BYTES" ]; then
			touch $IMAGE_PATH
			truncate -s ${SIZE_IN_BYTES} $IMAGE_PATH
		fi

		lo="$(losetup -j $IMAGE_PATH | cut -d':' -f1)"
		if [ -z "$lo" ]; then
			lo="$(losetup -f --show $IMAGE_PATH)"
		fi

		if ! file $IMAGE_PATH | grep BTRFS; then
			`+fs.mkfsBin+` --nodiscard $IMAGE_PATH
		fi

		mkdir -p $MOUNT_PATH

		if ! mountpoint -q $MOUNT_PATH; then
			mount -t btrfs $lo $MOUNT_PATH
		fi
	`)

	idempotent.Env = []string{
		"PATH=" + os.Getenv("PATH"),
		"MOUNT_PATH=" + fs.mountPath,
		"IMAGE_PATH=" + fs.imagePath,
		fmt.Sprintf("SIZE_IN_BYTES=%d", bytes),
	}

	_, err := fs.run(idempotent)
	return err
}
func Resolve(u *url.URL) (*net.IPAddr, error) {
	host, _, err := SplitHostPort(u)
	if err != nil {
		return nil, err
	}

	addr, err := net.ResolveIPAddr("ip", host)
	if err != nil {
		return nil, err
	}

	return addr, nil
}
func (a *Token) Equal(b *Token) bool {
	if a == nil || b == nil {
		return false
	}
	if a.tokenType != b.tokenType {
		return false
	}
	return a.value == b.value
}
func newDefaultClassifier() tokenClassifier {
	t := tokenClassifier{}
	t.addRuneClass(spaceRunes, spaceRuneClass)
	t.addRuneClass(escapingQuoteRunes, escapingQuoteRuneClass)
	t.addRuneClass(nonEscapingQuoteRunes, nonEscapingQuoteRuneClass)
	t.addRuneClass(escapeRunes, escapeRuneClass)
	t.addRuneClass(commentRunes, commentRuneClass)
	return t
}
func (l *Lexer) Next() (string, error) {
	for {
		token, err := (*Tokenizer)(l).Next()
		if err != nil {
			return "", err
		}
		switch token.tokenType {
		case WordToken:
			return token.value, nil
		case CommentToken:
			// skip comments
		default:
			return "", fmt.Errorf("Unknown token type: %v", token.tokenType)
		}
	}
}
func NewTokenizer(r io.Reader) *Tokenizer {
	input := bufio.NewReader(r)
	classifier := newDefaultClassifier()
	return &Tokenizer{
		input:      *input,
		classifier: classifier}
}
func Split(s string) ([]string, error) {
	l := NewLexer(strings.NewReader(s))
	subStrings := make([]string, 0)
	for {
		word, err := l.Next()
		if err != nil {
			if err == io.EOF {
				return subStrings, nil
			}
			return subStrings, err
		}
		subStrings = append(subStrings, word)
	}
}
func Between(s, left, right string) string {
	l := len(left)
	startPos := strings.Index(s, left)
	if startPos < 0 {
		return ""
	}
	endPos := IndexOf(s, right, startPos+l)
	//log.Printf("%s: left %s right %s start %d end %d", s, left, right, startPos+l, endPos)
	if endPos < 0 {
		return ""
	} else if right == "" {
		return s[endPos:]
	} else {
		return s[startPos+l : endPos]
	}
}
func BetweenF(left, right string) func(string) string {
	return func(s string) string {
		return Between(s, left, right)
	}
}
func Camelize(s string) string {
	return camelizeRe.ReplaceAllStringFunc(s, func(val string) string {
		val = strings.ToUpper(val)
		val = camelizeRe2.ReplaceAllString(val, "")
		return val
	})
}
func Capitalize(s string) string {
	return strings.ToUpper(s[0:1]) + strings.ToLower(s[1:])
}
func CharAt(s string, index int) string {
	l := len(s)
	shortcut := index < 0 || index > l-1 || l == 0
	if shortcut {
		return ""
	}
	return s[index : index+1]
}
func CharAtF(index int) func(string) string {
	return func(s string) string {
		return CharAt(s, index)
	}
}
func ChompLeft(s, prefix string) string {
	if strings.HasPrefix(s, prefix) {
		return s[len(prefix):]
	}
	return s
}
func ChompLeftF(prefix string) func(string) string {
	return func(s string) string {
		return ChompLeft(s, prefix)
	}
}
func ChompRight(s, suffix string) string {
	if strings.HasSuffix(s, suffix) {
		return s[:len(s)-len(suffix)]
	}
	return s
}
func ChompRightF(suffix string) func(string) string {
	return func(s string) string {
		return ChompRight(s, suffix)
	}
}
func ClassifyF(s string) func(string) string {
	return func(s string) string {
		return Classify(s)
	}
}
func Clean(s string) string {
	s = spacesRe.ReplaceAllString(s, " ")
	s = beginEndSpacesRe.ReplaceAllString(s, "")
	return s
}
func Dasherize(s string) string {
	s = strings.TrimSpace(s)
	s = spaceUnderscoreRe.ReplaceAllString(s, "-")
	s = capitalsRe.ReplaceAllString(s, "-$1")
	s = dashesRe.ReplaceAllString(s, "-")
	s = strings.ToLower(s)
	return s
}
func EscapeHTML(s string) string {
	if Verbose {
		fmt.Println("Use html.EscapeString instead of EscapeHTML")
	}
	return html.EscapeString(s)
}
func DecodeHTMLEntities(s string) string {
	if Verbose {
		fmt.Println("Use html.UnescapeString instead of DecodeHTMLEntities")
	}
	return html.UnescapeString(s)
}
func EnsurePrefixF(prefix string) func(string) string {
	return func(s string) string {
		return EnsurePrefix(s, prefix)
	}
}
func EnsureSuffixF(suffix string) func(string) string {
	return func(s string) string {
		return EnsureSuffix(s, suffix)
	}
}
func Humanize(s string) string {
	if s == "" {
		return s
	}
	s = Underscore(s)
	var humanizeRe = regexp.MustCompile(`_id$`)
	s = humanizeRe.ReplaceAllString(s, "")
	s = strings.Replace(s, "_", " ", -1)
	s = strings.TrimSpace(s)
	s = Capitalize(s)
	return s
}
func Iif(condition bool, truthy string, falsey string) string {
	if condition {
		return truthy
	}
	return falsey
}
func IndexOf(s string, needle string, start int) int {
	l := len(s)
	if needle == "" {
		if start < 0 {
			return 0
		} else if start < l {
			return start
		} else {
			return l
		}
	}
	if start < 0 || start > l-1 {
		return -1
	}
	pos := strings.Index(s[start:], needle)
	if pos == -1 {
		return -1
	}
	return start + pos
}
func IsLower(s string) bool {
	return IsAlpha(s) && s == strings.ToLower(s)
}
func IsUpper(s string) bool {
	return IsAlpha(s) && s == strings.ToUpper(s)
}
func Left(s string, n int) string {
	if n < 0 {
		return Right(s, -n)
	}
	return Substr(s, 0, n)
}
func LeftF(n int) func(string) string {
	return func(s string) string {
		return Left(s, n)
	}
}
func Letters(s string) []string {
	result := []string{}
	for _, r := range s {
		result = append(result, string(r))
	}
	return result
}
func Lines(s string) []string {
	s = strings.Replace(s, "\r\n", "\n", -1)
	return strings.Split(s, "\n")
}
func Map(arr []string, iterator func(string) string) []string {
	r := []string{}
	for _, item := range arr {
		r = append(r, iterator(item))
	}
	return r
}
func Match(s, pattern string) bool {
	r := regexp.MustCompile(pattern)
	return r.MatchString(s)
}
func tasks(p *do.Project) {
	p.Task("default", do.S{"readme"}, nil)

	p.Task("install", nil, func(c *do.Context) {
		c.Run("go get github.com/robertkrimen/godocdown/godocdown")
	})

	p.Task("lint", nil, func(c *do.Context) {
		c.Run("golint .")
		c.Run("gofmt -w -s .")
		c.Run("go vet .")
		c.Run("go test")
	})

	p.Task("readme", nil, func(c *do.Context) {
		c.Run("godocdown -output README.md")

		packageName, _ := util.PackageName("doc.go")

		// add godoc link
		goa.Pipe(
			f.Load("./README.md"),
			f.Str(str.ReplaceF("--", "\n[godoc](https://godoc.org/"+packageName+")\n", 1)),
			f.Write(),
		)
	}).Src("**/*.go")

	p.Task("test", nil, func(c *do.Context) {
		c.Run("go test")
	})
}
func Pad(s, c string, n int) string {
	L := len(s)
	if L >= n {
		return s
	}
	n -= L

	left := strings.Repeat(c, int(math.Ceil(float64(n)/2)))
	right := strings.Repeat(c, int(math.Floor(float64(n)/2)))
	return left + s + right
}
func PadF(c string, n int) func(string) string {
	return func(s string) string {
		return Pad(s, c, n)
	}
}
func PadLeft(s, c string, n int) string {
	L := len(s)
	if L > n {
		return s
	}
	return strings.Repeat(c, (n-L)) + s
}
func PadLeftF(c string, n int) func(string) string {
	return func(s string) string {
		return PadLeft(s, c, n)
	}
}
func PadRightF(c string, n int) func(string) string {
	return func(s string) string {
		return PadRight(s, c, n)
	}
}
func Pipe(s string, funcs ...func(string) string) string {
	for _, fn := range funcs {
		s = fn(s)
	}
	return s
}
func QuoteItems(arr []string) []string {
	return Map(arr, func(s string) string {
		return strconv.Quote(s)
	})
}
func ReplaceF(old, new string, n int) func(string) string {
	return func(s string) string {
		return strings.Replace(s, old, new, n)
	}
}
func ReplacePatternF(pattern, repl string) func(string) string {
	return func(s string) string {
		return ReplacePattern(s, pattern, repl)
	}
}
func Reverse(s string) string {
	cs := make([]rune, utf8.RuneCountInString(s))
	i := len(cs)
	for _, c := range s {
		i--
		cs[i] = c
	}
	return string(cs)
}
func RightF(n int) func(string) string {
	return func(s string) string {
		return Right(s, n)
	}
}
func Slice(s string, start, end int) string {
	if end > -1 {
		return s[start:end]
	}
	L := len(s)
	if L+end > 0 {
		return s[start : L-end]
	}
	return s[start:]
}
func SliceF(start, end int) func(string) string {
	return func(s string) string {
		return Slice(s, start, end)
	}
}
func SliceContains(slice []string, val string) bool {
	if slice == nil {
		return false
	}

	for _, it := range slice {
		if it == val {
			return true
		}
	}
	return false
}
func SliceIndexOf(slice []string, val string) int {
	if slice == nil {
		return -1
	}

	for i, it := range slice {
		if it == val {
			return i
		}
	}
	return -1
}
func Slugify(s string) string {
	sl := slugifyRe.ReplaceAllString(s, "")
	sl = strings.ToLower(sl)
	sl = Dasherize(sl)
	return sl
}
func StripPunctuation(s string) string {
	s = stripPuncRe.ReplaceAllString(s, "")
	s = nWhitespaceRe.ReplaceAllString(s, " ")
	return s
}
func StripTags(s string, tags ...string) string {
	if len(tags) == 0 {
		tags = append(tags, "")
	}
	for _, tag := range tags {
		stripTagsRe := regexp.MustCompile(`(?i)<\/?` + tag + `[^<>]*>`)
		s = stripTagsRe.ReplaceAllString(s, "")
	}
	return s
}
func Substr(s string, index int, n int) string {
	L := len(s)
	if index < 0 || index >= L || s == "" {
		return ""
	}
	end := index + n
	if end >= L {
		end = L
	}
	if end <= index {
		return ""
	}
	return s[index:end]
}
func SubstrF(index, n int) func(string) string {
	return func(s string) string {
		return Substr(s, index, n)
	}
}
func TemplateWithDelimiters(s string, values map[string]interface{}, opening, closing string) string {
	escapeDelimiter := func(delim string) string {
		result := templateRe.ReplaceAllString(delim, "\\$1")
		return templateRe2.ReplaceAllString(result, "\\$")
	}

	openingDelim := escapeDelimiter(opening)
	closingDelim := escapeDelimiter(closing)
	r := regexp.MustCompile(openingDelim + `(.+?)` + closingDelim)
	matches := r.FindAllStringSubmatch(s, -1)
	for _, submatches := range matches {
		match := submatches[0]
		key := submatches[1]
		//log.Printf("match %s key %s\n", match, key)
		if values[key] != nil {
			v := fmt.Sprintf("%v", values[key])
			s = strings.Replace(s, match, v, -1)
		}
	}

	return s
}
func ToArgv(s string) []string {
	const (
		InArg = iota
		InArgQuote
		OutOfArg
	)
	currentState := OutOfArg
	currentQuoteChar := "\x00" // to distinguish between ' and " quotations
	// this allows to use "foo'bar"
	currentArg := ""
	argv := []string{}

	isQuote := func(c string) bool {
		return c == `"` || c == `'`
	}

	isEscape := func(c string) bool {
		return c == `\`
	}

	isWhitespace := func(c string) bool {
		return c == " " || c == "\t"
	}

	L := len(s)
	for i := 0; i < L; i++ {
		c := s[i : i+1]

		//fmt.Printf("c %s state %v arg %s argv %v i %d\n", c, currentState, currentArg, args, i)
		if isQuote(c) {
			switch currentState {
			case OutOfArg:
				currentArg = ""
				fallthrough
			case InArg:
				currentState = InArgQuote
				currentQuoteChar = c

			case InArgQuote:
				if c == currentQuoteChar {
					currentState = InArg
				} else {
					currentArg += c
				}
			}

		} else if isWhitespace(c) {
			switch currentState {
			case InArg:
				argv = append(argv, currentArg)
				currentState = OutOfArg
			case InArgQuote:
				currentArg += c
			case OutOfArg:
				// nothing
			}

		} else if isEscape(c) {
			switch currentState {
			case OutOfArg:
				currentArg = ""
				currentState = InArg
				fallthrough
			case InArg:
				fallthrough
			case InArgQuote:
				if i == L-1 {
					if runtime.GOOS == "windows" {
						// just add \ to end for windows
						currentArg += c
					} else {
						panic("Escape character at end string")
					}
				} else {
					if runtime.GOOS == "windows" {
						peek := s[i+1 : i+2]
						if peek != `"` {
							currentArg += c
						}
					} else {
						i++
						c = s[i : i+1]
						currentArg += c
					}
				}
			}
		} else {
			switch currentState {
			case InArg, InArgQuote:
				currentArg += c

			case OutOfArg:
				currentArg = ""
				currentArg += c
				currentState = InArg
			}
		}
	}

	if currentState == InArg {
		argv = append(argv, currentArg)
	} else if currentState == InArgQuote {
		panic("Starting quote has no ending quote.")
	}

	return argv
}
func ToBool(s string) bool {
	s = strings.ToLower(s)
	return s == "true" || s == "yes" || s == "on" || s == "1"
}
func ToBoolOr(s string, defaultValue bool) bool {
	b, err := strconv.ParseBool(s)
	if err != nil {
		return defaultValue
	}
	return b
}
func ToIntOr(s string, defaultValue int) int {
	n, err := strconv.Atoi(s)
	if err != nil {
		return defaultValue
	}
	return n
}
func ToFloat32Or(s string, defaultValue float32) float32 {
	f, err := strconv.ParseFloat(s, 32)
	if err != nil {
		return defaultValue
	}
	return float32(f)
}
func ToFloat64Or(s string, defaultValue float64) float64 {
	f, err := strconv.ParseFloat(s, 64)
	if err != nil {
		return defaultValue
	}
	return f
}
func UnescapeHTML(s string) string {
	if Verbose {
		fmt.Println("Use html.UnescapeString instead of UnescapeHTML")
	}
	return html.UnescapeString(s)
}
func WrapHTML(s string, tag string, attrs map[string]string) string {
	escapeHTMLAttributeQuotes := func(v string) string {
		v = strings.Replace(v, "<", "&lt;", -1)
		v = strings.Replace(v, "&", "&amp;", -1)
		v = strings.Replace(v, "\"", "&quot;", -1)
		return v
	}
	if tag == "" {
		tag = "div"
	}
	el := "<" + tag
	for name, val := range attrs {
		el += " " + name + "=\"" + escapeHTMLAttributeQuotes(val) + "\""
	}
	el += ">" + s + "</" + tag + ">"
	return el
}
func WrapHTMLF(tag string, attrs map[string]string) func(string) string {
	return func(s string) string {
		return WrapHTML(s, tag, attrs)
	}
}
func ReapChildren(pids PidCh, errors ErrorCh, done chan struct{}, reapLock *sync.RWMutex) {
	c := make(chan os.Signal, 1)
	signal.Notify(c, unix.SIGCHLD)

	for {
		// Block for an incoming signal that a child has exited.
		select {
		case <-c:
			// Got a child signal, drop out and reap.
		case <-done:
			return
		}

		// Attempt to reap all abandoned child processes after getting
		// the reap lock, which makes sure the application isn't doing
		// any waiting of its own. Note that we do the full write lock
		// here.
		func() {
			if reapLock != nil {
				reapLock.Lock()
				defer reapLock.Unlock()
			}

		POLL:
			// Try to reap children until there aren't any more. We
			// never block in here so that we are always responsive
			// to signals, at the expense of possibly leaving a
			// child behind if we get here too quickly. Any
			// stragglers should get reaped the next time we see a
			// signal, so we won't leak in the long run.
			var status unix.WaitStatus
			pid, err := unix.Wait4(-1, &status, unix.WNOHANG, nil)
			switch err {
			case nil:
				// Got a child, clean this up and poll again.
				if pid > 0 {
					if pids != nil {
						pids <- pid
					}
					goto POLL
				}
				return

			case unix.ECHILD:
				// No more children, we are done.
				return

			case unix.EINTR:
				// We got interrupted, try again. This likely
				// can't happen since we are calling Wait4 in a
				// non-blocking fashion, but it's good to be
				// complete and handle this case rather than
				// fail.
				goto POLL

			default:
				// We got some other error we didn't expect.
				// Wait for another SIGCHLD so we don't
				// potentially spam in here and chew up CPU.
				if errors != nil {
					errors <- err
				}
				return
			}
		}()
	}
}
func SdNotify(state string) error {
	name := os.Getenv("NOTIFY_SOCKET")
	if name == "" {
		return ErrSdNotifyNoSocket
	}

	conn, err := net.DialUnix("unixgram", nil, &net.UnixAddr{Name: name, Net: "unixgram"})
	if err != nil {
		return err
	}
	defer conn.Close()

	_, err = conn.Write([]byte(state))
	return err
}
func NewAuthenticator(store *coal.Store, policy *Policy) *Authenticator {
	// initialize token
	coal.Init(policy.Token)

	// initialize clients
	for _, model := range policy.Clients {
		coal.Init(model)
	}

	return &Authenticator{
		store:  store,
		policy: policy,
	}
}
func (a *Authenticator) Endpoint(prefix string) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// create tracer
		tracer := fire.NewTracerFromRequest(r, "flame/Authenticator.Endpoint")
		tracer.Tag("prefix", prefix)
		defer tracer.Finish(true)

		// continue any previous aborts
		defer stack.Resume(func(err error) {
			// directly write oauth2 errors
			if oauth2Error, ok := err.(*oauth2.Error); ok {
				_ = oauth2.WriteError(w, oauth2Error)
				return
			}

			// set critical error on last span
			tracer.Tag("error", true)
			tracer.Log("error", err.Error())
			tracer.Log("stack", stack.Trace())

			// otherwise report critical errors
			if a.Reporter != nil {
				a.Reporter(err)
			}

			// ignore errors caused by writing critical errors
			_ = oauth2.WriteError(w, oauth2.ServerError(""))
		})

		// trim and split path
		s := strings.Split(strings.Trim(strings.TrimPrefix(r.URL.Path, prefix), "/"), "/")
		if len(s) != 1 || (s[0] != "authorize" && s[0] != "token" && s[0] != "revoke") {
			w.WriteHeader(http.StatusNotFound)
			return
		}

		// copy store
		store := a.store.Copy()
		defer store.Close()

		// create
		state := &state{
			request: r,
			writer:  w,
			store:   store,
			tracer:  tracer,
		}

		// call endpoints
		switch s[0] {
		case "authorize":
			a.authorizationEndpoint(state)
		case "token":
			a.tokenEndpoint(state)
		case "revoke":
			a.revocationEndpoint(state)
		}
	})
}
func Unique(ids []bson.ObjectId) []bson.ObjectId {
	// prepare map
	m := make(map[bson.ObjectId]bool)
	l := make([]bson.ObjectId, 0, len(ids))

	for _, id := range ids {
		if _, ok := m[id]; !ok {
			m[id] = true
			l = append(l, id)
		}
	}

	return l
}
func Contains(list []bson.ObjectId, id bson.ObjectId) bool {
	for _, item := range list {
		if item == id {
			return true
		}
	}

	return false
}
func Includes(all, subset []bson.ObjectId) bool {
	for _, item := range subset {
		if !Contains(all, item) {
			return false
		}
	}

	return true
}
func Require(m Model, flags ...string) {
	// check all flags
	for _, f := range flags {
		L(m, f, true)
	}
}
func Enqueue(store *coal.SubStore, name string, data Model, delay time.Duration) (*Job, error) {
	// set default data
	if data == nil {
		data = bson.M{}
	}

	// get time
	now := time.Now()

	// prepare job
	job := coal.Init(&Job{
		Name:      name,
		Status:    StatusEnqueued,
		Created:   now,
		Available: now.Add(delay),
	}).(*Job)

	// marshall data
	raw, err := bson.Marshal(data)
	if err != nil {
		return nil, err
	}

	// marshall into job
	err = bson.Unmarshal(raw, &job.Data)
	if err != nil {
		return nil, err
	}

	// insert job
	err = store.C(job).Insert(job)
	if err != nil {
		return nil, err
	}

	return job, nil
}
func A(name string, m fire.Matcher, h Handler) *Authorizer {
	// panic if matcher or handler is not set
	if m == nil || h == nil {
		panic("ash: missing matcher or handler")
	}

	// construct and return authorizer
	return &Authorizer{
		Matcher: m,
		Handler: func(ctx *fire.Context) ([]*Enforcer, error) {
			// begin trace
			ctx.Tracer.Push(name)

			// call handler
			enforcers, err := h(ctx)
			if err != nil {
				return nil, err
			}

			// finish trace
			ctx.Tracer.Pop()

			return enforcers, nil
		},
	}
}
func Run(enforcers ...*Enforcer) *Authorizer {
	return A("ash/Run", fire.All(), func(ctx *fire.Context) ([]*Enforcer, error) {
		return enforcers, nil
	})
}
func And(a, b *Authorizer) *Authorizer {
	return A("ash/And", func(ctx *fire.Context) bool {
		return a.Matcher(ctx) && b.Matcher(ctx)
	}, func(ctx *fire.Context) ([]*Enforcer, error) {
		// run first callback
		enforcers1, err := a.Handler(ctx)
		if err != nil {
			return nil, err
		} else if enforcers1 == nil {
			return nil, nil
		}

		// run second callback
		enforcers2, err := b.Handler(ctx)
		if err != nil {
			return nil, err
		} else if enforcers2 == nil {
			return nil, nil
		}

		// merge both sets
		enforcers := append(S{}, enforcers1...)
		enforcers = append(enforcers, enforcers2...)

		return enforcers, nil
	})
}
func Or(a, b *Authorizer) *Authorizer {
	return A("ash/Or", func(ctx *fire.Context) bool {
		return a.Matcher(ctx) || b.Matcher(ctx)
	}, func(ctx *fire.Context) ([]*Enforcer, error) {
		// check first authorizer
		if a.Matcher(ctx) {
			// run callback
			enforcers, err := a.Handler(ctx)
			if err != nil {
				return nil, err
			}

			// return on success
			if enforcers != nil {
				return enforcers, nil
			}
		}

		// check second authorizer
		if b.Matcher(ctx) {
			// run callback
			enforcers, err := b.Handler(ctx)
			if err != nil {
				return nil, err
			}

			// return on success
			if enforcers != nil {
				return enforcers, nil
			}
		}

		return nil, nil
	})
}
func (q *Queue) Enqueue(name string, data Model, delay time.Duration) (*Job, error) {
	// copy store
	store := q.store.Copy()
	defer store.Close()

	// enqueue job
	job, err := Enqueue(store, name, data, delay)
	if err != nil {
		return nil, err
	}

	return job, nil
}
func (q *Queue) Callback(name string, delay time.Duration, matcher fire.Matcher, cb func(ctx *fire.Context) Model) *fire.Callback {
	return fire.C("axe/Queue.Callback", matcher, func(ctx *fire.Context) error {
		// set task tag
		ctx.Tracer.Tag("task", name)

		// get data
		var data Model
		if cb != nil {
			data = cb(ctx)
		}

		// check if controller uses same store
		if q.store == ctx.Controller.Store {
			// enqueue job using context store
			_, err := Enqueue(ctx.Store, name, data, delay)
			if err != nil {
				return err
			}
		} else {
			// enqueue job using queue store
			_, err := q.Enqueue(name, data, delay)
			if err != nil {
				return err
			}
		}

		// respond with an empty object
		if ctx.Operation.Action() {
			err := ctx.Respond(fire.Map{})
			if err != nil {
				return err
			}
		}

		return nil
	})
}
func NewWatcher() *Watcher {
	// prepare watcher
	w := &Watcher{
		streams: make(map[string]*Stream),
	}

	// create and add manager
	w.manager = newManager(w)

	return w
}
func (w *Watcher) Add(stream *Stream) {
	// initialize model
	coal.Init(stream.Model)

	// check existence
	if w.streams[stream.Name()] != nil {
		panic(fmt.Sprintf(`spark: stream with name "%s" already exists`, stream.Name()))
	}

	// save stream
	w.streams[stream.Name()] = stream

	// open stream
	coal.OpenStream(stream.Store, stream.Model, nil, func(e coal.Event, id bson.ObjectId, m coal.Model, token []byte) {
		// ignore real deleted events when soft delete has been enabled
		if stream.SoftDelete && e == coal.Deleted {
			return
		}

		// handle soft deleted documents
		if stream.SoftDelete && e == coal.Updated {
			// get soft delete field
			softDeleteField := coal.L(stream.Model, "fire-soft-delete", true)

			// get deleted time
			t := m.MustGet(softDeleteField).(*time.Time)

			// change type if document has been soft deleted
			if t != nil && !t.IsZero() {
				e = coal.Deleted
			}
		}

		// create event
		evt := &Event{
			Type:   e,
			ID:     id,
			Model:  m,
			Stream: stream,
		}

		// broadcast event
		w.manager.broadcast(evt)
	}, nil, func(err error) bool {
		// report error
		w.Reporter(err)

		return true
	})
}
func (w *Watcher) Action() *fire.Action {
	return &fire.Action{
		Methods: []string{"GET"},
		Callback: fire.C("spark/Watcher.Action", fire.All(), func(ctx *fire.Context) error {
			// handle connection
			w.manager.handle(ctx)

			return nil
		}),
	}
}
func (o Operation) Write() bool {
	return o == Create || o == Update || o == Delete
}
func (o Operation) String() string {
	switch o {
	case List:
		return "List"
	case Find:
		return "Find"
	case Create:
		return "Create"
	case Update:
		return "Update"
	case Delete:
		return "Delete"
	case CollectionAction:
		return "CollectionAction"
	case ResourceAction:
		return "ResourceAction"
	}

	return ""
}
func (c *Context) Query() bson.M {
	return bson.M{"$and": append([]bson.M{c.Selector}, c.Filters...)}
}
func (c *Context) Respond(value interface{}) error {
	// encode response
	bytes, err := json.Marshal(value)
	if err != nil {
		return err
	}

	// write token
	_, err = c.ResponseWriter.Write(bytes)
	if err != nil {
		return err
	}

	return nil
}
func EnsureApplication(store *coal.Store, name, key, secret string) (string, error) {
	// copy store
	s := store.Copy()
	defer s.Close()

	// count main applications
	var apps []Application
	err := s.C(&Application{}).Find(bson.M{
		coal.F(&Application{}, "Name"): name,
	}).All(&apps)
	if err != nil {
		return "", err
	}

	// check existence
	if len(apps) > 1 {
		return "", errors.New("to many applications with that name")
	} else if len(apps) == 1 {
		return apps[0].Key, nil
	}

	// application is missing

	// create application
	app := coal.Init(&Application{}).(*Application)
	app.Key = key
	app.Name = name
	app.Secret = secret

	// validate model
	err = app.Validate()
	if err != nil {
		return "", err
	}

	// save application
	err = s.C(app).Insert(app)
	if err != nil {
		return "", err
	}

	return app.Key, nil
}
func EnsureFirstUser(store *coal.Store, name, email, password string) error {
	// copy store
	s := store.Copy()
	defer s.Close()

	// check existence
	n, err := s.C(&User{}).Count()
	if err != nil {
		return err
	} else if n > 0 {
		return nil
	}

	// user is missing

	// create user
	user := coal.Init(&User{}).(*User)
	user.Name = name
	user.Email = email
	user.Password = password

	// set key and secret
	err = user.Validate()
	if err != nil {
		return err
	}

	// save user
	err = s.C(user).Insert(user)
	if err != nil {
		return err
	}

	return nil
}
func (s *Strategy) Callback() *fire.Callback {
	// enforce defaults
	if s.CollectionAction == nil {
		s.CollectionAction = make(map[string][]*Authorizer)
	}
	if s.ResourceAction == nil {
		s.ResourceAction = make(map[string][]*Authorizer)
	}

	// construct and return callback
	return fire.C("ash/Strategy.Callback", fire.All(), func(ctx *fire.Context) (err error) {
		switch ctx.Operation {
		case fire.List:
			err = s.call(ctx, s.List, s.Read, s.All)
		case fire.Find:
			err = s.call(ctx, s.Find, s.Read, s.All)
		case fire.Create:
			err = s.call(ctx, s.Create, s.Write, s.All)
		case fire.Update:
			err = s.call(ctx, s.Update, s.Write, s.All)
		case fire.Delete:
			err = s.call(ctx, s.Delete, s.Write, s.All)
		case fire.CollectionAction:
			err = s.call(ctx, s.CollectionAction[ctx.JSONAPIRequest.CollectionAction], s.CollectionActions, s.Actions, s.All)
		case fire.ResourceAction:
			err = s.call(ctx, s.ResourceAction[ctx.JSONAPIRequest.ResourceAction], s.ResourceActions, s.Actions, s.All)
		}

		return err
	})
}
func OpenStream(store *Store, model Model, token []byte, receiver Receiver, opened func(), manager func(error) bool) *Stream {
	// prepare resume token
	var resumeToken *bson.Raw

	// create resume token if available
	if token != nil {
		resumeToken = &bson.Raw{
			Kind: bson.ElementDocument,
			Data: token,
		}
	}

	// create stream
	s := &Stream{
		store:    store,
		model:    model,
		token:    resumeToken,
		receiver: receiver,
		opened:   opened,
		manager:  manager,
	}

	// open stream
	go s.open()

	return s
}
func (s *Stream) Close() {
	// get mutex
	s.mutex.Lock()
	defer s.mutex.Unlock()

	// set flag
	s.closed = true

	// close active change stream
	if s.current != nil {
		_ = s.current.Close()
	}
}
func AddTokenIndexes(i *coal.Indexer, autoExpire bool) {
	i.Add(&Token{}, false, 0, "Type")
	i.Add(&Token{}, false, 0, "Application")
	i.Add(&Token{}, false, 0, "User")

	if autoExpire {
		i.Add(&Token{}, false, time.Minute, "ExpiresAt")
	}
}
func (t *Token) GetTokenData() (TokenType, []string, time.Time, bson.ObjectId, *bson.ObjectId) {
	return t.Type, t.Scope, t.ExpiresAt, t.Application, t.User
}
func (t *Token) SetTokenData(typ TokenType, scope []string, expiresAt time.Time, client Client, resourceOwner ResourceOwner) {
	t.Type = typ
	t.Scope = scope
	t.ExpiresAt = expiresAt
	t.Application = client.ID()
	if resourceOwner != nil {
		t.User = coal.P(resourceOwner.ID())
	}
}
func (a *Application) ValidSecret(secret string) bool {
	return bcrypt.CompareHashAndPassword(a.SecretHash, []byte(secret)) == nil
}
func (a *Application) HashSecret() error {
	// check length
	if len(a.Secret) == 0 {
		return nil
	}

	// generate hash from password
	hash, err := bcrypt.GenerateFromPassword([]byte(a.Secret), bcrypt.DefaultCost)
	if err != nil {
		return err
	}

	// save hash
	a.SecretHash = hash

	// clear password
	a.Secret = ""

	return nil
}
func (u *User) ValidPassword(password string) bool {
	return bcrypt.CompareHashAndPassword(u.PasswordHash, []byte(password)) == nil
}
func (u *User) HashPassword() error {
	// check length
	if len(u.Password) == 0 {
		return nil
	}

	// generate hash from password
	hash, err := bcrypt.GenerateFromPassword([]byte(u.Password), bcrypt.DefaultCost)
	if err != nil {
		return err
	}

	// save hash
	u.PasswordHash = hash

	// clear password
	u.Password = ""

	return nil
}
func MustCreateStore(uri string) *Store {
	store, err := CreateStore(uri)
	if err != nil {
		panic(err)
	}

	return store
}
func CreateStore(uri string) (*Store, error) {
	session, err := mgo.Dial(uri)
	if err != nil {
		return nil, err
	}

	return NewStore(session), nil
}
func (s *SubStore) C(model Model) *mgo.Collection {
	return s.DB().C(C(model))
}
func NewAssetServer(prefix, directory string) http.Handler {
	// ensure prefix
	prefix = "/" + strings.Trim(prefix, "/")

	// create dir server
	dir := http.Dir(directory)

	// create file server
	fs := http.FileServer(dir)

	h := func(w http.ResponseWriter, r *http.Request) {
		// pre-check if file does exist
		f, err := dir.Open(r.URL.Path)
		if err != nil {
			r.URL.Path = "/"
		} else if f != nil {
			_ = f.Close()
		}

		// serve file
		fs.ServeHTTP(w, r)
	}

	return http.StripPrefix(prefix, http.HandlerFunc(h))
}
func DefaultGrantStrategy(scope oauth2.Scope, _ Client, _ ResourceOwner) (oauth2.Scope, error) {
	// check scope
	if !scope.Empty() {
		return nil, ErrInvalidScope
	}

	return scope, nil
}
func DefaultTokenData(_ Client, ro ResourceOwner, _ GenericToken) map[string]interface{} {
	if ro != nil {
		return map[string]interface{}{
			"user": ro.ID(),
		}
	}

	return nil
}
func (p *Policy) GenerateToken(id bson.ObjectId, issuedAt, expiresAt time.Time, client Client, resourceOwner ResourceOwner, token GenericToken) (string, error) {
	// prepare claims
	claims := &TokenClaims{}
	claims.Id = id.Hex()
	claims.IssuedAt = issuedAt.Unix()
	claims.ExpiresAt = expiresAt.Unix()

	// set user data
	if p.TokenData != nil {
		claims.Data = p.TokenData(client, resourceOwner, token)
	}

	// create token
	tkn := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)

	// sign token
	str, err := tkn.SignedString(p.Secret)
	if err != nil {
		return "", nil
	}

	return str, nil
}
func (p *Policy) ParseToken(str string) (*TokenClaims, bool, error) {
	// parse token and check id
	var claims TokenClaims
	_, err := jwt.ParseWithClaims(str, &claims, func(_ *jwt.Token) (interface{}, error) {
		return p.Secret, nil
	})
	if valErr, ok := err.(*jwt.ValidationError); ok && valErr.Errors == jwt.ValidationErrorExpired {
		return nil, true, err
	} else if err != nil {
		return nil, false, err
	} else if !bson.IsObjectIdHex(claims.Id) {
		return nil, false, errors.New("invalid id")
	}

	return &claims, false, nil
}
func E(name string, m fire.Matcher, h fire.Handler) *Enforcer {
	return fire.C(name, m, h)
}
func (b *Base) MustGet(name string) interface{} {
	// find field
	field := b.meta.Fields[name]
	if field == nil {
		panic(fmt.Sprintf(`coal: field "%s" not found on "%s"`, name, b.meta.Name))
	}

	// read value from model struct
	structField := reflect.ValueOf(b.model).Elem().Field(field.index)
	return structField.Interface()
}
func (b *Base) MustSet(name string, value interface{}) {
	// find field
	field := b.meta.Fields[name]
	if field == nil {
		panic(fmt.Sprintf(`coal: field "%s" not found on "%s"`, name, b.meta.Name))
	}

	// set the value on model struct
	reflect.ValueOf(b.model).Elem().Field(field.index).Set(reflect.ValueOf(value))
}
func NewGroup() *Group {
	return &Group{
		controllers: make(map[string]*Controller),
		actions:     make(map[string]*GroupAction),
	}
}
func (g *Group) Add(controllers ...*Controller) {
	for _, controller := range controllers {
		// prepare controller
		controller.prepare()

		// get name
		name := controller.Model.Meta().PluralName

		// check existence
		if g.controllers[name] != nil {
			panic(fmt.Sprintf(`fire: controller with name "%s" already exists`, name))
		}

		// create entry in controller map
		g.controllers[name] = controller
	}
}
func (g *Group) Endpoint(prefix string) http.Handler {
	// trim prefix
	prefix = strings.Trim(prefix, "/")

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// create tracer
		tracer := NewTracerFromRequest(r, "fire/Group.Endpoint")
		defer tracer.Finish(true)

		// continue any previous aborts
		defer stack.Resume(func(err error) {
			// directly write jsonapi errors
			if jsonapiError, ok := err.(*jsonapi.Error); ok {
				_ = jsonapi.WriteError(w, jsonapiError)
				return
			}

			// set critical error on last span
			tracer.Tag("error", true)
			tracer.Log("error", err.Error())
			tracer.Log("stack", stack.Trace())

			// report critical errors if possible
			if g.Reporter != nil {
				g.Reporter(err)
			}

			// ignore errors caused by writing critical errors
			_ = jsonapi.WriteError(w, jsonapi.InternalServerError(""))
		})

		// trim path
		path := strings.Trim(r.URL.Path, "/")
		path = strings.TrimPrefix(path, prefix)
		path = strings.Trim(path, "/")

		// check path
		if path == "" {
			stack.Abort(jsonapi.NotFound("resource not found"))
		}

		// split path
		s := strings.Split(path, "/")

		// prepare context
		ctx := &Context{
			Data:           Map{},
			HTTPRequest:    r,
			ResponseWriter: w,
			Group:          g,
			Tracer:         tracer,
		}

		// get controller
		controller, ok := g.controllers[s[0]]
		if ok {
			// set controller
			ctx.Controller = controller

			// call controller with context
			controller.generalHandler(prefix, ctx)

			return
		}

		// get action
		action, ok := g.actions[s[0]]
		if ok {
			// check if action is allowed
			if Contains(action.Action.Methods, r.Method) {
				// check if action matches the context
				if action.Action.Callback.Matcher(ctx) {
					// run authorizers and handle errors
					for _, cb := range action.Authorizers {
						// check if callback should be run
						if !cb.Matcher(ctx) {
							continue
						}

						// call callback
						err := cb.Handler(ctx)
						if IsSafe(err) {
							stack.Abort(&jsonapi.Error{
								Status: http.StatusUnauthorized,
								Detail: err.Error(),
							})
						} else if err != nil {
							stack.Abort(err)
						}
					}

					// limit request body size
					LimitBody(ctx.ResponseWriter, ctx.HTTPRequest, int64(action.Action.BodyLimit))

					// call action with context
					stack.AbortIf(action.Action.Callback.Handler(ctx))

					return
				}
			}
		}

		// otherwise return error
		stack.Abort(jsonapi.NotFound("resource not found"))
	})
}
func (i *Indexer) Add(model Model, unique bool, expireAfter time.Duration, fields ...string) {
	// construct key from fields
	var key []string
	for _, f := range fields {
		key = append(key, F(model, f))
	}

	// add index
	i.AddRaw(C(model), mgo.Index{
		Key:         key,
		Unique:      unique,
		ExpireAfter: expireAfter,
		Background:  true,
	})
}
func (i *Indexer) AddRaw(coll string, idx mgo.Index) {
	i.indexes = append(i.indexes, index{
		coll:  coll,
		index: idx,
	})
}
func (i *Indexer) Ensure(store *Store) error {
	// copy store
	s := store.Copy()
	defer s.Close()

	// go through all raw indexes
	for _, i := range i.indexes {
		// ensure single index
		err := s.DB().C(i.coll).EnsureIndex(i.index)
		if err != nil {
			return err
		}
	}

	return nil
}
func NewCatalog(models ...Model) *Catalog {
	// create catalog
	c := &Catalog{
		models: make(map[string]Model),
	}

	// add models
	c.Add(models...)

	return c
}
func (c *Catalog) Add(models ...Model) {
	for _, model := range models {
		// get name
		name := Init(model).Meta().PluralName

		// check existence
		if c.models[name] != nil {
			panic(fmt.Sprintf(`coal: model with name "%s" already exists in catalog`, name))
		}

		// add model
		c.models[name] = model
	}
}
func (c *Catalog) All() []Model {
	// prepare models
	models := make([]Model, 0, len(c.models))

	// add models
	for _, model := range c.models {
		models = append(models, model)
	}

	return models
}
func (c *Catalog) Visualize(title string) string {
	// prepare buffer
	var out bytes.Buffer

	// start graph
	out.WriteString("graph G {\n")
	out.WriteString("  rankdir=\"LR\";\n")
	out.WriteString("  sep=\"0.3\";\n")
	out.WriteString("  ranksep=\"0.5\";\n")
	out.WriteString("  nodesep=\"0.4\";\n")
	out.WriteString("  pad=\"0.4,0.4\";\n")
	out.WriteString("  margin=\"0,0\";\n")
	out.WriteString("  labelloc=\"t\";\n")
	out.WriteString("  fontsize=\"13\";\n")
	out.WriteString("  fontname=\"Arial BoldMT\";\n")
	out.WriteString("  splines=\"spline\";\n")
	out.WriteString("  overlap=\"voronoi\";\n")
	out.WriteString("  outputorder=\"edgesfirst\";\n")
	out.WriteString("  edge[headclip=true, tailclip=false];\n")
	out.WriteString("  label=\"" + title + "\";\n")

	// get a sorted list of model names and lookup table
	var names []string
	lookup := make(map[string]string)
	for name, model := range c.models {
		names = append(names, name)
		lookup[name] = model.Meta().Name
	}
	sort.Strings(names)

	// add model nodes
	for _, name := range names {
		// get model
		model := c.models[name]

		// write begin of node
		out.WriteString(fmt.Sprintf(`  "%s" [ style=filled, fillcolor=white, label=`, lookup[name]))

		// write head table
		out.WriteString(fmt.Sprintf(`<<table border="0" align="center" cellspacing="0.5" cellpadding="0" width="134"><tr><td align="center" valign="bottom" width="130"><font face="Arial BoldMT" point-size="11">%s</font></td></tr></table>|`, lookup[name]))

		// write begin of tail table
		out.WriteString(fmt.Sprintf(`<table border="0" align="left" cellspacing="2" cellpadding="0" width="134">`))

		// write attributes
		for _, field := range model.Meta().OrderedFields {
			out.WriteString(fmt.Sprintf(`<tr><td align="left" width="130" port="%s">%s<font face="Arial ItalicMT" color="grey60"> %s</font></td></tr>`, field.Name, field.Name, field.Type.String()))
		}

		// write end of tail table
		out.WriteString(fmt.Sprintf(`</table>>`))

		// write end of node
		out.WriteString(`, shape=Mrecord, fontsize=10, fontname="ArialMT", margin="0.07,0.05", penwidth="1.0" ];` + "\n")
	}

	// define temporary struct
	type rel struct {
		from, to   string
		srcMany    bool
		dstMany    bool
		hasInverse bool
	}

	// prepare list
	list := make(map[string]*rel)
	var relNames []string

	// prepare relationships
	for _, name := range names {
		// get model
		model := c.models[name]

		// add all direct relationships
		for _, field := range model.Meta().OrderedFields {
			if field.RelName != "" && (field.ToOne || field.ToMany) {
				list[name+"-"+field.RelName] = &rel{
					from:    name,
					to:      field.RelType,
					srcMany: field.ToMany,
				}

				relNames = append(relNames, name+"-"+field.RelName)
			}
		}
	}

	// update relationships
	for _, name := range names {
		// get model
		model := c.models[name]

		// add all indirect relationships
		for _, field := range model.Meta().OrderedFields {
			if field.RelName != "" && (field.HasOne || field.HasMany) {
				r := list[field.RelType+"-"+field.RelInverse]
				r.dstMany = field.HasMany
				r.hasInverse = true
			}
		}
	}

	// sort relationship names
	sort.Strings(relNames)

	// add relationships
	for _, name := range relNames {
		// get relationship
		r := list[name]

		// get style
		style := "solid"
		if !r.hasInverse {
			style = "dotted"
		}

		// get color
		color := "black"
		if r.srcMany {
			color = "black:white:black"
		}

		// write edge
		out.WriteString(fmt.Sprintf(`  "%s"--"%s"[ fontname="ArialMT", fontsize=7, dir=both, arrowsize="0.9", penwidth="0.9", labelangle=32, labeldistance="1.8", style=%s, color="%s", arrowhead=%s, arrowtail=%s ];`, lookup[r.from], lookup[r.to], style, color, "normal", "none") + "\n")
	}

	// end graph
	out.WriteString("}\n")

	return out.String()
}
func NewErrorReporter(out io.Writer) func(error) {
	return func(err error) {
		_, _ = fmt.Fprintf(out, "===> Begin Error: %s\n", err.Error())
		_, _ = out.Write(debug.Stack())
		_, _ = fmt.Fprintln(out, "<=== End Error")
	}
}
func EnsureIndexes(store *coal.Store) error {
	// ensure model indexes
	err := indexer.Ensure(store)
	if err != nil {
		return err
	}

	return nil
}
func E(format string, a ...interface{}) error {
	return Safe(fmt.Errorf(format, a...))
}
func Compose(chain ...interface{}) http.Handler {
	// check length
	if len(chain) < 2 {
		panic("fire: expected chain to have at least two items")
	}

	// get handler
	h, ok := chain[len(chain)-1].(http.Handler)
	if !ok {
		panic(`fire: expected last chain item to be a "http.Handler"`)
	}

	// chain all middleware
	for i := len(chain) - 2; i >= 0; i-- {
		// get middleware
		m, ok := chain[i].(func(http.Handler) http.Handler)
		if !ok {
			panic(`fire: expected intermediary chain item to be a "func(http.handler) http.Handler"`)
		}

		// chain
		h = m(h)
	}

	return h
}
func Includes(all, subset []string) bool {
	for _, item := range subset {
		if !Contains(all, item) {
			return false
		}
	}

	return true
}
func Intersect(listA, listB []string) []string {
	// prepare new list
	list := make([]string, 0, len(listA))

	// add items that are part of both lists
	for _, item := range listA {
		if Contains(listB, item) {
			list = append(list, item)
		}
	}

	return list
}
func E(reason string, retry bool) *Error {
	return &Error{
		Reason: reason,
		Retry:  retry,
	}
}
func RootTracer() func(http.Handler) http.Handler {
	return func(next http.Handler) http.Handler {
		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
			// split url
			segments := strings.Split(r.URL.Path, "/")

			// replace ids
			for i, s := range segments {
				if bson.IsObjectIdHex(s) {
					segments[i] = ":id"
				}
			}

			// construct name
			path := strings.Join(segments, "/")
			name := fmt.Sprintf("%s %s", r.Method, path)

			// create root span from request
			tracer := NewTracerFromRequest(r, name)
			tracer.Tag("peer.address", r.RemoteAddr)
			tracer.Tag("http.proto", r.Proto)
			tracer.Tag("http.method", r.Method)
			tracer.Tag("http.host", r.Host)
			tracer.Log("http.url", r.URL.String())
			tracer.Log("http.length", r.ContentLength)
			tracer.Log("http.header", r.Header)
			r = r.WithContext(tracer.Context(r.Context()))
			defer tracer.Finish(true)

			// call next handler
			next.ServeHTTP(w, r)
		})
	}
}
func NewTracerFromRequest(r *http.Request, name string) *Tracer {
	span, _ := opentracing.StartSpanFromContext(r.Context(), name)
	return NewTracer(span)
}
func NewTracer(root opentracing.Span) *Tracer {
	return &Tracer{
		root:  root,
		spans: make([]opentracing.Span, 0, 32),
	}
}
func (t *Tracer) Push(name string) {
	// get context
	var ctx opentracing.SpanContext
	if len(t.spans) > 0 {
		ctx = t.Last().Context()
	} else {
		ctx = t.root.Context()
	}

	// create new span
	span := opentracing.StartSpan(name, opentracing.ChildOf(ctx))

	// push span
	t.spans = append(t.spans, span)
}
func (t *Tracer) Last() opentracing.Span {
	// return root if empty
	if len(t.spans) == 0 {
		return t.root
	}

	return t.spans[len(t.spans)-1]
}
func (t *Tracer) Tag(key string, value interface{}) {
	t.Last().SetTag(key, value)
}
func (t *Tracer) Log(key string, value interface{}) {
	t.Last().LogKV(key, value)
}
func (t *Tracer) Context(ctx context.Context) context.Context {
	return opentracing.ContextWithSpan(ctx, t.Last())
}
func (t *Tracer) Pop() {
	// check list
	if len(t.spans) == 0 {
		return
	}

	// finish last span
	t.Last().Finish()

	// resize slice
	t.spans = t.spans[:len(t.spans)-1]
}
func (t *Tracer) Finish(root bool) {
	for _, span := range t.spans {
		span.Finish()
	}

	if root {
		t.root.Finish()
	}
}
func NewPool() *Pool {
	return &Pool{
		tasks:  make(map[string]*Task),
		queues: make(map[*Queue]bool),
		closed: make(chan struct{}),
	}
}
func (p *Pool) Add(task *Task) {
	// check existence
	if p.tasks[task.Name] != nil {
		panic(fmt.Sprintf(`axe: task with name "%s" already exists`, task.Name))
	}

	// save task
	p.tasks[task.Name] = task

	// add task to queue
	task.Queue.tasks = append(task.Queue.tasks, task.Name)

	// save queue
	p.queues[task.Queue] = true
}
func (p *Pool) Run() {
	// start all queues
	for queue := range p.queues {
		queue.start(p)
	}

	// start all tasks
	for _, task := range p.tasks {
		task.start(p)
	}
}
func NewBodyLimiter(w http.ResponseWriter, r *http.Request, n int64) *BodyLimiter {
	return &BodyLimiter{
		Original:   r.Body,
		ReadCloser: http.MaxBytesReader(w, r.Body, n),
	}
}
func LimitBody(w http.ResponseWriter, r *http.Request, n int64) {
	// get original body from existing limiter
	if bl, ok := r.Body.(*BodyLimiter); ok {
		r.Body = bl.Original
	}

	// set new limiter
	r.Body = NewBodyLimiter(w, r, n)
}
func C(name string, m Matcher, h Handler) *Callback {
	// panic if matcher or handler is not set
	if m == nil || h == nil {
		panic("fire: missing matcher or handler")
	}

	return &Callback{
		Matcher: m,
		Handler: func(ctx *Context) error {
			// begin trace
			ctx.Tracer.Push(name)

			// call handler
			err := h(ctx)
			if err != nil {
				return err
			}

			// finish trace
			ctx.Tracer.Pop()

			return nil
		},
	}
}
func Only(ops ...Operation) Matcher {
	return func(ctx *Context) bool {
		// allow if operation is listed
		for _, op := range ops {
			if op == ctx.Operation {
				return true
			}
		}

		return false
	}
}
func BasicAuthorizer(credentials map[string]string) *Callback {
	return C("fire/BasicAuthorizer", All(), func(ctx *Context) error {
		// check for credentials
		user, password, ok := ctx.HTTPRequest.BasicAuth()
		if !ok {
			return ErrAccessDenied
		}

		// check if credentials match
		if val, ok := credentials[user]; !ok || val != password {
			return ErrAccessDenied
		}

		return nil
	})
}
func ModelValidator() *Callback {
	return C("fire/ModelValidator", Only(Create, Update), func(ctx *Context) error {
		// check model
		m, ok := ctx.Model.(ValidatableModel)
		if !ok {
			return fmt.Errorf("model is not validatable")
		}

		// validate model
		err := m.Validate()
		if err != nil {
			return err
		}

		return nil
	})
}
func TimestampValidator() *Callback {
	return C("fire/TimestampValidator", Only(Create, Update), func(ctx *Context) error {
		// get time
		now := time.Now()

		// get timestamp fields
		ctf := coal.L(ctx.Model, "fire-created-timestamp", false)
		utf := coal.L(ctx.Model, "fire-updated-timestamp", false)

		// set created timestamp on creation and set missing create timestamps
		// to the timestamp inferred from the model id
		if ctf != "" {
			if ctx.Operation == Create {
				ctx.Model.MustSet(ctf, now)
			} else if t := ctx.Model.MustGet(ctf).(time.Time); t.IsZero() {
				ctx.Model.MustSet(ctf, ctx.Model.ID().Time())
			}
		}

		// always set updated timestamp
		if utf != "" {
			ctx.Model.MustSet(utf, now)
		}

		return nil
	})
}
func RelationshipValidator(model coal.Model, catalog *coal.Catalog, excludedFields ...string) *Callback {
	// prepare lists
	dependentResources := make(map[coal.Model]string)
	references := make(map[string]coal.Model)

	// iterate through all fields
	for _, field := range coal.Init(model).Meta().Relationships {
		// exclude field if requested
		if Contains(excludedFields, field.Name) {
			continue
		}

		// handle has-one and has-many relationships
		if field.HasOne || field.HasMany {
			// get related model
			relatedModel := catalog.Find(field.RelType)
			if relatedModel == nil {
				panic(fmt.Sprintf(`fire: missing model in catalog: "%s"`, field.RelType))
			}

			// get related bson field
			bsonField := ""
			for _, relatedField := range relatedModel.Meta().Relationships {
				if relatedField.RelName == field.RelInverse {
					bsonField = relatedField.Name
				}
			}
			if bsonField == "" {
				panic(fmt.Sprintf(`fire: missing field for inverse relationship: "%s"`, field.RelInverse))
			}

			// add relationship
			dependentResources[relatedModel] = bsonField
		}

		// handle to-one and to-many relationships
		if field.ToOne || field.ToMany {
			// get related model
			relatedModel := catalog.Find(field.RelType)
			if relatedModel == nil {
				panic(fmt.Sprintf(`fire: missing model in catalog: "%s"`, field.RelType))
			}

			// add relationship
			references[field.Name] = relatedModel
		}
	}

	// create callbacks
	cb1 := DependentResourcesValidator(dependentResources)
	cb2 := VerifyReferencesValidator(references)

	return C("RelationshipValidator", func(ctx *Context) bool {
		return cb1.Matcher(ctx) || cb2.Matcher(ctx)
	}, func(ctx *Context) error {
		// run dependent resources validator
		if cb1.Matcher(ctx) {
			err := cb1.Handler(ctx)
			if err != nil {
				return err
			}
		}

		// run dependent resources validator
		if cb2.Matcher(ctx) {
			err := cb2.Handler(ctx)
			if err != nil {
				return err
			}
		}

		return nil
	})
}
func (s *Seekret) Inspect(Nworkers int) {
	jobs := make(chan workerJob)
	results := make(chan workerResult)

	for w := 1; w <= Nworkers; w++ {
		go inspect_worker(w, jobs, results)
	}

	objectGroupMap := s.GroupObjectsByPrimaryKeyHash()

	go func() {
		for _, objectGroup := range objectGroupMap {
			jobs <- workerJob{
				objectGroup:   objectGroup,
				ruleList:      s.ruleList,
				exceptionList: s.exceptionList,
			}
		}
		close(jobs)
	}()

	for i := 0; i < len(objectGroupMap); i++ {
		result := <-results
		s.secretList = append(s.secretList, result.secretList...)
	}
}
func NewRule(name string, match string) (*Rule, error) {
	matchRegexp, err := regexp.Compile("(?i)" + match)
	if err != nil {
		return nil, err
	}
	if err != nil {
		fmt.Println(err)
	}

	r := &Rule{
		Enabled: false,
		Name:    name,
		Match:   matchRegexp,
	}
	return r, nil
}
func (r *Rule) AddUnmatch(unmatch string) error {
	unmatchRegexp, err := regexp.Compile("(?i)" + unmatch)
	if err != nil {
		return err
	}

	r.Unmatch = append(r.Unmatch, unmatchRegexp)

	return nil
}
func (r *Rule) Run(content []byte) []RunResult {
	var results []RunResult

	b := bufio.NewScanner(bytes.NewReader(content))

	nLine := 0
	for b.Scan() {
		nLine = nLine + 1
		line := b.Text()

		if r.Match.MatchString(line) {
			unmatch := false
			for _, Unmatch := range r.Unmatch {
				if Unmatch.MatchString(line) {
					unmatch = true
				}
			}

			if !unmatch {
				results = append(results, RunResult{
					Line:  line,
					Nline: nLine,
				})
			}
		}
	}

	return results
}
func NewSecret(object *Object, rule *Rule, nLine int, line string) *Secret {
	s := &Secret{
		Object: object,
		Rule:   rule,
		Nline:  nLine,
		Line:   line,
	}
	return s
}
func NewObject(name string, t string, st string, content []byte) *Object {
	if len(content) > MaxObjectContentLen {
		content = content[:MaxObjectContentLen]
	}
	o := &Object{
		Type: t,
		SubType: st,

		Name:    name,
		Content: content,

		Metadata:       make(map[string]MetadataData),
		PrimaryKeyHash: nil,
	}
	return o
}
func (o *Object) SetMetadata(key string, value string, attr MetadataAttributes) error {
	o.Metadata[key] = MetadataData{
		value: value,
		attr:  attr,
	}

	if attr.PrimaryKey {
		o.updatePrimaryKeyHash()
	}

	return nil
}
func (o *Object) GetMetadata(key string) (string, error) {
	data, ok := o.Metadata[key]
	if !ok {
		return "", fmt.Errorf("%s unexistent key", key)
	}

	return data.value, nil
}
func (o *Object) GetMetadataAll(attr bool) map[string]string {
	metadataAll := make(map[string]string)
	for k, v := range o.Metadata {
		metadataAll[k] = v.value
	}
	return metadataAll
}
func (x *Exception) SetRule(rule string) error {
	ruleRegexp, err := regexp.Compile("(?i)" + rule)
	if err != nil {
		return err
	}
	x.Rule = ruleRegexp
	return nil
}
func (x *Exception) SetObject(object string) error {
	objectRegexp, err := regexp.Compile("(?i)" + object)
	if err != nil {
		return err
	}
	x.Object = objectRegexp
	return nil
}
func (x *Exception) SetNline(nLine int) error {
	x.Nline = &nLine
	return nil
}
func (x *Exception) SetContent(content string) error {
	contentRegexp, err := regexp.Compile("(?i)" + content)
	if err != nil {
		return err
	}
	x.Content = contentRegexp
	return nil
}
func (x *Exception) Run(s *Secret) bool {
	match := true

	if match && x.Rule != nil && !x.Rule.MatchString(s.Rule.Name) {
		match = false
	}

	if match && x.Object != nil && !x.Object.MatchString(s.Object.Name) {
		match = false
	}

	if match && x.Nline != nil && *x.Nline != s.Nline {
		match = false
	}

	if match && x.Content != nil && !x.Content.MatchString(s.Line) {
		match = false
	}

	return match
}
func (s *Seekret) AddRule(rule models.Rule, enabled bool) {
	if enabled {
		rule.Enable()
	}
	s.ruleList = append(s.ruleList, rule)
}
func (s *Seekret) LoadRulesFromFile(file string, defaulEnabled bool) error {
	var ruleYamlMap map[string]ruleYaml

	if file == "" {
		return nil
	}

	filename, _ := filepath.Abs(file)

	ruleBase := filepath.Base(filename)
	if filepath.Ext(ruleBase) == ".rule" {
		ruleBase = ruleBase[0 : len(ruleBase)-5]
	}

	yamlData, err := ioutil.ReadFile(filename)
	if err != nil {
		return err
	}

	err = yaml.Unmarshal(yamlData, &ruleYamlMap)
	if err != nil {
		return err
	}

	for k, v := range ruleYamlMap {
		rule, err := models.NewRule(ruleBase+"."+k, v.Match)
		if err != nil {
			return err
		}

		for _, e := range v.Unmatch {
			rule.AddUnmatch(e)
		}
		s.AddRule(*rule, defaulEnabled)
	}

	return nil
}
func (s *Seekret) LoadRulesFromDir(dir string, defaulEnabled bool) error {
	fi, err := os.Stat(dir)
	if err != nil {
		return err
	}

	if !fi.IsDir() {
		err := fmt.Errorf("%s is not a directory", dir)
		return err
	}

	fileList, err := filepath.Glob(dir + "/*")
	if err != nil {
		return err
	}
	for _, file := range fileList {
		if strings.HasSuffix(file, ".rule") {
			err := s.LoadRulesFromFile(file, defaulEnabled)
			if err != nil {
				return err
			}
		}
	}
	return nil
}
func DefaultRulesPath() string {
	rulesPath := os.Getenv("SEEKRET_RULES_PATH")
	if rulesPath == "" {
		rulesPath = os.ExpandEnv(defaultRulesDir)
	}
	return rulesPath
}
func (s *Seekret) EnableRule(name string) error {
	return setRuleEnabled(s.ruleList, name, true)
}
func (s *Seekret) DisableRule(name string) error {
	return setRuleEnabled(s.ruleList, name, false)
}
func (s *Seekret) EnableRuleByRegexp(name string) int {
	return setRuleEnabledByRegexp(s.ruleList, name, true)
}
func (s *Seekret) DisableRuleByRegexp(name string) int {
	return setRuleEnabledByRegexp(s.ruleList, name, false)
}
func (s *Seekret) LoadObjects(st SourceType, source string, opt LoadOptions) error {
	objectList, err := st.LoadObjects(source, opt)
	if err != nil {
		return err
	}
	s.objectList = append(s.objectList, objectList...)
	return nil
}
func (s *Seekret) GroupObjectsByMetadata(k string) map[string][]models.Object {
	return models.GroupObjectsByMetadata(s.objectList, k)
}
func (s *Seekret) GroupObjectsByPrimaryKeyHash() map[string][]models.Object {
	return models.GroupObjectsByPrimaryKeyHash(s.objectList)
}
func (s *Seekret) AddException(exception models.Exception) {
	s.exceptionList = append(s.exceptionList, exception)
}
func (s *Seekret) LoadExceptionsFromFile(file string) error {
	var exceptionYamlList []exceptionYaml

	if file == "" {
		return nil
	}

	filename, _ := filepath.Abs(file)
	yamlData, err := ioutil.ReadFile(filename)
	if err != nil {
		return err
	}

	err = yaml.Unmarshal(yamlData, &exceptionYamlList)
	if err != nil {
		return err
	}

	for _, v := range exceptionYamlList {
		x := models.NewException()

		if v.Rule != nil {
			err := x.SetRule(*v.Rule)
			if err != nil {
				return err
			}
		}

		if v.Object != nil {
			err := x.SetObject(*v.Object)
			if err != nil {
				return err
			}
		}

		if v.Line != nil {
			err := x.SetNline(*v.Line)
			if err != nil {
				return err
			}
		}

		if v.Content != nil {
			err := x.SetContent(*v.Content)
			if err != nil {
				return err
			}
		}

		s.AddException(*x)
	}

	return nil
}
func printPlainResults(results Results) error {
	for _, res := range results {
		// Explicitely start with the string and error output
		fmt.Printf("Source = %s\n", res.origString)
		fmt.Printf("    String = %s\n", res.String)

		if res.Error != "" {
			fmt.Printf("    Error = %s\n", res.Error)
			continue
		}

		// Dynamically loop over the rest of the fields
		typ := reflect.TypeOf(*res)
		val := reflect.ValueOf(*res)

		for i := 0; i < typ.NumField(); i++ {
			field := typ.Field(i)
			if field.Name == "Error" || field.Name == "String" {
				continue
			}
			if field.PkgPath != "" {
				// ignore unexported fields
				continue
			}
			fmt.Printf("    %s = %v\n", field.Name, val.Field(i).Interface())
		}

		fmt.Print("\n")
	}
	return nil
}
func printJsonResults(results Results) error {
	data, err := json.MarshalIndent(results, "", "    ")
	if err != nil {
		return fmt.Errorf("Failed to convert results to JSON: %s", err.Error())
	}

	if _, err = io.Copy(os.Stdout, bytes.NewReader(data)); err != nil {
		return fmt.Errorf("Failed to write json output: %s", err.Error())
	}

	fmt.Print("\n")
	return nil
}
func NewXor64Source(seed int64) *Xor64Source {
	var s Xor64Source
	s.Seed(seed)
	return &s
}
func xor64(x uint64) uint64 {
	x ^= x << 13
	x ^= x >> 7
	x ^= x << 17
	return x
}
func (s *Xor64Source) next() uint64 {
	x := xor64(uint64(*s))
	*s = Xor64Source(x)
	return x
}
func (s *Xor64Source) Seed(seed int64) {
	if seed == 0 {
		seed = seed0
	}
	*s = Xor64Source(seed)
}
func NewFrameSet(frange string) (*FrameSet, error) {
	// Process the frame range and get a slice of match slices
	matches, err := frameRangeMatches(frange)
	if err != nil {
		return nil, err
	}

	frameSet := &FrameSet{frange, &ranges.InclusiveRanges{}}

	// Process each slice match and add it to the frame set
	for _, match := range matches {
		if err = frameSet.handleMatch(match); err != nil {
			return nil, err
		}
	}

	return frameSet, nil
}
func (s *FrameSet) handleMatch(match []string) error {
	switch len(match) {

	// Single frame match
	case 1:
		f, err := parseInt(match[0])
		if err != nil {
			return err
		}
		s.rangePtr.AppendUnique(f, f, 1)

	// Simple frame range
	case 2:
		start, err := parseInt(match[0])
		if err != nil {
			return err
		}
		end, err := parseInt(match[1])
		if err != nil {
			return err
		}

		// Handle descending frame ranges, like 10-1
		var inc int
		if start > end {
			inc = -1
		} else {
			inc = 1
		}

		s.rangePtr.AppendUnique(start, end, inc)

	// Complex frame range
	case 4:
		var (
			err               error
			mod               string
			start, end, chunk int
		)
		chunk, err = parseInt(match[3])
		if err != nil {
			return err
		}
		if chunk == 0 {
			return fmt.Errorf("Failed to parse part of range %v. "+
				"Encountered invalid 0 value", match[3])
		}
		if start, err = parseInt(match[0]); err != nil {
			return err
		}
		if end, err = parseInt(match[1]); err != nil {
			return err
		}
		if mod = match[2]; !isModifier(mod) {
			return fmt.Errorf("%q is not one of the valid modifier 'xy:'", mod)
		}

		switch mod {
		case `x`:
			s.rangePtr.AppendUnique(start, end, chunk)

		case `y`:
			// TODO: Add proper support for adding inverse of range.
			// This approach will add excessive amounts of singe
			// range elements. They could be compressed into chunks
			skip := start
			aRange := ranges.NewInclusiveRange(start, end, 1)
			var val int
			for it := aRange.IterValues(); !it.IsDone(); {
				val = it.Next()
				if val == skip {
					skip += chunk
					continue
				}
				s.rangePtr.AppendUnique(val, val, 1)
			}

		case `:`:
			for ; chunk > 0; chunk-- {
				s.rangePtr.AppendUnique(start, end, chunk)
			}
		}

	default:
		return fmt.Errorf("Unexpected match []string size: %v", match)
	}

	return nil
}
func (s *FrameSet) Index(frame int) int {
	return s.rangePtr.Index(frame)
}
func (s *FrameSet) Frame(index int) (int, error) {
	return s.rangePtr.Value(index)
}
func (s *FrameSet) HasFrame(frame int) bool {
	return s.rangePtr.Contains(frame)
}
func (s *FrameSet) FrameRangePadded(pad int) string {
	return PadFrameRange(s.frange, pad)
}
func (s *FrameSet) Normalize() *FrameSet {
	ptr := s.rangePtr.Normalized()
	return &FrameSet{ptr.String(), ptr}
}
func FramesToFrameRange(frames []int, sorted bool, zfill int) string {
	count := len(frames)
	if count == 0 {
		return ""
	}

	if count == 1 {
		return zfillInt(frames[0], zfill)
	}

	if sorted {
		sort.Ints(frames)
	}

	var i, frame, step int
	var start, end string
	var buf strings.Builder

	// Keep looping until all frames are consumed
	for len(frames) > 0 {
		count = len(frames)
		// If we get to the last element, just write it
		// and end
		if count <= 2 {
			for _, frame = range frames {
				if buf.Len() > 0 {
					buf.WriteString(",")
				}
				buf.WriteString(zfillInt(frame, zfill))
			}
			break
		}
		// At this point, we have 3 or more frames to check.
		// Scan the current window of the slice to see how
		// many frames we can consume into a group
		step = frames[1] - frames[0]
		for i = 0; i < len(frames)-1; i++ {
			// We have scanned as many frames as we can
			// for this group. Now write them and stop
			// looping on this window
			if (frames[i+1] - frames[i]) != step {
				break
			}
		}

		// Subsequent groups are comma-separated
		if buf.Len() > 0 {
			buf.WriteString(",")
		}

		// We only have a single frame to write for this group
		if i == 0 {
			buf.WriteString(zfillInt(frames[0], zfill))
			frames = frames[1:]
			continue
		}

		// First do a check to see if we could have gotten a larger range
		// out of subsequent values with a different step size
		if i == 1 && count > 3 {
			// Check if the next two pairwise frames have the same step.
			// If so, then it is better than our current grouping.
			if (frames[2] - frames[1]) == (frames[3] - frames[2]) {
				// Just consume the first frame, and allow the next
				// loop to scan the new stepping
				buf.WriteString(zfillInt(frames[0], zfill))
				frames = frames[1:]
				continue
			}
		}

		// Otherwise write out this step range
		start = zfillInt(frames[0], zfill)
		end = zfillInt(frames[i], zfill)
		buf.WriteString(fmt.Sprintf("%s-%s", start, end))
		if step > 1 {
			buf.WriteString(fmt.Sprintf("x%d", step))
		}
		frames = frames[i+1:]
	}

	return buf.String()
}
func frameRangeMatches(frange string) ([][]string, error) {
	for _, k := range defaultPadding.AllChars() {
		frange = strings.Replace(frange, k, "", -1)
	}

	var (
		matched bool
		match   []string
		rx      *regexp.Regexp
	)

	frange = strings.Replace(frange, " ", "", -1)

	// For each comma-sep component, we will parse a frame range
	parts := strings.Split(frange, ",")
	size := len(parts)
	matches := make([][]string, size, size)

	for i, part := range parts {

		matched = false

		// Build up frames for all comma-sep components
		for _, rx = range rangePatterns {
			if match = rx.FindStringSubmatch(part); match == nil {
				continue
			}
			matched = true
			matches[i] = match[1:]
		}

		// If any component of the comma-sep frame range fails to
		// parse, we bail out
		if !matched {
			err := fmt.Errorf("Failed to parse frame range: %s on part %q", frange, part)
			return nil, err
		}
	}

	return matches, nil
}
func toRange(start, end, step int) []int {
	nums := []int{}
	if step < 1 {
		step = 1
	}
	if start <= end {
		for i := start; i <= end; {
			nums = append(nums, i)
			i += step
		}
	} else {
		for i := start; i >= end; {
			nums = append(nums, i)
			i -= step
		}
	}
	return nums
}
func NewWorkManager() *workManager {
	var fileopts []fileseq.FileOption
	if Options.AllFiles {
		fileopts = append(fileopts, fileseq.HiddenFiles)
	}
	if !Options.SeqsOnly {
		fileopts = append(fileopts, fileseq.SingleFiles)
	}

	s := &workManager{
		inDirs:   make(chan string),
		inSeqs:   make(chan *fileseq.FileSequence),
		outSeqs:  make(chan fileseq.FileSequences),
		fileOpts: fileopts,
	}
	return s
}
func (w *workManager) processSources() {
	var (
		ok   bool
		path string
		seq  *fileseq.FileSequence
	)

	fileopts := w.fileOpts

	inDirs := w.inDirs
	inSeqs := w.inSeqs
	outSeqs := w.outSeqs

	isDone := func() bool {
		return (inDirs == nil && inSeqs == nil)
	}

	for !isDone() {
		select {

		// Directory paths will be scanned for contents
		case path, ok = <-inDirs:
			if !ok {
				inDirs = nil
				continue
			}
			seqs, err := fileseq.FindSequencesOnDisk(path, fileopts...)
			if err != nil {
				fmt.Fprintf(errOut, "%s %q: %s\n", ErrorPath, path, err)
				continue
			}
			outSeqs <- seqs

		// Sequence paths will be scanned for a direct match
		// against the sequence pattern
		case seq, ok = <-inSeqs:
			if !ok {
				inSeqs = nil
				continue
			}

			path, err := seq.Format("{{dir}}{{base}}{{pad}}{{ext}}")
			if err != nil {
				fmt.Fprintf(errOut, "%s %q: Not a valid path\n", ErrorPattern, path)
				continue
			}

			seq, err := fileseq.FindSequenceOnDisk(path)
			if err != nil {
				if !os.IsNotExist(err) {
					fmt.Fprintf(errOut, "%s %q: %s\n", ErrorPattern, path, err)
				}
				continue
			}

			if seq != nil {
				outSeqs <- fileseq.FileSequences{seq}
			}
		}
	}
}
func (w *workManager) isInputDone() bool {
	if w.inDirs != nil {
		return false
	}
	if w.inSeqs != nil {
		return false
	}
	return true
}
func (w *workManager) closeInputs() {
	if w.inDirs != nil {
		close(w.inDirs)

	}
	if w.inSeqs != nil {
		close(w.inSeqs)
	}
}
func (w *workManager) load(paths []string) {
	dirs, seqs := preparePaths(paths)

	for _, s := range seqs {
		w.inSeqs <- s
	}

	for _, r := range dirs {
		w.inDirs <- r
	}
}
func (w *workManager) loadRecursive(paths []string) {

	walkFn := func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return nil
		}

		var isDir bool

		if info.IsDir() {
			isDir = true
		} else if info, err = os.Stat(path); err == nil && info.IsDir() {
			isDir = true
		}

		if isDir {

			if !Options.AllFiles {
				// Skip tranversing into hidden dirs
				if len(info.Name()) > 1 && strings.HasPrefix(info.Name(), ".") {
					return walk.SkipDir
				}
			}

			// Add the path to the input channel for sequence scanning
			w.inDirs <- path
		}

		return nil
	}

	dirs, seqs := preparePaths(paths)

	for _, s := range seqs {
		w.inSeqs <- s
	}

	for _, r := range dirs {
		r := r
		if err := walk.Walk(r, walkFn); err != nil {
			if err != walk.SkipDir {
				fmt.Fprintf(errOut, "%s %q: %s\n", ErrorPath, r, err)
			}
		}
	}
}
func preparePaths(paths []string) ([]string, fileseq.FileSequences) {
	var (
		fi  os.FileInfo
		err error
	)

	dirs := make([]string, 0)
	seqs := make(fileseq.FileSequences, 0)
	previous := make(map[string]struct{})

	for _, p := range paths {
		p := strings.TrimSpace(filepath.Clean(p))
		if p == "" {
			continue
		}

		if _, seen := previous[p]; seen {
			continue
		}
		previous[p] = struct{}{}

		if fi, err = os.Stat(p); err != nil {

			// If the path doesn't exist, test it for
			// a valid fileseq pattern
			if seq, err := fileseq.NewFileSequence(p); err == nil {
				seqs = append(seqs, seq)
				continue
			}

			fmt.Fprintf(errOut, "%s %q: %s\n", ErrorPath, p, err)
			continue
		}

		if !fi.IsDir() {
			continue
		}

		dirs = append(dirs, p)
	}

	return dirs, seqs
}
func PadFrameRange(frange string, pad int) string {
	// We don't need to do anything if they gave us
	// an invalid pad number
	if pad < 2 {
		return frange
	}

	size := strings.Count(frange, ",") + 1
	parts := make([]string, size, size)

	for i, part := range strings.Split(frange, ",") {

		didMatch := false

		for _, rx := range rangePatterns {
			matched := rx.FindStringSubmatch(part)
			if len(matched) == 0 {
				continue
			}
			matched = matched[1:]
			size = len(matched)
			switch size {
			case 1:
				parts[i] = zfillString(matched[0], pad)
			case 2:
				parts[i] = fmt.Sprintf("%s-%s",
					zfillString(matched[0], pad),
					zfillString(matched[1], pad))
			case 4:
				parts[i] = fmt.Sprintf("%s-%s%s%s",
					zfillString(matched[0], pad),
					zfillString(matched[1], pad),
					matched[2], matched[3])
			default:
				// No match. Try the next pattern
				continue
			}
			// If we got here, we matched a case and can stop
			// checking the rest of the patterns
			didMatch = true
			break
		}
		// If we didn't match one of our expected patterns
		// then just take the original part and add it unmodified
		if !didMatch {
			parts = append(parts, part)
		}
	}
	return strings.Join(parts, ",")
}
func zfillString(src string, z int) string {
	size := len(src)
	if size >= z {
		return src
	}

	fill := strings.Repeat("0", z-size)
	if strings.HasPrefix(src, "-") {
		return fmt.Sprintf("-%s%s", fill, src[1:])
	}
	return fmt.Sprintf("%s%s", fill, src)
}
func zfillInt(src int, z int) string {
	if z < 2 {
		return strconv.Itoa(src)
	}
	return fmt.Sprintf(fmt.Sprintf("%%0%dd", z), src)
}
func NewInclusiveRange(start, end, step int) *InclusiveRange {
	if step == 0 {
		if start <= end {
			step = 1

		} else {
			step = -1
		}
	}

	r := &InclusiveRange{
		start: start,
		end:   end,
		step:  step,
	}
	return r
}
func (r *InclusiveRange) String() string {
	var buf strings.Builder

	// Always for a single value
	buf.WriteString(strconv.Itoa(r.Start()))

	// If we have a range, express the end value
	if r.End() != r.Start() {
		buf.WriteString(`-`)
		buf.WriteString(strconv.Itoa(r.End()))

		// Express the stepping, if its not 1
		step := r.Step()
		if step > 1 || step < -1 {
			buf.WriteString(`x`)
			buf.WriteString(strconv.Itoa(r.Step()))
		}
	}
	return buf.String()
}
func (r *InclusiveRange) End() int {
	if r.isEndCached {
		return r.cachedEnd
	}

	r.isEndCached = true

	// If we aren't stepping, or we don't have
	// a full range, then just use the end value
	if r.step == 1 || r.step == -1 || r.start == r.end {
		r.cachedEnd = r.end
		return r.cachedEnd
	}

	// If the step is in the wrong direction,
	// compared to the range direction, then
	// just use the start as the end.
	if (r.end < r.start) && r.step < (r.end-r.start) {
		r.cachedEnd = r.start
		return r.cachedEnd

	} else if (r.end > r.start) && r.step > (r.end-r.start) {
		r.cachedEnd = r.start
		return r.cachedEnd
	}

	// Calculate the end, taking into account the stepping
	r.cachedEnd = r.closestInRange(r.end, r.start, r.end, r.step)
	return r.cachedEnd
}
func (r *InclusiveRange) Len() int {
	if r.isLenCached {
		return r.cachedLen
	}

	// Offset by one to include the end value
	diff := math.Abs(float64(r.end-r.start)) + 1
	r.cachedLen = int(math.Ceil(diff / math.Abs(float64(r.step))))
	r.isLenCached = true
	return r.cachedLen
}
func (r *InclusiveRange) Min() int {
	start := r.Start()
	end := r.End()
	if start < end {
		return start
	}
	return end
}
func (r *InclusiveRange) Max() int {
	start := r.Start()
	end := r.End()
	if start > end {
		return start
	}
	return end
}
func (r *InclusiveRange) Contains(value int) bool {
	// If we attempt to find the closest value, given
	// the start of the range and the step, we can check
	// if it is still the same number. If it hasn't changed,
	// then it is in the range.
	closest := r.closestInRange(value, r.start, r.End(), r.step)
	return closest == value
}
func (*InclusiveRange) closestInRange(value, start, end, step int) int {
	// Possibly clamp the value if it is outside the range
	if end >= start {
		if value < start {
			return start
		} else if value > end {
			return end
		}

	} else {
		if value > start {
			return start
		} else if value < end {
			return end
		}
	}

	// No calculation needed if there is no stepping
	if step == 1 || step == -1 {
		return value
	}

	// Modified the value so that it is a properly stepped
	// increment within the range
	return (((value - start) / step) * step) + start
}
func (f *InclusiveRange) Index(value int) int {
	closest := f.closestInRange(value, f.start, f.End(), f.step)
	if closest != value {
		return -1
	}
	idx := (value - f.start) / f.step
	if idx < 0 {
		idx *= -1
	}
	return idx
}
func (l *InclusiveRanges) String() string {
	var buf strings.Builder
	for i, b := range l.blocks {
		if i > 0 {
			buf.WriteString(`,`)
		}
		buf.WriteString(b.String())
	}
	return buf.String()
}
func (l *InclusiveRanges) Len() int {
	var totalLen int
	for _, b := range l.blocks {
		totalLen += b.Len()
	}
	return totalLen
}
func (l *InclusiveRanges) Start() int {
	for _, b := range l.blocks {
		return b.Start()
	}
	return 0
}
func (l *InclusiveRanges) End() int {
	if l.blocks == nil {
		return 0
	}
	return l.blocks[len(l.blocks)-1].End()
}
func (l *InclusiveRanges) Min() int {
	val := l.Start()
	for _, aRange := range l.blocks {
		next := aRange.Min()
		if next < val {
			val = next
		}
	}
	return val
}
func (l *InclusiveRanges) Max() int {
	val := l.End()
	for _, aRange := range l.blocks {
		next := aRange.Max()
		if next > val {
			val = next
		}
	}
	return val
}
func (l *InclusiveRanges) numRanges() int {
	if l.blocks == nil {
		return 0
	}
	return len(l.blocks)
}
func (l *InclusiveRanges) rangeAt(idx int) *InclusiveRange {
	if idx < 0 || idx >= l.numRanges() {
		return nil
	}
	return l.blocks[idx]
}
func (l *InclusiveRanges) Append(start, end, step int) {
	block := NewInclusiveRange(start, end, step)
	l.blocks = append(l.blocks, block)
}
func (l *InclusiveRanges) AppendUnique(start, end, step int) {
	if step == 0 {
		return
	}

	subStart := start
	subEnd := start
	subStep := step
	last := start
	pending := 0 // Track unique value count

	// Handle loop test for both increasing
	// and decreasing ranges
	var pred func() bool
	if start <= end {
		if step < 0 {
			step *= -1
		}
		pred = func() bool { return subEnd <= end }
	} else {
		if step > 0 {
			step *= -1
		}
		pred = func() bool { return subEnd >= end }
	}

	// Short-circuit if this is the first range being added
	if len(l.blocks) == 0 {
		l.Append(start, end, step)
		return
	}

	// TODO: More intelligent fast-paths for easy-to-identify
	// overlapping ranges. Such as when the existing range is:
	// 1-100x1 and we are appending 50-150x1. Should be easy
	// enough to just know we can Append(101,150,1)

	for ; pred(); subEnd += step {
		if !l.Contains(subEnd) {
			// Is a unique value in the range
			last = subEnd
			if pending == 0 {
				subStart = last
			}
			pending++
			continue
		}

		if pending == 0 {
			// Nothing to add yet
			continue
		}

		// Current value is already in range.
		// Add previous values
		l.Append(subStart, last, subStep)
		subStart = subEnd + step
		pending = 0
	}

	// Flush the remaining values
	if pending > 0 {
		l.Append(subStart, last, subStep)
	}
}
func (l *InclusiveRanges) Contains(value int) bool {
	for _, b := range l.blocks {
		if b.Contains(value) {
			return true
		}
	}
	return false
}
func (l *InclusiveRanges) Index(value int) int {
	var idx, n int

	for _, b := range l.blocks {
		// If the value is within the current block
		// then return the local index, offset by the
		// number of previous values we have tracked
		if idx = b.Index(value); idx >= 0 {
			return idx + n
		}
		// Update the offset for the values we have seen
		n += b.Len()
	}

	// The previous loop ended in error
	return -1
}
func (s *FileSequence) FrameRange() string {
	if s.frameSet == nil {
		return ""
	}
	return s.frameSet.FrameRange()
}
func (s *FileSequence) FrameRangePadded() string {
	if s.frameSet == nil {
		return ""
	}
	return s.frameSet.FrameRangePadded(s.zfill)
}
func (s *FileSequence) Index(idx int) string {
	if s.frameSet == nil {
		return s.String()
	}
	frame, err := s.frameSet.Frame(idx)
	if err != nil {
		return ""
	}
	path, err := s.Frame(frame)
	if err != nil {
		return ""
	}
	return path
}
func (s *FileSequence) SetDirname(dir string) {
	if !strings.HasSuffix(dir, string(filepath.Separator)) {
		dir = dir + string(filepath.Separator)
	}
	s.dir = dir
}
func (s *FileSequence) SetPadding(padChars string) {
	s.padChar = padChars
	s.zfill = s.padMapper.PaddingCharsSize(padChars)
}
func (s *FileSequence) SetPaddingStyle(style PadStyle) {
	s.padMapper = padders[style]
	s.SetPadding(s.padMapper.PaddingChars(s.ZFill()))
}
func (s *FileSequence) SetExt(ext string) {
	if !strings.HasPrefix(ext, ".") {
		ext = "." + ext
	}
	s.ext = ext
}
func (s *FileSequence) SetFrameRange(frameRange string) error {
	frameSet, err := NewFrameSet(frameRange)
	if err != nil {
		return err
	}
	s.frameSet = frameSet
	return nil
}
func (s *FileSequence) Len() int {
	if s.frameSet == nil {
		return 1
	}
	return s.frameSet.Len()
}
func (s *FileSequence) String() string {
	var fs string
	if s.frameSet != nil {
		fs = s.frameSet.String()
	}
	buf := bytes.NewBufferString(s.dir)
	buf.WriteString(s.basename)
	buf.WriteString(fs)
	buf.WriteString(s.padChar)
	buf.WriteString(s.ext)
	return buf.String()
}
func (s *FileSequence) Copy() *FileSequence {
	seq, _ := NewFileSequence(s.String())
	return seq
}
func NewClient(url string, tls bool, header interface{}) *Client {
	return &Client{
		url:    url,
		tls:    tls,
		header: header,
	}
}
func (h *Header) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {
	var (
		token xml.Token
		err   error
	)
Loop:
	for {
		if token, err = d.Token(); err != nil {
			return err
		}
		if token == nil {
			break
		}
		switch se := token.(type) {
		case xml.StartElement:
			if err = d.DecodeElement(h.Content, &se); err != nil {
				return err
			}
		case xml.EndElement:
			break Loop
		}
	}
	return nil
}
func (b *Body) UnmarshalXML(d *xml.Decoder, start xml.StartElement) error {
	if b.Content == nil {
		return xml.UnmarshalError("Content must be a pointer to a struct")
	}
	var (
		token    xml.Token
		err      error
		consumed bool
	)
Loop:
	for {
		if token, err = d.Token(); err != nil {
			return err
		}
		if token == nil {
			break
		}
		envelopeNameSpace := "http://schemas.xmlsoap.org/soap/envelope/"
		switch se := token.(type) {
		case xml.StartElement:
			if consumed {
				return xml.UnmarshalError(
					"Found multiple elements inside SOAP body; not wrapped-document/literal WS-I compliant")
			} else if se.Name.Space == envelopeNameSpace && se.Name.Local == "Fault" {
				b.Fault = &Fault{}
				b.Content = nil
				err = d.DecodeElement(b.Fault, &se)
				if err != nil {
					return err
				}
				consumed = true
			} else {
				if err = d.DecodeElement(b.Content, &se); err != nil {
					return err
				}
				consumed = true
			}
		case xml.EndElement:
			break Loop
		}
	}
	return nil
}
func (s *Client) Call(soapAction string, request, response, header interface{}) error {
	var envelope Envelope
	if s.header != nil {
		envelope = Envelope{
			Header: &Header{
				Content: s.header,
			},
			Body: Body{
				Content: request,
			},
		}
	} else {
		envelope = Envelope{
			Body: Body{
				Content: request,
			},
		}
	}
	buffer := new(bytes.Buffer)
	encoder := xml.NewEncoder(buffer)
	encoder.Indent("  ", "    ")
	if err := encoder.Encode(envelope); err != nil {
		return errors.Wrap(err, "failed to encode envelope")
	}
	if err := encoder.Flush(); err != nil {
		return errors.Wrap(err, "failed to flush encoder")
	}

	req, err := http.NewRequest("POST", s.url, buffer)
	if err != nil {
		return errors.Wrap(err, "failed to create POST request")
	}
	req.Header.Add("Content-Type", "text/xml; charset=\"utf-8\"")
	req.Header.Set("SOAPAction", soapAction)
	req.Header.Set("User-Agent", s.userAgent)
	req.Close = true

	tr := &http.Transport{
		TLSClientConfig: &tls.Config{
			InsecureSkipVerify: s.tls,
		},
		Dial: dialTimeout,
	}

	client := &http.Client{Transport: tr}
	res, err := client.Do(req)
	if err != nil {
		return errors.Wrap(err, "failed to send SOAP request")
	}
	defer res.Body.Close()
	if res.StatusCode != http.StatusOK {
		soapFault, err := ioutil.ReadAll(res.Body)
		if err != nil {
			return errors.Wrap(err, "failed to read SOAP fault response body")
		}
		msg := fmt.Sprintf("HTTP Status Code: %d, SOAP Fault: \n%s", res.StatusCode, string(soapFault))
		return errors.New(msg)
	}

	rawbody, err := ioutil.ReadAll(res.Body)
	if err != nil {
		return errors.Wrap(err, "failed to read SOAP body")
	}
	if len(rawbody) == 0 {
		return nil
	}
	respEnvelope := Envelope{}
	respEnvelope.Body = Body{Content: response}
	if header != nil {
		respEnvelope.Header = &Header{Content: header}
	}

	if err = xml.Unmarshal(rawbody, &respEnvelope); err != nil {
		return errors.Wrap(err, "failed to unmarshal response SOAP Envelope")
	}
	return nil
}
func JSONDoc(path string) (json.RawMessage, error) {
	data, err := swag.LoadFromFileOrHTTP(path)
	if err != nil {
		return nil, err
	}
	return json.RawMessage(data), nil
}
func AddLoader(predicate DocMatcher, load DocLoader) {
	prev := loaders
	loaders = &loader{
		Match: predicate,
		Fn:    load,
		Next:  prev,
	}
	spec.PathLoader = loaders.Fn
}
func JSONSpec(path string) (*Document, error) {
	data, err := JSONDoc(path)
	if err != nil {
		return nil, err
	}
	// convert to json
	return Analyzed(data, "")
}
func Embedded(orig, flat json.RawMessage) (*Document, error) {
	var origSpec, flatSpec spec.Swagger
	if err := json.Unmarshal(orig, &origSpec); err != nil {
		return nil, err
	}
	if err := json.Unmarshal(flat, &flatSpec); err != nil {
		return nil, err
	}
	return &Document{
		raw:      orig,
		origSpec: &origSpec,
		spec:     &flatSpec,
	}, nil
}
func Spec(path string) (*Document, error) {
	specURL, err := url.Parse(path)
	if err != nil {
		return nil, err
	}
	var lastErr error
	for l := loaders.Next; l != nil; l = l.Next {
		if loaders.Match(specURL.Path) {
			b, err2 := loaders.Fn(path)
			if err2 != nil {
				lastErr = err2
				continue
			}
			doc, err3 := Analyzed(b, "")
			if err3 != nil {
				return nil, err3
			}
			if doc != nil {
				doc.specFilePath = path
			}
			return doc, nil
		}
	}
	if lastErr != nil {
		return nil, lastErr
	}
	b, err := defaultLoader.Fn(path)
	if err != nil {
		return nil, err
	}

	document, err := Analyzed(b, "")
	if document != nil {
		document.specFilePath = path
	}

	return document, err
}
func Analyzed(data json.RawMessage, version string) (*Document, error) {
	if version == "" {
		version = "2.0"
	}
	if version != "2.0" {
		return nil, fmt.Errorf("spec version %q is not supported", version)
	}

	raw := data
	trimmed := bytes.TrimSpace(data)
	if len(trimmed) > 0 {
		if trimmed[0] != '{' && trimmed[0] != '[' {
			yml, err := swag.BytesToYAMLDoc(trimmed)
			if err != nil {
				return nil, fmt.Errorf("analyzed: %v", err)
			}
			d, err := swag.YAMLToJSON(yml)
			if err != nil {
				return nil, fmt.Errorf("analyzed: %v", err)
			}
			raw = d
		}
	}

	swspec := new(spec.Swagger)
	if err := json.Unmarshal(raw, swspec); err != nil {
		return nil, err
	}

	origsqspec, err := cloneSpec(swspec)
	if err != nil {
		return nil, err
	}

	d := &Document{
		Analyzer: analysis.New(swspec),
		schema:   spec.MustLoadSwagger20Schema(),
		spec:     swspec,
		raw:      raw,
		origSpec: origsqspec,
	}
	return d, nil
}
func (d *Document) Expanded(options ...*spec.ExpandOptions) (*Document, error) {
	swspec := new(spec.Swagger)
	if err := json.Unmarshal(d.raw, swspec); err != nil {
		return nil, err
	}

	var expandOptions *spec.ExpandOptions
	if len(options) > 0 {
		expandOptions = options[0]
	} else {
		expandOptions = &spec.ExpandOptions{
			RelativeBase: d.specFilePath,
		}
	}

	if err := spec.ExpandSpec(swspec, expandOptions); err != nil {
		return nil, err
	}

	dd := &Document{
		Analyzer:     analysis.New(swspec),
		spec:         swspec,
		specFilePath: d.specFilePath,
		schema:       spec.MustLoadSwagger20Schema(),
		raw:          d.raw,
		origSpec:     d.origSpec,
	}
	return dd, nil
}
func (d *Document) ResetDefinitions() *Document {
	defs := make(map[string]spec.Schema, len(d.origSpec.Definitions))
	for k, v := range d.origSpec.Definitions {
		defs[k] = v
	}

	d.spec.Definitions = defs
	return d
}
func (d *Document) Pristine() *Document {
	dd, _ := Analyzed(d.Raw(), d.Version())
	return dd
}
func OpenDb(files []string, flag int) (*GeoIP, error) {
	if len(files) == 0 {
		files = []string{
			"/usr/share/GeoIP/GeoIP.dat",       // Linux default
			"/usr/share/local/GeoIP/GeoIP.dat", // source install?
			"/usr/local/share/GeoIP/GeoIP.dat", // FreeBSD
			"/opt/local/share/GeoIP/GeoIP.dat", // MacPorts
			"/usr/share/GeoIP/GeoIP.dat",       // ArchLinux
		}
	}

	g := &GeoIP{}
	runtime.SetFinalizer(g, (*GeoIP).free)

	var err error

	for _, file := range files {

		// libgeoip prints errors if it can't open the file, so check first
		if _, err := os.Stat(file); err != nil {
			if os.IsExist(err) {
				log.Println(err)
			}
			continue
		}

		cbase := C.CString(file)
		defer C.free(unsafe.Pointer(cbase))

		g.db, err = C.GeoIP_open(cbase, C.int(flag))
		if g.db != nil && err != nil {
			break
		}
	}
	if err != nil {
		return nil, fmt.Errorf("Error opening GeoIP database (%s): %s", files, err)
	}

	if g.db == nil {
		return nil, fmt.Errorf("Didn't open GeoIP database (%s)", files)
	}

	C.GeoIP_set_charset(g.db, C.GEOIP_CHARSET_UTF8)
	return g, nil
}
func (gi *GeoIP) GetOrg(ip string) string {
	name, _ := gi.GetName(ip)
	return name
}
func (gi *GeoIP) GetRegion(ip string) (string, string) {
	if gi.db == nil {
		return "", ""
	}

	cip := C.CString(ip)
	defer C.free(unsafe.Pointer(cip))

	gi.mu.Lock()
	region := C.GeoIP_region_by_addr(gi.db, cip)
	gi.mu.Unlock()

	if region == nil {
		return "", ""
	}

	countryCode := C.GoString(&region.country_code[0])
	regionCode := C.GoString(&region.region[0])
	defer C.free(unsafe.Pointer(region))

	return countryCode, regionCode
}
func GetRegionName(countryCode, regionCode string) string {

	cc := C.CString(countryCode)
	defer C.free(unsafe.Pointer(cc))

	rc := C.CString(regionCode)
	defer C.free(unsafe.Pointer(rc))

	region := C.GeoIP_region_name_by_code(cc, rc)
	if region == nil {
		return ""
	}

	// it's a static string constant, don't free this
	regionName := C.GoString(region)

	return regionName
}
func (gi *GeoIP) GetCountry(ip string) (cc string, netmask int) {
	if gi.db == nil {
		return
	}

	gi.mu.Lock()
	defer gi.mu.Unlock()

	cip := C.CString(ip)
	defer C.free(unsafe.Pointer(cip))
	ccountry := C.GeoIP_country_code_by_addr(gi.db, cip)

	if ccountry != nil {
		cc = C.GoString(ccountry)
		netmask = int(C.GeoIP_last_netmask(gi.db))
		return
	}
	return
}
func NewRotatingFileHandler(fileName string, maxBytes int, backupCount int) (*RotatingFileHandler, error) {
	dir := path.Dir(fileName)
	os.MkdirAll(dir, 0777)

	h := new(RotatingFileHandler)

	if maxBytes <= 0 {
		return nil, fmt.Errorf("invalid max bytes")
	}

	h.fileName = fileName
	h.maxBytes = maxBytes
	h.backupCount = backupCount

	var err error
	h.fd, err = os.OpenFile(fileName, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)
	if err != nil {
		return nil, err
	}

	f, err := h.fd.Stat()
	if err != nil {
		return nil, err
	}
	h.curBytes = int(f.Size())

	return h, nil
}
func (h *RotatingFileHandler) Close() error {
	if h.fd != nil {
		return h.fd.Close()
	}
	return nil
}
func (l Level) String() string {
	switch l {
	case LevelTrace:
		return "trace"
	case LevelDebug:
		return "debug"
	case LevelInfo:
		return "info"
	case LevelWarn:
		return "warn"
	case LevelError:
		return "error"
	case LevelFatal:
		return "fatal"
	}
	// return default info
	return "info"
}
func New(handler Handler, flag int) *Logger {
	var l = new(Logger)

	l.level = LevelInfo
	l.handler = handler

	l.flag = flag

	l.bufs = sync.Pool{
		New: func() interface{} {
			return make([]byte, 0, 1024)
		},
	}

	return l
}
func (l *Logger) Close() {
	l.hLock.Lock()
	defer l.hLock.Unlock()
	l.handler.Close()
}
func (l *Logger) SetLevelByName(name string) {
	level := LevelInfo
	switch strings.ToLower(name) {
	case "trace":
		level = LevelTrace
	case "debug":
		level = LevelDebug
	case "warn", "warning":
		level = LevelWarn
	case "error":
		level = LevelError
	case "fatal":
		level = LevelFatal
	default:
		level = LevelInfo
	}

	l.SetLevel(level)
}
func (l *Logger) Output(callDepth int, level Level, msg string) {
	if l.level > level {
		return
	}

	buf := l.bufs.Get().([]byte)
	buf = buf[0:0]
	defer l.bufs.Put(buf)

	if l.flag&Ltime > 0 {
		now := time.Now().Format(timeFormat)
		buf = append(buf, '[')
		buf = append(buf, now...)
		buf = append(buf, "] "...)
	}

	if l.flag&Llevel > 0 {
		buf = append(buf, '[')
		buf = append(buf, level.String()...)
		buf = append(buf, "] "...)
	}

	if l.flag&Lfile > 0 {
		_, file, line, ok := runtime.Caller(callDepth)
		if !ok {
			file = "???"
			line = 0
		} else {
			for i := len(file) - 1; i > 0; i-- {
				if file[i] == '/' {
					file = file[i+1:]
					break
				}
			}
		}

		buf = append(buf, file...)
		buf = append(buf, ':')

		buf = strconv.AppendInt(buf, int64(line), 10)
		buf = append(buf, ' ')
	}

	buf = append(buf, msg...)
	if len(msg) == 0 || msg[len(msg)-1] != '\n' {
		buf = append(buf, '\n')
	}

	l.hLock.Lock()
	l.handler.Write(buf)
	l.hLock.Unlock()
}
func (l *Logger) OutputJson(callDepth int, level Level, body interface{}) {
	if l.level > level {
		return
	}

	buf := l.bufs.Get().([]byte)
	buf = buf[0:0]
	defer l.bufs.Put(buf)

	type JsonLog struct {
		Time string `json:"log_time"`
		Level string `json:"log_level"`
		File string `json:"log_file"`
		Line string `json:"log_line"`
		Body interface{} `json:"log_body"`
	}

	var jsonlog JsonLog
	if l.flag&Ltime > 0 {
		now := time.Now().Format(timeFormat)
		jsonlog.Time = now
	}

	if l.flag&Llevel > 0 {
		jsonlog.Level = level.String()
	}

	if l.flag&Lfile > 0 {
		_, file, line, ok := runtime.Caller(callDepth)
		if !ok {
			file = "???"
			line = 0
		} else {
			for i := len(file) - 1; i > 0; i-- {
				if file[i] == '/' {
					file = file[i+1:]
					break
				}
			}
		}

		jsonlog.File = file
		jsonlog.Line = string(strconv.AppendInt(buf, int64(line), 10))
	}

	jsonlog.Body = body

	msg, _ := json.Marshal(jsonlog)
	msg = append(msg, '\n')

	l.hLock.Lock()
	l.handler.Write(msg)
	l.hLock.Unlock()
}
func (l *Logger) Print(args ...interface{}) {
	l.Output(2, LevelTrace, fmt.Sprint(args...))
}
func (l *Logger) Println(args ...interface{}) {
	l.Output(2, LevelTrace, fmt.Sprintln(args...))
}
func (l *Logger) Debug(args ...interface{}) {
	l.Output(2, LevelDebug, fmt.Sprint(args...))
}
func (l *Logger) Debugln(args ...interface{}) {
	l.Output(2, LevelDebug, fmt.Sprintln(args...))
}
func (l *Logger) Error(args ...interface{}) {
	l.Output(2, LevelError, fmt.Sprint(args...))
}
func (l *Logger) Errorln(args ...interface{}) {
	l.Output(2, LevelError, fmt.Sprintln(args...))
}
func (l *Logger) Info(args ...interface{}) {
	l.Output(2, LevelInfo, fmt.Sprint(args...))
}
func (l *Logger) Infoln(args ...interface{}) {
	l.Output(2, LevelInfo, fmt.Sprintln(args...))
}
func (l *Logger) Warn(args ...interface{}) {
	l.Output(2, LevelWarn, fmt.Sprint(args...))
}
func (l *Logger) Warnln(args ...interface{}) {
	l.Output(2, LevelWarn, fmt.Sprintln(args...))
}
func NewStreamHandler(w io.Writer) (*StreamHandler, error) {
	h := new(StreamHandler)

	h.w = w

	return h, nil
}
func Right(str string, length int, pad string) string {
	return str + times(pad, length-len(str))
}
func New(h string, a rsapi.Authenticator) *API {
	api := rsapi.New(h, a)
	api.Metadata = GenMetadata
	return &API{API: api}
}
func setupMetadata() (result map[string]*metadata.Resource) {
	result = make(map[string]*metadata.Resource)
	for n, r := range ssd.GenMetadata {
		result[n] = r
		for _, a := range r.Actions {
			for _, p := range a.PathPatterns {
				// remove "/api/designer" prefix
				p.Regexp = removePrefixes(p.Regexp, 2)
			}
		}
	}
	for n, r := range ssc.GenMetadata {
		result[n] = r
		for _, a := range r.Actions {
			for _, p := range a.PathPatterns {
				// remove "/api/catalog" prefix
				p.Regexp = removePrefixes(p.Regexp, 2)
			}
		}
	}
	for n, r := range ssm.GenMetadata {
		result[n] = r
		for _, a := range r.Actions {
			for _, p := range a.PathPatterns {
				// remove "/api/manager" prefix
				p.Regexp = removePrefixes(p.Regexp, 2)
			}
		}
	}
	return
}
func (p *ParamAnalyzer) recordTypes(root gen.DataType) {
	if o, ok := root.(*gen.ObjectDataType); ok {
		if _, found := p.ParamTypes[o.TypeName]; !found {
			p.ParamTypes[o.TypeName] = o
			for _, f := range o.Fields {
				p.recordTypes(f.Type)
			}
		}
	} else if a, ok := root.(*gen.ArrayDataType); ok {
		p.recordTypes(a.ElemType.Type)
	}
}
func appendSorted(params []*gen.ActionParam, param *gen.ActionParam) []*gen.ActionParam {
	params = append(params, param)
	sort.Sort(gen.ByName(params))
	return params
}
func (p *ParamAnalyzer) parseDataType(path string, child *gen.ActionParam) gen.DataType {
	param := p.rawParams[path].(map[string]interface{})
	class := "String"
	if c, ok := param["class"].(string); ok {
		class = c
	}
	var res gen.DataType
	switch class {
	case "Integer":
		i := gen.BasicDataType("int")
		res = &i
	case "String":
		s := gen.BasicDataType("string")
		res = &s
	case "Array":
		if child != nil {
			res = &gen.ArrayDataType{child}
		} else {
			s := gen.BasicDataType("string")
			p := p.newParam(fmt.Sprintf("%s[item]", path),
				map[string]interface{}{}, &s)
			res = &gen.ArrayDataType{p}
		}
	case "SourceUpload":
		res = new(gen.SourceUploadDataType)
	case "FileUpload", "Tempfile":
		res = &gen.UploadDataType{TypeName: "FileUpload"}
	case "Enumerable":
		res = new(gen.EnumerableDataType)
	case "Hash":
		if current, ok := p.parsed[path]; ok {
			res = current.Type
			o := res.(*gen.ObjectDataType)
			o.Fields = appendSorted(o.Fields, child)
		} else {
			oname := p.typeName(path)
			res = &gen.ObjectDataType{oname, []*gen.ActionParam{child}}
		}
	}
	return res
}
func (p *ParamAnalyzer) parseParam(path string, param map[string]interface{}, child *gen.ActionParam) *gen.ActionParam {
	dType := p.parseDataType(path, child)
	return p.newParam(path, param, dType)
}
func (p *ParamAnalyzer) newParam(path string, param map[string]interface{}, dType gen.DataType) *gen.ActionParam {
	var description, regexp string
	var mandatory, nonBlank bool
	var validValues []interface{}
	if d, ok := param["description"]; ok {
		description = d.(string)
	}
	if m, ok := param["mandatory"]; ok {
		mandatory = m.(bool)
	}
	if n, ok := param["non_blank"]; ok {
		nonBlank = n.(bool)
	}
	if r, ok := param["regexp"]; ok {
		regexp = r.(string)
	}
	if v, ok := param["valid_values"]; ok {
		validValues = v.([]interface{})
	}
	native := nativeNameFromPath(path)
	isLeaf := false
	if _, ok := dType.(*gen.EnumerableDataType); ok {
		isLeaf = true
	} else {
		for _, l := range p.leafParamNames {
			if path == l {
				isLeaf = true
				break
			}
		}
	}
	queryName := path
	if _, ok := dType.(*gen.ArrayDataType); ok {
		queryName += "[]"
	}
	actionParam := &gen.ActionParam{
		Name:        native,
		QueryName:   queryName,
		Description: removeBlankLines(description),
		VarName:     parseParamName(native),
		Type:        dType,
		Mandatory:   mandatory,
		NonBlank:    nonBlank,
		Regexp:      regexp,
		ValidValues: validValues,
	}
	if isLeaf {
		p.LeafParams = append(p.LeafParams, actionParam)
	}
	return actionParam
}
func toGoReturnTypeName(name string, slice bool) string {
	slicePrefix := ""
	if slice {
		slicePrefix = "[]"
	}
	return fmt.Sprintf("%s*%s", slicePrefix, toGoTypeName(name))
}
func toGoTypeName(name string) string {
	switch name {
	case "String", "Symbol":
		return "string"
	case "Integer":
		return "int"
	case "Boolean":
		return "bool"
	case "Struct", "Collection":
		panic("Uh oh, trying to infer a go type name for a unnamed struct or collection (" + name + ")")
	default:
		if strings.Contains(name, "::") {
			elems := strings.Split(name, "::")
			return strings.Join(elems[2:len(elems)], "")
		}
		return name
	}

}
func prettify(o interface{}) string {
	s, err := json.MarshalIndent(o, "", "    ")
	if err != nil {
		return fmt.Sprintf("%+v", o)
	}
	return string(s)
}
func isBuiltInType(name string) bool {
	for _, n := range BuiltInTypes {
		if name == n {
			return true
		}
	}
	return false
}
func (a *Action) MatchHref(href string) bool {
	hrefs := []string{href, href + "/"}
	for _, pattern := range a.PathPatterns {
		for _, href := range hrefs {
			indices := pattern.Regexp.FindStringIndex(href)
			// it is only an exact match if the pattern matches from the beginning to the length of the string
			if indices != nil && indices[0] == 0 && indices[1] == len(href) {
				return true
			}
		}
	}
	return false
}
func (p *PathPattern) Substitute(vars []*PathVariable) (string, []string) {
	values := make([]interface{}, len(p.Variables))
	var missing []string
	var used []string
	for i, n := range p.Variables {
		for _, v := range vars {
			if v.Name == n {
				values[i] = v.Value
				used = append(used, n)
				break
			}
		}
		if values[i] == nil {
			missing = append(missing, n)
		}
	}
	if len(missing) > 0 {
		return "", missing
	}
	return fmt.Sprintf(p.Pattern, values...), used
}
func (f *FileUpload) MarshalJSON() ([]byte, error) {
	b, err := ioutil.ReadAll(f.Reader)
	if err != nil {
		return nil, err
	}
	return json.Marshal(string(b))
}
func writeMultipartParams(w *multipart.Writer, payload APIParams, prefix string) error {
	for k, v := range payload {
		fieldName := k
		if prefix != "" {
			fieldName = fmt.Sprintf("%s[%s]", prefix, k)
		}
		// Add more types as needed. These two cover both CM15 and SS.
		switch v.(type) {
		case string:
			err := w.WriteField(fieldName, v.(string))
			if err != nil {
				return err
			}
		case APIParams:
			err := writeMultipartParams(w, v.(APIParams), fieldName)
			if err != nil {
				return err
			}
		default:
			return fmt.Errorf("Unknown type for multipart form section %s: %#v", fieldName, v)
		}
	}
	return nil
}
func (a *API) PerformRequest(req *http.Request) (*http.Response, error) {
	// Sign last so auth headers don't get printed or logged
	if a.Auth != nil {
		if err := a.Auth.Sign(req); err != nil {
			return nil, err
		}
	}
	resp, err := a.Client.Do(req)
	if err != nil {
		return nil, err
	}

	return resp, err
}
func (a *API) PerformRequestWithContext(ctx context.Context, req *http.Request) (*http.Response, error) {
	// Sign last so auth headers don't get printed or logged
	if a.Auth != nil {
		if err := a.Auth.Sign(req); err != nil {
			return nil, err
		}
	}
	resp, err := a.Client.DoWithContext(ctx, req)
	if err != nil {
		return nil, err
	}

	return resp, err
}
func (a *API) LoadResponse(resp *http.Response) (interface{}, error) {
	defer resp.Body.Close()
	var respBody interface{}
	jsonResp, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("Failed to read response (%s)", err)
	}
	if len(jsonResp) > 0 {
		err = json.Unmarshal(jsonResp, &respBody)
		if err != nil {
			return nil, fmt.Errorf("Failed to load response (%s)", err)
		}
	}
	// Special case for "Location" header, assume that if there is a location there is no body
	loc := resp.Header.Get("Location")
	if len(loc) > 0 {
		var bodyMap = make(map[string]interface{})
		bodyMap["Location"] = loc
		respBody = interface{}(bodyMap)
	}
	return respBody, err
}
func (api *API) ScheduleLocator(href string) *ScheduleLocator {
	return &ScheduleLocator{Href(href), api}
}
func (api *API) TemplateLocator(href string) *TemplateLocator {
	return &TemplateLocator{Href(href), api}
}
func fetchDetails(client *cm15.API, envName string, envDetail EnvironmentDetail, sshConfig *[]SSHConfig) {
	for nickname, name := range envDetail.ServerArrays {
		// Obtain the resource
		instances := serverArray(client, name)
		// Obtain the IP address of the first instance (only one instance is considered here -- for now)
		for _, instance := range instances {
			ipAddress := instance.PublicIpAddresses[0]
			number := getInstanceNumber(instance.Name)
			*sshConfig = append(*sshConfig, SSHConfig{Name: envName + "_" + nickname + number, IPAddress: ipAddress})
		}
	}
	for nickname, name := range envDetail.Servers {
		instance := server(client, name)
		ipAddress := instance.PublicIpAddresses[0]
		*sshConfig = append(*sshConfig, SSHConfig{Name: envName + "_" + nickname, IPAddress: ipAddress})
	}
}
func buildAliases(sshConfig []SSHConfig, sshOptions, sshUser string) string {
	var aliases string
	for _, conf := range sshConfig {
		aliases = aliases + fmt.Sprintf("alias %v='ssh %v %v@%v'\n", conf.Name, sshOptions, sshUser, conf.IPAddress)
	}
	return aliases
}
func serverArray(client *cm15.API, name string) []*cm15.Instance {
	serverArrayLocator := client.ServerArrayLocator("/api/server_arrays")
	serverArrays, err := serverArrayLocator.Index(rsapi.APIParams{"view": "default", "filter": []string{"name==" + name}})
	if err != nil {
		fail("Failed to retrieve server array: %v\n", err.Error())
	}
	if len(serverArrays) == 0 {
		fail("Could not find server array with name: %v\n", name)
	} else if len(serverArrays) != 1 {
		fail("More than one server array found with name: %v\n", name)
	}
	array := serverArrays[0]
	var instancesHref string
	for _, l := range array.Links {
		if l["rel"] == "current_instances" {
			instancesHref = l["href"]
			break
		}
	}
	instanceLocator := client.InstanceLocator(instancesHref)
	instances, err := instanceLocator.Index(rsapi.APIParams{})
	if err != nil {
		fail("Failed to retrieve current instances of the server array: %v\n", err.Error())
	}
	if len(instances) == 0 {
		fail("No instances found in server array: %v\n", name)
	}
	return instances
}
func server(client *cm15.API, name string) *cm15.Instance {
	serverLocator := client.ServerLocator("/api/servers")
	servers, err := serverLocator.Index(rsapi.APIParams{"view": "instance_detail", "filter": []string{"name==" + name}})
	if err != nil {
		fail("Failed to retrieve server: %v\n", err.Error())
	}
	if len(servers) == 0 {
		fail("Could not find server with name: %v\n", name)
	} else if len(servers) != 1 {
		fail("More than one server found with name: %v\n", name)
	}
	return servers[0].CurrentInstance
}
func toPackageName(version string) string {
	if version == "unversioned" {
		return "v0"
	}
	parts := strings.Split(version, ".")
	i := 1
	p := parts[len(parts)-i]
	for p == "0" && i <= len(parts) {
		i++
		p = parts[len(parts)-i]
	}
	version = strings.Join(parts, "_")
	return fmt.Sprintf("v%s", version)
}
func loadFile(file string) ([]byte, error) {
	if _, err := os.Stat(file); err != nil {
		return nil, fmt.Errorf("Cannot find '%s'", file)
	}
	js, err := ioutil.ReadFile(file)
	if err != nil {
		return nil, fmt.Errorf("Cannot read '%s': %s", file, err)
	}
	return js, nil
}
func main() {
	app := kingpin.New("rsc", "A RightScale API client")
	app.Writer(os.Stdout)
	app.Version(VV)

	cmdLine, err := ParseCommandLine(app)
	if err != nil {
		line := strings.Join(os.Args, " ")
		PrintFatal("%s: %s", line, err.Error())
	}

	resp, err := ExecuteCommand(app, cmdLine)
	if err != nil {
		PrintFatal("%s", err.Error())
	}
	if resp == nil {
		return // No results, just exit (e.g. setup, printed help...)
	}

	var notExactlyOneError bool
	displayer, err := NewDisplayer(resp)
	if err != nil {
		PrintFatal("%s", err.Error())
	}
	if resp.StatusCode < 200 || resp.StatusCode > 299 {
		// Let user know if something went wrong
		fmt.Fprintln(errOut, resp.Status)
		if len(displayer.body) > 0 {
			fmt.Fprintln(errOut, displayer.body)
		}
	} else if cmdLine.ExtractOneSelect != "" {
		err = displayer.ApplySingleExtract(cmdLine.ExtractOneSelect)
		if err != nil {
			notExactlyOneError = strings.Contains(err.Error(),
				"instead of one value") // Ugh, there has to be a better way
			PrintError(err.Error())
		}
		fmt.Fprint(out, displayer.Output())
	} else {
		if cmdLine.ExtractSelector != "" {
			err = displayer.ApplyExtract(cmdLine.ExtractSelector, false)
		} else if cmdLine.ExtractSelectorJSON != "" {
			err = displayer.ApplyExtract(cmdLine.ExtractSelectorJSON, true)
		} else if cmdLine.ExtractHeader != "" {
			err = displayer.ApplyHeaderExtract(cmdLine.ExtractHeader)
		}
		if err != nil {
			PrintFatal("%s", err.Error())
		} else if cmdLine.Pretty {
			displayer.Pretty()
		}
		fmt.Fprint(out, displayer.Output())
	}
	// Figure out exit code
	exitStatus := 0
	switch {
	case notExactlyOneError:
		exitStatus = 6
	case resp.StatusCode == 401:
		exitStatus = 1
	case resp.StatusCode == 403:
		exitStatus = 3
	case resp.StatusCode == 404:
		exitStatus = 4
	case resp.StatusCode > 399 && resp.StatusCode < 500:
		exitStatus = 2
	case resp.StatusCode > 499:
		exitStatus = 5
	}
	//fmt.Fprintf(os.Stderr, "exitStatus=%d\n", exitStatus)
	osExit(exitStatus)
}
func runCommand(client cmd.CommandClient, cmdLine *cmd.CommandLine) (resp *http.Response, err error) {
	cmds := strings.Split(cmdLine.Command, " ")
	if cmdLine.ShowHelp {
		err = client.ShowCommandHelp(cmdLine.Command)
	} else if len(cmds) > 1 && cmds[1] == "actions" {
		err = client.ShowAPIActions(cmdLine.Command)
	} else {
		resp, err = client.RunCommand(cmdLine.Command)
	}
	return
}
func CreateJSONResponse(b []byte) (resp *http.Response) {
	// Remove UTF-8 Byte Order Mark if it exists
	b = bytes.TrimPrefix(b, []byte{0xef, 0xbb, 0xbf})
	resp = &http.Response{
		StatusCode: 200,
		Body:       ioutil.NopCloser(bytes.NewBuffer(b)),
	}
	return resp
}
func (api *API) AccountLocator(href string) *AccountLocator {
	return &AccountLocator{Href(href), api}
}
func (api *API) AccountGroupLocator(href string) *AccountGroupLocator {
	return &AccountGroupLocator{Href(href), api}
}
func (api *API) AlertLocator(href string) *AlertLocator {
	return &AlertLocator{Href(href), api}
}
func (api *API) AlertSpecLocator(href string) *AlertSpecLocator {
	return &AlertSpecLocator{Href(href), api}
}
func (api *API) AuditEntryLocator(href string) *AuditEntryLocator {
	return &AuditEntryLocator{Href(href), api}
}
func (api *API) BackupLocator(href string) *BackupLocator {
	return &BackupLocator{Href(href), api}
}
func (api *API) ChildAccountLocator(href string) *ChildAccountLocator {
	return &ChildAccountLocator{Href(href), api}
}
func (api *API) CloudLocator(href string) *CloudLocator {
	return &CloudLocator{Href(href), api}
}
func (api *API) CloudAccountLocator(href string) *CloudAccountLocator {
	return &CloudAccountLocator{Href(href), api}
}
func (api *API) CookbookLocator(href string) *CookbookLocator {
	return &CookbookLocator{Href(href), api}
}
func (api *API) CookbookAttachmentLocator(href string) *CookbookAttachmentLocator {
	return &CookbookAttachmentLocator{Href(href), api}
}
func (api *API) CredentialLocator(href string) *CredentialLocator {
	return &CredentialLocator{Href(href), api}
}
func (api *API) DatacenterLocator(href string) *DatacenterLocator {
	return &DatacenterLocator{Href(href), api}
}
func (api *API) DeploymentLocator(href string) *DeploymentLocator {
	return &DeploymentLocator{Href(href), api}
}
func (api *API) HealthCheckLocator(href string) *HealthCheckLocator {
	return &HealthCheckLocator{Href(href), api}
}
func (api *API) IdentityProviderLocator(href string) *IdentityProviderLocator {
	return &IdentityProviderLocator{Href(href), api}
}
func (api *API) ImageLocator(href string) *ImageLocator {
	return &ImageLocator{Href(href), api}
}
func (api *API) InputLocator(href string) *InputLocator {
	return &InputLocator{Href(href), api}
}
func (api *API) InstanceLocator(href string) *InstanceLocator {
	return &InstanceLocator{Href(href), api}
}
func (api *API) InstanceTypeLocator(href string) *InstanceTypeLocator {
	return &InstanceTypeLocator{Href(href), api}
}
func (api *API) IpAddressLocator(href string) *IpAddressLocator {
	return &IpAddressLocator{Href(href), api}
}
func (api *API) IpAddressBindingLocator(href string) *IpAddressBindingLocator {
	return &IpAddressBindingLocator{Href(href), api}
}
func (api *API) MonitoringMetricLocator(href string) *MonitoringMetricLocator {
	return &MonitoringMetricLocator{Href(href), api}
}
func (api *API) MultiCloudImageLocator(href string) *MultiCloudImageLocator {
	return &MultiCloudImageLocator{Href(href), api}
}
func (api *API) MultiCloudImageMatcherLocator(href string) *MultiCloudImageMatcherLocator {
	return &MultiCloudImageMatcherLocator{Href(href), api}
}
func (api *API) MultiCloudImageSettingLocator(href string) *MultiCloudImageSettingLocator {
	return &MultiCloudImageSettingLocator{Href(href), api}
}
func (api *API) NetworkLocator(href string) *NetworkLocator {
	return &NetworkLocator{Href(href), api}
}
func (api *API) NetworkGatewayLocator(href string) *NetworkGatewayLocator {
	return &NetworkGatewayLocator{Href(href), api}
}
func (api *API) NetworkOptionGroupLocator(href string) *NetworkOptionGroupLocator {
	return &NetworkOptionGroupLocator{Href(href), api}
}
func (api *API) NetworkOptionGroupAttachmentLocator(href string) *NetworkOptionGroupAttachmentLocator {
	return &NetworkOptionGroupAttachmentLocator{Href(href), api}
}
func (api *API) Oauth2Locator(href string) *Oauth2Locator {
	return &Oauth2Locator{Href(href), api}
}
func (api *API) PermissionLocator(href string) *PermissionLocator {
	return &PermissionLocator{Href(href), api}
}
func (api *API) PlacementGroupLocator(href string) *PlacementGroupLocator {
	return &PlacementGroupLocator{Href(href), api}
}
func (api *API) PreferenceLocator(href string) *PreferenceLocator {
	return &PreferenceLocator{Href(href), api}
}
func (api *API) PublicationLocator(href string) *PublicationLocator {
	return &PublicationLocator{Href(href), api}
}
func (api *API) PublicationLineageLocator(href string) *PublicationLineageLocator {
	return &PublicationLineageLocator{Href(href), api}
}
func (api *API) RecurringVolumeAttachmentLocator(href string) *RecurringVolumeAttachmentLocator {
	return &RecurringVolumeAttachmentLocator{Href(href), api}
}
func (api *API) RepositoryLocator(href string) *RepositoryLocator {
	return &RepositoryLocator{Href(href), api}
}
func (api *API) RepositoryAssetLocator(href string) *RepositoryAssetLocator {
	return &RepositoryAssetLocator{Href(href), api}
}
func (api *API) ResourceGroupLocator(href string) *ResourceGroupLocator {
	return &ResourceGroupLocator{Href(href), api}
}
func (api *API) RightScriptLocator(href string) *RightScriptLocator {
	return &RightScriptLocator{Href(href), api}
}
func (api *API) RightScriptAttachmentLocator(href string) *RightScriptAttachmentLocator {
	return &RightScriptAttachmentLocator{Href(href), api}
}
func (api *API) RouteLocator(href string) *RouteLocator {
	return &RouteLocator{Href(href), api}
}
func (api *API) RouteTableLocator(href string) *RouteTableLocator {
	return &RouteTableLocator{Href(href), api}
}
func (api *API) RunnableBindingLocator(href string) *RunnableBindingLocator {
	return &RunnableBindingLocator{Href(href), api}
}
func (api *API) SchedulerLocator(href string) *SchedulerLocator {
	return &SchedulerLocator{Href(href), api}
}
func (api *API) SecurityGroupLocator(href string) *SecurityGroupLocator {
	return &SecurityGroupLocator{Href(href), api}
}
func (api *API) SecurityGroupRuleLocator(href string) *SecurityGroupRuleLocator {
	return &SecurityGroupRuleLocator{Href(href), api}
}
func (api *API) ServerLocator(href string) *ServerLocator {
	return &ServerLocator{Href(href), api}
}
func (api *API) ServerArrayLocator(href string) *ServerArrayLocator {
	return &ServerArrayLocator{Href(href), api}
}
func (api *API) ServerTemplateLocator(href string) *ServerTemplateLocator {
	return &ServerTemplateLocator{Href(href), api}
}
func (api *API) ServerTemplateMultiCloudImageLocator(href string) *ServerTemplateMultiCloudImageLocator {
	return &ServerTemplateMultiCloudImageLocator{Href(href), api}
}
func (api *API) SessionLocator(href string) *SessionLocator {
	return &SessionLocator{Href(href), api}
}
func (api *API) SshKeyLocator(href string) *SshKeyLocator {
	return &SshKeyLocator{Href(href), api}
}
func (api *API) SubnetLocator(href string) *SubnetLocator {
	return &SubnetLocator{Href(href), api}
}
func (api *API) TagLocator(href string) *TagLocator {
	return &TagLocator{Href(href), api}
}
func (api *API) TaskLocator(href string) *TaskLocator {
	return &TaskLocator{Href(href), api}
}
func (api *API) UserLocator(href string) *UserLocator {
	return &UserLocator{Href(href), api}
}
func (api *API) UserDataLocator(href string) *UserDataLocator {
	return &UserDataLocator{Href(href), api}
}
func (api *API) VolumeLocator(href string) *VolumeLocator {
	return &VolumeLocator{Href(href), api}
}
func (api *API) VolumeAttachmentLocator(href string) *VolumeAttachmentLocator {
	return &VolumeAttachmentLocator{Href(href), api}
}
func (api *API) VolumeSnapshotLocator(href string) *VolumeSnapshotLocator {
	return &VolumeSnapshotLocator{Href(href), api}
}
func (api *API) VolumeTypeLocator(href string) *VolumeTypeLocator {
	return &VolumeTypeLocator{Href(href), api}
}
func RegisterCommands(registrar rsapi.APICommandRegistrar) {
	commandValues = rsapi.ActionCommands{}
	registrar.RegisterActionCommands(APIName, GenMetadata, commandValues)
}
func (a *API) ShowCommandHelp(cmd string) error {
	return a.ShowHelp(cmd, "/rll", commandValues)
}
func (a *API) ShowAPIActions(cmd string) error {
	return a.ShowActions(cmd, "/rll", commandValues)
}
func (a *API) ShowHelp(cmd, hrefPrefix string, values ActionCommands) error {
	target, _, err := a.ParseCommandAndFlags(cmd, hrefPrefix, values)
	if err != nil {
		return err
	}
	_, action, href := target.Resource, target.Action, target.Href
	if len(action.CommandFlags) == 0 {
		fmt.Printf("usage: rsc [<FLAGS>] %s %s %s\n\n%s\n", strings.Split(cmd, " ")[0],
			action.Name, href, action.Description)
		return nil
	}
	flagHelp := make([]string, len(action.CommandFlags))
	for i, f := range action.CommandFlags {
		var attrs string
		if f.Mandatory {
			attrs = "required"
		} else {
			attrs = "optional"
		}
		if len(f.ValidValues) > 0 {
			attrs += ", [" + strings.Join(f.ValidValues, "|") + "]"
		}
		if f.Regexp != nil {
			attrs += ", /" + f.Regexp.String() + "/"
		}
		flagHelp[i] = fmt.Sprintf("%s=%s\n    <%s> %s", f.Name, f.Type, attrs, f.Description)
	}
	fmt.Printf("usage: rsc [<FLAGS>] %s %s %s [<PARAMS>]\n\n%s\n\n", strings.Split(cmd, " ")[0],
		action.Name, href, action.Description)
	fmt.Printf("PARAMS:\n%s\n", strings.Join(flagHelp, "\n\n"))
	return nil
}
func (a *API) ParseCommandAndFlags(cmd, hrefPrefix string, values ActionCommands) (*CommandTarget, []string, error) {
	resource, vars, err := a.parseResource(cmd, hrefPrefix, values)
	if err != nil {
		return nil, nil, err
	}
	var action *metadata.Action
	elems := strings.Split(cmd, " ")
	actionName := elems[len(elems)-1]
	for _, a := range resource.Actions {
		if a.Name == actionName {
			action = a
			break
		}
	}
	if action == nil {
		supported := make([]string, len(resource.Actions))
		for i, a := range resource.Actions {
			supported[i] = a.Name
		}
		return nil, nil, fmt.Errorf("Unknown %s action '%s'. Supported actions are: %s",
			resource.Name, actionName, strings.Join(supported, ", "))
	}

	path, err := action.URL(vars)
	if err != nil {
		return nil, nil, err
	}
	flags := values[cmd]

	return &CommandTarget{resource, action, path, flags.Href}, flags.Params, nil
}
func validateFlagValue(value string, param *metadata.ActionParam) error {
	if param.Regexp != nil {
		if !param.Regexp.MatchString(value) {
			return fmt.Errorf("Invalid value '%s' for '%s', value must validate /%s/",
				value, param.Name, param.Regexp.String())
		}
	}
	if param.NonBlank && value == "" {
		return fmt.Errorf("Invalid value for '%s', value must not be blank",
			param.Name)
	}
	if len(param.ValidValues) > 0 && param.Name != "filter[]" { // filter[] is special: it has values just so --help can list them
		found := false
		for _, v := range param.ValidValues {
			if v == value {
				found = true
				break
			}
		}
		if !found {
			return fmt.Errorf("Invalid value for '%s', value must be one of %s, value provided was '%s'",
				param.Name, strings.Join(param.ValidValues, ", "), value)
		}
	}

	return nil
}
func buildQuery(values []APIParams) (APIParams, error) {
	query := APIParams{}
	for _, value := range values {
		// Only one iteration below, flatten params only have one element each
		for name, param := range value {
			if q, ok := query[name]; ok {
				if a, ok := q.([]interface{}); ok {
					query[name] = append(a, param)
				} else {
					query[name] = []interface{}{q, param}
				}
			} else {
				query[name] = param
			}
		}
	}
	return query, nil
}
func buildPayload(values []APIParams) (APIParams, error) {
	payload := APIParams{}
	for _, value := range values {
		// Only one iteration below, flatten params only have one element each
		for name, param := range value {
			if _, err := Normalize(payload, name, param); err != nil {
				return nil, err
			}
		}
	}
	return payload, nil
}
func shortenPattern(res *metadata.Resource, pattern, suffix string) (string, bool) {
	if strings.HasSuffix(pattern, suffix) {
		pat := strings.TrimSuffix(pattern, suffix)
		for _, action := range res.Actions {
			for _, pattern2 := range action.PathPatterns {
				vars := pattern2.Variables
				ivars := make([]interface{}, len(vars))
				for i, v := range vars {
					ivars[i] = interface{}(":" + v)
				}
				subPattern := pattern2.Pattern
				pat2 := fmt.Sprintf(subPattern, ivars...)
				if pat == pat2 {
					return pat, true
				}
			}
		}
	}
	return pattern, false
}
func cleanDescription(doc string) string {
	docBits := strings.Split(doc, "Required security scope")
	doc = docBits[0]

	lines := strings.Split(doc, "\n")
	fullLines := make([]string, len(lines))
	i := 0
	for _, line := range lines {
		if len(line) > 0 && !blankRegexp.MatchString(line) {
			fullLines[i] = line
			i++
		}
	}
	return strings.Join(fullLines[:i], "\n")
}
func fileExists(file string) bool {
	_, err := os.Stat(file)
	return err == nil
}
func (api *API) AnalysisSnapshotLocator(href string) *AnalysisSnapshotLocator {
	return &AnalysisSnapshotLocator{Href(href), api}
}
func (api *API) BudgetAlertLocator(href string) *BudgetAlertLocator {
	return &BudgetAlertLocator{Href(href), api}
}
func (api *API) CloudBillLocator(href string) *CloudBillLocator {
	return &CloudBillLocator{Href(href), api}
}
func (api *API) CloudBillMetricLocator(href string) *CloudBillMetricLocator {
	return &CloudBillMetricLocator{Href(href), api}
}
func (api *API) CurrentUserLocator(href string) *CurrentUserLocator {
	return &CurrentUserLocator{Href(href), api}
}
func (api *API) InstanceCombinationLocator(href string) *InstanceCombinationLocator {
	return &InstanceCombinationLocator{Href(href), api}
}
func (api *API) InstanceMetricLocator(href string) *InstanceMetricLocator {
	return &InstanceMetricLocator{Href(href), api}
}
func (api *API) InstanceUsagePeriodLocator(href string) *InstanceUsagePeriodLocator {
	return &InstanceUsagePeriodLocator{Href(href), api}
}
func (api *API) PatternLocator(href string) *PatternLocator {
	return &PatternLocator{Href(href), api}
}
func (api *API) ReservedInstanceLocator(href string) *ReservedInstanceLocator {
	return &ReservedInstanceLocator{Href(href), api}
}
func (api *API) ReservedInstancePurchaseLocator(href string) *ReservedInstancePurchaseLocator {
	return &ReservedInstancePurchaseLocator{Href(href), api}
}
func (api *API) ScenarioLocator(href string) *ScenarioLocator {
	return &ScenarioLocator{Href(href), api}
}
func (api *API) ScheduledReportLocator(href string) *ScheduledReportLocator {
	return &ScheduledReportLocator{Href(href), api}
}
func (api *API) TempInstancePriceLocator(href string) *TempInstancePriceLocator {
	return &TempInstancePriceLocator{Href(href), api}
}
func (api *API) UserSettingLocator(href string) *UserSettingLocator {
	return &UserSettingLocator{Href(href), api}
}
func readAllAsync(f io.ReadCloser) (*[]byte, chan struct{}) {
	done := make(chan struct{}, 1) // signal that the read is done
	var buf []byte                 // placeholder buffer for the result
	go func() {
		var err error
		buf, err = ioutil.ReadAll(f)
		if err != nil {
			buf = make([]byte, 0)
		}
		f.Close()
		done <- struct{}{}
	}()
	return &buf, done
}
func extractArg(name string, args []string) (string, []string) {
	var val string
	var newArgs []string
	var skip bool
	for i, a := range args {
		if skip {
			skip = false
			continue
		}
		if strings.Contains(a, "=") {
			elems := strings.SplitN(a, "=", 2)
			if elems[0] == name {
				val = elems[1]
			} else {
				newArgs = append(newArgs, a)
			}
		} else if a == name && len(args) > (i+1) {
			val = args[i+1]
			skip = true
		} else {
			newArgs = append(newArgs, a)
		}
	}
	return val, newArgs
}
func write(b []byte) {
	f, err := os.OpenFile(output, os.O_APPEND|os.O_WRONLY|os.O_CREATE, 0644)
	if err != nil {
		fail("failed to open output file")
	}
	f.Write(b)
	f.WriteString("\n")
	f.Close()
}
func (api *API) AppliedPolicyLocator(href string) *AppliedPolicyLocator {
	return &AppliedPolicyLocator{Href(href), api}
}
func (api *API) ApprovalLocator(href string) *ApprovalLocator {
	return &ApprovalLocator{Href(href), api}
}
func (api *API) IncidentLocator(href string) *IncidentLocator {
	return &IncidentLocator{Href(href), api}
}
func (api *API) PolicyTemplateLocator(href string) *PolicyTemplateLocator {
	return &PolicyTemplateLocator{Href(href), api}
}
func (api *API) PublishedTemplateLocator(href string) *PublishedTemplateLocator {
	return &PublishedTemplateLocator{Href(href), api}
}
func (api *API) DebugCookbookPathLocator(href string) *DebugCookbookPathLocator {
	return &DebugCookbookPathLocator{Href(href), api}
}
func (api *API) DockerControlLocator(href string) *DockerControlLocator {
	return &DockerControlLocator{Href(href), api}
}
func (api *API) EnvLocator(href string) *EnvLocator {
	return &EnvLocator{Href(href), api}
}
func (api *API) LoginControlLocator(href string) *LoginControlLocator {
	return &LoginControlLocator{Href(href), api}
}
func (api *API) ProcLocator(href string) *ProcLocator {
	return &ProcLocator{Href(href), api}
}
func (api *API) Rl10Locator(href string) *Rl10Locator {
	return &Rl10Locator{Href(href), api}
}
func (api *API) TSSLocator(href string) *TSSLocator {
	return &TSSLocator{Href(href), api}
}
func (api *API) TSSControlLocator(href string) *TSSControlLocator {
	return &TSSControlLocator{Href(href), api}
}
func (api *API) TSSPluginLocator(href string) *TSSPluginLocator {
	return &TSSPluginLocator{Href(href), api}
}
func NewAngularWriter() (*AngularWriter, error) {
	funcMap := template.FuncMap{
		"comment":     comment,
		"commandLine": commandLine,
		"path":        path,
		"mandatory":   mandatory,
	}
	resourceT, err := template.New("resource-client").Funcs(funcMap).Parse(angularTmpl)
	if err != nil {
		return nil, err
	}
	return &AngularWriter{
		angularTmpl: resourceT,
	}, nil
}
func (c *AngularWriter) WriteResource(resource *gen.Resource, w io.Writer) error {
	return c.angularTmpl.Execute(w, resource)
}
func path(a *gen.Action) string {
	pattern := a.PathPatterns[0]
	vars := pattern.Variables
	ivars := make([]interface{}, len(vars))
	for i, v := range vars {
		ivars[i] = interface{}(":" + v)
	}
	return fmt.Sprintf(pattern.Pattern, ivars...)
}
func mandatory(a gen.Action, param string) bool {
	for _, p := range a.Params {
		if p.Name == param {
			return p.Mandatory
		}
	}
	panic("praxisgen bug: Unknown param " + param + " for action " + a.Name)
}
func New(host string, auth Authenticator) *API {
	client := httpclient.New()
	if strings.HasPrefix(host, "http://") {
		host = host[7:]
	} else if strings.HasPrefix(host, "https://") {
		host = host[8:]
	}
	a := &API{
		Auth:   auth,
		Host:   host,
		Client: client,
	}
	if auth != nil {
		auth.SetHost(host)
	}
	return a
}
func FromCommandLine(cmdLine *cmd.CommandLine) (*API, error) {
	var client *API
	ss := strings.HasPrefix(cmdLine.Command, "ss")
	if cmdLine.RL10 {
		var err error
		if client, err = NewRL10(); err != nil {
			return nil, err
		}
	} else if cmdLine.OAuthToken != "" {
		auth := NewOAuthAuthenticator(cmdLine.OAuthToken, cmdLine.Account)
		if ss {
			auth = NewSSAuthenticator(auth, cmdLine.Account)
		}
		client = New(cmdLine.Host, auth)
	} else if cmdLine.OAuthAccessToken != "" {
		auth := NewTokenAuthenticator(cmdLine.OAuthAccessToken, cmdLine.Account)
		if ss {
			auth = NewSSAuthenticator(auth, cmdLine.Account)
		}
		client = New(cmdLine.Host, auth)
	} else if cmdLine.APIToken != "" {
		auth := NewInstanceAuthenticator(cmdLine.APIToken, cmdLine.Account)
		if ss {
			auth = NewSSAuthenticator(auth, cmdLine.Account)
		}
		client = New(cmdLine.Host, auth)
	} else if cmdLine.Username != "" && cmdLine.Password != "" {
		auth := NewBasicAuthenticator(cmdLine.Username, cmdLine.Password, cmdLine.Account)
		if ss {
			auth = NewSSAuthenticator(auth, cmdLine.Account)
		}
		client = New(cmdLine.Host, auth)
	} else {
		// No auth, used by tests
		client = New(cmdLine.Host, nil)
		httpclient.Insecure = true
	}
	if !cmdLine.ShowHelp && !cmdLine.NoAuth {
		if cmdLine.OAuthToken == "" && cmdLine.OAuthAccessToken == "" && cmdLine.APIToken == "" && cmdLine.Username == "" && !cmdLine.RL10 {
			return nil, fmt.Errorf("Missing authentication information, use '--email EMAIL --password PWD', '--token TOKEN' or 'setup'")
		}
		if cmdLine.Verbose || cmdLine.Dump == "debug" {
			httpclient.DumpFormat = httpclient.Debug
		}
		if cmdLine.Dump == "json" {
			httpclient.DumpFormat = httpclient.JSON
		}
		if cmdLine.Dump == "record" {
			httpclient.DumpFormat = httpclient.JSON | httpclient.Record
		}
		if cmdLine.Verbose {
			httpclient.DumpFormat |= httpclient.Verbose
		}
		client.FetchLocationResource = cmdLine.FetchResource
	}
	return client, nil
}
func (a *API) CanAuthenticate() error {
	res := a.Auth.CanAuthenticate(a.Host)
	return res
}
func Encrypt(text string) (string, error) {
	bytes := []byte(text)
	key := seekret()
	block, err := aes.NewCipher(key)
	if err != nil {
		return "", err
	}
	b := encodeBase64(bytes)
	ciphertext := make([]byte, aes.BlockSize+len(b))
	iv := ciphertext[:aes.BlockSize]
	if _, err := io.ReadFull(rand.Reader, iv); err != nil {
		return "", err
	}
	cfb := cipher.NewCFBEncrypter(block, iv)
	cfb.XORKeyStream(ciphertext[aes.BlockSize:], b)
	return string(encodeBase64(ciphertext)), nil
}
func Decrypt(text string) (string, error) {
	if text == "" {
		return "", nil
	}
	key := seekret()
	bytes := decodeBase64([]byte(text))
	block, err := aes.NewCipher(key)
	if err != nil {
		return "", err
	}
	if len(bytes) < aes.BlockSize {
		return "", errors.New("ciphertext too short")
	}
	iv := bytes[:aes.BlockSize]
	bytes = bytes[aes.BlockSize:]
	cfb := cipher.NewCFBDecrypter(block, iv)
	cfb.XORKeyStream(bytes, bytes)
	return string(decodeBase64(bytes)), nil
}
func (a *APIAnalyzer) guessType(ec EvalCtx, d *Definition, refID string) string {
	// First get the type name and and view from the swagger reference definition
	// name -- are a few cases where that's the only place that has the view
	if t, ok := a.TypeOverrides[refID]; ok {
		return t
	}
	var name, view string
	if strings.Contains(refID, "RequestBody") {
		bits := strings.Split(refID, "RequestBody")
		name = bits[0]
		if len(bits) > 1 {
			view = strings.ToLower(bits[1])
		}
	} else if strings.Contains(refID, "ResponseBody") {
		bits := strings.Split(refID, "ResponseBody")
		name = bits[0]
		if len(bits) > 1 {
			view = strings.ToLower(bits[1])
		}
	} else {
		name = refID
	}

	// Now try and get it from the media type -- this is preferred if its set.
	if mt := mediaType(d.Title); mt != "" {
		if strings.Contains(mt, "application") {
			bits := strings.Split(mt, ".")
			name := bits[len(bits)-1]
			attrs := mediaTypeAttrs(d.Title)
			if attrs["type"] != "" {
				name += "_" + attrs["type"]
			}
			if attrs["view"] != "" && attrs["view"] != "default" {
				name += "_" + attrs["view"]
			} else if view != "" {
				name += "_" + view
			}
			dbg("DEBUG media type refID:%#v title:%#v name:%#v view:%#v -> type:%#v\n", refID, d.Title, name, view, name)
			return toTypeName(name)
		} else if strings.Contains(mt, "text/") {
			return "string"
		} else {
			fail("Don't know how to handle media type %s", mt)
		}
	}
	if view != "" {
		return name + "_" + view
	}

	return name
}
func (a *APIAnalyzer) addType(ec EvalCtx, dt *gen.ObjectDataType, r Ref) {
	a.api.NeedJSON = true
	if a.refByType[dt.TypeName] == r.ID() {
		return
	}

	if other, ok := a.api.Types[dt.TypeName]; ok {
		if !ec.IsResult {
			// If its an input parameter, fix the collision by amending this types name
			dt.TypeName += "Param"
			if a.refByType[dt.TypeName] == r.ID() {
				return
			}
		}
		oldFields := []string{}
		newFields := []string{}
		for _, f := range other.Fields {
			oldFields = append(oldFields, f.Name)
		}
		for _, f := range dt.Fields {
			newFields = append(newFields, f.Name)
		}
		use := "Old"
		if len(newFields) > len(oldFields) {
			use = "New"
		}
		if strings.Join(oldFields, ",") != strings.Join(newFields, ",") {
			warn("Warning: Type collision when adding new type %s!\n  New: id %s fields %v\n  Old: id %s fields %v\n  Using %s, which has more fields\n",
				dt.TypeName, r.ID(), newFields, a.refByType[dt.TypeName], oldFields, use)
		}
		if use == "Old" {
			return
		}
	}
	dbg("DEBUG NEW TYPE %s\n", prettify(dt))
	a.refByType[dt.TypeName] = r.ID()
	a.api.Types[dt.TypeName] = dt
}
func extractCmdLineParams(a *gen.ActionParam, root string, seen map[string]*[]*gen.ActionParam, parentNotMandatory bool) []*gen.ActionParam {
	switch t := a.Type.(type) {
	case *gen.BasicDataType, *gen.EnumerableDataType, *gen.UploadDataType:
		dup := gen.ActionParam{
			Name:        a.Name,
			QueryName:   root,
			Description: a.Description,
			VarName:     a.VarName,
			Location:    a.Location,
			Type:        a.Type,
			Mandatory:   a.Mandatory && !parentNotMandatory, // yay for double negations!
			NonBlank:    a.NonBlank,
			Regexp:      a.Regexp,
			ValidValues: a.ValidValues,
			Min:         a.Min,
			Max:         a.Max,
		}
		return []*gen.ActionParam{&dup}
	case *gen.ArrayDataType:
		p := t.ElemType
		eq, ok := seen[p.Name]
		if !ok {
			eq = &[]*gen.ActionParam{}
			seen[p.Name] = eq
			*eq = extractCmdLineParams(p, root+"[]", seen, parentNotMandatory || !a.Mandatory)
		}
		return *eq
	case *gen.ObjectDataType:
		params := []*gen.ActionParam{}
		for _, f := range t.Fields {
			eq, ok := seen[f.Name]
			if !ok {
				eq = &[]*gen.ActionParam{}
				seen[f.Name] = eq
				*eq = extractCmdLineParams(f, fmt.Sprintf("%s[%s]", root, f.Name), seen, parentNotMandatory || !a.Mandatory)
			}
			params = append(params, *eq...)
		}
		return params
	}
	return nil
}
func NewMetadataWriter() (*MetadataWriter, error) {
	funcMap := template.FuncMap{
		"comment":         comment,
		"join":            strings.Join,
		"commandLine":     commandLine,
		"toStringArray":   toStringArray,
		"flagType":        flagType,
		"location":        location,
		"escapeBackticks": escapeBackticks,
	}
	headerT, err := template.New("header-metadata").Funcs(funcMap).Parse(headerMetadataTmpl)
	if err != nil {
		return nil, err
	}
	resourceT, err := template.New("resource-metadata").Funcs(funcMap).Parse(resourceMetadataTmpl)
	if err != nil {
		return nil, err
	}
	return &MetadataWriter{
		headerTmpl:   headerT,
		resourceTmpl: resourceT,
	}, nil
}
func (c *MetadataWriter) WriteHeader(pkg string, w io.Writer) error {
	return c.headerTmpl.Execute(w, pkg)
}
func (c *MetadataWriter) WriteMetadata(d *gen.APIDescriptor, w io.Writer) error {
	resources := make([]*gen.Resource, len(d.ResourceNames))
	for i, n := range d.ResourceNames {
		resources[i] = d.Resources[n]
	}
	return c.resourceTmpl.Execute(w, resources)
}
func location(p *gen.ActionParam) string {
	switch p.Location {
	case gen.PathParam:
		return "metadata.PathParam"
	case gen.QueryParam:
		return "metadata.QueryParam"
	case gen.PayloadParam:
		return "metadata.PayloadParam"
	default:
		return ""
	}
}
func New(host string, auth rsapi.Authenticator) *API {
	return fromAPI(rsapi.New(host, auth))
}
func fromAPI(api *rsapi.API) *API {
	api.Metadata = GenMetadata
	return &API{api}
}
func (a *API) BuildRequest(resource, action, href string, params rsapi.APIParams) (*http.Request, error) {
	// First lookup metadata
	res, ok := GenMetadata[resource]
	if !ok {
		return nil, fmt.Errorf("No resource with name '%s'", resource)
	}
	act := res.GetAction(action)
	if act == nil {
		return nil, fmt.Errorf("No action with name '%s' on %s", action, resource)
	}

	// Now lookup action request HTTP method, url, params and payload.
	vars, err := res.ExtractVariables(href)
	if err != nil {
		return nil, err
	}
	actionURL, err := act.URL(vars)
	if err != nil {
		return nil, err
	}
	_, queryParams := rsapi.IdentifyParams(act, params)
	return a.BuildHTTPRequest("GET", actionURL.Path, "1.6", queryParams, nil)
}
func setupMetadata() (result map[string]*metadata.Resource) {
	result = make(map[string]*metadata.Resource)
	for n, r := range cac.GenMetadata {
		result[n] = r
	}
	return
}
func NewDisplayer(resp *http.Response) (*Displayer, error) {
	defer resp.Body.Close()
	js, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("Failed to read response (%s)", err)
	}
	disp := Displayer{response: resp, body: string(js)}
	if len(js) > 2 {
		err = json.Unmarshal(js, &disp.RawOutput)
		if err != nil {
			disp.RawOutput = string(js)
		}
	}
	return &disp, nil
}
func (d *Displayer) ApplySingleExtract(extract string) error {
	if err := d.ApplyExtract(extract, true); err != nil {
		return err
	}
	outputs := d.RawOutput.([]interface{})
	if len(outputs) != 1 {
		d.RawOutput = nil
		return fmt.Errorf("JSON selector '%s' returned %d instead of one value",
			extract, len(outputs))
	}
	if len(outputs) == 0 {
		d.RawOutput = ""
	} else {
		switch v := outputs[0].(type) {
		case nil:
			d.RawOutput = ""
		case float64, bool:
			d.RawOutput = fmt.Sprint(v)
		case string:
			d.RawOutput = v
		default:
			d.RawOutput = v
		}
		d.RawOutput = outputs[0]
	}
	return nil
}
func (d *Displayer) ApplyExtract(selector string, js bool) error {
	parser, err := jsonselect.CreateParserFromString(d.body)
	if err != nil {
		return fmt.Errorf("Failed to load response JSON: %s, JSON was:\n%s", err, d.body)
	}
	outputs, err := parser.GetValues(selector)
	if !js {
		out := ""
		for _, o := range outputs {
			b, _ := json.Marshal(o)
			out += string(b) + "\n"
		}
		d.RawOutput = out
	} else {
		d.RawOutput = outputs
	}
	if err != nil {
		return fmt.Errorf("Failed to apply JSON selector '%s' to response: %s, JSON was:\n%s",
			selector, err, d.body)
	}
	return nil
}
func (d *Displayer) ApplyHeaderExtract(header string) error {
	d.RawOutput = d.response.Header.Get(header)
	if d.RawOutput == "" {
		return fmt.Errorf("Response does not contain the '%s' header", header)
	}
	return nil
}
func (d *Displayer) Output() string {
	output := d.RawOutput
	if output == nil {
		return ""
	}
	if outputStr, ok := d.RawOutput.(string); ok {
		suffix := ""
		if d.prettify {
			suffix = "\n"
		}
		return outputStr + suffix
	}
	var out string
	var err error
	if d.prettify {
		var b []byte
		b, err = json.MarshalIndent(output, "", "    ")
		if err == nil {
			out = string(b) + "\n"
		}
	} else {
		var b []byte
		b, err = json.Marshal(output)
		out = string(b)
	}
	if err != nil {
		fm := "%v"
		if d.prettify {
			fm += "\n"
		}
		return fmt.Sprintf(fm, output)
	}
	return out
}
func NewAPIAnalyzer(resources map[string]interface{}, attributeTypes map[string]string) *APIAnalyzer {
	return &APIAnalyzer{
		rawResources:   resources,
		attributeTypes: attributeTypes,
		rawTypes:       make(map[string][]*gen.ObjectDataType),
	}
}
func (a *APIAnalyzer) Analyze() *gen.APIDescriptor {
	a.AnalyzeAliases()
	var descriptor = &gen.APIDescriptor{
		Resources: make(map[string]*gen.Resource),
		Types:     make(map[string]*gen.ObjectDataType),
	}
	var rawResourceNames = make([]string, len(a.rawResources))
	var idx = 0
	for n := range a.rawResources {
		rawResourceNames[idx] = n
		idx++
	}
	sort.Strings(rawResourceNames)
	for _, name := range rawResourceNames {
		var resource = a.rawResources[name]
		a.AnalyzeResource(name, resource, descriptor)
	}
	descriptor.FinalizeTypeNames(a.rawTypes)
	return descriptor
}
func (a *APIAnalyzer) AnalyzeAliases() {
	for from, to := range aliases {
		splits := strings.SplitN(from, "#", 2)
		fromResName := splits[0]
		fromActionName := splits[1]
		splits = strings.SplitN(to, "#", 2)
		toResName := splits[0]
		toActionName := splits[1]

		fromRes := a.rawResources[fromResName]
		fromAct := fromRes.(map[string]interface{})["methods"].(map[string]interface{})[fromActionName].(map[string]interface{})

		toRes := a.rawResources[toResName]
		toAct := toRes.(map[string]interface{})["methods"].(map[string]interface{})[toActionName].(map[string]interface{})
		fromAct["parameters"] = toAct["parameters"]
		fromAct["status_code"] = toAct["status_code"]
		fromAct["access_rules"] = toAct["access_rules"]
	}
}
func LocatorFunc(attributes []*gen.Attribute, name string) string {
	hasLinks := false
	for _, a := range attributes {
		if a.FieldName == "Links" {
			hasLinks = true
			break
		}
	}
	if !hasLinks {
		return ""
	}
	return `for _, l := range r.Links {
			if l["rel"] == "self" {
				return api.` + name + `Locator(l["href"])
			}
		}
		return nil`
}
func ParseRoute(moniker string, routes []string) (pathPatterns []*gen.PathPattern) {
	// :(((( some routes are empty
	var paths []string
	var method string
	switch moniker {
	case "Deployments#servers":
		method, paths = "GET", []string{"/api/deployments/:id/servers"}
	case "ServerArrays#current_instances":
		method, paths = "GET", []string{"/api/server_arrays/:id/current_instances"}
	case "ServerArrays#launch":
		method, paths = "POST", []string{"/api/server_arrays/:id/launch"}
	case "ServerArrays#multi_run_executable":
		method, paths = "POST", []string{"/api/server_arrays/:id/multi_run_executable"}
	case "ServerArrays#multi_terminate":
		method, paths = "POST", []string{"/api/server_arrays/:id/multi_terminate"}
	case "Servers#launch":
		method, paths = "POST", []string{"/api/servers/:id/launch"}
	case "Servers#terminate":
		method, paths = "POST", []string{"/api/servers/:id/terminate"}
	default:
		for _, route := range routes {
			bound := routeRegexp.FindStringIndex(route)
			match := route[0:bound[0]]
			method = strings.TrimRight(match[0:7], " ")
			path := strings.TrimRight(match[7:], " ")
			path = strings.TrimSuffix(path, "(.:format)?")
			if isDeprecated(path) || isCustom(method, path) {
				continue
			}
			paths = append(paths, path)
		}
	}
	pathPatterns = make([]*gen.PathPattern, len(paths))
	for i, p := range paths {
		rx := routeVariablesRegexp.ReplaceAllLiteralString(regexp.QuoteMeta(p), `/([^/]+)`)
		rx = fmt.Sprintf("^%s$", rx)
		pattern := gen.PathPattern{
			HTTPMethod: method,
			Path:       p,
			Pattern:    routeVariablesRegexp.ReplaceAllLiteralString(p, "/%s"),
			Regexp:     rx,
		}
		matches := routeVariablesRegexp.FindAllStringSubmatch(p, -1)
		if len(matches) > 0 {
			pattern.Variables = make([]string, len(matches))
			for i, m := range matches {
				pattern.Variables[i] = m[1]
			}
		}
		pathPatterns[i] = &pattern
	}
	return
}
func isDeprecated(path string) bool {
	return strings.Contains(path, "/api/session") && !strings.Contains(path, "/api/sessions")
}
func isQueryParam(a, n string) bool {
	return n == "view" || n == "filter" || (a == "index" && (n == "with_deleted" || n == "with_inherited" || n == "latest_only" || n == "lineage"))
}
func isPathParam(p string, pathPatterns []*gen.PathPattern) bool {
	for _, pattern := range pathPatterns {
		for _, v := range pattern.Variables {
			if p == v {
				return true
			}
		}
	}
	return false
}
func fetchAuditEntries(client *cm15.API, filterEmail string) ([]*cm15.AuditEntry, error) {
	auditLocator := client.AuditEntryLocator("/api/audit_entries")
	var apiParams = rsapi.APIParams{"filter": []string{"user_email==" + filterEmail}}
	auditEntries, err := auditLocator.Index(
		tomorrow(),  // End date
		"100",       // Limit
		yesterday(), // Start date
		apiParams,
	)
	if err != nil {
		return auditEntries, err
	}
	return auditEntries, nil
}
func formatTime(tm time.Time) string {
	year, month, date := tm.Date()
	return time.Date(year, month, date, 0, 0, 0, 0, time.UTC).Format("2006/01/02 15:04:05 -0700")
}
func printAudits(entries []*cm15.AuditEntry) {
	for _, a := range entries {
		fmt.Printf("[%v] <%v>: %v\n", a.UpdatedAt, a.UserEmail, a.Summary)
	}
}
func extractUnique(oldEntries, newEntries []*cm15.AuditEntry) []*cm15.AuditEntry {
	var uniqueEntries = make([]*cm15.AuditEntry, 0)
	var oldHrefs = make([]string, len(oldEntries))
	for i, e := range oldEntries {
		oldHrefs[i] = getHref(e)
	}
	for _, newEntry := range newEntries {
		if !stringInSlice(getHref(newEntry), oldHrefs) {
			uniqueEntries = append(uniqueEntries, newEntry)
		}
	}
	return uniqueEntries
}
func getHref(entry *cm15.AuditEntry) string {
	var href string
	for _, link := range entry.Links {
		if link["rel"] == "self" {
			href = link["href"]
			break
		}
	}
	return href
}
func fail(format string, v ...interface{}) {
	if !strings.HasSuffix(format, "\n") {
		format += "\n"
	}
	fmt.Println(fmt.Sprintf(format, v...))
	os.Exit(1)
}
func parameters(a *gen.Action) string {
	var m = a.MandatoryParams()
	var hasOptional = a.HasOptionalParams()
	var countParams = len(m)
	if hasOptional {
		countParams++
	}
	var params = make([]string, countParams)
	for i, param := range m {
		params[i] = fmt.Sprintf("%s %s", fixReserved(param.VarName), param.Signature())
	}
	if hasOptional {
		params[countParams-1] = "options rsapi.APIParams"
	}

	return strings.Join(params, ", ")
}
func paramsInitializer(action *gen.Action, location int, varName string) string {
	var fields []string
	var optionals []*gen.ActionParam
	varName = fixReserved(varName)
	for _, param := range action.Params {
		if param.Location != location {
			continue
		}
		if param.Mandatory {
			name := param.Name
			if location == 1 { // QueryParam
				name = param.QueryName
			}
			fields = append(fields, fmt.Sprintf("\"%s\": %s,", name, fixReserved(param.VarName)))
		} else {
			optionals = append(optionals, param)
		}
	}
	if len(fields) == 0 && len(optionals) == 0 {
		return ""
	}
	var paramsDecl = fmt.Sprintf("rsapi.APIParams{\n%s\n}", strings.Join(fields, "\n\t"))
	if len(optionals) == 0 {
		return fmt.Sprintf("\n%s = %s", varName, paramsDecl)
	}
	var inits = make([]string, len(optionals))
	for i, opt := range optionals {
		name := opt.Name
		if location == 1 { // QueryParam
			name = opt.QueryName
		}
		inits[i] = fmt.Sprintf("\tvar %sOpt = options[\"%s\"]\n\tif %sOpt != nil {\n\t\t%s[\"%s\"] = %sOpt\n\t}",
			opt.VarName, opt.Name, opt.VarName, varName, name, opt.VarName)
	}
	var paramsInits = strings.Join(inits, "\n\t")
	return fmt.Sprintf("\n%s = %s\n%s", varName, paramsDecl, paramsInits)
}
func commandLine() string {
	return fmt.Sprintf("$ %s %s", os.Args[0], strings.Join(os.Args[1:], " "))
}
func toVerb(text string) (res string) {
	res = strings.ToUpper(string(text[0])) + strings.ToLower(text[1:])
	if text == "GET" || text == "POST" {
		res += "Raw"
	}
	return
}
func escapeBackticks(d string) string {
	elems := strings.Split(d, "`")
	return strings.Join(elems, "` + `")
}
func (api *API) AccountPreferenceLocator(href string) *AccountPreferenceLocator {
	return &AccountPreferenceLocator{Href(href), api}
}
func (api *API) ApplicationLocator(href string) *ApplicationLocator {
	return &ApplicationLocator{Href(href), api}
}
func (api *API) EndUserLocator(href string) *EndUserLocator {
	return &EndUserLocator{Href(href), api}
}
func (api *API) NotificationRuleLocator(href string) *NotificationRuleLocator {
	return &NotificationRuleLocator{Href(href), api}
}
func (api *API) UserPreferenceLocator(href string) *UserPreferenceLocator {
	return &UserPreferenceLocator{Href(href), api}
}
func (api *API) UserPreferenceInfoLocator(href string) *UserPreferenceInfoLocator {
	return &UserPreferenceInfoLocator{Href(href), api}
}
func LoadConfig(path string) (*ClientConfig, error) {
	content, err := ioutil.ReadFile(path)
	if err != nil {
		return nil, err
	}
	var config ClientConfig
	err = json.Unmarshal(content, &config)
	if err != nil {
		return nil, err
	}
	config.Password, err = Decrypt(config.Password)
	if err != nil {
		return nil, err
	}
	config.RefreshToken, err = Decrypt(config.RefreshToken)
	return &config, err
}
func CreateConfig(path string) error {
	config, _ := LoadConfig(path)
	var emailDef, passwordDef, accountDef, hostDef, refreshTokenDef string
	if config != nil {
		yn := PromptConfirmation("Found existing configuration file %v, overwrite? (y/N): ", path)
		if yn != "y" {
			PrintSuccess("Exiting")
			return nil
		}
		emailDef = fmt.Sprintf(" (%v)", config.Email)
		accountDef = fmt.Sprintf(" (%v)", config.Account)
		passwordDef = " (leave blank to leave unchanged)"
		if config.LoginHost == "" {
			config.LoginHost = "my.rightscale.com"
		}
		hostDef = fmt.Sprintf(" (%v)", config.LoginHost)
		refreshTokenDef = " (leave blank to leave unchanged)"
	} else {
		config = &ClientConfig{}
	}

	fmt.Fprintf(out, "Account ID%v: ", accountDef)
	var newAccount string
	fmt.Fscanln(in, &newAccount)
	if newAccount != "" {
		a, err := strconv.Atoi(newAccount)
		if err != nil {
			return fmt.Errorf("Account ID must be an integer, got '%s'.", newAccount)
		}
		config.Account = a
	}

	fmt.Fprintf(out, "Login email%v: ", emailDef)
	var newEmail string
	fmt.Fscanln(in, &newEmail)
	if newEmail != "" {
		config.Email = newEmail
	}

	fmt.Fprintf(out, "Login password%v: ", passwordDef)
	var newPassword string
	fmt.Fscanln(in, &newPassword)
	if newPassword != "" {
		config.Password = newPassword
	}

	fmt.Fprintf(out, "API Login host%v: ", hostDef)
	var newLoginHost string
	fmt.Fscanln(in, &newLoginHost)
	if newLoginHost != "" {
		config.LoginHost = newLoginHost
	}

	fmt.Fprintf(out, "API Refresh Token%v: ", refreshTokenDef)
	var newRefreshToken string
	fmt.Fscanln(in, &newRefreshToken)
	if newRefreshToken != "" {
		config.RefreshToken = newRefreshToken
	}

	err := config.Save(path)
	if err != nil {
		return fmt.Errorf("Failed to save config: %s", err)
	}

	return nil
}
func fromAPI(api *rsapi.API) *API {
	api.FileEncoding = rsapi.FileEncodingJSON
	api.Host = HostFromLogin(api.Host)
	api.Metadata = GenMetadata
	api.VersionHeader = "Api-Version"
	return &API{api}
}
func HostFromLogin(host string) string {
	urlElems := strings.Split(host, ".")
	hostPrefix := urlElems[0]
	elems := strings.Split(hostPrefix, "-")

	if len(elems) == 1 && elems[0] == "cm" {
		// accommodates micromoo host inference, such as "cm.rightscale.local" => "selfservice.rightscale.local"
		elems[0] = "governance"
	} else if len(elems) < 2 {
		// don't know how to compute this policy host; use the cm host
		return host
	} else {
		elems[len(elems)-2] = "governance"
	}
	policyHostPrefix := strings.Join(elems, "-")
	return strings.Join(append([]string{policyHostPrefix}, urlElems[1:]...), ".")
}
func (a *APIAnalyzer) AnalyzeParam(ec EvalCtx, p *Parameter) *gen.ActionParam {
	location, ok := loc[p.In]
	if !ok {
		location = -1
	}
	ap := &gen.ActionParam{
		Name:        p.Name,
		QueryName:   p.Name,
		Description: cleanDescription(p.Description),
		VarName:     toVarName(p.Name),
		Location:    location,
		Mandatory:   p.Required,
		NonBlank:    p.Required || p.Pattern != "",
		Regexp:      p.Pattern,
		ValidValues: p.Enum,
	}
	if p.Schema != nil {
		ap.Type = a.typeForRef(ec, p.Schema)
	} else {
		ap.Type = basicType(p.Type)
	}

	return ap
}
func (a *APIAnalyzer) AnalyzeAttribute(name, query string, attr map[string]interface{}) (*gen.ActionParam, error) {
	param := gen.ActionParam{Name: name, QueryName: query, VarName: toVarName(name)}
	if d, ok := attr["description"]; ok {
		param.Description = removeBlankLines(d.(string))
	}
	if r, ok := attr["required"]; ok {
		if r.(bool) {
			param.Mandatory = true
		}
	}
	if options, ok := attr["options"]; ok {
		opts, ok := options.(map[string]interface{})
		if ok {
			for n, o := range opts {
				switch n {
				case "max":
					param.Max = int(o.(float64))
				case "min":
					param.Min = int(o.(float64))
				case "regexp":
					param.Regexp = o.(string)
				}
			}
		}
	}
	if values, ok := attr["values"]; ok {
		param.ValidValues = values.([]interface{})
	}
	t := attr["type"].(map[string]interface{})
	dataType, err := a.AnalyzeType(t, query)
	if err != nil {
		return nil, err
	}
	param.Type = dataType
	switch dataType.(type) {
	case *gen.ArrayDataType:
		param.QueryName += "[]"
	}

	return &param, nil
}
func (a *APIAnalyzer) AnalyzeType(typeDef map[string]interface{}, query string) (gen.DataType, error) {
	n, ok := typeDef["name"].(string)
	if !ok {
		n = "Struct" // Assume inline struct (e.g. payload types)
	}
	if strings.HasSuffix(n, "FileUpload") {
		// A little bit hacky but this is to make upload work with resticle
		// The idea is that a type named "FileUpload" is assumed to define a multipart
		// request with a file part.
		// The type must define a "name" and  "filename" string fields.
		t, ok := a.RawTypes[n]
		if !ok {
			return nil, fmt.Errorf("Unknown type %s for %s", n, prettify(typeDef))
		}
		attrs, ok := t["attributes"]
		if !ok {
			return nil, fmt.Errorf("Invalid file upload type %s for %s: no attributes", n, prettify(typeDef))
		}
		mattrs, ok := attrs.(map[string]interface{})
		if !ok {
			return nil, fmt.Errorf("Invalid file upload type %s for %s: basic type", n, prettify(typeDef))
		}
		_, ok = mattrs["name"]
		if !ok {
			return nil, fmt.Errorf("Invalid file upload type %s for %s: no name", n, prettify(typeDef))
		}
		_, ok = mattrs["filename"]
		if !ok {
			return nil, fmt.Errorf("Invalid file upload type %s for %s: no filename", n, prettify(typeDef))
		}
		return &gen.UploadDataType{TypeName: n}, nil
	}
	if isBuiltInType(n) {
		n = "String"
	}
	var dataType gen.DataType
	switch n {
	case "Integer":
		i := gen.BasicDataType("int")
		dataType = &i
	case "Float":
		f := gen.BasicDataType("float64")
		dataType = &f
	case "String":
		s := gen.BasicDataType("string")
		dataType = &s
	case "Boolean":
		b := gen.BasicDataType("bool")
		dataType = &b
	case "Object":
		o := gen.BasicDataType("interface{}")
		dataType = &o
	case "DateTime":
		t := gen.BasicDataType("*time.Time") // Need pointer for case where value is null
		a.descriptor.NeedTime = true
		dataType = &t
	case "Collection", "Ids":
		member, ok := typeDef["member_attribute"].(map[string]interface{})
		if !ok {
			return nil, fmt.Errorf("Missing \"member_attribute\" for %s", prettify(typeDef))
		}
		elemType, err := a.AnalyzeAttribute(n+"Member", query+"[]", member)
		if err != nil {
			return nil, fmt.Errorf("Failed to compute type of \"member_attribute\": %s", err)
		}
		dataType = &gen.ArrayDataType{elemType}
	case "Struct":
		attrs, ok := typeDef["attributes"].(map[string]interface{})
		if !ok {
			return nil, fmt.Errorf("Failed to retrieve attributes of struct for %s", prettify(typeDef))
		}
		obj, err := a.CreateType(query, attrs)
		if err != nil {
			return nil, err
		}
		dataType = obj
	case "Hash":
		keys, ok := typeDef["keys"].(map[string]interface{})
		if !ok {
			dataType = new(gen.EnumerableDataType)
		} else {
			obj, err := a.CreateType(query, keys)
			if err != nil {
				return nil, err
			}
			dataType = obj
		}
	default:
		// First check if we already analyzed that type
		if t := a.Registry.GetNamedType(n); t != nil {
			return t, nil
		}

		// No then analyze it
		t, ok := a.RawTypes[n]
		if !ok {
			return nil, fmt.Errorf("Unknown type %s for %s", n, prettify(typeDef))
		}
		attrs, ok := t["attributes"]
		if !ok {
			// No attribute, it's a string
			s := gen.BasicDataType("string")
			dataType = &s
		} else {
			att := attrs.(map[string]interface{})
			obj := a.Registry.CreateNamedType(n)
			obj.Fields = make([]*gen.ActionParam, len(att))

			for idx, an := range sortedKeys(att) {
				at := att[an]
				aq := fmt.Sprintf("%s[%s]", query, an)
				ap, err := a.AnalyzeAttribute(an, aq, at.(map[string]interface{}))
				if err != nil {
					return nil, err
				}
				obj.Fields[idx] = ap
			}

			// We're done
			dataType = obj
		}
	}

	return dataType, nil
}
func (a *APIAnalyzer) CreateType(query string, attributes map[string]interface{}) (*gen.ObjectDataType, error) {
	name := inflect.Camelize(bracketRegexp.ReplaceAllLiteralString(query, "_") + "_struct")
	obj := a.Registry.CreateInlineType(name)
	obj.Fields = make([]*gen.ActionParam, len(attributes))
	for idx, an := range sortedKeys(attributes) {
		at := attributes[an]
		var childQ string
		if query == "payload" {
			childQ = an
		} else {
			childQ = fmt.Sprintf("%s[%s]", query, an)
		}
		att, err := a.AnalyzeAttribute(an, childQ, at.(map[string]interface{}))
		if err != nil {
			return nil, fmt.Errorf("Failed to compute type of attribute %s: %s", an, err)
		}
		obj.Fields[idx] = att
	}
	return obj, nil
}
func (d *Doc) Ref(r Ref) *Definition {
	if refIF, ok := r["$ref"]; ok {
		refKey := strings.TrimPrefix(refIF.(string), "#/definitions/")
		return d.Definitions[refKey]
	}
	return nil
}
func (r Ref) Type() string {
	if _, ok := r["$ref"]; ok {
		return "object"
	}
	if refIF, ok := r["type"]; ok {
		return refIF.(string)
	}
	return ""
}
func (r Ref) Required() []string {
	if refIF, ok := r["required"]; ok {
		return refIF.([]string)
	}
	return []string{}
}
func (r Ref) ID() string {
	if refIF, ok := r["$ref"]; ok {
		return strings.TrimPrefix(refIF.(string), "#/definitions/")
	}
	return ""
}
func (ep *Endpoint) Service() string {
	if len(ep.Tags) > 0 {
		return ep.Tags[0]
	}
	if len(ep.OperationID) > 0 {
		return strings.Split(ep.OperationID, "#")[0]
	}
	return ""
}
func (ep *Endpoint) Method() string {
	if strings.Contains(ep.OperationID, "#") {
		return strings.Split(ep.OperationID, "#")[1]
	}
	return ""
}
func NewTypeRegistry() *TypeRegistry {
	return &TypeRegistry{
		NamedTypes:  make(map[string]*gen.ObjectDataType),
		InlineTypes: make(map[string][]*gen.ObjectDataType),
	}
}
func (reg *TypeRegistry) GetNamedType(name string) *gen.ObjectDataType {
	return reg.NamedTypes[toGoTypeName(name)]
}
func (reg *TypeRegistry) CreateNamedType(name string) *gen.ObjectDataType {
	goName := toGoTypeName(name)
	obj := gen.ObjectDataType{TypeName: goName}
	if _, ok := reg.NamedTypes[goName]; ok {
		panic("BUG: Can't create two named types with same name....")
	}
	reg.NamedTypes[goName] = &obj
	return &obj
}
func (reg *TypeRegistry) CreateInlineType(name string) *gen.ObjectDataType {
	goName := toGoTypeName(name)
	obj := gen.ObjectDataType{TypeName: goName}
	reg.InlineTypes[goName] = append(reg.InlineTypes[goName], &obj)
	return &obj
}
func (reg *TypeRegistry) FinalizeTypeNames(d *gen.APIDescriptor) {
	for n, named := range reg.NamedTypes {
		reg.InlineTypes[n] = append(reg.InlineTypes[n], named)
	}
	d.FinalizeTypeNames(reg.InlineTypes)
}
func (d *APIDescriptor) Merge(other *APIDescriptor) error {
	if d.Version != other.Version {
		return fmt.Errorf("Can't merge API descriptors with different versions")
	}
	for _, name := range d.ResourceNames {
		for _, otherName := range other.ResourceNames {
			if name == otherName {
				return fmt.Errorf("%s is a resource that exists in multiple APIs, generate separate clients", name)
			}
		}
	}
	for _, name := range d.TypeNames {
		for i, otherName := range other.TypeNames {
			if name == otherName {
				newName := MakeUniq(otherName, append(d.TypeNames, other.TypeNames...))
				first := other.TypeNames[:i]
				last := append([]string{newName}, other.TypeNames[i+1:]...)
				other.TypeNames = append(first, last...)
				typ := other.Types[name]
				delete(other.Types, name)
				typ.TypeName = newName
				other.Types[newName] = typ
			}
		}
	}
	for name, resource := range other.Resources {
		d.Resources[name] = resource
	}
	for name, typ := range other.Types {
		d.Types[name] = typ
	}
	d.ResourceNames = append(d.ResourceNames, other.ResourceNames...)
	d.TypeNames = append(d.TypeNames, other.TypeNames...)
	return nil
}
func (d *APIDescriptor) FinalizeTypeNames(rawTypes map[string][]*ObjectDataType) {

	// 1. Make sure data type names don't clash with resource names
	rawTypeNames := make([]string, len(rawTypes))
	idx := 0
	for n := range rawTypes {
		rawTypeNames[idx] = n
		idx++
	}
	sort.Strings(rawTypeNames)
	for _, tn := range rawTypeNames {
		types := rawTypes[tn]
		for rn := range d.Resources {
			if tn == rn {
				oldTn := tn
				if strings.HasSuffix(tn, "Param") {
					tn = fmt.Sprintf("%s2", tn)
				} else {
					tn = fmt.Sprintf("%sParam", tn)
				}
				for _, ty := range types {
					ty.TypeName = tn
				}
				rawTypes[tn] = types
				delete(rawTypes, oldTn)
			}
		}
	}

	// 2. Make data type names unique
	idx = 0
	for n := range rawTypes {
		rawTypeNames[idx] = n
		idx++
	}
	sort.Strings(rawTypeNames)
	for _, tn := range rawTypeNames {
		types := rawTypes[tn]
		first := types[0]
		d.Types[tn] = first
		if len(types) > 1 {
			for i, ty := range types[1:] {
				found := false
				for j := 0; j < i+1; j++ {
					if ty.IsEquivalent(types[j]) {
						found = true
						break
					}
				}
				if !found {
					newName := d.uniqueTypeName(tn)
					ty.TypeName = newName
					d.Types[newName] = ty
					d.TypeNames = append(d.TypeNames, newName)
				}
			}
		}
	}

	// 3. Finally initialize .ResourceNames and .TypeNames
	idx = 0
	resourceNames := make([]string, len(d.Resources))
	for n := range d.Resources {
		resourceNames[idx] = n
		idx++
	}
	sort.Strings(resourceNames)
	d.ResourceNames = resourceNames

	typeNames := make([]string, len(d.Types))
	idx = 0
	for tn := range d.Types {
		typeNames[idx] = tn
		idx++
	}
	sort.Strings(typeNames)
	d.TypeNames = typeNames
}
func (d *APIDescriptor) uniqueTypeName(prefix string) string {
	u := fmt.Sprintf("%s%d", prefix, 2)
	taken := false
	for _, tn := range d.TypeNames {
		if tn == u {
			taken = true
			break
		}
	}
	idx := 3
	for taken {
		u = fmt.Sprintf("%s%d", prefix, idx)
		taken = false
		for _, tn := range d.TypeNames {
			if tn == u {
				taken = true
				break
			}
		}
		if taken {
			idx++
		}
	}
	return u
}
func (a *Action) MandatoryParams() []*ActionParam {
	m := make([]*ActionParam, len(a.Params))
	i := 0
	for _, p := range a.Params {
		if p.Mandatory {
			m[i] = p
			i++
		}
	}
	return m[:i]
}
func (a *Action) HasOptionalParams() bool {
	for _, param := range a.Params {
		if !param.Mandatory {
			return true
		}
	}
	return false
}
func MakeUniq(base string, taken []string) string {
	idx := 1
	uniq := base
	inuse := true
	for inuse {
		inuse = false
		for _, gn := range taken {
			if gn == uniq {
				inuse = true
				break
			}
		}
		if inuse {
			idx++
			uniq = base + strconv.Itoa(idx)
		}
	}
	return uniq
}
func NewClientWriter() (*ClientWriter, error) {
	funcMap := template.FuncMap{
		"comment":           comment,
		"commandLine":       commandLine,
		"parameters":        parameters,
		"paramsInitializer": paramsInitializer,
		"blankCondition":    blankCondition,
		"stripStar":         stripStar,
	}
	headerT, err := template.New("header-client").Funcs(funcMap).Parse(headerTmpl)
	if err != nil {
		return nil, err
	}
	resourceT, err := template.New("resource-client").Funcs(funcMap).Parse(resourceTmpl)
	if err != nil {
		return nil, err
	}
	return &ClientWriter{
		headerTmpl:   headerT,
		resourceTmpl: resourceT,
	}, nil
}
func (c *ClientWriter) WriteHeader(pkg, version string, needTime, needJSON bool, w io.Writer) error {
	ctx := map[string]interface{}{
		"Pkg":        pkg,
		"APIVersion": version,
		"NeedTime":   needTime,
		"NeedJSON":   needJSON,
	}
	return c.headerTmpl.Execute(w, ctx)
}
func (c *ClientWriter) WriteResourceHeader(name string, w io.Writer) {
	fmt.Fprintf(w, "/****** %s ******/\n\n", name)
}
func (c *ClientWriter) WriteType(o *gen.ObjectDataType, w io.Writer) {
	fields := make([]string, len(o.Fields))
	for i, f := range o.Fields {
		fields[i] = fmt.Sprintf("%s %s `json:\"%s,omitempty\"`", strings.Title(f.VarName),
			f.Signature(), f.Name)
	}
	decl := fmt.Sprintf("type %s struct {\n%s\n}", o.TypeName,
		strings.Join(fields, "\n\t"))
	fmt.Fprintf(w, "%s\n\n", decl)
}
func (c *ClientWriter) WriteResource(resource *gen.Resource, w io.Writer) error {
	return c.resourceTmpl.Execute(w, resource)
}
func (ec EvalCtx) WithTrail(t string) EvalCtx {
	newEC := ec
	trailCopy := make([]string, 0, len(ec.Trail)+1)
	for _, val := range ec.Trail {
		trailCopy = append(trailCopy, val)
	}
	newEC.Trail = append(trailCopy, t)
	return newEC
}
func (a *APIAnalyzer) AnalyzeEndpoint(verb string, path string, ep *Endpoint) error {
	path = joinPath(a.Doc.BasePath, path)
	dbg("\n------\nDEBUG AnalyzeEndpoint %s %s %+v\n", verb, path, ep)
	pattern := toPattern(verb, path)
	dbg("DEBUG AnalyzeEndpoint pattern %v\n", pattern)
	svc := ep.Service()

	// Get Resource -- base it on the service name for now
	res := a.api.Resources[svc]
	if res == nil {
		res = &gen.Resource{
			Name:       svc,
			ClientName: a.ClientName,
			Actions:    []*gen.Action{},
		}
		a.api.Resources[svc] = res
	}
	action := &gen.Action{
		Name:         ep.Method(),
		MethodName:   toMethodName(ep.Method()),
		Description:  cleanDescription(ep.Description),
		ResourceName: svc,
		PathPatterns: []*gen.PathPattern{pattern},
	}
	res.Actions = append(res.Actions, action)

	var returnDT gen.DataType
	var hasLocation bool
	for code, response := range ep.Responses {
		if code >= 300 {
			continue
		}

		if response.Headers != nil {
			if _, ok := response.Headers["Location"]; ok {
				hasLocation = true
			}
		}

		if response.Schema == nil {
			dbg("DEBUG MISSING SCHEMA SKIP!\n")
			break
		}
		dbg("DEBUG AnalyzeEndpoint %d RESPONSE %#v\n", code, response)

		returnDef := a.Doc.Ref(response.Schema)
		if returnDef != nil {
			ec := EvalCtx{IsResult: true, Trail: nil, Svc: res, Method: action}
			if mediaType(returnDef.Title) == "" {
				warn("Warning: AnalyzeEndpoint: MediaType not set for %s, will be hard to guess the result type\n", response.Schema.ID())
				continue
			}
			returnDT = a.AnalyzeDefinition(ec, returnDef, response.Schema.ID())
			if returnObj, ok := returnDT.(*gen.ObjectDataType); ok {
				isResourceType := verb == "get" && returnObj.TypeName == svc
				dbg("DEBUG AnalyzeEndpoint Path %s Verb %s returnTypeName %s svc %s\n", path, verb, returnObj.TypeName, svc)

				if isResourceType {
					res.Description = cleanDescription(ep.Description)
					res.Identifier = mediaType(returnDef.Title)
				}
				a.addType(ec, returnObj, response.Schema)
			}
		} else {
			returnDT = basicType(response.Schema.Type())
		}

		break
	}

	for _, p := range ep.Parameters {
		ec := EvalCtx{IsResult: false, Trail: nil, Svc: res, Method: action}
		ap := a.AnalyzeParam(ec, p)

		switch p.In {
		case "header":
			action.HeaderParamNames = append(action.HeaderParamNames, p.Name)
			action.Params = append(action.Params, ap)
		case "query":
			action.QueryParamNames = append(action.QueryParamNames, p.Name)
			action.Params = append(action.Params, ap)
		case "path":
			action.PathParamNames = append(action.PathParamNames, p.Name)
			action.Params = append(action.Params, ap)
		case "body":
			def := a.Doc.Ref(p.Schema)
			if def != nil {
				if def.Type == "array" {
					fail("Array type for body not implemented yet")
				} else if def.Type == "object" {
					// Flatten the first level of object
					dt := a.AnalyzeDefinition(ec, def, p.Schema.ID())
					if obj, ok := dt.(*gen.ObjectDataType); ok {
						a.addType(ec, obj, p.Schema)
						action.Payload = obj
						for _, f := range obj.Fields {
							action.PayloadParamNames = append(action.PayloadParamNames, f.Name)
							action.Params = append(action.Params, f)
						}
					}

				}
			}
		}
	}
	if returnDT != nil {
		action.Return = signature(returnDT)

	}
	action.ReturnLocation = hasLocation
	dbg("DEBUG ACTION %s", prettify(action))
	return nil
}
func (api *API) NetworkInterfaceLocator(href string) *NetworkInterfaceLocator {
	return &NetworkInterfaceLocator{Href(href), api}
}
func (api *API) NetworkInterfaceAttachmentLocator(href string) *NetworkInterfaceAttachmentLocator {
	return &NetworkInterfaceAttachmentLocator{Href(href), api}
}
func (r *RubyTime) UnmarshalJSON(b []byte) (err error) {
	s := string(b)
	t, err := time.Parse("2006/01/02 15:04:05 -0700", s[1:len(s)-1])
	if err != nil {
		return err
	}
	r.Time = t
	return nil
}
func (api *API) ExecutionLocator(href string) *ExecutionLocator {
	return &ExecutionLocator{Href(href), api}
}
func (api *API) NotificationLocator(href string) *NotificationLocator {
	return &NotificationLocator{Href(href), api}
}
func (api *API) OperationLocator(href string) *OperationLocator {
	return &OperationLocator{Href(href), api}
}
func (api *API) ScheduledActionLocator(href string) *ScheduledActionLocator {
	return &ScheduledActionLocator{Href(href), api}
}
func NewBasicAuthenticator(username, password string, accountID int) Authenticator {
	builder := basicLoginRequestBuilder{username: username, password: password, accountID: accountID}
	return newCookieSigner(&builder, accountID)
}
func NewSSAuthenticator(auther Authenticator, accountID int) Authenticator {
	if _, ok := auther.(*ssAuthenticator); ok {
		// Only wrap if not wrapped already
		return auther
	}
	return &ssAuthenticator{
		auther:    auther,
		accountID: accountID,
		refreshAt: time.Now().Add(-2 * time.Minute),
		client:    httpclient.NewNoRedirect(),
	}
}
func newCookieSigner(builder loginRequestBuilder, accountID int) Authenticator {
	return &cookieSigner{
		builder:   builder,
		accountID: accountID,
		refreshAt: time.Now().Add(-2 * time.Minute),
		client:    httpclient.NewNoRedirect(),
	}
}
func (s *cookieSigner) Sign(req *http.Request) error {
	if time.Now().After(s.refreshAt) {
		authReq, authErr := s.builder.BuildLoginRequest(s.host)
		if authErr != nil {
			return authErr
		}
		resp, err := s.client.DoHidden(authReq)
		if err != nil {
			return err
		}
		url, err := extractRedirectURL(resp)
		if err != nil {
			return err
		}
		if url != nil {
			authReq, authErr = s.builder.BuildLoginRequest(url.Host)
			if authErr != nil {
				return authErr
			}
			s.host = url.Host
			req.Host = url.Host
			req.URL.Host = url.Host
			resp, err = s.client.DoHidden(authReq)
		}
		if err != nil {
			return fmt.Errorf("Authentication failed: %s", err)
		}
		if err := s.refresh(resp); err != nil {
			return err
		}
	}
	for _, c := range s.cookies {
		req.AddCookie(c)
	}
	req.Header.Set("X-Account", strconv.Itoa(s.accountID))
	return nil
}
func (s *cookieSigner) CanAuthenticate(host string) error {
	_, instance := s.builder.(*instanceLoginRequestBuilder)
	return testAuth(s, s.client, host, instance)
}
func (s *cookieSigner) refresh(resp *http.Response) error {
	if resp.StatusCode != 204 {
		return fmt.Errorf("Authentication failed: %s", resp.Status)
	}
	s.cookies = resp.Cookies()
	s.refreshAt = time.Now().Add(time.Duration(2) * time.Hour)
	return nil
}
func (t *tokenAuthenticator) Sign(r *http.Request) error {
	r.Header.Set("Authorization", "Bearer "+t.token)
	if t.accountID != 0 {
		r.Header.Set("X-Account", strconv.Itoa(t.accountID))
	}
	return nil
}
func (a *rl10Authenticator) Sign(r *http.Request) error {
	r.Header.Set("X-RLL-Secret", a.secret)
	return nil
}
func (a *ssAuthenticator) Sign(r *http.Request) error {
	if time.Now().After(a.refreshAt) {
		u := buildURL(a.host, "api/catalog/new_session")
		u += "?account_id=" + strconv.Itoa(a.accountID)
		authReq, err := http.NewRequest("GET", u, nil)
		if err != nil {
			return err
		}
		if err := a.auther.Sign(authReq); err != nil {
			return err
		}

		// A bit tricky: if the auther is the cookie signer it could have updated the
		// host after being redirected.
		if ca, ok := a.auther.(*cookieSigner); ok {
			a.SetHost(ca.host)
			authReq.Host = a.host
			authReq.URL.Host = a.host
		}

		authReq.Header.Set("Content-Type", "application/json")
		resp, err := a.client.DoHidden(authReq)
		if err != nil {
			return fmt.Errorf("Authentication failed: %s", err)
		}
		if resp.StatusCode != 303 {
			body, err := ioutil.ReadAll(resp.Body)
			var msg string
			if err != nil {
				msg = " - <failed to read body>"
			}
			if len(body) > 0 {
				msg = " - " + string(body)
			}
			return fmt.Errorf("Authentication failed: %s%s", resp.Status, msg)
		}
		a.refreshAt = time.Now().Add(2 * time.Hour)
	}
	a.auther.Sign(r)
	r.Header.Set("X-Api-Version", "1.0")
	r.Host = a.host
	r.URL.Host = a.host

	return nil
}
func (a *ssAuthenticator) SetHost(host string) {
	a.auther.SetHost(host)
	urlElems := strings.Split(host, ".")
	hostPrefix := urlElems[0]
	elems := strings.Split(hostPrefix, "-")

	if len(elems) == 1 && elems[0] == "cm" {
		// accommodates micromoo host inference, such as "cm.rightscale.local" => "selfservice.rightscale.local"
		elems[0] = "selfservice"
	} else if len(elems) < 2 {
		// don't know how to compute this ss host; use the cm host
		a.host = host
		return
	} else {
		elems[len(elems)-2] = "selfservice"
	}
	ssLoginHostPrefix := strings.Join(elems, "-")
	a.host = strings.Join(append([]string{ssLoginHostPrefix}, urlElems[1:]...), ".")
}
func (a *ssAuthenticator) CanAuthenticate(host string) error {
	url := fmt.Sprintf("api/catalog/accounts/%d/user_preferences", a.accountID)
	req, err := http.NewRequest("GET", buildURL(host, url), nil)
	if err != nil {
		return err
	}
	req.Header.Set("X-Api-Version", "1.0")
	if err := a.Sign(req); err != nil {
		return err
	}
	resp, err := a.client.DoHidden(req)
	if err != nil {
		return err
	}
	if resp.StatusCode != 200 {
		var body string
		if b, err := ioutil.ReadAll(resp.Body); err != nil {
			body = ": " + string(b)
		}
		return fmt.Errorf("%s%s", resp.Status, body)
	}
	return nil
}
func extractRedirectURL(resp *http.Response) (*url.URL, error) {
	var u *url.URL
	if resp.StatusCode > 299 && resp.StatusCode < 399 {
		loc := resp.Header.Get("Location")
		if loc != "" {
			var err error
			u, err = url.Parse(loc)
			if err != nil {
				return nil, fmt.Errorf("invalid Location header '%s': %s", loc, err)
			}
		}
	}
	return u, nil
}
func buildURL(host, path string) string {
	scheme := "https"
	if httpclient.Insecure {
		scheme = "http"
	}
	u := url.URL{
		Scheme: scheme,
		Host:   host,
		Path:   path,
	}
	return u.String()
}
func (r *Resource) GetAction(name string) *Action {
	for _, a := range r.Actions {
		if a.Name == name {
			return a
		}
	}
	return nil
}
func (r *Resource) HasLink(name string) bool {
	for n, _ := range r.Links {
		if n == name {
			return true
		}
	}
	return false
}
func (r *Resource) findMatches(href string) []*PathPattern {
	var matches []*PathPattern
	for _, action := range r.Actions {
		for _, pattern := range action.PathPatterns {
			if pattern.Regexp.MatchString(href) || pattern.Regexp.MatchString(href+"/") {
				matches = append(matches, pattern)
			}
		}
	}
	return matches
}
func NewPB(pb *ParamBlock) HTTPClient {
	responseHeaderTimeout := pb.ResponseHeaderTimeout
	if responseHeaderTimeout == 0 {
		responseHeaderTimeout = defaultResponseHeaderTimeout
	}
	dumpFormat := pb.DumpFormat
	if dumpFormat == 0 {
		dumpFormat = NoDump
	}
	hiddenHeaders := pb.HiddenHeaders
	if hiddenHeaders == nil {
		hiddenHeaders = defaultHiddenHeaders // immutable
	} else {
		hiddenHeaders = copyHiddenHeaders(hiddenHeaders) // copy to avoid side-effects
	}
	dc := &dumpClient{Client: newRawClient(pb.NoRedirect, pb.NoCertCheck, pb.DisableKeepAlives, responseHeaderTimeout)}
	dc.isInsecure = func() bool {
		return pb.Insecure
	}
	dc.dumpFormat = func() Format {
		return dumpFormat
	}
	dc.hiddenHeaders = func() map[string]bool {
		return hiddenHeaders
	}
	return dc
}
func newVariableDumpClient(c *http.Client) HTTPClient {
	dc := &dumpClient{Client: c}
	dc.isInsecure = func() bool {
		return Insecure
	}
	dc.dumpFormat = func() Format {
		return DumpFormat
	}
	dc.hiddenHeaders = func() map[string]bool {
		return HiddenHeaders
	}
	return dc
}
func newRawClient(
	noRedirect, noCertCheck, disableKeepAlives bool,
	responseHeaderTimeout time.Duration) *http.Client {

	tr := http.Transport{
		DisableKeepAlives:     disableKeepAlives,
		ResponseHeaderTimeout: responseHeaderTimeout,
		Proxy: http.ProxyFromEnvironment,
	}
	tr.TLSClientConfig = &tls.Config{InsecureSkipVerify: noCertCheck}
	c := http.Client{Transport: &tr}
	if noRedirect {
		c.CheckRedirect = func(*http.Request, []*http.Request) error {
			return fmt.Errorf(noRedirectError)
		}
	}
	return &c
}
func (d *dumpClient) DoHidden(req *http.Request) (*http.Response, error) {
	return d.doImp(req, true, nil)
}
func (d *dumpClient) Do(req *http.Request) (*http.Response, error) {
	return d.doImp(req, false, nil)
}
func (d *dumpClient) doImp(req *http.Request, hidden bool, ctx context.Context) (*http.Response, error) {
	if req.URL.Scheme == "" {
		if d.isInsecure() {
			req.URL.Scheme = "http"
		} else {
			req.URL.Scheme = "https"
		}
	}

	//set user-agent if one is not provided.
	ua := req.Header.Get("User-Agent")
	if ua == "" {
		req.Header.Set("User-Agent", UA)
	}

	var reqBody []byte
	startedAt := time.Now()

	// prefer the X-Request-Id header as request token for logging, if present.
	id := req.Header.Get(requestIdHeader)
	if id == "" {
		id = ShortToken()
	}
	log.Info("started", "id", id, req.Method, req.URL.String())
	df := d.dumpFormat()
	hide := (df == NoDump) || (hidden && !df.IsVerbose())
	if !hide {
		startedAt = time.Now()
		reqBody = d.dumpRequest(req)
	}
	var resp *http.Response
	var err error
	if ctx == nil {
		resp, err = d.Client.Do(req)
	} else {
		resp, err = ctxhttpDo(ctx, d.getClientWithoutTimeout(), req)
	}
	if urlError, ok := err.(*url.Error); ok {
		if urlError.Err.Error() == noRedirectError {
			err = nil
		}
	}
	if err != nil {
		return nil, err
	}
	if !hide {
		d.dumpResponse(resp, req, reqBody)
	}
	log.Info("completed", "id", id, "status", resp.Status, "time", time.Since(startedAt).String())

	return resp, nil
}
func (d *dumpClient) getClientWithoutTimeout() *http.Client {
	// Get a copy of the client and modify as multiple concurrent go routines can be using this client.
	client := *d.Client
	tr, ok := client.Transport.(*http.Transport)
	if ok {
		// note that the http.Transport struct has internal mutex fields that are
		// not safe to copy. we have to be selective in copying fields.
		trCopy := &http.Transport{
			Proxy:                  tr.Proxy,
			DialContext:            tr.DialContext,
			Dial:                   tr.Dial,
			DialTLS:                tr.DialTLS,
			TLSClientConfig:        tr.TLSClientConfig,
			TLSHandshakeTimeout:    tr.TLSHandshakeTimeout,
			DisableKeepAlives:      tr.DisableKeepAlives,
			DisableCompression:     tr.DisableCompression,
			MaxIdleConns:           tr.MaxIdleConns,
			MaxIdleConnsPerHost:    tr.MaxIdleConnsPerHost,
			IdleConnTimeout:        tr.IdleConnTimeout,
			ResponseHeaderTimeout:  0, // explicitly zeroed-out
			ExpectContinueTimeout:  tr.ExpectContinueTimeout,
			TLSNextProto:           tr.TLSNextProto,
			MaxResponseHeaderBytes: tr.MaxResponseHeaderBytes,
		}
		tr = trCopy
	} else {
		// note that the following code has a known issue in that it depends on the
		// current value of the NoCertCheck global. if that global changes after
		// creation of this client then the behavior is undefined.
		tr = &http.Transport{Proxy: http.ProxyFromEnvironment}
		tr.TLSClientConfig = &tls.Config{InsecureSkipVerify: NoCertCheck}
	}
	client.Transport = tr
	return &client
}
func (d *dumpClient) dumpRequest(req *http.Request) []byte {
	df := d.dumpFormat()
	if df == NoDump {
		return nil
	}
	reqBody, err := dumpReqBody(req)
	if err != nil {
		log.Error("Failed to load request body for dump", "error", err.Error())
	}
	if df.IsDebug() {
		var buffer bytes.Buffer
		buffer.WriteString(req.Method + " " + req.URL.String() + "\n")
		d.writeHeaders(&buffer, req.Header)
		if reqBody != nil {
			buffer.WriteString("\n")
			buffer.Write(reqBody)
			buffer.WriteString("\n")
		}
		fmt.Fprint(OsStderr, buffer.String())
	} else if df.IsJSON() {
		return reqBody
	}
	return nil
}
func (d *dumpClient) writeHeaders(buffer *bytes.Buffer, headers http.Header) {
	filterHeaders(
		d.dumpFormat(),
		d.hiddenHeaders(),
		headers,
		func(name string, value []string) {
			buffer.WriteString(name)
			buffer.WriteString(": ")
			buffer.WriteString(strings.Join(value, ", "))
			buffer.WriteString("\n")
		})
}
func copyHiddenHeaders(from map[string]bool) (to map[string]bool) {
	to = make(map[string]bool)
	for k, v := range from {
		to[k] = v
	}
	return
}
func validateCommandLine(cmdLine *cmd.CommandLine) {
	if cmdLine.Command == "setup" ||
		cmdLine.Command == "actions" ||
		cmdLine.Command == "json" ||
		cmdLine.ShowHelp ||
		cmdLine.RL10 {
		return
	}
	if cmdLine.Account == 0 && cmdLine.OAuthToken == "" && cmdLine.OAuthAccessToken == "" && cmdLine.APIToken == "" && !cmdLine.NoAuth {
		kingpin.Fatalf("missing --account option")
	}
	if cmdLine.Host == "" {
		kingpin.Fatalf("missing --host option")
	}
	if cmdLine.Password == "" && cmdLine.OAuthToken == "" && cmdLine.OAuthAccessToken == "" && cmdLine.APIToken == "" && !cmdLine.NoAuth {
		kingpin.Fatalf("missing login info, use --email and --pwd or use --key, --apiToken or --rl10")
	}
}
func APIClient(name string, cmdLine *cmd.CommandLine) (cmd.CommandClient, error) {
	switch name {
	case Cm15Command:
		return cm15.FromCommandLine(cmdLine)
	case Cm16Command:
		return cm16.FromCommandLine(cmdLine)
	case SsCommand:
		return ss.FromCommandLine(cmdLine)
	case Rl10Command:
		return rl10.FromCommandLine(cmdLine)
	case CaCommand:
		return ca.FromCommandLine(cmdLine)
	case PolicyCommand:
		return policy.FromCommandLine(cmdLine)
	default:
		return nil, fmt.Errorf("No client for '%s'", name)
	}
}
func RegisterClientCommands(app *kingpin.Application) {
	cm15Cmd := app.Command(Cm15Command, cm15.APIName)
	registrar := rsapi.Registrar{APICmd: cm15Cmd}
	cm15.RegisterCommands(&registrar)

	cm16Cmd := app.Command(Cm16Command, cm16.APIName)
	registrar = rsapi.Registrar{APICmd: cm16Cmd}
	cm16.RegisterCommands(&registrar)

	ssCmd := app.Command(SsCommand, ss.APIName)
	registrar = rsapi.Registrar{APICmd: ssCmd}
	ss.RegisterCommands(&registrar)

	rl10Cmd := app.Command(Rl10Command, rl10.APIName)
	registrar = rsapi.Registrar{APICmd: rl10Cmd}
	rl10.RegisterCommands(&registrar)

	caCmd := app.Command(CaCommand, ca.APIName)
	registrar = rsapi.Registrar{APICmd: caCmd}
	ca.RegisterCommands(&registrar)

	policyCmd := app.Command(PolicyCommand, policy.APIName)
	registrar = rsapi.Registrar{APICmd: policyCmd}
	policy.RegisterCommands(&registrar)
}
func Interactive() {
	Logger.SetHandler(log15.MultiHandler(
		log15.LvlFilterHandler(
			log15.LvlError,
			log15.StderrHandler)))
}
func toPattern(verb, path string) *gen.PathPattern {
	pattern := gen.PathPattern{
		HTTPMethod: verb,
		Path:       path,
		Pattern:    pathVariablesRegexp.ReplaceAllLiteralString(path, "/%s"),
		Regexp: pathVariablesRegexp.ReplaceAllLiteralString(regexp.QuoteMeta(path),
			`/([^/]+)`),
	}
	matches := pathVariablesRegexp.FindAllStringSubmatch(path, -1)
	if len(matches) > 0 {
		pattern.Variables = make([]string, len(matches))
		for i, m := range matches {
			pattern.Variables[i] = m[1]
		}
	}
	return &pattern
}
func WithClientIP(ctx context.Context, ip net.IP) context.Context {
	if ip == nil {
		return ctx
	}
	return context.WithValue(ctx, clientIPKey{}, ip)
}
func ClientIP(ctx context.Context) net.IP {
	ip, _ := ctx.Value(clientIPKey{}).(net.IP)
	return ip
}
func NewProducer(config ProducerConfig) (p *Producer, err error) {
	config.defaults()

	p = &Producer{
		reqs:         make(chan ProducerRequest, config.MaxConcurrency),
		done:         make(chan struct{}),
		address:      config.Address,
		topic:        config.Topic,
		dialTimeout:  config.DialTimeout,
		readTimeout:  config.ReadTimeout,
		writeTimeout: config.WriteTimeout,
	}

	return
}
func StartProducer(config ProducerConfig) (p *Producer, err error) {
	p, err = NewProducer(config)
	if err != nil {
		return
	}

	p.Start()
	return
}
func (p *Producer) Start() {
	if p.started {
		panic("(*Producer).Start has already been called")
	}

	concurrency := cap(p.reqs)
	p.join.Add(concurrency)
	for i := 0; i != concurrency; i++ {
		go p.run()
	}

	p.started = true
}
func (p *Producer) Stop() {
	p.once.Do(p.stop)
	err := errors.New("publishing to a producer that was already stopped")

	for req := range p.reqs {
		req.complete(err)
	}

	p.join.Wait()
}
func (p *Producer) Publish(message []byte) (err error) {
	return p.PublishTo(p.topic, message)
}
func (p *Producer) PublishTo(topic string, message []byte) (err error) {
	defer func() {
		if recover() != nil {
			err = errors.New("publishing to a producer that was already stopped")
		}
	}()

	if len(topic) == 0 {
		return errors.New("no topic set for publishing message")
	}

	response := make(chan error, 1)
	deadline := time.Now().Add(p.dialTimeout + p.readTimeout + p.writeTimeout)

	// Attempts to queue the request so one of the active connections can pick
	// it up.
	p.reqs <- ProducerRequest{
		Topic:    topic,
		Message:  message,
		Response: response,
		Deadline: deadline,
	}

	// This will always trigger, either if the connection was lost or if a
	// response was successfully sent.
	err = <-response
	return
}
func NewLocalEngine(config LocalConfig) *LocalEngine {
	if config.NodeTimeout == 0 {
		config.NodeTimeout = DefaultLocalEngineNodeTimeout
	}

	if config.TombstoneTimeout == 0 {
		config.TombstoneTimeout = DefaultLocalEngineTombstoneTimeout
	}

	e := &LocalEngine{
		nodeTimeout: config.NodeTimeout,
		tombTimeout: config.TombstoneTimeout,

		done: make(chan struct{}),
		join: make(chan struct{}),

		nodes: make(map[string]*LocalNode),
	}

	go e.run()
	return e
}
func (c *ConsumerConfig) validate() error {
	if len(c.Topic) == 0 {
		return errors.New("creating a new consumer requires a non-empty topic")
	}

	if len(c.Channel) == 0 {
		return errors.New("creating a new consumer requires a non-empty channel")
	}

	return nil
}
func (c *ConsumerConfig) defaults() {
	if c.MaxInFlight == 0 {
		c.MaxInFlight = DefaultMaxInFlight
	}

	if c.DialTimeout == 0 {
		c.DialTimeout = DefaultDialTimeout
	}

	if c.ReadTimeout == 0 {
		c.ReadTimeout = DefaultReadTimeout
	}

	if c.WriteTimeout == 0 {
		c.WriteTimeout = DefaultWriteTimeout
	}
}
func NewConsumer(config ConsumerConfig) (c *Consumer, err error) {
	if err = config.validate(); err != nil {
		return
	}

	config.defaults()

	c = &Consumer{
		msgs: make(chan Message, config.MaxInFlight),
		done: make(chan struct{}),

		topic:        config.Topic,
		channel:      config.Channel,
		address:      config.Address,
		lookup:       append([]string{}, config.Lookup...),
		maxInFlight:  config.MaxInFlight,
		identify:     setIdentifyDefaults(config.Identify),
		dialTimeout:  config.DialTimeout,
		readTimeout:  config.ReadTimeout,
		writeTimeout: config.WriteTimeout,

		conns: make(map[string](chan<- Command)),
	}

	return
}
func StartConsumer(config ConsumerConfig) (c *Consumer, err error) {
	c, err = NewConsumer(config)
	if err != nil {
		return
	}

	c.Start()
	return
}
func (c *Consumer) Start() {
	if c.started {
		panic("(*Consumer).Start has already been called")
	}

	go c.run()

	c.started = true
}
func RateLimit(limit int, messages <-chan Message) <-chan Message {
	if limit <= 0 {
		return messages
	}

	output := make(chan Message)

	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()
		defer close(output)

		input := messages
		count := 0

		for {
			select {
			case <-ticker.C:
				count = 0
				input = messages

			case msg, ok := <-input:
				if !ok {
					return
				}

				output <- msg

				if count++; count >= limit {
					input = nil
				}
			}
		}
	}()

	return output
}
func (r RawResponse) Write(w *bufio.Writer) error {
	return writeResponse(w, []byte(r))
}
func ReadResponse(r *bufio.Reader) (res Response, err error) {
	var data []byte
	var size int32

	if err = binary.Read(r, binary.BigEndian, &size); err != nil {
		return
	}

	data = make([]byte, int(size))

	if _, err = io.ReadFull(r, data); err != nil {
		return
	}

	switch {
	case bytes.Equal(data, []byte("OK")):
		res = OK{}

	case bytes.HasPrefix(data, []byte("E_")):
		res = readError(data)

	default:
		res = RawResponse(data)
	}

	return
}
func backoff(rand *rand.Rand, attempt int, min, max time.Duration) time.Duration {
	if attempt <= 0 {
		panic("tube.Backoff: attempt <= 0")
	}

	if min > max {
		panic("tube.Backoff: min > max")
	}

	// Hardcoded backoff coefficient, maybe we'll make it configuration in the
	// future?
	const coeff = 2.0
	return jitteredBackoff(rand, attempt, min, max, coeff)
}
func (t FrameType) String() string {
	switch t {
	case FrameTypeResponse:
		return "response"

	case FrameTypeError:
		return "error"

	case FrameTypeMessage:
		return "message"

	default:
		return "frame <" + strconv.Itoa(int(t)) + ">"
	}
}
func NewConsulEngine(config ConsulConfig) *ConsulEngine {
	if len(config.Address) == 0 {
		config.Address = DefaultConsulAddress
	}

	if len(config.Namespace) == 0 {
		config.Namespace = DefaultConsulNamespace
	}

	if config.NodeTimeout == 0 {
		config.NodeTimeout = DefaultLocalEngineNodeTimeout
	}

	if config.TombstoneTimeout == 0 {
		config.TombstoneTimeout = DefaultLocalEngineTombstoneTimeout
	}

	if !strings.Contains(config.Address, "://") {
		config.Address = "http://" + config.Address
	}

	return &ConsulEngine{
		client:      http.Client{Transport: config.Transport},
		address:     config.Address,
		namespace:   config.Namespace,
		nodeTimeout: config.NodeTimeout,
		tombTimeout: config.TombstoneTimeout,
	}
}
func ParseMessageID(s string) (id MessageID, err error) {
	var v uint64
	v, err = strconv.ParseUint(s, 16, 64)
	id = MessageID(v)
	return
}
func (id MessageID) WriteTo(w io.Writer) (int64, error) {
	a := [16]byte{}
	b := strconv.AppendUint(a[:0], uint64(id), 16)
	n := len(a) - len(b)
	copy(a[n:], b)

	for i := 0; i != n; i++ {
		a[i] = '0'
	}

	c, e := w.Write(a[:])
	return int64(c), e
}
func NewMessage(id MessageID, body []byte, cmdChan chan<- Command) *Message {
	return &Message{
		ID:      id,
		Body:    body,
		cmdChan: cmdChan,
	}
}
func (m *Message) Finish() {
	if m.Complete() {
		panic("(*Message).Finish or (*Message).Requeue has already been called")
	}
	defer func() { recover() }() // the connection may have been closed asynchronously
	m.cmdChan <- Fin{MessageID: m.ID}
	m.cmdChan = nil
}
func (m *Message) Requeue(timeout time.Duration) {
	if m.Complete() {
		panic("(*Message).Finish or (*Message).Requeue has already been called")
	}
	defer func() { recover() }() // the connection may have been closed asynchronously
	m.cmdChan <- Req{MessageID: m.ID, Timeout: timeout}
	m.cmdChan = nil
}
func ReadCommand(r *bufio.Reader) (cmd Command, err error) {
	var line string

	if line, err = r.ReadString('\n'); err != nil {
		return
	}

	parts := strings.Split(strings.TrimSpace(line), " ")

	if len(parts) == 0 {
		err = makeErrInvalid("invalid empty command")
		return
	}

	switch name, args := parts[0], parts[1:]; name {
	case "PING":
		return readPing(args...)

	case "IDENTIFY":
		return readIdentify(r, args...)

	case "REGISTER":
		return readRegister(args...)

	case "UNREGISTER":
		return readUnregister(args...)

	default:
		err = makeErrInvalid("invalid command %s", name)
		return
	}
}
func (tmpl *Template) funcMapMaker(req *http.Request, writer http.ResponseWriter) template.FuncMap {
	var funcMap = template.FuncMap{}

	for key, fc := range tmpl.render.funcMaps {
		funcMap[key] = fc
	}

	if tmpl.render.Config.FuncMapMaker != nil {
		for key, fc := range tmpl.render.Config.FuncMapMaker(tmpl.render, req, writer) {
			funcMap[key] = fc
		}
	}

	for key, fc := range tmpl.funcMap {
		funcMap[key] = fc
	}
	return funcMap
}
func (tmpl *Template) Funcs(funcMap template.FuncMap) *Template {
	tmpl.funcMap = funcMap
	return tmpl
}
func (tmpl *Template) Execute(templateName string, obj interface{}, req *http.Request, w http.ResponseWriter) error {
	result, err := tmpl.Render(templateName, obj, req, w)
	if err == nil {
		if w.Header().Get("Content-Type") == "" {
			w.Header().Set("Content-Type", "text/html")
		}

		_, err = w.Write([]byte(result))
	}
	return err
}
func (fs *AssetFileSystem) RegisterPath(pth string) error {
	if _, err := os.Stat(pth); !os.IsNotExist(err) {
		var existing bool
		for _, p := range fs.paths {
			if p == pth {
				existing = true
				break
			}
		}
		if !existing {
			fs.paths = append(fs.paths, pth)
		}
		return nil
	}
	return errors.New("not found")
}
func (fs *AssetFileSystem) Asset(name string) ([]byte, error) {
	for _, pth := range fs.paths {
		if _, err := os.Stat(filepath.Join(pth, name)); err == nil {
			return ioutil.ReadFile(filepath.Join(pth, name))
		}
	}
	return []byte{}, fmt.Errorf("%v not found", name)
}
func (fs *AssetFileSystem) Glob(pattern string) (matches []string, err error) {
	for _, pth := range fs.paths {
		if results, err := filepath.Glob(filepath.Join(pth, pattern)); err == nil {
			for _, result := range results {
				matches = append(matches, strings.TrimPrefix(result, pth))
			}
		}
	}
	return
}
func (fs *AssetFileSystem) NameSpace(nameSpace string) Interface {
	if fs.nameSpacedFS == nil {
		fs.nameSpacedFS = map[string]Interface{}
	}
	fs.nameSpacedFS[nameSpace] = &AssetFileSystem{}
	return fs.nameSpacedFS[nameSpace]
}
func New(config *Config, viewPaths ...string) *Render {
	if config == nil {
		config = &Config{}
	}

	if config.DefaultLayout == "" {
		config.DefaultLayout = DefaultLayout
	}

	if config.AssetFileSystem == nil {
		config.AssetFileSystem = assetfs.AssetFS().NameSpace("views")
	}

	config.ViewPaths = append(append(config.ViewPaths, viewPaths...), DefaultViewPath)

	render := &Render{funcMaps: map[string]interface{}{}, Config: config}

	for _, viewPath := range config.ViewPaths {
		render.RegisterViewPath(viewPath)
	}

	return render
}
func (render *Render) RegisterViewPath(paths ...string) {
	for _, pth := range paths {
		if filepath.IsAbs(pth) {
			render.ViewPaths = append(render.ViewPaths, pth)
			render.AssetFileSystem.RegisterPath(pth)
		} else {
			if absPath, err := filepath.Abs(pth); err == nil && isExistingDir(absPath) {
				render.ViewPaths = append(render.ViewPaths, absPath)
				render.AssetFileSystem.RegisterPath(absPath)
			} else if isExistingDir(filepath.Join(utils.AppRoot, "vendor", pth)) {
				render.AssetFileSystem.RegisterPath(filepath.Join(utils.AppRoot, "vendor", pth))
			} else {
				for _, gopath := range utils.GOPATH() {
					if p := filepath.Join(gopath, "src", pth); isExistingDir(p) {
						render.ViewPaths = append(render.ViewPaths, p)
						render.AssetFileSystem.RegisterPath(p)
					}
				}
			}
		}
	}
}
func (render *Render) SetAssetFS(assetFS assetfs.Interface) {
	for _, viewPath := range render.ViewPaths {
		assetFS.RegisterPath(viewPath)
	}

	render.AssetFileSystem = assetFS
}
func (render *Render) Layout(name string) *Template {
	return &Template{render: render, layout: name}
}
func (render *Render) Funcs(funcMap template.FuncMap) *Template {
	tmpl := &Template{render: render, usingDefaultLayout: true}
	return tmpl.Funcs(funcMap)
}
func (render *Render) Execute(name string, context interface{}, request *http.Request, writer http.ResponseWriter) error {
	tmpl := &Template{render: render, usingDefaultLayout: true}
	return tmpl.Execute(name, context, request, writer)
}
func (render *Render) RegisterFuncMap(name string, fc interface{}) {
	if render.funcMaps == nil {
		render.funcMaps = template.FuncMap{}
	}
	render.funcMaps[name] = fc
}
func (render *Render) Asset(name string) ([]byte, error) {
	return render.AssetFileSystem.Asset(name)
}
func NewPlainClient(identity, username, password string) Client {
	return &plainClient{identity, username, password}
}
func Create(url string, h http.Header, c *Config) (io.WriteCloser, error) {
	if c == nil {
		c = DefaultConfig
	}
	return newUploader(url, h, c)
}
func Open(url string, c *Config) (io.ReadCloser, error) {
	if c == nil {
		c = DefaultConfig
	}
	// TODO(kr): maybe parallel range fetching
	r, _ := http.NewRequest("GET", url, nil)
	r.Header.Set("Date", time.Now().UTC().Format(http.TimeFormat))
	c.Sign(r, *c.Keys)
	client := c.Client
	if client == nil {
		client = http.DefaultClient
	}
	resp, err := client.Do(r)
	if err != nil {
		return nil, err
	}
	if resp.StatusCode != 200 {
		return nil, newRespError(resp)
	}
	return resp.Body, nil
}
func Sign(r *http.Request, k Keys) {
	DefaultService.Sign(r, k)
}
func (s *Service) Sign(r *http.Request, k Keys) {
	if k.SecurityToken != "" {
		r.Header.Set("X-Amz-Security-Token", k.SecurityToken)
	}
	h := hmac.New(sha1.New, []byte(k.SecretKey))
	s.writeSigData(h, r)
	sig := make([]byte, base64.StdEncoding.EncodedLen(h.Size()))
	base64.StdEncoding.Encode(sig, h.Sum(nil))
	r.Header.Set("Authorization", "AWS "+k.AccessKey+":"+string(sig))
}
func (f *File) Readdir(n int) ([]os.FileInfo, error) {
	if f.result != nil && !f.result.IsTruncated {
		return make([]os.FileInfo, 0), io.EOF
	}

	reader, err := f.sendRequest(n)
	if err != nil {
		return nil, err
	}
	defer reader.Close()

	return f.parseResponse(reader)
}
func Find(x tree.Node, p pathexpr.PathExpr) []tree.Node {
	ret := []tree.Node{}

	if p.Axis == "" {
		findChild(x, &p, &ret)
		return ret
	}

	f := findMap[p.Axis]
	f(x, &p, &ret)

	return ret
}
func Lex(xpath string) chan XItem {
	l := &Lexer{
		input: xpath,
		items: make(chan XItem),
	}
	go l.run()
	return l.items
}
func MustParseXML(r io.Reader, op ...ParseSettings) tree.Node {
	ret, err := ParseXML(r, op...)

	if err != nil {
		panic(err)
	}

	return ret
}
func ParseXML(r io.Reader, op ...ParseSettings) (tree.Node, error) {
	ov := ParseOptions{
		Strict:  true,
		XMLRoot: xmlele.Root,
	}
	for _, i := range op {
		i(&ov)
	}

	dec := xml.NewDecoder(r)
	dec.CharsetReader = charset.NewReaderLabel
	dec.Strict = ov.Strict

	ordrPos := 1
	xmlTree := ov.XMLRoot()

	t, err := dec.Token()

	if err != nil {
		return nil, err
	}

	if head, ok := t.(xml.ProcInst); ok && head.Target == "xml" {
		t, err = dec.Token()
	}

	opts := xmlbuilder.BuilderOpts{
		Dec: dec,
	}

	for err == nil {
		switch xt := t.(type) {
		case xml.StartElement:
			setEle(&opts, xmlTree, xt, &ordrPos)
			xmlTree = xmlTree.CreateNode(&opts)
		case xml.CharData:
			setNode(&opts, xmlTree, xt, tree.NtChd, &ordrPos)
			xmlTree = xmlTree.CreateNode(&opts)
		case xml.Comment:
			setNode(&opts, xmlTree, xt, tree.NtComm, &ordrPos)
			xmlTree = xmlTree.CreateNode(&opts)
		case xml.ProcInst:
			setNode(&opts, xmlTree, xt, tree.NtPi, &ordrPos)
			xmlTree = xmlTree.CreateNode(&opts)
		case xml.EndElement:
			xmlTree = xmlTree.EndElem()
		case xml.Directive:
			if dp, ok := xmlTree.(DirectiveParser); ok {
				dp.Directive(xt.Copy(), dec)
			}
		}

		t, err = dec.Token()
	}

	if err == io.EOF {
		err = nil
	}

	return xmlTree, err
}
func (w Wrap) Call(c Ctx, args ...Result) (Result, error) {
	switch w.LastArgOpt {
	case Optional:
		if len(args) == w.NArgs || len(args) == w.NArgs-1 {
			return w.Fn(c, args...)
		}
	case Variadic:
		if len(args) >= w.NArgs-1 {
			return w.Fn(c, args...)
		}
	default:
		if len(args) == w.NArgs {
			return w.Fn(c, args...)
		}
	}
	return nil, fmt.Errorf("Invalid number of arguments")
}
func Parse(xp string) (XPathExec, error) {
	n, err := parser.Parse(xp)
	return XPathExec{n: n}, err
}
func MustParse(xp string) XPathExec {
	ret, err := Parse(xp)
	if err != nil {
		panic(err)
	}
	return ret
}
func (xp XPathExec) Exec(t tree.Node, opts ...FuncOpts) (tree.Result, error) {
	o := &Opts{
		NS:    make(map[string]string),
		Funcs: make(map[xml.Name]tree.Wrap),
		Vars:  make(map[string]tree.Result),
	}
	for _, i := range opts {
		i(o)
	}
	return execxp.Exec(xp.n, t, o.NS, o.Funcs, o.Vars)
}
func (xp XPathExec) ExecBool(t tree.Node, opts ...FuncOpts) (bool, error) {
	res, err := xp.Exec(t, opts...)
	if err != nil {
		return false, err
	}

	b, ok := res.(tree.IsBool)
	if !ok {
		return false, fmt.Errorf("Cannot convert result to a boolean")
	}

	return bool(b.Bool()), nil
}
func (xp XPathExec) ExecNum(t tree.Node, opts ...FuncOpts) (float64, error) {
	res, err := xp.Exec(t, opts...)
	if err != nil {
		return 0, err
	}

	n, ok := res.(tree.IsNum)
	if !ok {
		return 0, fmt.Errorf("Cannot convert result to a number")
	}

	return float64(n.Num()), nil
}
func (xp XPathExec) ExecNode(t tree.Node, opts ...FuncOpts) (tree.NodeSet, error) {
	res, err := xp.Exec(t, opts...)
	if err != nil {
		return nil, err
	}

	n, ok := res.(tree.NodeSet)
	if !ok {
		return nil, fmt.Errorf("Cannot convert result to a node-set")
	}

	return n, nil
}
func (xp XPathExec) MustExec(t tree.Node, opts ...FuncOpts) tree.Result {
	res, err := xp.Exec(t, opts...)
	if err != nil {
		panic(err)
	}
	return res
}
func ParseExec(xpstr string, t tree.Node, opts ...FuncOpts) (tree.Result, error) {
	xp, err := Parse(xpstr)
	if err != nil {
		return nil, err
	}
	return xp.Exec(t, opts...)
}
func (x *XMLEle) CreateNode(opts *xmlbuilder.BuilderOpts) xmlbuilder.XMLBuilder {
	if opts.NodeType == tree.NtElem {
		ele := &XMLEle{
			StartElement: opts.Tok.(xml.StartElement),
			NSBuilder:    tree.NSBuilder{NS: opts.NS},
			Attrs:        make([]tree.Node, len(opts.Attrs)),
			Parent:       x,
			NodePos:      tree.NodePos(opts.NodePos),
			NodeType:     opts.NodeType,
		}
		for i := range opts.Attrs {
			ele.Attrs[i] = xmlnode.XMLNode{
				Token:    opts.Attrs[i],
				NodePos:  tree.NodePos(opts.AttrStartPos + i),
				NodeType: tree.NtAttr,
				Parent:   ele,
			}
		}
		x.Children = append(x.Children, ele)
		return ele
	}

	node := xmlnode.XMLNode{
		Token:    opts.Tok,
		NodePos:  tree.NodePos(opts.NodePos),
		NodeType: opts.NodeType,
		Parent:   x,
	}
	x.Children = append(x.Children, node)
	return x
}
func (x *XMLEle) GetChildren() []tree.Node {
	ret := make([]tree.Node, len(x.Children))

	for i := range x.Children {
		ret[i] = x.Children[i]
	}

	return ret
}
func (x *XMLEle) GetAttrs() []tree.Node {
	ret := make([]tree.Node, len(x.Attrs))
	for i := range x.Attrs {
		ret[i] = x.Attrs[i]
	}
	return ret
}
func (x *XMLEle) ResValue() string {
	ret := ""
	for i := range x.Children {
		switch x.Children[i].GetNodeType() {
		case tree.NtChd, tree.NtElem, tree.NtRoot:
			ret += x.Children[i].ResValue()
		}
	}
	return ret
}
func Parse(xp string) (*Node, error) {
	var err error
	c := lexer.Lex(xp)
	n := &Node{}
	p := &parseStack{cur: n}

	for next := range c {
		if next.Typ != lexer.XItemError {
			parseMap[next.Typ](p, next)
		} else if err == nil {
			err = fmt.Errorf(next.Val)
		}
	}

	return n, err
}
func (a XMLNode) GetToken() xml.Token {
	if a.NodeType == tree.NtAttr {
		ret := a.Token.(*xml.Attr)
		return *ret
	}
	return a.Token
}
func (a XMLNode) ResValue() string {
	switch a.NodeType {
	case tree.NtAttr:
		return a.Token.(*xml.Attr).Value
	case tree.NtChd:
		return string(a.Token.(xml.CharData))
	case tree.NtComm:
		return string(a.Token.(xml.Comment))
	}
	//case tree.NtPi:
	return string(a.Token.(xml.ProcInst).Inst)
}
func Exec(n *parser.Node, t tree.Node, ns map[string]string, fns map[xml.Name]tree.Wrap, v map[string]tree.Result) (tree.Result, error) {
	f := xpFilt{
		t:         t,
		ns:        ns,
		ctx:       tree.NodeSet{t},
		fns:       fns,
		variables: v,
	}

	return exec(&f, n)
}
func (n Num) String() string {
	if math.IsInf(float64(n), 0) {
		if math.IsInf(float64(n), 1) {
			return "Infinity"
		}
		return "-Infinity"
	}
	return fmt.Sprintf("%g", float64(n))
}
func (s String) Num() Num {
	num, err := strconv.ParseFloat(strings.TrimSpace(string(s)), 64)
	if err != nil {
		return Num(math.NaN())
	}
	return Num(num)
}
func BuildNS(t Elem) (ret []NS) {
	vals := make(map[xml.Name]string)

	if nselem, ok := t.(NSElem); ok {
		buildNS(nselem, vals)

		ret = make([]NS, 0, len(vals))
		i := 1

		for k, v := range vals {
			if !(k.Local == "xmlns" && k.Space == "" && v == "") {
				ret = append(ret, NS{
					Attr:     xml.Attr{Name: k, Value: v},
					Parent:   t,
					NodeType: NtNs,
				})
				i++
			}
		}

		sort.Sort(nsValueSort(ret))
		for i := range ret {
			ret[i].NodePos = NodePos(t.Pos() + i + 1)
		}
	}

	return ret
}
func GetAttribute(n Elem, local, space string) (xml.Attr, bool) {
	attrs := n.GetAttrs()
	for _, i := range attrs {
		attr := i.GetToken().(xml.Attr)
		if local == attr.Name.Local && space == attr.Name.Space {
			return attr, true
		}
	}
	return xml.Attr{}, false
}
func GetAttributeVal(n Elem, local, space string) (string, bool) {
	attr, ok := GetAttribute(n, local, space)
	return attr.Value, ok
}
func GetAttrValOrEmpty(n Elem, local, space string) string {
	val, ok := GetAttributeVal(n, local, space)
	if !ok {
		return ""
	}
	return val
}
func FindNodeByPos(n Node, pos int) Node {
	if n.Pos() == pos {
		return n
	}

	if elem, ok := n.(Elem); ok {
		chldrn := elem.GetChildren()
		for i := 1; i < len(chldrn); i++ {
			if chldrn[i-1].Pos() <= pos && chldrn[i].Pos() > pos {
				return FindNodeByPos(chldrn[i-1], pos)
			}
		}

		if len(chldrn) > 0 {
			if chldrn[len(chldrn)-1].Pos() <= pos {
				return FindNodeByPos(chldrn[len(chldrn)-1], pos)
			}
		}

		attrs := elem.GetAttrs()
		for _, i := range attrs {
			if i.Pos() == pos {
				return i
			}
		}

		ns := BuildNS(elem)
		for _, i := range ns {
			if i.Pos() == pos {
				return i
			}
		}
	}

	return nil
}
func Marshal(n tree.Node, w io.Writer) error {
	return marshal(n, w)
}
func MarshalStr(n tree.Node) (string, error) {
	ret := bytes.NewBufferString("")
	err := marshal(n, ret)

	return ret.String(), err
}
func NewLexer(r io.Reader, posix, whitespacesplit bool) *Lexer {
	return &Lexer{
		reader:          bufio.NewReader(r),
		tokenizer:       &DefaultTokenizer{},
		posix:           posix,
		whitespacesplit: whitespacesplit,
	}
}
func NewLexerString(s string, posix, whitespacesplit bool) *Lexer {
	return NewLexer(strings.NewReader(s), posix, whitespacesplit)
}
func Split(s string, posix bool) ([]string, error) {
	return NewLexerString(s, posix, true).Split()
}
func (r *registry) Register(err *ErrDescriptor) {
	r.Lock()
	defer r.Unlock()

	if err.Code == NoCode {
		panic(fmt.Errorf("No code defined in error descriptor (message: `%s`)", err.MessageFormat))
	}

	if r.byCode[err.Code] != nil {
		panic(fmt.Errorf("errors: Duplicate error code %v registered", err.Code))
	}

	err.registered = true
	r.byCode[err.Code] = err
}
func (r *registry) Get(code Code) *ErrDescriptor {
	r.RLock()
	defer r.RUnlock()
	return r.byCode[code]
}
func (r *registry) GetAll() []*ErrDescriptor {
	r.RLock()
	defer r.RUnlock()

	res := make([]*ErrDescriptor, 0, len(r.byCode))
	for _, d := range r.byCode {
		res = append(res, d)
	}
	return res
}
func From(in error) Error {
	if err, ok := in.(Error); ok {
		return err
	}

	return FromGRPC(in)
}
func Descriptor(in error) (desc *ErrDescriptor) {
	err := From(in)
	descriptor := Get(err.Code())
	if descriptor != nil {
		return descriptor
	}

	// return a new error descriptor with sane defaults
	return &ErrDescriptor{
		MessageFormat: err.Error(),
		Type:          err.Type(),
		Code:          err.Code(),
	}
}
func GetAttributes(err error) Attributes {
	e, ok := err.(Error)
	if ok {
		return e.Attributes()
	}

	return Attributes{}
}
func (t Type) HTTPStatusCode() int {
	switch t {
	case Canceled:
		return http.StatusRequestTimeout
	case InvalidArgument:
		return http.StatusBadRequest
	case OutOfRange:
		return http.StatusBadRequest
	case NotFound:
		return http.StatusNotFound
	case Conflict:
		return http.StatusConflict
	case AlreadyExists:
		return http.StatusConflict
	case Unauthorized:
		return http.StatusUnauthorized
	case PermissionDenied:
		return http.StatusForbidden
	case Timeout:
		return http.StatusRequestTimeout
	case NotImplemented:
		return http.StatusNotImplemented
	case TemporarilyUnavailable:
		return http.StatusBadGateway
	case PermanentlyUnavailable:
		return http.StatusGone
	case ResourceExhausted:
		return http.StatusForbidden
	case Internal:
		return http.StatusInternalServerError
	case Unknown:
		return http.StatusInternalServerError
	}

	return http.StatusInternalServerError
}
func HTTPStatusCode(err error) int {
	e, ok := err.(Error)
	if ok {
		return e.Type().HTTPStatusCode()
	}

	return http.StatusInternalServerError
}
func HTTPStatusToType(status int) Type {
	switch status {
	case http.StatusBadRequest:
		return InvalidArgument
	case http.StatusNotFound:
		return NotFound
	case http.StatusConflict:
		return Conflict
	case http.StatusUnauthorized:
		return Unauthorized
	case http.StatusForbidden:
		return PermissionDenied
	case http.StatusRequestTimeout:
		return Timeout
	case http.StatusNotImplemented:
		return NotImplemented
	case http.StatusBadGateway:
	case http.StatusServiceUnavailable:
		return TemporarilyUnavailable
	case http.StatusGone:
		return PermanentlyUnavailable
	case http.StatusTooManyRequests:
		return ResourceExhausted
	case http.StatusInternalServerError:
		return Unknown
	}
	return Unknown
}
func ToHTTP(in error, w http.ResponseWriter) error {
	w.Header().Set("Content-Type", "application/json")
	if err, ok := in.(Error); ok {
		w.Header().Set(CodeHeader, err.Code().String())
		w.WriteHeader(err.Type().HTTPStatusCode())
		return json.NewEncoder(w).Encode(toJSON(err))
	}

	w.WriteHeader(http.StatusInternalServerError)
	return json.NewEncoder(w).Encode(&jsonError{
		Message: in.Error(),
		Type:    Unknown,
	})
}
func toImpl(err Error) *impl {
	if i, ok := err.(*impl); ok {
		return i
	}

	return &impl{
		message:    err.Error(),
		code:       err.Code(),
		typ:        err.Type(),
		attributes: err.Attributes(),
	}
}
func MetadataFromIncomingContext(ctx context.Context) metadata.MD {
	md, _ := metadata.FromIncomingContext(ctx)
	return md
}
func MetadataFromOutgoingContext(ctx context.Context) metadata.MD {
	md, _ := metadata.FromOutgoingContext(ctx)
	return md
}
func TokenFromMetadata(md metadata.MD) (string, error) {
	token, ok := md["token"]
	if !ok || len(token) == 0 {
		return "", ErrNoToken
	}
	return token[0], nil
}
func TokenFromIncomingContext(ctx context.Context) (string, error) {
	md := MetadataFromIncomingContext(ctx)
	return TokenFromMetadata(md)
}
func OutgoingContextWithToken(ctx context.Context, token string) context.Context {
	return outgoingContextWithMergedMetadata(ctx, "token", token)
}
func KeyFromMetadata(md metadata.MD) (string, error) {
	key, ok := md["key"]
	if !ok || len(key) == 0 {
		return "", ErrNoKey
	}
	return key[0], nil
}
func KeyFromIncomingContext(ctx context.Context) (string, error) {
	md := MetadataFromIncomingContext(ctx)
	return KeyFromMetadata(md)
}
func OutgoingContextWithKey(ctx context.Context, key string) context.Context {
	return outgoingContextWithMergedMetadata(ctx, "key", key)
}
func IDFromMetadata(md metadata.MD) (string, error) {
	id, ok := md["id"]
	if !ok || len(id) == 0 {
		return "", ErrNoID
	}
	return id[0], nil
}
func IDFromIncomingContext(ctx context.Context) (string, error) {
	md := MetadataFromIncomingContext(ctx)
	return IDFromMetadata(md)
}
func OutgoingContextWithID(ctx context.Context, id string) context.Context {
	return outgoingContextWithMergedMetadata(ctx, "id", id)
}
func ServiceInfoFromMetadata(md metadata.MD) (serviceName, serviceVersion, netAddress string, err error) {
	serviceNameL, ok := md["service-name"]
	if ok && len(serviceNameL) > 0 {
		serviceName = serviceNameL[0]
	}
	serviceVersionL, ok := md["service-version"]
	if ok && len(serviceVersionL) > 0 {
		serviceVersion = serviceVersionL[0]
	}
	netAddressL, ok := md["net-address"]
	if ok && len(netAddressL) > 0 {
		netAddress = netAddressL[0]
	}
	return
}
func ServiceInfoFromIncomingContext(ctx context.Context) (serviceName, serviceVersion, netAddress string, err error) {
	md := MetadataFromIncomingContext(ctx)
	return ServiceInfoFromMetadata(md)
}
func OutgoingContextWithServiceInfo(ctx context.Context, serviceName, serviceVersion, netAddress string) context.Context {
	return outgoingContextWithMergedMetadata(ctx, "service-name", serviceName, "service-version", serviceVersion, "net-address", netAddress)
}
func LimitFromMetadata(md metadata.MD) (uint64, error) {
	limit, ok := md["limit"]
	if !ok || len(limit) == 0 {
		return 0, nil
	}
	return strconv.ParseUint(limit[0], 10, 64)
}
func OffsetFromMetadata(md metadata.MD) (uint64, error) {
	offset, ok := md["offset"]
	if !ok || len(offset) == 0 {
		return 0, nil
	}
	return strconv.ParseUint(offset[0], 10, 64)
}
func LimitAndOffsetFromIncomingContext(ctx context.Context) (limit, offset uint64, err error) {
	md := MetadataFromIncomingContext(ctx)
	limit, err = LimitFromMetadata(md)
	if err != nil {
		return 0, 0, err
	}
	offset, err = OffsetFromMetadata(md)
	if err != nil {
		return 0, 0, err
	}
	return limit, offset, nil
}
func OutgoingContextWithLimitAndOffset(ctx context.Context, limit, offset uint64) context.Context {
	var pairs []string
	if limit != 0 {
		pairs = append(pairs, "limit", strconv.FormatUint(limit, 10))
	}
	if offset != 0 {
		pairs = append(pairs, "offset", strconv.FormatUint(offset, 10))
	}
	if len(pairs) == 0 {
		return ctx
	}
	return outgoingContextWithMergedMetadata(ctx, pairs...)
}
func before(i, j ScheduleItem) bool {
	iEnd := i.Time().UnixNano() + i.Duration().Nanoseconds()
	jStart := j.Time().UnixNano()
	if i, ok := i.(ScheduleItemWithTimestamp); ok {
		if j, ok := j.(ScheduleItemWithTimestamp); ok {
			iEnd = i.Timestamp() + i.Duration().Nanoseconds()
			jStart = j.Timestamp()
		}
	}
	return iEnd < jStart
}
func (err *ErrDescriptor) New(attributes Attributes) Error {
	if err.Code != NoCode && !err.registered {
		panic(fmt.Errorf("Error descriptor with code %v was not registered", err.Code))
	}

	return &impl{
		message:    Format(err.MessageFormat, attributes),
		code:       err.Code,
		typ:        err.Type,
		attributes: attributes,
	}
}
func WithNamespace(namespace string, ctx log.Interface) log.Interface {
	return ctx.WithField(NamespaceKey, namespace)
}
func Wrap(ctx log.Interface, namespaces ...string) *Namespaced {
	return &Namespaced{
		Interface: ctx,
		namespaces: &ns{
			namespaces: namespaces,
		},
	}
}
func (n *Namespaced) WithField(k string, v interface{}) log.Interface {
	if k == NamespaceKey {
		if str, ok := v.(string); ok {
			return &Namespaced{
				Interface:  n.Interface,
				namespaces: n.namespaces,
				namespace:  str,
			}
		}
	}

	return &Namespaced{
		Interface:  n.Interface.WithField(k, v),
		namespaces: n.namespaces,
		namespace:  n.namespace,
	}
}
func (n *Namespaced) WithFields(fields log.Fields) log.Interface {
	return &Namespaced{
		Interface:  n.Interface.WithFields(fields),
		namespaces: n.namespaces,
		namespace:  n.namespace,
	}
}
func Format(format string, values Attributes) string {
	formatter, err := messageformat.New()
	if err != nil {
		return format
	}

	fm, err := formatter.Parse(format)
	if err != nil {
		return format
	}

	fixed := make(map[string]interface{}, len(values))
	for k, v := range values {
		fixed[k] = fix(v)
	}

	// todo format unsupported types
	res, err := fm.FormatMap(fixed)
	if err != nil {
		fmt.Println("err", err)
		return format
	}

	return res
}
func fix(v interface{}) interface{} {
	if v == nil {
		return "<nil>"
	}

	switch reflect.TypeOf(v).Kind() {
	case reflect.Bool:
	case reflect.Int:
	case reflect.Int8:
	case reflect.Int16:
	case reflect.Int32:
	case reflect.Int64:
	case reflect.Uint:
	case reflect.Uint8:
	case reflect.Uint16:
	case reflect.Uint32:
	case reflect.Uint64:
	case reflect.Uintptr:
	case reflect.Float32:
	case reflect.Float64:
		return v
	case reflect.Ptr:
		// dereference and fix
		return fix(reflect.ValueOf(v).Elem())
	}
	return fmt.Sprintf("%v", v)
}
func (t Type) GRPCCode() codes.Code {
	switch t {
	case InvalidArgument:
		return codes.InvalidArgument
	case OutOfRange:
		return codes.OutOfRange
	case NotFound:
		return codes.NotFound
	case Conflict:
	case AlreadyExists:
		return codes.AlreadyExists
	case Unauthorized:
		return codes.Unauthenticated
	case PermissionDenied:
		return codes.PermissionDenied
	case Timeout:
		return codes.DeadlineExceeded
	case NotImplemented:
		return codes.Unimplemented
	case TemporarilyUnavailable:
		return codes.Unavailable
	case PermanentlyUnavailable:
		return codes.FailedPrecondition
	case Canceled:
		return codes.Canceled
	case ResourceExhausted:
		return codes.ResourceExhausted
	case Internal:
	case Unknown:
		return codes.Unknown
	}

	return codes.Unknown
}
func GRPCCodeToType(code codes.Code) Type {
	switch code {
	case codes.InvalidArgument:
		return InvalidArgument
	case codes.OutOfRange:
		return OutOfRange
	case codes.NotFound:
		return NotFound
	case codes.AlreadyExists:
		return AlreadyExists
	case codes.Unauthenticated:
		return Unauthorized
	case codes.PermissionDenied:
		return PermissionDenied
	case codes.DeadlineExceeded:
		return Timeout
	case codes.Unimplemented:
		return NotImplemented
	case codes.Unavailable:
		return TemporarilyUnavailable
	case codes.FailedPrecondition:
		return PermanentlyUnavailable
	case codes.Canceled:
		return Canceled
	case codes.ResourceExhausted:
		return ResourceExhausted
	case codes.Unknown:
		return Unknown
	}
	return Unknown
}
func GRPCCode(err error) codes.Code {
	e, ok := err.(Error)
	if ok {
		return e.Type().GRPCCode()
	}

	return grpc.Code(err)
}
func FromGRPC(in error) Error {
	out := &impl{
		message: grpc.ErrorDesc(in),
		typ:     GRPCCodeToType(grpc.Code(in)),
		code:    NoCode,
	}

	matches := grpcMessageFormat.FindStringSubmatch(in.Error())

	if len(matches) < 4 {
		return out
	}

	out.message = matches[1]
	out.code = parseCode(matches[2])
	_ = json.Unmarshal([]byte(matches[3]), &out.attributes)

	got := Get(Code(out.code))
	if got == nil {
		return out
	}

	return got.New(out.attributes)
}
func ToGRPC(in error) error {
	if err, ok := in.(Error); ok {
		attrs, _ := json.Marshal(err.Attributes())
		return grpc.Errorf(err.Type().GRPCCode(), format, err.Error(), err.Code(), attrs)
	}

	return grpc.Errorf(codes.Unknown, in.Error())
}
func (n *ns) IsEnabled(namespace string) bool {
	n.RLock()
	defer n.RUnlock()

	if namespace == "" {
		return true
	}

	hasStar := false
	included := false

	for _, ns := range n.namespaces {
		// if the namspace is negated, it can never be enabled
		if ns == negate(namespace) {
			return false
		}

		// if the namespace is explicitly enabled, mark it as included
		if ns == namespace {
			included = true
		}

		// mark that we have a *
		if ns == "*" {
			hasStar = true
		}
	}

	// non-mentioned namespaces are only enabled if we got the catch-all *
	return hasStar || included
}
func (n *ns) Set(namespaces []string) {
	n.Lock()
	defer n.Unlock()
	n.namespaces = namespaces
}
func Cause(err Error) error {
	attributes := err.Attributes()
	if attributes == nil {
		return nil
	}

	cause, ok := attributes[causeKey]
	if !ok {
		return nil
	}

	switch v := cause.(type) {
	case error:
		return v
	case string:
		return errors.New(v)
	default:
		return nil
	}
}
func parseCode(str string) Code {
	code, err := strconv.Atoi(str)
	if err != nil {
		return Code(0)
	}
	return Code(code)
}
func UnaryServerInterceptor(fn ConvertFunc) grpc.UnaryServerInterceptor {
	return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {
		resp, err = handler(ctx, req)
		return resp, fn(err)
	}
}
func StreamServerInterceptor(fn ConvertFunc) grpc.StreamServerInterceptor {
	return func(srv interface{}, ss grpc.ServerStream, info *grpc.StreamServerInfo, handler grpc.StreamHandler) (err error) {
		return fn(handler(srv, ss))
	}
}
func UnaryClientInterceptor(fn ConvertFunc) grpc.UnaryClientInterceptor {
	return func(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) (err error) {
		return fn(invoker(ctx, method, req, reply, cc, opts...))
	}
}
func StreamClientInterceptor(fn ConvertFunc) grpc.StreamClientInterceptor {
	return func(ctx context.Context, desc *grpc.StreamDesc, cc *grpc.ClientConn, method string, streamer grpc.Streamer, opts ...grpc.CallOption) (stream grpc.ClientStream, err error) {
		stream, err = streamer(ctx, desc, cc, method, opts...)
		return stream, fn(err)
	}
}
func Interceptor(settings Settings) grpc.StreamClientInterceptor {
	return func(ctx context.Context, desc *grpc.StreamDesc, cc *grpc.ClientConn, method string, streamer grpc.Streamer, opts ...grpc.CallOption) (stream grpc.ClientStream, err error) {
		s := &restartingStream{
			log: log.Get().WithField("method", method),

			ctx:      ctx,
			desc:     desc,
			cc:       cc,
			method:   method,
			streamer: streamer,
			opts:     opts,

			retryableCodes: settings.RetryableCodes,
			backoff:        settings.Backoff,
			retries:        -1,
		}

		err = s.start()

		stream = s
		return
	}
}
func Wrap(logger *logrus.Logger) log.Interface {
	return &logrusEntryWrapper{logrus.NewEntry(logger)}
}
func NewCounter(bucketSize, retention time.Duration) Counter {
	return &counter{
		bucketSize: bucketSize,
		retention:  retention,
		buckets:    make([]uint64, 2*retention/bucketSize),
	}
}
func NewRedisCounter(client *redis.Client, key string, bucketSize, retention time.Duration) Counter {
	return &redisCounter{
		client:     client,
		key:        key,
		bucketSize: bucketSize,
		retention:  retention,
	}
}
func NewLimiter(counter Counter, duration time.Duration, limit uint64) Limiter {
	return &limiter{
		Counter:  counter,
		duration: duration,
		limit:    limit,
	}
}
func (c *TokenCredentials) WithInsecure() *TokenCredentials {
	return &TokenCredentials{token: c.token, tokenFunc: c.tokenFunc, allowInsecure: true}
}
func WithTokenFunc(k string, tokenFunc func(v string) string) *TokenCredentials {
	return &TokenCredentials{
		tokenFunc:    tokenFunc,
		tokenFuncKey: k,
	}
}
func (c *TokenCredentials) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) {
	md := ttnctx.MetadataFromOutgoingContext(ctx)
	token, _ := ttnctx.TokenFromMetadata(md)
	if token != "" {
		return map[string]string{tokenKey: token}, nil
	}
	if c.tokenFunc != nil {
		var k string
		if v, ok := md[c.tokenFuncKey]; ok && len(v) > 0 {
			k = v[0]
		}
		return map[string]string{tokenKey: c.tokenFunc(k)}, nil
	}
	if c.token != "" {
		return map[string]string{tokenKey: c.token}, nil
	}
	return map[string]string{tokenKey: ""}, nil
}
func FieldsFromIncomingContext(ctx context.Context) ttnlog.Fields {
	fields := make(fieldMap)
	if peer, ok := peer.FromContext(ctx); ok {
		fields.addFromPeer(peer)
	}
	if md, ok := metadata.FromIncomingContext(ctx); ok {
		fields.addFromMD(md)
	}
	return fields.LogFields()
}
func (t Type) String() string {
	switch t {
	case Unknown:
		return "Unknown"
	case Internal:
		return "Internal"
	case InvalidArgument:
		return "Invalid argument"
	case OutOfRange:
		return "Out of range"
	case NotFound:
		return "Not found"
	case Conflict:
		return "Conflict"
	case AlreadyExists:
		return "Already exists"
	case Unauthorized:
		return "Unauthorized"
	case PermissionDenied:
		return "Permission denied"
	case Timeout:
		return "Timeout"
	case NotImplemented:
		return "Not implemented"
	case TemporarilyUnavailable:
		return "Temporarily unavailable"
	case PermanentlyUnavailable:
		return "Permanently unavailable"
	case Canceled:
		return "Canceled"
	case ResourceExhausted:
		return "Resource exhausted"
	default:
		return "Unknown"
	}
}
func (t *Type) UnmarshalText(text []byte) error {
	e, err := fromString(string(text))
	if err != nil {
		return err
	}

	*t = e

	return nil
}
func fromString(str string) (Type, error) {
	enum := strings.ToLower(str)
	switch enum {
	case "unknown":
		return Unknown, nil
	case "internal":
		return Internal, nil
	case "invalid argument":
		return InvalidArgument, nil
	case "out of range":
		return OutOfRange, nil
	case "not found":
		return NotFound, nil
	case "conflict":
		return Conflict, nil
	case "already exists":
		return AlreadyExists, nil
	case "unauthorized":
		return Unauthorized, nil
	case "permission denied":
		return PermissionDenied, nil
	case "timeout":
		return Timeout, nil
	case "not implemented":
		return NotImplemented, nil
	case "temporarily unavailable":
		return TemporarilyUnavailable, nil
	case "permanently unavailable":
		return PermanentlyUnavailable, nil
	case "canceled":
		return Canceled, nil
	case "resource exhausted":
		return ResourceExhausted, nil
	default:
		return Unknown, fmt.Errorf("Invalid error type")
	}
}
func Start(ctx log.Interface, interval time.Duration) {
	ctx.WithField("interval", interval).Debug("starting stats loop")
	go func() {
		memstats := new(runtime.MemStats)
		for range time.Tick(interval) {
			runtime.ReadMemStats(memstats)
			ctx.WithFields(log.Fields{
				"goroutines": runtime.NumGoroutine(),
				"memory":     float64(memstats.Alloc) / megaByte, // MegaBytes allocated and not yet freed
			}).Debugf("memory stats")
		}
	}()
}
func NewSimple() Simple {
	q := &simpleQueue{
		queue: make([]interface{}, 0),
	}
	q.available = sync.NewCond(&q.mu)
	return q
}
func Wrap(logger log.Interface, filters ...Filter) *Filtered {
	return &Filtered{
		Interface: logger,
		filters:   filters,
	}
}
func (f *Filtered) WithFilters(filters ...Filter) *Filtered {
	return &Filtered{
		Interface: f.Interface,
		filters:   append(f.filters, filters...),
	}
}
func (f *Filtered) WithField(k string, v interface{}) log.Interface {
	val := v

	// apply the filters
	for _, filter := range f.filters {
		val = filter.Filter(k, val)
	}

	return &Filtered{
		Interface: f.Interface.WithField(k, val),
		filters:   f.filters,
	}
}
func (f *Filtered) WithFields(fields log.Fields) log.Interface {
	res := make(map[string]interface{}, len(fields))

	for k, v := range fields {
		val := v

		// apply the filters
		for _, filter := range f.filters {
			val = filter.Filter(k, val)
		}

		res[k] = val
	}

	return &Filtered{
		Interface: f.Interface.WithFields(res),
		filters:   f.filters,
	}
}
func FilterSensitive(sensitive []string, elided interface{}) Filter {
	return FilterFunc(func(key string, v interface{}) interface{} {
		lower := strings.ToLower(key)
		for _, s := range sensitive {
			if lower == s {
				return elided
			}
		}

		return v
	})
}
func SliceFilter(filter Filter) Filter {
	return FilterFunc(func(k string, v interface{}) interface{} {
		r := reflect.ValueOf(v)
		if r.Kind() == reflect.Slice {
			res := make([]interface{}, 0, r.Len())
			for i := 0; i < r.Len(); i++ {
				el := r.Index(i).Interface()
				res = append(res, filter.Filter(k, el))
			}

			return res
		}

		return filter.Filter(k, v)
	})
}
func MapFilter(filter Filter) Filter {
	return FilterFunc(func(k string, v interface{}) interface{} {
		r := reflect.ValueOf(v)
		if r.Kind() == reflect.Map {
			// res will be the filtered map
			res := make(map[string]interface{}, r.Len())
			for _, key := range r.MapKeys() {
				str := key.String()
				val := r.MapIndex(key).Interface()
				res[str] = filter.Filter(str, val)
			}

			return res
		}

		return v
	})
}
func RestrictFilter(fieldName string, filter Filter) Filter {
	return FilterFunc(func(k string, v interface{}) interface{} {
		if fieldName == k {
			return filter.Filter(k, v)
		}

		return v
	})
}
func LowerCaseFilter(filter Filter) Filter {
	return FilterFunc(func(k string, v interface{}) interface{} {
		return filter.Filter(strings.ToLower(k), v)
	})
}
func newBatchPoints(bpConf influxdb.BatchPointsConfig) influxdb.BatchPoints {
	bp, err := influxdb.NewBatchPoints(bpConf)
	if err != nil {
		// Can only happen if there's an error in the code
		panic(fmt.Errorf("Invalid batch point configuration: %s", err))
	}
	return bp
}
func NewSinglePointWriter(log ttnlog.Interface, w BatchPointsWriter) *SinglePointWriter {
	return &SinglePointWriter{
		log:    log,
		writer: w,
	}
}
func (w *SinglePointWriter) Write(bpConf influxdb.BatchPointsConfig, p *influxdb.Point) error {
	bp := newBatchPoints(bpConf)
	bp.AddPoint(p)
	return w.writer.Write(bp)
}
func WithScalingInterval(v time.Duration) BatchingWriterOption {
	return func(w *BatchingWriter) {
		w.scalingInterval = v
	}
}
func NewBatchingWriter(log ttnlog.Interface, w BatchPointsWriter, opts ...BatchingWriterOption) *BatchingWriter {
	bw := &BatchingWriter{
		log:             log,
		writer:          w,
		scalingInterval: DefaultScalingInterval,
		limit:           DefaultInstanceLimit,
		pointChans:      make(map[influxdb.BatchPointsConfig]chan *batchPoint),
	}
	for _, opt := range opts {
		opt(bw)
	}
	bw.log = bw.log.WithFields(ttnlog.Fields{
		"limit":           bw.limit,
		"scalingInterval": bw.scalingInterval,
	})
	return bw
}
func (w *BatchingWriter) Write(bpConf influxdb.BatchPointsConfig, p *influxdb.Point) error {
	log := w.log.WithField("config", bpConf)

	w.pointChanMutex.RLock()
	ch, ok := w.pointChans[bpConf]
	w.pointChanMutex.RUnlock()
	if !ok {
		w.pointChanMutex.Lock()
		ch, ok = w.pointChans[bpConf]
		if !ok {
			w.mutex.Lock()
			w.active++
			w.limit++
			w.mutex.Unlock()

			ch = make(chan *batchPoint)
			w.pointChans[bpConf] = ch
			go writeInBatches(log, w.writer, bpConf, w.scalingInterval, ch, true)
		}
		w.pointChanMutex.Unlock()
	}

	point := &batchPoint{
		Point: p,
		errch: make(chan error, 1),
	}
	select {
	case ch <- point:
	case <-time.After(w.scalingInterval):
		w.mutex.Lock()
		if w.active < w.limit {
			w.active++
			go writeInBatches(w.log, w.writer, bpConf, w.scalingInterval, ch, false)
		}
		w.mutex.Unlock()
		ch <- point
	}
	return <-point.errch
}
func (w *apexInterfaceWrapper) MustParseLevel(s string) {
	level, err := ParseLevel(s)
	if err != nil {
		w.WithError(err).WithField("level", s).Fatal("Could not parse log level")
	}
	w.Level = level
}
func New(bufferSize int, setup func() (grpc.ClientStream, error)) *Stream {
	return &Stream{
		setupFunc:  setup,
		sendBuffer: make(chan interface{}, bufferSize),
		log:        ttnlog.Get(),
	}
}
func (s *Stream) SetLogger(log ttnlog.Interface) {
	s.mu.Lock()
	s.log = log
	s.mu.Unlock()
}
func (s *Stream) CloseRecv() {
	s.mu.Lock()
	if s.recvBuffer != nil {
		close(s.recvBuffer)
		s.recvBuffer = nil
	}
	s.mu.Unlock()
}
func (s *Stream) Stats() (sent, dropped uint64) {
	return atomic.LoadUint64(&s.sent), atomic.LoadUint64(&s.dropped)
}
func (s *Stream) Run() (err error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	defer func() {
		if err != nil {
			if grpc.Code(err) == codes.Canceled {
				s.log.Debug("streambuffer: context canceled")
				err = context.Canceled
				return
			}
			if grpc.Code(err) == codes.DeadlineExceeded {
				s.log.Debug("streambuffer: context deadline exceeded")
				err = context.DeadlineExceeded
				return
			}
		}
	}()

	stream, err := s.setupFunc()
	if err != nil {
		s.log.WithError(err).Debug("streambuffer: setup returned error")
		return err
	}

	recvErr := make(chan error)
	defer func() {
		go func() { // empty the recvErr channel on return
			<-recvErr
		}()
	}()

	go func() {
		for {
			var r interface{}
			if s.recvFunc != nil {
				r = s.recvFunc()
			} else {
				r = new(empty.Empty) // Generic proto message if not interested in received values
			}
			err := stream.RecvMsg(r)
			if err != nil {
				s.log.WithError(err).Debug("streambuffer: error from stream.RecvMsg")
				recvErr <- err
				close(recvErr)
				return
			}
			if s.recvFunc != nil {
				s.recvMsg(r)
			}
		}
	}()

	defer stream.CloseSend()

	for {
		select {
		case err := <-recvErr:
			return err
		case <-stream.Context().Done():
			s.log.WithError(stream.Context().Err()).Debug("streambuffer: context done")
			return stream.Context().Err()
		case msg := <-s.sendBuffer:
			if err = stream.SendMsg(msg); err != nil {
				s.log.WithError(err).Debug("streambuffer: error from stream.SendMsg")
				return err
			}
			atomic.AddUint64(&s.sent, 1)
		}
	}
}
func ServerOptions(log ttnlog.Interface) []grpc.ServerOption {
	return []grpc.ServerOption{
		grpc.UnaryInterceptor(UnaryServerInterceptor(log)),
		grpc.StreamInterceptor(StreamServerInterceptor(log)),
	}
}
func ClientOptions(log ttnlog.Interface) []grpc.DialOption {
	return []grpc.DialOption{
		grpc.WithUnaryInterceptor(UnaryClientInterceptor(log)),
		grpc.WithStreamInterceptor(StreamClientInterceptor(log)),
	}
}
func UnaryServerInterceptor(log ttnlog.Interface) grpc.UnaryServerInterceptor {
	return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {
		log := getLog(log).WithField("method", info.FullMethod)
		log = log.WithFields(FieldsFromIncomingContext(ctx))
		start := time.Now()
		resp, err = handler(ctx, req)
		log = log.WithField("duration", time.Since(start))
		if err != nil {
			log.WithError(err).Debug("rpc-server: call failed")
			return
		}
		log.Debug("rpc-server: call done")
		return
	}
}
func StreamServerInterceptor(log ttnlog.Interface) grpc.StreamServerInterceptor {
	return func(srv interface{}, ss grpc.ServerStream, info *grpc.StreamServerInfo, handler grpc.StreamHandler) (err error) {
		log := getLog(log).WithField("method", info.FullMethod)
		log = log.WithFields(FieldsFromIncomingContext(ss.Context()))
		start := time.Now()
		log.Debug("rpc-server: stream starting")
		err = handler(srv, ss)
		log = log.WithField("duration", time.Since(start))
		if err != nil {
			if err == context.Canceled || grpc.Code(err) == codes.Canceled {
				log.Debug("rpc-server: stream canceled")
				return
			}
			log.WithError(err).Debug("rpc-server: stream failed")
			return
		}
		log.Debug("rpc-server: stream done")
		return
	}
}
func UnaryClientInterceptor(log ttnlog.Interface) grpc.UnaryClientInterceptor {
	return func(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) (err error) {
		log := getLog(log).WithField("method", method)
		log = log.WithFields(FieldsFromOutgoingContext(ctx))
		start := time.Now()
		err = invoker(ctx, method, req, reply, cc, opts...)
		log = log.WithField("duration", time.Since(start))
		if err != nil {
			log.WithError(err).Debug("rpc-client: call failed")
			return
		}
		log.Debug("rpc-client: call done")
		return
	}
}
func StreamClientInterceptor(log ttnlog.Interface) grpc.StreamClientInterceptor {
	return func(ctx context.Context, desc *grpc.StreamDesc, cc *grpc.ClientConn, method string, streamer grpc.Streamer, opts ...grpc.CallOption) (stream grpc.ClientStream, err error) {
		log := getLog(log).WithField("method", method)
		log = log.WithFields(FieldsFromOutgoingContext(ctx))
		log.Debug("rpc-client: stream starting")
		stream, err = streamer(ctx, desc, cc, method, opts...)
		if err != nil {
			if err == context.Canceled || grpc.Code(err) == codes.Canceled {
				log.Debug("rpc-client: stream canceled")
				return
			}
			log.WithError(err).Debug("rpc-client: stream failed")
			return
		}
		go func() {
			<-stream.Context().Done()
			if err := stream.Context().Err(); err != nil {
				log = log.WithError(err)
			}
			log.Debug("rpc-client: stream done")
		}()
		return
	}
}
func (c *Config) defaults() {
	if c.BufferSize == 0 {
		c.BufferSize = 100
	}

	if c.Prefix == "" {
		c.Prefix = "logs"
	}
}
func (h *Handler) Flush() {
	h.mu.Lock()
	defer h.mu.Unlock()
	if h.batch != nil {
		go h.flush(h.batch)
		h.batch = nil
	}
}
func New(w io.Writer) *Handler {
	var useColor bool
	if os.Getenv("COLORTERM") != "" {
		useColor = true
	}
	if term := os.Getenv("TERM"); term != "" {
		for _, substring := range colorTermSubstrings {
			if strings.Contains(term, substring) {
				useColor = true
				break
			}
		}
	}
	return &Handler{
		Writer:   w,
		UseColor: useColor,
	}
}
func (h *Handler) HandleLog(e *log.Entry) error {
	color := Colors[e.Level]
	level := Strings[e.Level]

	var fields []field

	for k, v := range e.Fields {
		fields = append(fields, field{k, v})
	}

	sort.Sort(byName(fields))

	h.mu.Lock()
	defer h.mu.Unlock()

	if h.UseColor {
		fmt.Fprintf(h.Writer, "\033[%dm%6s\033[0m %-40s", color, level, e.Message)
	} else {
		fmt.Fprintf(h.Writer, "%6s %-40s", level, e.Message)
	}

	for _, f := range fields {
		var value interface{}
		switch t := f.Value.(type) {
		case []byte: // addresses and EUIs are []byte
			value = fmt.Sprintf("%X", t)
		case [21]byte: // bundle IDs [21]byte
			value = fmt.Sprintf("%X-%X-%X-%X", t[0], t[1:9], t[9:17], t[17:])
		default:
			value = f.Value
		}

		if h.UseColor {
			fmt.Fprintf(h.Writer, " \033[%dm%s\033[0m=%v", color, f.Name, value)
		} else {
			fmt.Fprintf(h.Writer, " %s=%v", f.Name, value)
		}

	}

	fmt.Fprintln(h.Writer)

	return nil
}
func New(seed int64) random.Interface {
	return &TTNRandom{
		Interface: &random.TTNRandom{
			Source: rand.New(rand.NewSource(seed)),
		},
	}
}
func (self *UriTemplate) Names() []string {
	names := make([]string, 0, len(self.parts))

	for _, p := range self.parts {
		if len(p.raw) > 0 || len(p.terms) == 0 {
			continue
		}

		for _, term := range p.terms {
			names = append(names, term.name)
		}
	}

	return names
}
func (p PerfDatum) String() string {
	val := fmtPerfFloat(p.value)
	value := fmt.Sprintf("%s=%s%s", p.label, val, p.unit)
	value += fmt.Sprintf(";%s;%s", fmtThreshold(p.warn), fmtThreshold(p.crit))
	value += fmt.Sprintf(";%s;%s", fmtThreshold(p.min), fmtThreshold(p.max))
	return value
}
func RenderPerfdata(perfdata []PerfDatum) string {
	value := ""
	if len(perfdata) == 0 {
		return value
	}
	// Demarcate start of perfdata in check output.
	value += " |"
	for _, datum := range perfdata {
		value += fmt.Sprintf(" %v", datum)
	}
	return value
}
func Exit(status Status, message string) {
	fmt.Printf("%v: %s\n", status, message)
	os.Exit(int(status))
}
func NewCheckWithOptions(options CheckOptions) *Check {
	c := NewCheck()
	if options.StatusPolicy != nil {
		c.statusPolicy = options.StatusPolicy
	}
	return c
}
func (c *Check) AddResult(status Status, message string) {
	var result Result
	result.status = status
	result.message = message
	c.results = append(c.results, result)

	if (*c.statusPolicy)[result.status] > (*c.statusPolicy)[c.status] {
		c.status = result.status
	}
}
func (c *Check) AddResultf(status Status, format string, v ...interface{}) {
	msg := fmt.Sprintf(format, v...)
	c.AddResult(status, msg)
}
func (c Check) String() string {
	value := fmt.Sprintf("%v: %s", c.status, c.exitInfoText())
	value += RenderPerfdata(c.perfdata)
	return value
}
func (c *Check) Exitf(status Status, format string, v ...interface{}) {
	info := fmt.Sprintf(format, v...)
	c.AddResult(status, info)
	c.Finish()
}
func (c *Check) Criticalf(format string, v ...interface{}) {
	c.Exitf(CRITICAL, format, v...)
}
func (c *Check) Unknownf(format string, v ...interface{}) {
	c.Exitf(UNKNOWN, format, v...)
}
func NewDefaultStatusPolicy() *statusPolicy {
	return &statusPolicy{
		OK:       statusSeverity(OK),
		WARNING:  statusSeverity(WARNING),
		CRITICAL: statusSeverity(CRITICAL),
		UNKNOWN:  statusSeverity(UNKNOWN),
	}
}
func NewStatusPolicy(statuses []Status) (*statusPolicy, error) {
	newPol := make(statusPolicy)
	for i, status := range statuses {
		newPol[status] = statusSeverity(i)
	}

	// Ensure all statuses are covered by the new policy.
	defaultPol := NewDefaultStatusPolicy()
	for status := range *defaultPol {
		_, ok := newPol[status]
		if !ok {
			return nil, fmt.Errorf("missing status: %v", status)
		}
	}

	return &newPol, nil
}
func ParseRange(rangeStr string) (*Range, error) {
	// Set defaults
	t := &Range{
		Start:         0,
		End:           math.Inf(1),
		AlertOnInside: false,
	}
	// Remove leading and trailing whitespace
	rangeStr = strings.Trim(rangeStr, " \n\r")

	// Check for inverted semantics
	if rangeStr[0] == '@' {
		t.AlertOnInside = true
		rangeStr = rangeStr[1:]
	}

	// Parse lower limit
	endPos := strings.Index(rangeStr, ":")
	if endPos > -1 {
		if rangeStr[0] == '~' {
			t.Start = math.Inf(-1)
		} else {
			min, err := strconv.ParseFloat(rangeStr[0:endPos], 64)
			if err != nil {
				return nil, fmt.Errorf("failed to parse lower limit: %v", err)
			}
			t.Start = min
		}
		rangeStr = rangeStr[endPos+1:]
	}

	// Parse upper limit
	if len(rangeStr) > 0 {
		max, err := strconv.ParseFloat(rangeStr, 64)
		if err != nil {
			return nil, fmt.Errorf("failed to parse upper limit: %v", err)
		}
		t.End = max
	}

	if t.End < t.Start {
		return nil, errors.New("Invalid range definition. min <= max violated!")
	}

	// OK
	return t, nil
}
func (r *Range) Check(value float64) bool {
	// Ranges are treated as a closed interval.
	if r.Start <= value && value <= r.End {
		return r.AlertOnInside
	}
	return !r.AlertOnInside
}
func (r *Range) CheckInt(val int) bool {
	return r.Check(float64(val))
}
func (r *Range) CheckUint64(val uint64) bool {
	return r.Check(float64(val))
}
func NewClient(config *ClientConfig) (*Client, error) {
	t := &http.Transport{
		TLSClientConfig: &tls.Config{
			InsecureSkipVerify: config.AllowUnverifiedSSL,
		},
	}
	httpClient := &http.Client{
		Transport: t,
	}

	apiPath, _ := url.Parse("api/13/")
	baseURL, err := url.Parse(config.BaseURL)
	if err != nil {
		return nil, fmt.Errorf("invalid base URL: %s", err.Error())
	}
	apiURL := baseURL.ResolveReference(apiPath)

	return &Client{
		httpClient: httpClient,
		apiURL:     apiURL,
		authToken:  config.AuthToken,
	}, nil
}
func (c *Client) GetKeyMeta(path string) (*KeyMeta, error) {
	k := &KeyMeta{}
	err := c.get([]string{"storage", "keys", path}, nil, k)
	return k, err
}
func (c *Client) GetKeysInDirMeta(path string) ([]KeyMeta, error) {
	r := &keyMetaListContents{}
	err := c.get([]string{"storage", "keys", path}, nil, r)
	if err != nil {
		return nil, err
	}
	return r.Keys, nil
}
func (c *Client) GetKeyContent(path string) (string, error) {
	return c.rawGet([]string{"storage", "keys", path}, nil, "application/pgp-keys")
}
func (c *Client) GetJobSummariesForProject(projectName string) ([]JobSummary, error) {
	jobList := &jobSummaryList{}
	err := c.get([]string{"project", projectName, "jobs"}, nil, jobList)
	return jobList.Jobs, err
}
func (c *Client) GetJobsForProject(projectName string) ([]JobDetail, error) {
	jobList := &jobDetailList{}
	err := c.get([]string{"jobs", "export"}, map[string]string{"project": projectName}, jobList)
	if err != nil {
		return nil, err
	}
	return jobList.Jobs, nil
}
func (c *Client) GetJob(id string) (*JobDetail, error) {
	jobList := &jobDetailList{}
	err := c.get([]string{"job", id}, nil, jobList)
	if err != nil {
		return nil, err
	}
	return &jobList.Jobs[0], nil
}
func (c *Client) CreateJob(job *JobDetail) (*JobSummary, error) {
	return c.importJob(job, "create")
}
func (c *Client) CreateOrUpdateJob(job *JobDetail) (*JobSummary, error) {
	return c.importJob(job, "update")
}
func (c *Client) DeleteJob(id string) error {
	return c.delete([]string{"job", id})
}
func (r *jobImportResult) JobSummary() *JobSummary {
	return &JobSummary{
		ID:          r.ID,
		Name:        r.Name,
		GroupName:   r.GroupName,
		ProjectName: r.ProjectName,
	}
}
func (c *Client) GetSystemInfo() (*SystemInfo, error) {
	sysInfo := &SystemInfo{}
	err := c.get([]string{"system", "info"}, nil, sysInfo)
	return sysInfo, err
}
func (ts *SystemTimestamp) DateTime() time.Time {
	// Assume the server will always give us a valid timestamp,
	// so we don't need to handle the error case.
	// (Famous last words?)
	t, _ := time.Parse(time.RFC3339, ts.DateTimeStr)
	return t
}
func (c *Client) GetAllProjects() ([]ProjectSummary, error) {
	p := &projects{}
	err := c.get([]string{"projects"}, nil, p)
	return p.Projects, err
}
func (c *Client) GetProject(name string) (*Project, error) {
	p := &Project{}
	err := c.get([]string{"project", name}, nil, p)
	return p, err
}
func (c *Client) CreateProject(project *Project) (*Project, error) {
	p := &Project{}
	err := c.post([]string{"projects"}, nil, project, p)
	return p, err
}
func (c *Client) DeleteProject(name string) error {
	return c.delete([]string{"project", name})
}
func (c *Client) SetProjectConfig(projectName string, config ProjectConfig) error {
	return c.put(
		[]string{"project", projectName, "config"},
		config,
		nil,
	)
}
func NewClient(username, password string) *Client {

	c := newPBRestClient(username, password, "", "", true)
	return &Client{
		userName: c.username,
		password: c.password,
		client:   c,
	}
}
func NewClientbyToken(token string) *Client {

	c := newPBRestClientbyToken(token, "", "", true)
	return &Client{
		token:  c.token,
		client: c,
	}
}
func (c *Client) SetDepth(depth int) {
	c.client.depth = strconv.Itoa(depth)
}
func (c *Client) ListDatacenters() (*Datacenters, error) {
	url := dcColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Datacenters{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) CreateDatacenter(dc Datacenter) (*Datacenter, error) {
	url := dcColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Datacenter{}
	err := c.client.Post(url, dc, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) GetDatacenter(dcid string) (*Datacenter, error) {
	url := dcPath(dcid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Datacenter{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) UpdateDataCenter(dcid string, obj DatacenterProperties) (*Datacenter, error) {
	url := dcPath(dcid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Datacenter{}
	err := c.client.Patch(url, obj, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteDatacenter(dcid string) (*http.Header, error) {
	url := dcPath(dcid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	return ret, c.client.Delete(url, ret, http.StatusAccepted)
}
func (c *Client) WaitTillProvisioned(path string) error {
	waitCount := 300
	for i := 0; i < waitCount; i++ {
		request, err := c.GetRequestStatus(path)
		if err != nil {
			return err
		}
		if request.Metadata.Status == "DONE" {
			return nil
		}
		time.Sleep(1 * time.Second)
		i++
	}
	return fmt.Errorf("timeout expired while waiting for request to complete")
}
func (c *Client) ListFirewallRules(dcID string, serverID string, nicID string) (*FirewallRules, error) {
	url := fwruleColPath(dcID, serverID, nicID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &FirewallRules{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetFirewallRule(dcID string, serverID string, nicID string, fwID string) (*FirewallRule, error) {
	url := fwrulePath(dcID, serverID, nicID, fwID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &FirewallRule{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) CreateFirewallRule(dcID string, serverID string, nicID string, fw FirewallRule) (*FirewallRule, error) {
	url := fwruleColPath(dcID, serverID, nicID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &FirewallRule{}
	err := c.client.Post(url, fw, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) UpdateFirewallRule(dcID string, serverID string, nicID string, fwID string, obj FirewallruleProperties) (*FirewallRule, error) {
	url := fwrulePath(dcID, serverID, nicID, fwID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &FirewallRule{}
	err := c.client.Patch(url, obj, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteFirewallRule(dcID string, serverID string, nicID string, fwID string) (*http.Header, error) {
	url := fwrulePath(dcID, serverID, nicID, fwID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListLoadbalancers(dcid string) (*Loadbalancers, error) {

	url := lbalColPath(dcid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Loadbalancers{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetLoadbalancer(dcid, lbalid string) (*Loadbalancer, error) {
	url := lbalPath(dcid, lbalid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Loadbalancer{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) UpdateLoadbalancer(dcid string, lbalid string, obj LoadbalancerProperties) (*Loadbalancer, error) {
	url := lbalPath(dcid, lbalid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Loadbalancer{}
	err := c.client.Patch(url, obj, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteLoadbalancer(dcid, lbalid string) (*http.Header, error) {
	url := lbalPath(dcid, lbalid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListBalancedNics(dcid, lbalid string) (*Nics, error) {
	url := balnicColPath(dcid, lbalid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nics{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) AssociateNic(dcid string, lbalid string, nicid string) (*Nic, error) {
	sm := map[string]string{"id": nicid}
	url := balnicColPath(dcid, lbalid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nic{}
	err := c.client.Post(url, sm, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) GetBalancedNic(dcid, lbalid, balnicid string) (*Nic, error) {
	url := balnicPath(dcid, lbalid, balnicid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nic{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) DeleteBalancedNic(dcid, lbalid, balnicid string) (*http.Header, error) {
	url := balnicPath(dcid, lbalid, balnicid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListLans(dcid string) (*Lans, error) {
	url := lanColPath(dcid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Lans{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetLan(dcid, lanid string) (*Lan, error) {
	url := lanPath(dcid, lanid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Lan{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) DeleteLan(dcid, lanid string) (*http.Header, error) {
	url := lanPath(dcid, lanid)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListNics(dcid, srvid string) (*Nics, error) {
	url := nicColPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nics{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) CreateNic(dcid string, srvid string, nic Nic) (*Nic, error) {

	url := nicColPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nic{}
	err := c.client.Post(url, nic, ret, http.StatusAccepted)

	return ret, err
}
func (c *Client) GetNic(dcid, srvid, nicid string) (*Nic, error) {

	url := nicPath(dcid, srvid, nicid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nic{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) UpdateNic(dcid string, srvid string, nicid string, obj NicProperties) (*Nic, error) {

	url := nicPath(dcid, srvid, nicid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Nic{}
	err := c.client.Patch(url, obj, ret, http.StatusAccepted)
	return ret, err

}
func (c *Client) DeleteNic(dcid, srvid, nicid string) (*http.Header, error) {
	url := nicPath(dcid, srvid, nicid)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListSnapshots() (*Snapshots, error) {
	url := snapshotColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Snapshots{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetSnapshot(snapshotID string) (*Snapshot, error) {
	url := snapshotColPath() + slash(snapshotID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Snapshot{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) DeleteSnapshot(snapshotID string) (*http.Header, error) {
	url := snapshotColPath() + slash(snapshotID) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) UpdateSnapshot(snapshotID string, request SnapshotProperties) (*Snapshot, error) {
	url := snapshotColPath() + slash(snapshotID)
	ret := &Snapshot{}
	err := c.client.Patch(url, request, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListIPBlocks() (*IPBlocks, error) {
	url := ipblockColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &IPBlocks{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) ReserveIPBlock(request IPBlock) (*IPBlock, error) {
	url := ipblockColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &IPBlock{}
	err := c.client.Post(url, request, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) GetIPBlock(ipblockid string) (*IPBlock, error) {
	url := ipblockPath(ipblockid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &IPBlock{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) UpdateIPBlock(ipblockid string, props IPBlockProperties) (*IPBlock, error) {
	url := ipblockPath(ipblockid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &IPBlock{}
	err := c.client.Patch(url, props, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ReleaseIPBlock(ipblockid string) (*http.Header, error) {
	url := ipblockPath(ipblockid)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListVolumes(dcid string) (*Volumes, error) {
	url := volumeColPath(dcid) + `?depth=` + c.client.depth
	ret := &Volumes{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetVolume(dcid string, volumeID string) (*Volume, error) {
	url := volumePath(dcid, volumeID) + `?depth=` + c.client.depth
	ret := &Volume{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) UpdateVolume(dcid string, volid string, request VolumeProperties) (*Volume, error) {
	url := volumePath(dcid, volid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Volume{}
	err := c.client.Patch(url, request, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) CreateVolume(dcid string, request Volume) (*Volume, error) {
	url := volumeColPath(dcid) + `?depth=` + c.client.depth
	ret := &Volume{}
	err := c.client.Post(url, request, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteVolume(dcid, volid string) (*http.Header, error) {
	url := volumePath(dcid, volid)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) CreateSnapshot(dcid string, volid string, name string, description string) (*Snapshot, error) {
	path := volumePath(dcid, volid) + "/create-snapshot"
	data := url.Values{}
	data.Set("name", name)
	data.Add("description", description)

	ret := &Snapshot{}
	err := c.client.Post(path, data, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) RestoreSnapshot(dcid string, volid string, snapshotID string) (*http.Header, error) {
	path := volumePath(dcid, volid) + "/restore-snapshot"
	data := url.Values{}
	data.Set("snapshotId", snapshotID)
	ret := &http.Header{}
	err := c.client.Post(path, data, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListServers(dcid string) (*Servers, error) {
	url := serverColPath(dcid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Servers{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetServer(dcid, srvid string) (*Server, error) {
	url := serverPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Server{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) DeleteServer(dcid, srvid string) (*http.Header, error) {
	ret := &http.Header{}
	err := c.client.Delete(serverPath(dcid, srvid), ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListAttachedCdroms(dcid, srvid string) (*Images, error) {
	url := serverCdromColPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Images{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) AttachCdrom(dcid string, srvid string, cdid string) (*Image, error) {
	data := struct {
		ID string `json:"id,omitempty"`
	}{
		cdid,
	}
	url := serverCdromColPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Image{}
	err := c.client.Post(url, data, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) GetAttachedCdrom(dcid, srvid, cdid string) (*Image, error) {
	url := serverCdromPath(dcid, srvid, cdid) // + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Image{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) DetachCdrom(dcid, srvid, cdid string) (*http.Header, error) {
	url := serverCdromPath(dcid, srvid, cdid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListAttachedVolumes(dcid, srvid string) (*Volumes, error) {
	url := serverVolumeColPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Volumes{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) AttachVolume(dcid string, srvid string, volid string) (*Volume, error) {
	data := struct {
		ID string `json:"id,omitempty"`
	}{
		volid,
	}
	url := serverVolumeColPath(dcid, srvid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Volume{}
	err := c.client.Post(url, data, ret, http.StatusAccepted)

	return ret, err
}
func (c *Client) GetAttachedVolume(dcid, srvid, volid string) (*Volume, error) {
	url := serverVolumePath(dcid, srvid, volid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Volume{}
	err := c.client.Get(url, ret, http.StatusOK)

	return ret, err
}
func (c *Client) DetachVolume(dcid, srvid, volid string) (*http.Header, error) {
	url := serverVolumePath(dcid, srvid, volid)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) StartServer(dcid, srvid string) (*http.Header, error) {
	url := serverPath(dcid, srvid) + "/start"
	ret := &http.Header{}
	err := c.client.Post(url, nil, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListLocations() (*Locations, error) {
	url := locationColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Locations{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetRegionalLocations(regid string) (*Locations, error) {
	url := locationRegPath(regid) + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Locations{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetLocation(locid string) (*Location, error) {
	url := locationPath(locid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Location{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetContractResources() (*ContractResources, error) {
	url := contractResourcePath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &ContractResources{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err

}
func (c *Client) ListImages() (*Images, error) {
	url := imageColPath() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Images{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetImage(imageid string) (*Image, error) {
	url := imagePath(imageid)
	ret := &Image{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (r *Resp) PrintHeaders() {
	for key, value := range r.Headers {
		fmt.Println(key, " : ", value[0])
	}

}
func (c *Client) ListGroups() (*Groups, error) {
	url := umGroups() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Groups{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetGroup(groupid string) (*Group, error) {
	url := umGroupPath(groupid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Group{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) CreateGroup(grp Group) (*Group, error) {
	url := umGroups() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Group{}
	err := c.client.Post(url, grp, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) UpdateGroup(groupid string, obj Group) (*Group, error) {
	url := umGroupPath(groupid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Group{}
	err := c.client.Put(url, obj, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteGroup(groupid string) (*http.Header, error) {
	url := umGroupPath(groupid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListShares(grpid string) (*Shares, error) {
	url := umGroupShares(grpid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Shares{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetShare(groupid string, resourceid string) (*Share, error) {
	url := umGroupSharePath(groupid, resourceid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Share{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) AddShare(groupid string, resourceid string, share Share) (*Share, error) {
	url := umGroupSharePath(groupid, resourceid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Share{}
	err := c.client.Post(url, share, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) UpdateShare(groupid string, resourceid string, obj Share) (*Share, error) {
	url := umGroupSharePath(groupid, resourceid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Share{}
	err := c.client.Put(url, obj, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteShare(groupid string, resourceid string) (*http.Header, error) {
	url := umGroupSharePath(groupid, resourceid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListGroupUsers(groupid string) (*Users, error) {
	url := umGroupUsers(groupid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Users{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) AddUserToGroup(groupid string, userid string) (*User, error) {
	var usr User
	usr.ID = userid
	url := umGroupUsers(groupid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &User{}
	err := c.client.Post(url, usr, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteUserFromGroup(groupid string, userid string) (*http.Header, error) {
	url := umGroupUsersPath(groupid, userid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListUsers() (*Users, error) {
	url := umUsers() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Users{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetUser(usrid string) (*User, error) {
	url := umUsersPath(usrid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &User{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) CreateUser(usr User) (*User, error) {
	url := umUsers() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &User{}
	err := c.client.Post(url, usr, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) UpdateUser(userid string, obj User) (*User, error) {
	url := umUsersPath(userid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &User{}
	err := c.client.Put(url, obj, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) DeleteUser(userid string) (*http.Header, error) {
	url := umUsersPath(userid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &http.Header{}
	err := c.client.Delete(url, ret, http.StatusAccepted)
	return ret, err
}
func (c *Client) ListResources() (*Resources, error) {
	url := umResources() + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Resources{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetResourceByType(resourcetype string, resourceid string) (*Resource, error) {
	url := umResourcesTypePath(resourcetype, resourceid) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Resource{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) ListResourcesByType(resourcetype string) (*Resources, error) {
	url := umResourcesType(resourcetype) + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Resources{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) ListRequests() (*Requests, error) {
	url := "/requests" + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Requests{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetRequest(reqID string) (*Request, error) {
	url := "/requests/" + reqID + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &Request{}
	err := c.client.Get(url, ret, http.StatusOK)
	return ret, err
}
func (c *Client) GetRequestStatus(path string) (*RequestStatus, error) {
	url := path + `?depth=` + c.client.depth + `&pretty=` + strconv.FormatBool(c.client.pretty)
	ret := &RequestStatus{}
	err := c.client.GetRequestStatus(url, ret, http.StatusOK)
	return ret, err
}
func (l *Writer) Close() error {
	l.mutex.Lock()
	defer l.mutex.Unlock()

	return l.file.Close()
}
func (serialize *SerializableMeta) GetSerializableArgument(serializableMetaInterface SerializableMetaInterface) interface{} {
	if serialize.Value.OriginalValue != nil {
		return serialize.Value.OriginalValue
	}

	if res := serializableMetaInterface.GetSerializableArgumentResource(); res != nil {
		value := res.NewStruct()
		json.Unmarshal([]byte(serialize.Value.SerializedValue), value)
		return value
	}
	return nil
}
func BoolVar(p *bool, name string, value bool, usage string) {
	EnvironmentFlags.BoolVar(p, name, value, usage)
}
func Bool(name string, value bool, usage string) *bool {
	return EnvironmentFlags.Bool(name, value, usage)
}
func IntVar(p *int, name string, value int, usage string) {
	EnvironmentFlags.IntVar(p, name, value, usage)
}
func Int(name string, value int, usage string) *int {
	return EnvironmentFlags.Int(name, value, usage)
}
func Int64Var(p *int64, name string, value int64, usage string) {
	EnvironmentFlags.Int64Var(p, name, value, usage)
}
func Int64(name string, value int64, usage string) *int64 {
	return EnvironmentFlags.Int64(name, value, usage)
}
func UintVar(p *uint, name string, value uint, usage string) {
	EnvironmentFlags.UintVar(p, name, value, usage)
}
func Uint(name string, value uint, usage string) *uint {
	return EnvironmentFlags.Uint(name, value, usage)
}
func Uint64Var(p *uint64, name string, value uint64, usage string) {
	EnvironmentFlags.Uint64Var(p, name, value, usage)
}
func Uint64(name string, value uint64, usage string) *uint64 {
	return EnvironmentFlags.Uint64(name, value, usage)
}
func StringVar(p *string, name string, value string, usage string) {
	EnvironmentFlags.StringVar(p, name, value, usage)
}
func String(name string, value string, usage string) *string {
	return EnvironmentFlags.String(name, value, usage)
}
func Float64Var(p *float64, name string, value float64, usage string) {
	EnvironmentFlags.Float64Var(p, name, value, usage)
}
func Float64(name string, value float64, usage string) *float64 {
	return EnvironmentFlags.Float64(name, value, usage)
}
func DurationVar(p *time.Duration, name string, value time.Duration, usage string) {
	EnvironmentFlags.DurationVar(p, name, value, usage)
}
func Duration(name string, value time.Duration, usage string) *time.Duration {
	return EnvironmentFlags.Duration(name, value, usage)
}
func Parse() {
	env := os.Environ()
	// Clean up and "fake" some flag k/v pairs.
	args := make([]string, 0, len(env))
	for _, value := range env {
		if Lookup(value[:strings.Index(value, "=")]) == nil {
			continue
		}
		args = append(args, fmt.Sprintf("-%s", value))
	}
	EnvironmentFlags.Parse(args)
}
func WriteStringToFile(text, path string) error {
	f, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY, 0644)
	defer f.Close()

	if err != nil {
		return err
	}

	f.WriteString(text)
	return nil
}
func ReadFileToString(path string) (string, error) {

	f, err := os.Open(path)
	if err != nil {
		return "", err
	}
	defer f.Close()

	scanner := bufio.NewScanner(f)
	scanner.Scan()

	return scanner.Text(), nil
}
func LineReader(r io.Reader) (<-chan string, error) {
	return lineReader(func() (io.Reader, func(), error) { return r, nil, nil })
}
func LineReaderFrom(path string) (<-chan string, error) {
	return lineReader(func() (io.Reader, func(), error) {
		if !FileExists(path) {
			return nil, nil, nil
		}
		f, err := os.Open(path)
		if err != nil {
			return nil, nil, err
		}
		return f, func() { f.Close() }, nil
	})
}
func FileExists(filePath string) bool {
	if _, err := os.Stat(filePath); !os.IsNotExist(err) {
		return true
	}
	return false
}
func FileExistsInPath(fileName string) bool {
	_, err := exec.LookPath(fileName)
	return err == nil
}
func GetPathParts(path string) (dirPath, fileName, absPath string) {
	lookup, lookupErr := exec.LookPath(path)
	if lookupErr == nil {
		path = lookup
	}
	absPath, _ = filepath.Abs(path)
	dirPath = filepath.Dir(absPath)
	fileName = filepath.Base(absPath)
	return
}
func GetThisPathParts() (dirPath, fileName, absPath string) {
	exeFile, _ := osext.Executable()
	return GetPathParts(exeFile)
}
func RandomString(length int) string {
	src := rand.NewSource(time.Now().UnixNano())
	b := make([]byte, length)
	for i, cache, remain := length-1, src.Int63(), letterIndexMax; i >= 0; {
		if remain == 0 {
			cache, remain = src.Int63(), letterIndexMax
		}
		if idx := int(cache & letterIndexMask); idx < len(letterBytes) {
			b[i] = letterBytes[idx]
			i--
		}
		cache >>= letterIndexBits
		remain--
	}

	return string(b)
}
func ParseAddress(addr string) (proto string, path string, err error) {
	m := netAddrRx.FindStringSubmatch(addr)
	if m == nil {
		return "", "", goof.WithField("address", addr, "invalid address")
	}
	return m[1], m[2], nil
}
func HomeDir() string {
	if homeDirSet {
		return homeDir
	}
	if user, err := user.Current(); err == nil {
		homeDir = user.HomeDir
	}
	homeDirSet = true
	return homeDir
}
func IsTCPPortAvailable(port int) bool {
	if port < minTCPPort || port > maxTCPPort {
		return false
	}
	conn, err := net.Listen("tcp", fmt.Sprintf("127.0.0.1:%d", port))
	if err != nil {
		return false
	}
	conn.Close()
	return true
}
func RandomTCPPort() int {
	for i := maxReservedTCPPort; i < maxTCPPort; i++ {
		p := tcpPortRand.Intn(maxRandTCPPort) + maxReservedTCPPort + 1
		if IsTCPPortAvailable(p) {
			return p
		}
	}
	return -1
}
func HashString(key string, numBuckets int) int32 {
	// jump.Hash returns values from 0.
	k := Hash(Sum64(key), numBuckets)

	return k
}
func (sm *sessionManager) Context(ctx context.Context, req *empty.Empty) (*mnemosynerpc.ContextResponse, error) {
	md, ok := metadata.FromIncomingContext(ctx)
	if !ok {
		return nil, status.Errorf(codes.InvalidArgument, "missing metadata in context, access token cannot be retrieved")
	}

	if len(md[mnemosyne.AccessTokenMetadataKey]) == 0 {
		return nil, status.Errorf(codes.InvalidArgument, "missing access token in metadata")
	}

	at := md[mnemosyne.AccessTokenMetadataKey][0]

	res, err := sm.Get(ctx, &mnemosynerpc.GetRequest{AccessToken: at})
	if err != nil {
		return nil, err
	}
	return &mnemosynerpc.ContextResponse{
		Session: res.Session,
	}, nil
}
func (s *Session) Token() (*oauth2.Token, error) {
	var (
		err      error
		expireAt time.Time
	)
	if s.ExpireAt != nil {
		expireAt, err = ptypes.Timestamp(s.ExpireAt)
		if err != nil {
			return nil, err
		}
	}
	token := &oauth2.Token{
		AccessToken: string(s.AccessToken),
		Expiry:      expireAt,
	}
	if s.Bag != nil && len(s.Bag) > 0 {
		token = token.WithExtra(bagToURLValues(s.Bag))
	}

	return token, nil
}
func NewDaemon(opts *DaemonOpts) (*Daemon, error) {
	d := &Daemon{
		done:          make(chan struct{}),
		opts:          opts,
		logger:        opts.Logger,
		serverOptions: opts.RPCOptions,
		rpcListener:   opts.RPCListener,
		debugListener: opts.DebugListener,
	}

	if err := d.setPostgresConnectionParameters(); err != nil {
		return nil, err
	}
	if d.opts.SessionTTL == 0 {
		d.opts.SessionTTL = storage.DefaultTTL
	}
	if d.opts.SessionTTC == 0 {
		d.opts.SessionTTC = storage.DefaultTTC
	}
	if d.opts.Storage == "" {
		d.opts.Storage = storage.EnginePostgres
	}
	if d.opts.PostgresTable == "" {
		d.opts.PostgresTable = "session"
	}
	if d.opts.PostgresSchema == "" {
		d.opts.PostgresSchema = "mnemosyne"
	}

	return d, nil
}
func (d *Daemon) Close() (err error) {
	d.done <- struct{}{}
	d.server.GracefulStop()
	if d.postgres != nil {
		if err = d.postgres.Close(); err != nil {
			return
		}
	}
	if d.debugListener != nil {
		if err = d.debugListener.Close(); err != nil {
			return
		}
	}
	if d.tracerCloser != nil {
		if err = d.tracerCloser.Close(); err != nil {
			return
		}
	}
	return nil
}
func NewAccessTokenContext(ctx context.Context, at string) context.Context {
	return context.WithValue(ctx, accessTokenContextKey, at)
}
func AccessTokenFromContext(ctx context.Context) (string, bool) {
	at, ok := ctx.Value(accessTokenContextKey).(string)

	return at, ok
}
func RandomAccessToken() (string, error) {
	buf, err := generateRandomBytes(128)
	if err != nil {
		return "", err
	}

	// A hash needs to be 64 bytes long to have 256-bit collision resistance.
	hash := make([]byte, 64)
	// Compute a 64-byte hash of buf and put it in h.
	sha3.ShakeSum256(hash, buf)
	hash2 := make([]byte, hex.EncodedLen(len(hash)))
	hex.Encode(hash2, hash)
	return string(hash2), nil
}
func Init(opts Opts) (logger *zap.Logger, err error) {
	var (
		cfg     zap.Config
		options []zap.Option
		lvl     zapcore.Level
	)
	switch opts.Environment {
	case "production":
		cfg = zap.NewProductionConfig()
	case "stackdriver":
		cfg = NewStackdriverConfig()
		options = append(options, zap.Fields(zap.Object("serviceContext", &ServiceContext{
			Service: "mnemosyned",
			Version: opts.Version,
		})))
	case "development":
		cfg = zap.NewDevelopmentConfig()
	default:
		cfg = zap.NewProductionConfig()
	}

	if err = lvl.Set(opts.Level); err != nil {
		return nil, err
	}
	cfg.Level.SetLevel(lvl)

	logger, err = cfg.Build(options...)
	if err != nil {
		return nil, err
	}
	logger.Info("logger has been initialized", zap.String("environment", opts.Environment))

	return logger, nil
}
func Load(path string) (d *Dic, err error) {
	d = new(Dic)
	r, err := zip.OpenReader(path)
	if err != nil {
		return d, err
	}
	defer r.Close()

	for _, f := range r.File {
		if err = func() error {
			rc, e := f.Open()
			if e != nil {
				return e
			}
			defer rc.Close()
			switch f.Name {
			case "morph.dic":
				if e = d.loadMorphDicPart(rc); e != nil {
					return e
				}
			case "pos.dic":
				if e = d.loadPOSDicPart(rc); e != nil {
					return e
				}
			case "content.dic":
				if e = d.loadContentDicPart(rc); e != nil {
					return e
				}
			case "index.dic":
				if e = d.loadIndexDicPart(rc); e != nil {
					return e
				}
			case "connection.dic":
				if e = d.loadConnectionDicPart(rc); e != nil {
					return e
				}
			case "chardef.dic":
				if e = d.loadCharDefDicPart(rc); e != nil {
					return e
				}
			case "unk.dic":
				if e = d.loadUnkDicPart(rc); e != nil {
					return e
				}
			}
			return nil
		}(); err != nil {
			return
		}
	}
	return
}
func (s *Storage) Start(ctx context.Context, accessToken, refreshToken, sid, sc string, b map[string]string) (*mnemosynerpc.Session, error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.start")
	defer span.Finish()

	ent := &sessionEntity{
		AccessToken:   accessToken,
		RefreshToken:  refreshToken,
		SubjectID:     sid,
		SubjectClient: sc,
		Bag:           model.Bag(b),
	}

	if err := s.save(ctx, ent); err != nil {
		return nil, err
	}

	return ent.session()
}
func (s *Storage) Get(ctx context.Context, accessToken string) (*mnemosynerpc.Session, error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.get")
	defer span.Finish()

	var entity sessionEntity
	start := time.Now()
	labels := prometheus.Labels{"query": "get"}

	err := s.db.QueryRowContext(ctx, s.queryGet, accessToken).Scan(
		&entity.RefreshToken,
		&entity.SubjectID,
		&entity.SubjectClient,
		&entity.Bag,
		&entity.ExpireAt,
	)
	s.incQueries(labels, start)
	if err != nil {
		s.incError(labels)
		if err == sql.ErrNoRows {
			return nil, storage.ErrSessionNotFound
		}
		return nil, err
	}

	expireAt, err := ptypes.TimestampProto(entity.ExpireAt)
	if err != nil {
		return nil, err
	}
	return &mnemosynerpc.Session{
		AccessToken:   accessToken,
		RefreshToken:  entity.RefreshToken,
		SubjectId:     entity.SubjectID,
		SubjectClient: entity.SubjectClient,
		Bag:           entity.Bag,
		ExpireAt:      expireAt,
	}, nil
}
func (s *Storage) List(ctx context.Context, offset, limit int64, expiredAtFrom, expiredAtTo *time.Time) ([]*mnemosynerpc.Session, error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.list")
	defer span.Finish()

	if limit == 0 {
		return nil, errors.New("cannot retrieve list of sessions, limit needs to be higher than 0")
	}

	args := []interface{}{offset, limit}
	query := "SELECT access_token, refresh_token, subject_id, subject_client, bag, expire_at FROM " + s.schema + "." + s.table + " "
	if expiredAtFrom != nil || expiredAtTo != nil {
		query += " WHERE "
	}
	switch {
	case expiredAtFrom != nil && expiredAtTo == nil:
		query += "expire_at > $3"
		args = append(args, expiredAtFrom)
	case expiredAtFrom == nil && expiredAtTo != nil:
		query += "expire_at < $3"
		args = append(args, expiredAtTo)
	case expiredAtFrom != nil && expiredAtTo != nil:
		query += "expire_at > $3 AND expire_at < $4"
		args = append(args, expiredAtFrom, expiredAtTo)
	}

	query += " OFFSET $1 LIMIT $2"
	labels := prometheus.Labels{"query": "list"}

	start := time.Now()
	rows, err := s.db.QueryContext(ctx, query, args...)
	s.incQueries(labels, start)
	if err != nil {
		s.incError(labels)
		return nil, err
	}
	defer rows.Close()

	sessions := make([]*mnemosynerpc.Session, 0, limit)
	for rows.Next() {
		var ent sessionEntity

		err = rows.Scan(
			&ent.AccessToken,
			&ent.RefreshToken,
			&ent.SubjectID,
			&ent.SubjectClient,
			&ent.Bag,
			&ent.ExpireAt,
		)
		if err != nil {
			s.incError(labels)
			return nil, err
		}

		expireAt, err := ptypes.TimestampProto(ent.ExpireAt)
		if err != nil {
			return nil, err
		}
		sessions = append(sessions, &mnemosynerpc.Session{
			AccessToken:   ent.AccessToken,
			RefreshToken:  ent.RefreshToken,
			SubjectId:     ent.SubjectID,
			SubjectClient: ent.SubjectClient,
			Bag:           ent.Bag,
			ExpireAt:      expireAt,
		})
	}
	if rows.Err() != nil {
		s.incError(labels)
		return nil, rows.Err()
	}

	return sessions, nil
}
func (s *Storage) Exists(ctx context.Context, accessToken string) (exists bool, err error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.exists")
	defer span.Finish()

	start := time.Now()
	labels := prometheus.Labels{"query": "exists"}

	err = s.db.QueryRowContext(ctx, s.queryExists, accessToken).Scan(
		&exists,
	)
	s.incQueries(labels, start)
	if err != nil {
		s.incError(labels)
	}

	return
}
func (s *Storage) Abandon(ctx context.Context, accessToken string) (bool, error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.abandon")
	defer span.Finish()

	start := time.Now()
	labels := prometheus.Labels{"query": "abandon"}

	result, err := s.db.ExecContext(ctx, s.queryAbandon, accessToken)
	s.incQueries(labels, start)
	if err != nil {
		s.incError(labels)
		return false, err
	}

	affected, err := result.RowsAffected()
	if err != nil {
		return false, err
	}
	if affected == 0 {
		return false, storage.ErrSessionNotFound
	}

	return true, nil
}
func (s *Storage) SetValue(ctx context.Context, accessToken string, key, value string) (map[string]string, error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.set-value")
	defer span.Finish()

	var err error
	if accessToken == "" {
		return nil, storage.ErrMissingAccessToken
	}

	entity := &sessionEntity{
		AccessToken: accessToken,
	}
	selectQuery := `
		SELECT bag
		FROM ` + s.schema + `.` + s.table + `
		WHERE access_token = $1
		FOR UPDATE
	`
	updateQuery := `
		UPDATE ` + s.schema + `.` + s.table + `
		SET
			bag = $2
		WHERE access_token = $1
	`

	tx, err := s.db.BeginTx(ctx, nil)
	if err != nil {
		tx.Rollback()
		return nil, err
	}

	startSelect := time.Now()
	err = tx.QueryRowContext(ctx, selectQuery, accessToken).Scan(
		&entity.Bag,
	)
	s.incQueries(prometheus.Labels{"query": "set_value_select"}, startSelect)
	if err != nil {
		s.incError(prometheus.Labels{"query": "set_value_select"})
		tx.Rollback()
		if err == sql.ErrNoRows {
			return nil, storage.ErrSessionNotFound
		}
		return nil, err
	}

	entity.Bag.Set(key, value)

	startUpdate := time.Now()
	_, err = tx.ExecContext(ctx, updateQuery, accessToken, entity.Bag)
	s.incQueries(prometheus.Labels{"query": "set_value_update"}, startUpdate)
	if err != nil {
		s.incError(prometheus.Labels{"query": "set_value_update"})
		tx.Rollback()
		return nil, err
	}

	tx.Commit()

	return entity.Bag, nil
}
func (s *Storage) Delete(ctx context.Context, subjectID, accessToken, refreshToken string, expiredAtFrom, expiredAtTo *time.Time) (int64, error) {
	span, ctx := opentracing.StartSpanFromContext(ctx, "postgres.storage.delete")
	defer span.Finish()

	where, args := s.where(subjectID, accessToken, refreshToken, expiredAtFrom, expiredAtTo)
	if where.Len() == 0 {
		return 0, fmt.Errorf("session cannot be deleted, no where parameter provided: %s", where.String())
	}
	query := "DELETE FROM " + s.schema + "." + s.table + " WHERE " + where.String()
	labels := prometheus.Labels{"query": "delete"}
	start := time.Now()

	result, err := s.db.Exec(query, args...)
	s.incQueries(labels, start)
	if err != nil {
		s.incError(labels)
		return 0, err
	}

	return result.RowsAffected()
}
func (s *Storage) Setup() error {
	query := fmt.Sprintf(`
		CREATE SCHEMA IF NOT EXISTS %s;
		CREATE TABLE IF NOT EXISTS %s.%s (
			access_token BYTEA PRIMARY KEY,
			refresh_token BYTEA,
			subject_id TEXT NOT NULL,
			subject_client TEXT,
			bag bytea NOT NULL,
			expire_at TIMESTAMPTZ NOT NULL DEFAULT (NOW() + '%d seconds')

		);
		CREATE INDEX ON %s.%s (refresh_token);
		CREATE INDEX ON %s.%s (subject_id);
		CREATE INDEX ON %s.%s (expire_at DESC);
	`, s.schema, s.schema, s.table, int64(s.ttl.Seconds()),
		s.schema, s.table,
		s.schema, s.table,
		s.schema, s.table,
	)
	_, err := s.db.Exec(query)

	return err
}
func (s *Storage) TearDown() error {
	_, err := s.db.Exec(`DROP SCHEMA IF EXISTS ` + s.schema + ` CASCADE`)

	return err
}
func initJaeger(service, node, agentAddress string, log *zap.Logger) (opentracing.Tracer, io.Closer, error) {
	cfg := &config.Configuration{
		Sampler: &config.SamplerConfig{
			Type:  "const",
			Param: 1,
		},
		Tags: []opentracing.Tag{{
			Key:   constant.Subsystem + ".listen",
			Value: node,
		}},
		Reporter: &config.ReporterConfig{
			LogSpans:           true,
			LocalAgentHostPort: agentAddress,
		},
	}

	tracer, closer, err := cfg.New(service, config.Logger(zapjaeger.NewLogger(log)))
	if err != nil {
		return nil, nil, err
	}
	return tracer, closer, nil
}
func Usage() {
	fmt.Fprintf(errorWriter, "Japanese Morphological Analyzer -- github.com/ikawaha/kagome.ipadic\n")
	fmt.Fprintf(errorWriter, "usage: %s <command>\n", path.Base(os.Args[0]))
}
func Build(keywords []string) (DoubleArray, error) {
	s := len(keywords)
	if s == 0 {
		return DoubleArray{}, nil
	}
	ids := make([]int, s, s)
	for i := range ids {
		ids[i] = i + 1
	}
	return BuildWithIDs(keywords, ids)
}
func Read(r io.Reader) (DoubleArray, error) {
	var sz int64
	if e := binary.Read(r, binary.LittleEndian, &sz); e != nil {
		return DoubleArray{}, e
	}
	//fmt.Println("read data len:", sz)
	d := make(DoubleArray, sz, sz)
	for i := range d {
		if e := binary.Read(r, binary.LittleEndian, &d[i].Base); e != nil {
			return d, e
		}
		if e := binary.Read(r, binary.LittleEndian, &d[i].Check); e != nil {
			return d, e
		}
	}
	return d, nil
}
func (c *Cluster) Get(k int32) (*Node, bool) {
	if len(c.nodes) == 0 {
		return nil, false
	}
	if len(c.nodes)-1 < int(k) {
		return nil, false
	}
	return c.nodes[k], true
}
func (c *Cluster) ExternalNodes() (res []*Node) {
	for _, n := range c.nodes {
		if n.Addr != c.listen {
			res = append(res, n)
		}
	}
	return
}
func (c *Cluster) GetOther(accessToken string) (*Node, bool) {
	if c == nil {
		return nil, false
	}
	if c.Len() == 1 {
		return nil, false
	}

	if node, ok := c.Get(jump.HashString(accessToken, c.Len())); ok {
		if node.Addr != c.listen {
			if node.Client != nil {
				return node, true
			}
		}
	}
	return nil, false
}
func (c *Cluster) GoString() string {
	buf, _ := json.Marshal(map[string]interface{}{
		"listen":  c.listen,
		"nodes":   c.nodes,
		"buckets": strconv.FormatInt(int64(c.buckets), 10),
	})
	return string(buf)
}
func (b *Bag) Scan(src interface{}) (err error) {
	switch t := src.(type) {
	case []byte:
		err = gob.NewDecoder(bytes.NewReader(t)).Decode(b)
	default:
		return errors.New("unsupported data source type")
	}

	return
}
func (b Bag) Value() (driver.Value, error) {
	buf := bytes.NewBuffer(nil)
	err := gob.NewEncoder(buf).Encode(b)
	if err != nil {
		return nil, err
	}

	return buf.Bytes(), nil
}
func (b *Bag) Has(key string) bool {
	_, ok := (*b)[key]

	return ok
}
func (idx IndexTable) WriteTo(w io.Writer) (n int64, err error) {
	n, err = idx.Da.WriteTo(w)
	var b bytes.Buffer
	enc := gob.NewEncoder(&b)
	if err = enc.Encode(idx.Dup); err != nil {
		return
	}
	x, err := b.WriteTo(w)
	if err != nil {
		return
	}
	n += x
	return
}
func ReadIndexTable(r io.Reader) (IndexTable, error) {
	idx := IndexTable{}
	d, err := da.Read(r)
	if err != nil {
		return idx, fmt.Errorf("read index error, %v", err)
	}
	idx.Da = d

	dec := gob.NewDecoder(r)
	if e := dec.Decode(&idx.Dup); e != nil {
		return idx, fmt.Errorf("read index dup table error, %v", e)
	}

	return idx, nil
}
func New(admin *admin.Admin) *ActionBar {
	bar := &ActionBar{Admin: admin}
	ctr := &controller{ActionBar: bar}
	admin.GetRouter().Get("/action_bar/switch_mode", ctr.SwitchMode)
	admin.GetRouter().Get("/action_bar/inline_edit", ctr.InlineEdit)
	return bar
}
func (bar *ActionBar) RegisterAction(action ActionInterface) {
	bar.GlobalActions = append(bar.GlobalActions, action)
	bar.actions = bar.GlobalActions
}
func (bar *ActionBar) Actions(actions ...ActionInterface) *ActionBar {
	newBar := &ActionBar{Admin: bar.Admin, actions: bar.GlobalActions}
	newBar.actions = append(newBar.actions, actions...)
	return newBar
}
func (bar *ActionBar) Render(w http.ResponseWriter, r *http.Request) template.HTML {
	var (
		actions, inlineActions []ActionInterface
		context                = bar.Admin.NewContext(w, r)
	)
	for _, action := range bar.actions {
		if action.InlineAction() {
			inlineActions = append(inlineActions, action)
		} else {
			actions = append(actions, action)
		}
	}
	context.Context.CurrentUser = bar.Admin.Auth.GetCurrentUser(context)
	result := map[string]interface{}{
		"EditMode":      bar.EditMode(w, r),
		"Auth":          bar.Admin.Auth,
		"CurrentUser":   context.Context.CurrentUser,
		"Actions":       actions,
		"InlineActions": inlineActions,
		"RouterPrefix":  bar.Admin.GetRouter().Prefix,
	}
	return context.Render("action_bar/action_bar", result)
}
func (bar *ActionBar) FuncMap(w http.ResponseWriter, r *http.Request) template.FuncMap {
	funcMap := template.FuncMap{}

	funcMap["render_edit_button"] = func(value interface{}, resources ...*admin.Resource) template.HTML {
		return bar.RenderEditButtonWithResource(w, r, value, resources...)
	}

	return funcMap
}
func (bar *ActionBar) EditMode(w http.ResponseWriter, r *http.Request) bool {
	return isEditMode(bar.Admin.NewContext(w, r))
}
func (controller) SwitchMode(context *admin.Context) {
	utils.SetCookie(http.Cookie{Name: "qor-action-bar", Value: context.Request.URL.Query().Get("checked")}, context.Context)

	referrer := context.Request.Referer()
	if referrer == "" {
		referrer = "/"
	}

	http.Redirect(context.Writer, context.Request, referrer, http.StatusFound)
}
func (controller) InlineEdit(context *admin.Context) {
	context.Writer.Write([]byte(context.Render("action_bar/inline_edit")))
}
func (e *Error) Error() string {
	return fmt.Sprintf("%s:%d: %s", e.Filename, e.LineNum, e.Msg)
}
func ParseFile(filename string) ([]*Machine, Macros, error) {
	// TODO(fhs): Check if file is readable by anyone besides the user if there is password in it.
	fd, err := os.Open(filename)
	if err != nil {
		return nil, nil, err
	}
	defer fd.Close()
	return parse(fd, &filePos{filename, 1})
}
func FindMachine(filename, name string) (*Machine, error) {
	mach, _, err := ParseFile(filename)
	if err != nil {
		return nil, err
	}
	var def *Machine
	for _, m := range mach {
		if m.Name == name {
			return m, nil
		}
		if m.Name == "" {
			def = m
		}
	}
	if def == nil {
		return nil, errors.New("no machine found")
	}
	return def, nil
}
func New(initialisms map[string]bool) (*Kace, error) {
	ci := initialisms
	if ci == nil {
		ci = map[string]bool{}
	}

	ci = sanitizeCI(ci)

	t, err := ktrie.NewKTrie(ci)
	if err != nil {
		return nil, fmt.Errorf("kace: cannot create trie: %s", err)
	}

	k := &Kace{
		t: t,
	}

	return k, nil
}
func (k *Kace) Camel(s string) string {
	return camelCase(k.t, s, false)
}
func (k *Kace) Pascal(s string) string {
	return camelCase(k.t, s, true)
}
func (k *Kace) Snake(s string) string {
	return delimitedCase(s, snakeDelim, false)
}
func (k *Kace) SnakeUpper(s string) string {
	return delimitedCase(s, snakeDelim, true)
}
func (k *Kace) Kebab(s string) string {
	return delimitedCase(s, kebabDelim, false)
}
func (k *Kace) KebabUpper(s string) string {
	return delimitedCase(s, kebabDelim, true)
}
func isPathSafe(s string) error {
	u, err := url.Parse(s)
	if err != nil {
		return err
	}

	e, err := url.PathUnescape(u.Path)
	if err != nil {
		return err
	}

	if strings.Contains(e, "..") {
		return fmt.Errorf(errorMessage)
	}

	if !whitelistPattern.MatchString(e) {
		return fmt.Errorf(errorMessage)
	}

	return nil
}
func (t *WriterTracer) Start(r *http.Request) {
	t.StartTime = time.Now().UTC()
	t.Request.URL = r.URL.String()
	t.Request.Method = r.Method
}
func ParseAuthHeaders(r *http.Request) (*AuthCreds, error) {
	// according to the doc below oauth 2.0 bearer access token can
	// come with query parameter
	// http://self-issued.info/docs/draft-ietf-oauth-v2-bearer.html#query-param
	// we are going to support this
	if r.URL.Query().Get(AccessTokenQueryParam) != "" {
		return &AuthCreds{
			Type:     AuthBearer,
			Password: r.URL.Query().Get(AccessTokenQueryParam),
		}, nil
	}

	authHeader := r.Header.Get("Authorization")
	if authHeader == "" {
		return nil, trace.Wrap(&AccessDeniedError{Message: "unauthorized"})
	}

	auth := strings.SplitN(authHeader, " ", 2)

	if len(auth) != 2 {
		return nil, trace.Wrap(
			&ParameterError{
				Name:    "Authorization",
				Message: "invalid auth header"})
	}

	switch auth[0] {
	case AuthBasic:
		payload, err := base64.StdEncoding.DecodeString(auth[1])
		if err != nil {
			return nil, trace.Wrap(
				&ParameterError{
					Name:    "Authorization",
					Message: err.Error()})
		}
		pair := strings.SplitN(string(payload), ":", 2)
		if len(pair) != 2 {
			return nil, trace.Wrap(
				&ParameterError{
					Name:    "Authorization",
					Message: "bad header"})
		}
		return &AuthCreds{Type: AuthBasic, Username: pair[0], Password: pair[1]}, nil
	case AuthBearer:
		return &AuthCreds{Type: AuthBearer, Password: auth[1]}, nil
	}
	return nil, trace.Wrap(
		&ParameterError{
			Name:    "Authorization",
			Message: "unsupported auth scheme"})
}
func Tracer(newTracer NewTracer) ClientParam {
	return func(c *Client) error {
		c.newTracer = newTracer
		return nil
	}
}
func HTTPClient(h *http.Client) ClientParam {
	return func(c *Client) error {
		c.client = h
		return nil
	}
}
func BasicAuth(username, password string) ClientParam {
	return func(c *Client) error {
		c.auth = &basicAuth{username: username, password: password}
		return nil
	}
}
func BearerAuth(token string) ClientParam {
	return func(c *Client) error {
		c.auth = &bearerAuth{token: token}
		return nil
	}
}
func CookieJar(jar http.CookieJar) ClientParam {
	return func(c *Client) error {
		c.jar = jar
		return nil
	}
}
func SanitizerEnabled(sanitizerEnabled bool) ClientParam {
	return func(c *Client) error {
		c.sanitizerEnabled = sanitizerEnabled
		return nil
	}
}
func (c *Client) OpenFile(ctx context.Context, endpoint string, params url.Values) (ReadSeekCloser, error) {
	// If the sanitizer is enabled, make sure the requested path is safe.
	if c.sanitizerEnabled {
		err := isPathSafe(endpoint)
		if err != nil {
			return nil, err
		}
	}

	u, err := url.Parse(endpoint)
	if err != nil {
		return nil, err
	}
	u.RawQuery = params.Encode()

	return newSeeker(c, ctx, u.String())
}
func (c *Client) RoundTrip(fn RoundTripFn) (*Response, error) {
	re, err := fn()
	if err != nil {
		return nil, err
	}
	defer re.Body.Close()
	buf := &bytes.Buffer{}
	_, err = io.Copy(buf, re.Body)
	if err != nil {
		return nil, err
	}
	return &Response{
		code:    re.StatusCode,
		headers: re.Header,
		body:    buf,
		cookies: re.Cookies(),
	}, nil
}
func (c *Client) SetAuthHeader(h http.Header) {
	if c.auth != nil {
		h.Set("Authorization", c.auth.String())
	}
}
func (r *FileResponse) FileName() string {
	value := r.headers.Get("Content-Disposition")
	if len(value) == 0 {
		return ""
	}
	_, params, err := mime.ParseMediaType(value)
	if err != nil {
		return ""
	}
	return params["filename"]
}
func newBuffersFromFiles(files []File) []fileBuffer {
	buffers := make([]fileBuffer, 0, len(files))
	for _, file := range files {
		buffers = append(buffers, newFileBuffer(file))
	}
	return buffers
}
func newFileBuffer(file File) fileBuffer {
	buf := &bytes.Buffer{}
	return fileBuffer{
		Reader: io.TeeReader(file.Reader, buf),
		File:   file,
		cache:  buf,
	}
}
func (r *fileBuffer) rewind() {
	r.Reader = io.MultiReader(r.cache, r.File.Reader)
}
func ConvertResponse(re *Response, err error) (*Response, error) {
	if err != nil {
		if uerr, ok := err.(*url.Error); ok && uerr != nil && uerr.Err != nil {
			return nil, trace.Wrap(uerr.Err)
		}
		return nil, trace.Wrap(err)
	}
	return re, trace.ReadError(re.Code(), re.Bytes())
}
func (o *BoolOption) Set(s string) error {
	err := convertString(s, &o.Value)
	if err != nil {
		return err
	}
	o.Source = "override"
	o.Defined = true
	return nil
}
func (o *BoolOption) WriteAnswer(name string, value interface{}) error {
	if v, ok := value.(bool); ok {
		o.Value = v
		o.Defined = true
		o.Source = "prompt"
		return nil
	}
	return fmt.Errorf("Got %T expected %T type: %v", value, o.Value, value)
}
func (o BoolOption) String() string {
	if StringifyValue {
		return fmt.Sprintf("%v", o.Value)
	}
	return fmt.Sprintf("{Source:%s Defined:%t Value:%v}", o.Source, o.Defined, o.Value)
}
func TriggerIncidentKey(description string, key string) (incidentKey string, err error) {
	return trigger(description, key, map[string]interface{}{})
}
func TriggerWithDetails(description string, details map[string]interface{}) (incidentKey string, err error) {
	return trigger(description, "", details)
}
func TriggerIncidentKeyWithDetails(description string, key string, details map[string]interface{}) (incidentKey string, err error) {
	return trigger(description, key, details)
}
func Merge(dst, src interface{}) {
	m := NewMerger()
	m.mergeStructs(reflect.ValueOf(dst), reflect.ValueOf(src))
}
func (m *Merger) setSource(v reflect.Value) {
	if v.Kind() == reflect.Ptr {
		v = v.Elem()
	}
	switch v.Kind() {
	case reflect.Map:
		for _, key := range v.MapKeys() {
			keyval := v.MapIndex(key)
			if keyval.Kind() == reflect.Struct && keyval.FieldByName("Source").IsValid() {
				// map values are immutable, so we need to copy the value
				// update the value, then re-insert the value to the map
				newval := reflect.New(keyval.Type())
				newval.Elem().Set(keyval)
				m.setSource(newval)
				v.SetMapIndex(key, newval.Elem())
			}
		}
	case reflect.Struct:
		if v.CanAddr() {
			if option, ok := v.Addr().Interface().(option); ok {
				if option.IsDefined() {
					option.SetSource(m.sourceFile)
				}
				return
			}
		}
		for i := 0; i < v.NumField(); i++ {
			structField := v.Type().Field(i)
			// PkgPath is empty for upper case (exported) field names.
			if structField.PkgPath != "" {
				// unexported field, skipping
				continue
			}
			m.setSource(v.Field(i))
		}
	case reflect.Array:
		fallthrough
	case reflect.Slice:
		for i := 0; i < v.Len(); i++ {
			m.setSource(v.Index(i))
		}
	}
}
func convertString(src string, dst interface{}) (err error) {
	switch v := dst.(type) {
	case *bool:
		*v, err = strconv.ParseBool(src)
	case *string:
		*v = src
	case *int:
		var tmp int64
		// this is a cheat, we only know int is at least 32 bits
		// but we have to make a compromise here
		tmp, err = strconv.ParseInt(src, 10, 32)
		*v = int(tmp)
	case *int8:
		var tmp int64
		tmp, err = strconv.ParseInt(src, 10, 8)
		*v = int8(tmp)
	case *int16:
		var tmp int64
		tmp, err = strconv.ParseInt(src, 10, 16)
		*v = int16(tmp)
	case *int32:
		var tmp int64
		tmp, err = strconv.ParseInt(src, 10, 32)
		*v = int32(tmp)
	case *int64:
		var tmp int64
		tmp, err = strconv.ParseInt(src, 10, 64)
		*v = int64(tmp)
	case *uint:
		var tmp uint64
		// this is a cheat, we only know uint is at least 32 bits
		// but we have to make a compromise here
		tmp, err = strconv.ParseUint(src, 10, 32)
		*v = uint(tmp)
	case *uint8:
		var tmp uint64
		tmp, err = strconv.ParseUint(src, 10, 8)
		*v = uint8(tmp)
	case *uint16:
		var tmp uint64
		tmp, err = strconv.ParseUint(src, 10, 16)
		*v = uint16(tmp)
	case *uint32:
		var tmp uint64
		tmp, err = strconv.ParseUint(src, 10, 32)
		*v = uint32(tmp)
	case *uint64:
		var tmp uint64
		tmp, err = strconv.ParseUint(src, 10, 64)
		*v = uint64(tmp)
	// hmm, collides with uint8
	// case *byte:
	// 	tmp := []byte(src)
	// 	if len(tmp) == 1 {
	// 		*v = tmp[0]
	// 	} else {
	// 		err = fmt.Errorf("Cannot convert string %q to byte, length: %d", src, len(tmp))
	// 	}
	// hmm, collides with int32
	// case *rune:
	// 	tmp := []rune(src)
	// 	if len(tmp) == 1 {
	// 		*v = tmp[0]
	// 	} else {
	// 		err = fmt.Errorf("Cannot convert string %q to rune, lengt: %d", src, len(tmp))
	// 	}
	case *float32:
		var tmp float64
		tmp, err = strconv.ParseFloat(src, 32)
		*v = float32(tmp)
	case *float64:
		var tmp float64
		tmp, err = strconv.ParseFloat(src, 64)
		*v = float64(tmp)
	default:
		err = fmt.Errorf("Cannot convert string %q to type %T", src, dst)
	}
	if err != nil {
		return err
	}

	return nil
}
func Do(d DB, f func(t Tx) error) error {
	t, err := d.TxBegin()
	if err != nil {
		return err
	}
	defer t.TxFinish()
	err = f(t)
	if err != nil {
		return err
	}
	return t.TxCommit()
}
func NewFile(filename string) io.Writer {
	if err, _ := os.Open(filename); err != nil {
		os.Remove(filename)
	}

	file, _ := os.Create(filename)

	return file
}
func (b *Bench) internalRun(showProgress bool) results.ResultSet {

	startTime := time.Now()
	endTime := startTime.Add(b.duration)

	sem := semaphore.NewSemaphore(b.threads, b.rampUp) // create a new semaphore with an initiall capacity or 0
	out := make(chan results.Result)
	resultsChan := make(chan []results.Result)

	go handleResult(showProgress, out, resultsChan)

	for run := true; run; run = (time.Now().Before(endTime)) {

		sem.Lock() // blocks when channel is full

		// execute a request
		go doRequest(b.request, b.timeout, sem, out)
	}

	fmt.Print("\nWaiting for threads to finish ")
	for i := sem.Length(); i != 0; i = sem.Length() {

		//abandon <- true
		time.Sleep(200 * time.Millisecond)
	}

	fmt.Println(" OK")
	fmt.Println("")

	close(out)

	return <-resultsChan
}
func (r Row) String() string {

	rStr := fmt.Sprintf("Start Time: %v\n", r.StartTime.UTC())
	rStr = fmt.Sprintf("%vElapsed Time: %v\n", rStr, r.ElapsedTime)
	rStr = fmt.Sprintf("%vThreads: %v\n", rStr, r.Threads)
	rStr = fmt.Sprintf("%vTotal Requests: %v\n", rStr, r.TotalRequests)
	rStr = fmt.Sprintf("%vAvg Request Time: %v\n", rStr, r.AvgRequestTime)
	rStr = fmt.Sprintf("%vTotal Success: %v\n", rStr, r.TotalSuccess)
	rStr = fmt.Sprintf("%vTotal Timeouts: %v\n", rStr, r.TotalTimeouts)
	return fmt.Sprintf("%vTotal Failures: %v\n", rStr, r.TotalFailures)
}
func (t *TabularResults) Tabulate(results []ResultSet) []Row {

	var rows []Row
	startTime := time.Unix(0, 0)

	for _, bucket := range results {

		if len(bucket) > 0 {

			var elapsedTime time.Duration

			if startTime == time.Unix(0, 0) {
				startTime = bucket[0].Timestamp
			}
			elapsedTime = bucket[0].Timestamp.Sub(startTime)

			row := Row{
				StartTime:      bucket[0].Timestamp,
				ElapsedTime:    elapsedTime,
				Threads:        0,
				TotalRequests:  0,
				TotalFailures:  0,
				TotalSuccess:   0,
				TotalTimeouts:  0,
				AvgRequestTime: 0,
			}

			totalRequestTime := 0 * time.Second
			maxThreads := 0

			for _, r := range bucket {

				row.TotalRequests++

				if r.Error != nil {
					if _, ok := r.Error.(errors.Timeout); ok {
						row.TotalTimeouts++
					}

					row.TotalFailures++
				} else {
					row.TotalSuccess++
					totalRequestTime += r.RequestTime
				}

				if r.Threads > maxThreads {
					maxThreads = r.Threads
					row.Threads = maxThreads
				}
			}

			if totalRequestTime != 0 && row.TotalSuccess != 0 {
				avgTime := int64(totalRequestTime) / int64(row.TotalSuccess)
				row.AvgRequestTime = time.Duration(avgTime)
			}

			rows = append(rows, row)
		}
	}

	return rows
}
func AmazonRequest() error {

	resp, err := http.Get("http://www.amazon.co.uk/")
	defer func(response *http.Response) {
		if response != nil && response.Body != nil {
			response.Body.Close()
		}
	}(resp)

	if err != nil || resp.StatusCode != 200 {
		return err
	}

	return nil
}
func WriteTabularData(interval time.Duration, r results.ResultSet, w io.Writer) {

	set := r.Reduce(interval)
	t := results.TabularResults{}
	rows := t.Tabulate(set)

	for _, row := range rows {
		w.Write([]byte(row.String()))
		w.Write([]byte("\n"))
	}
}
func (r ResultSet) Reduce(interval time.Duration) []ResultSet {
	sort.Sort(r)

	start := r[0].Timestamp
	end := r[len(r)-1].Timestamp

	// create the buckets
	bucketCount := getBucketCount(start, end, interval)
	buckets := make([]ResultSet, bucketCount)

	for _, result := range r {

		currentBucket := getBucketNumber(result.Timestamp, start, end, interval, bucketCount)
		buckets[currentBucket] = append(buckets[currentBucket], result)
	}

	return buckets
}
func NewSemaphore(capacity int, rampUp time.Duration) *Semaphore {
	s := Semaphore{
		lockDone: make(chan struct{}),
		lockLock: make(chan struct{}),
		rampUp:   rampUp,
	}

	// if the rampup time is less than 1 then return immediately
	if rampUp < 1 {
		s.s = make(chan struct{}, capacity)
	} else {
		s.s = make(chan struct{}, 1)
		go s.rampUpThreads(capacity, rampUp)
	}

	s.resizeUnlock()

	return &s
}
func (t *Semaphore) Release() {
	t.waitIfResizing()

	// we need a read lock to ensure we do not resize whilst resizing
	t.readMutex.RLock()
	defer t.readMutex.RUnlock()

	// make sure we have not called Release without Lock
	if len(t.s) == 0 {
		return
	}
	<-t.s
}
func (t *Semaphore) Resize(capacity int) {

	// only allow one resize to be called from one thread
	t.resizeMutex.Lock()

	if capacity == cap(t.s) {
		t.resizeMutex.Unlock()
		return
	}

	// lock the locks
	t.resizeLock()
	t.readMutex.Lock()
	defer t.resizeUnlock()
	defer t.resizeMutex.Unlock()
	defer t.readMutex.Unlock()

	new := make(chan struct{}, capacity) // create the new semaphore with the new capcity

	// copy the old values
	for n := len(t.s); n != 0; n = len(t.s) {
		new <- <-t.s // copy elements to the new channel
	}

	t.s = new
}
func (b *Bench) AddOutput(interval time.Duration, writer io.Writer, output output.OutputFunc) {

	o := outputContainer{
		interval: interval,
		writer:   writer,
		function: output,
	}

	b.outputs = append(b.outputs, o)
}
func (b *Bench) RunBenchmarks(r RequestFunc) {

	b.request = r
	results := b.internalRun(b.showProgress)
	b.processResults(results)
}
func parseBool(bytes []byte) (ret bool, err error) {
	if len(bytes) != 1 {
		err = asn1.SyntaxError{Msg: "invalid boolean"}
		return
	}

	// DER demands that "If the encoding represents the boolean value TRUE,
	// its single contents octet shall have all eight bits set to one."
	// Thus only 0 and 255 are valid encoded values.
	switch bytes[0] {
	case 0:
		ret = false
	case 0xff:
		ret = true
	default:
		err = asn1.SyntaxError{Msg: "invalid boolean"}
	}

	return
}
func checkInteger(bytes []byte) error {
	if len(bytes) == 0 {
		return asn1.StructuralError{Msg: "empty integer"}
	}
	if len(bytes) == 1 {
		return nil
	}
	if (bytes[0] == 0 && bytes[1]&0x80 == 0) || (bytes[0] == 0xff && bytes[1]&0x80 == 0x80) {
		return asn1.StructuralError{Msg: "integer not minimally-encoded"}
	}
	return nil
}
func parseInt64(bytes []byte) (ret int64, err error) {
	err = checkInteger(bytes)
	if err != nil {
		return
	}
	if len(bytes) > 8 {
		// We'll overflow an int64 in this case.
		err = asn1.StructuralError{Msg: "integer too large"}
		return
	}
	for bytesRead := 0; bytesRead < len(bytes); bytesRead++ {
		ret <<= 8
		ret |= int64(bytes[bytesRead])
	}

	// Shift up and down in order to sign extend the result.
	ret <<= 64 - uint8(len(bytes))*8
	ret >>= 64 - uint8(len(bytes))*8
	return
}
func parseInt32(bytes []byte) (int32, error) {
	if err := checkInteger(bytes); err != nil {
		return 0, err
	}
	ret64, err := parseInt64(bytes)
	if err != nil {
		return 0, err
	}
	if ret64 != int64(int32(ret64)) {
		return 0, asn1.StructuralError{Msg: "integer too large"}
	}
	return int32(ret64), nil
}
func parseBigInt(bytes []byte) (*big.Int, error) {
	if err := checkInteger(bytes); err != nil {
		return nil, err
	}
	ret := new(big.Int)
	if len(bytes) > 0 && bytes[0]&0x80 == 0x80 {
		// This is a negative number.
		notBytes := make([]byte, len(bytes))
		for i := range notBytes {
			notBytes[i] = ^bytes[i]
		}
		ret.SetBytes(notBytes)
		ret.Add(ret, bigOne)
		ret.Neg(ret)
		return ret, nil
	}
	ret.SetBytes(bytes)
	return ret, nil
}
func parseBitString(bytes []byte) (ret asn1.BitString, err error) {
	if len(bytes) == 0 {
		err = asn1.SyntaxError{Msg: "zero length BIT STRING"}
		return
	}
	paddingBits := int(bytes[0])
	if paddingBits > 7 ||
		len(bytes) == 1 && paddingBits > 0 ||
		bytes[len(bytes)-1]&((1<<bytes[0])-1) != 0 {
		err = asn1.SyntaxError{Msg: "invalid padding bits in BIT STRING"}
		return
	}
	ret.BitLength = (len(bytes)-1)*8 - paddingBits
	ret.Bytes = bytes[1:]
	return
}
func parseObjectIdentifier(bytes []byte) (s []int, err error) {
	if len(bytes) == 0 {
		err = asn1.SyntaxError{Msg: "zero length OBJECT IDENTIFIER"}
		return
	}

	// In the worst case, we get two elements from the first byte (which is
	// encoded differently) and then every varint is a single byte long.
	s = make([]int, len(bytes)+1)

	// The first varint is 40*value1 + value2:
	// According to this packing, value1 can take the values 0, 1 and 2 only.
	// When value1 = 0 or value1 = 1, then value2 is <= 39. When value1 = 2,
	// then there are no restrictions on value2.
	v, offset, err := _parseBase128Int(bytes, 0)
	if err != nil {
		return
	}
	if v < 80 {
		s[0] = v / 40
		s[1] = v % 40
	} else {
		s[0] = 2
		s[1] = v - 80
	}

	i := 2
	for ; offset < len(bytes); i++ {
		v, offset, err = _parseBase128Int(bytes, offset)
		if err != nil {
			return
		}
		s[i] = v
	}
	s = s[0:i]
	return
}
func parseBase128Int(bytes []byte, initOffset int) (ret, offset int, err error) {
	ret, offset, err = _parseBase128Int(bytes, initOffset)

	if offset-initOffset >= 4 {
		err = asn1.StructuralError{Msg: "base 128 integer too large"}
		return
	}

	return
}
func parseGeneralizedTime(bytes []byte) (ret time.Time, err error) {
	const formatStr = "20060102150405Z0700"
	s := string(bytes)

	if ret, err = time.Parse(formatStr, s); err != nil {
		return
	}

	if serialized := ret.Format(formatStr); serialized != s {
		err = fmt.Errorf("asn1: time did not serialize back to the original value and may be invalid: given %q, but serialized as %q", s, serialized)
	}

	return
}
func parsePrintableString(bytes []byte) (ret string, err error) {
	for _, b := range bytes {
		if !isPrintable(b) {
			err = asn1.SyntaxError{Msg: "PrintableString contains invalid character"}
			return
		}
	}
	ret = string(bytes)
	return
}
func isPrintable(b byte) bool {
	return 'a' <= b && b <= 'z' ||
		'A' <= b && b <= 'Z' ||
		'0' <= b && b <= '9' ||
		'\'' <= b && b <= ')' ||
		'+' <= b && b <= '/' ||
		b == ' ' ||
		b == ':' ||
		b == '=' ||
		b == '?' ||
		// This is technically not allowed in a PrintableString.
		// However, x509 certificates with wildcard strings don't
		// always use the correct string type so we permit it.
		b == '*'
}
func parseSequenceOf(bytes []byte, sliceType reflect.Type, elemType reflect.Type) (ret reflect.Value, err error) {
	expectedTag, compoundType, ok := getUniversalType(elemType)
	if !ok {
		err = asn1.StructuralError{Msg: "unknown Go type for slice"}
		return
	}

	// First we iterate over the input and count the number of elements,
	// checking that the types are correct in each case.
	numElements := 0
	for offset := 0; offset < len(bytes); {
		var t tagAndLength
		t, offset, err = parseTagAndLength(bytes, offset)
		if err != nil {
			return
		}
		switch t.tag {
		case tagIA5String, tagGeneralString, tagT61String, tagUTF8String:
			// We pretend that various other string types are
			// PRINTABLE STRINGs so that a sequence of them can be
			// parsed into a []string.
			t.tag = tagPrintableString
		case tagGeneralizedTime, tagUTCTime:
			// Likewise, both time types are treated the same.
			t.tag = tagUTCTime
		}

		if t.class != classUniversal || t.isCompound != compoundType || t.tag != expectedTag {
			err = asn1.StructuralError{Msg: "sequence tag mismatch"}
			return
		}
		if invalidLength(offset, t.length, len(bytes)) {
			err = asn1.SyntaxError{Msg: "truncated sequence"}
			return
		}
		offset += t.length
		numElements++
	}
	ret = reflect.MakeSlice(sliceType, numElements, numElements)
	params := fieldParameters{}
	offset := 0
	for i := 0; i < numElements; i++ {
		offset, err = parseField(ret.Index(i), bytes, offset, params)
		if err != nil {
			return
		}
	}
	return
}
func invalidLength(offset, length, sliceLength int) bool {
	return offset+length < offset || offset+length > sliceLength
}
func setDefaultValue(v reflect.Value, params fieldParameters) (ok bool) {
	if !params.optional {
		return
	}
	ok = true
	if params.defaultValue == nil {
		return
	}
	if canHaveDefaultValue(v.Kind()) {
		v.SetInt(*params.defaultValue)
	}
	return
}
func UnmarshalWithParams(b []byte, val interface{}, params string) (rest []byte, err error) {
	v := reflect.ValueOf(val).Elem()
	offset, err := parseField(v, b, 0, parseFieldParameters(params))
	if err != nil {
		return nil, err
	}
	return b[offset:], nil
}
func parseFieldParameters(str string) (ret fieldParameters) {
	for _, part := range strings.Split(str, ",") {
		switch {
		case part == "optional":
			ret.optional = true
		case part == "explicit":
			ret.explicit = true
			if ret.tag == nil {
				ret.tag = new(int)
			}
		case part == "generalized":
			ret.timeType = tagGeneralizedTime
		case part == "utc":
			ret.timeType = tagUTCTime
		case part == "ia5":
			ret.stringType = tagIA5String
		case part == "printable":
			ret.stringType = tagPrintableString
		case part == "utf8":
			ret.stringType = tagUTF8String
		case strings.HasPrefix(part, "default:"):
			i, err := strconv.ParseInt(part[8:], 10, 64)
			if err == nil {
				ret.defaultValue = new(int64)
				*ret.defaultValue = i
			}
		case strings.HasPrefix(part, "tag:"):
			i, err := strconv.Atoi(part[4:])
			if err == nil {
				ret.tag = new(int)
				*ret.tag = i
			}
		case part == "set":
			ret.set = true
		case part == "application":
			ret.application = true
			if ret.tag == nil {
				ret.tag = new(int)
			}
		case part == "omitempty":
			ret.omitEmpty = true
		}
	}
	return
}
func getUniversalType(t reflect.Type) (tagNumber int, isCompound, ok bool) {
	switch t {
	case objectIdentifierType:
		return tagOID, false, true
	case bitStringType:
		return tagBitString, false, true
	case timeType:
		return tagUTCTime, false, true
	case enumeratedType:
		return tagEnum, false, true
	case bigIntType:
		return tagInteger, false, true
	}
	switch t.Kind() {
	case reflect.Bool:
		return tagBoolean, false, true
	case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:
		return tagInteger, false, true
	case reflect.Struct:
		return tagSequence, true, true
	case reflect.Slice:
		if t.Elem().Kind() == reflect.Uint8 {
			return tagOctetString, false, true
		}
		if strings.HasSuffix(t.Name(), "SET") {
			return tagSet, true, true
		}
		return tagSequence, true, true
	case reflect.String:
		return tagPrintableString, false, true
	}
	return 0, false, false
}
func DecodeString(raw string) ([]byte, error) {
	pad := 8 - (len(raw) % 8)
	nb := []byte(raw)
	if pad != 8 {
		nb = make([]byte, len(raw)+pad)
		copy(nb, raw)
		for i := 0; i < pad; i++ {
			nb[len(raw)+i] = '='
		}
	}

	return lowerBase32.DecodeString(string(nb))
}
func (m MongoDb) MailNotifier(ctxt string) (models.MailNotifier, error) {
	c := m.Connect(models.COLL_NAME_MAIL_NOTIFIER)
	defer m.Close(c)
	var notifier []models.MailNotifier
	if err := c.Find(nil).All(&notifier); err != nil || len(notifier) == 0 {
		logger.Get().Error("%s-Unable to read MailNotifier from DB: %v", ctxt, err)
		return models.MailNotifier{}, ErrMissingNotifier
	} else {
		return notifier[0], nil
	}
}
func (m MongoDb) SaveMailNotifier(ctxt string, notifier models.MailNotifier) error {
	c := m.Connect(models.COLL_NAME_MAIL_NOTIFIER)
	defer m.Close(c)
	_, err := c.Upsert(bson.M{}, bson.M{"$set": notifier})
	if err != nil {
		logger.Get().Error("%s-Error Updating the mail notifier info for: %s Error: %v", ctxt, notifier.MailId, err)
		return errors.New(fmt.Sprintf("Error Updating the mail notifier info for: %s Error: %v", notifier.MailId, err))
	}
	return nil
}
func RegisterProvider(name string, factory ProvidersFactory) {
	providersMutex.Lock()
	defer providersMutex.Unlock()
	if _, found := providers[name]; found {
		logger.Get().Critical("Auth provider %s was registered twice", name)
	}
	providers[name] = factory
}
func (m MongoDb) InitDb() error {
	if err := m.InitUser(); err != nil {
		logger.Get().Error("Error Initilaizing User Table", err)
		return err
	}
	return nil
}
func Until(f func(), period time.Duration, stopCh <-chan struct{}) {
	for {
		select {
		case <-stopCh:
			return
		default:
		}
		func() {
			defer HandleCrash()
			f()
		}()
		time.Sleep(period)
	}
}
func logPanic(r interface{}) {
	callers := ""
	for i := 0; true; i++ {
		_, file, line, ok := runtime.Caller(i)
		if !ok {
			break
		}
		callers = callers + fmt.Sprintf("%v:%v\n", file, line)
	}
	logger.Get().Error("Recovered from panic: %#v (%v)\n%v", r, r, callers)
}
func (m MongoDb) User(username string) (user models.User, e error) {
	c := m.Connect(models.COLL_NAME_USER)
	defer m.Close(c)
	err := c.Find(bson.M{"username": username}).One(&user)
	if err != nil {
		return user, ErrMissingUser
	}
	return user, nil
}
func (m MongoDb) Users(filter interface{}) (us []models.User, e error) {
	c := m.Connect(models.COLL_NAME_USER)
	defer m.Close(c)

	err := c.Find(filter).All(&us)
	if err != nil {
		logger.Get().Error("Error getting record from DB. error: %v", err)
		return us, mkmgoerror(err.Error())
	}
	return us, nil
}
func (m MongoDb) SaveUser(user models.User) error {
	c := m.Connect(models.COLL_NAME_USER)
	defer m.Close(c)

	_, err := c.Upsert(bson.M{"username": user.Username}, bson.M{"$set": user})
	if err != nil {
		logger.Get().Error("Error deleting record from DB for user: %s. error: %v", user.Username, err)
		return mkmgoerror(err.Error())
	}
	return nil
}
func (m MongoDb) DeleteUser(username string) error {
	c := m.Connect(models.COLL_NAME_USER)
	defer m.Close(c)

	// raises error if "username" doesn't exist
	err := c.Remove(bson.M{"username": username})
	if err != nil {
		logger.Get().Error("Error deleting record from DB for user: %s. error: %v", username, err)
		return mkmgoerror(err.Error())
	}
	return err
}
func LoadCACertFile(cert string) (*x509.CertPool, error) {
	// validate caCert, and setup certpool
	ca, err := ioutil.ReadFile(cert)
	if err != nil {
		return nil, fmt.Errorf("could not load CA Certificate: %s ", err.Error())
	}

	certPool := x509.NewCertPool()
	if err := certPool.AppendCertsFromPEM(ca); !err {
		return nil, errors.New("could not append CA Certificate to CertPool")
	}

	return certPool, nil
}
func NewAuth(opts ...Options) *Auth {
	o := Options{}
	if len(opts) != 0 {
		o = opts[0]
	}

	h := defaultAuthErrorHandler
	if o.AuthErrorHandler != nil {
		h = o.AuthErrorHandler
	}

	return &Auth{
		opt:            o,
		authErrHandler: http.HandlerFunc(h),
	}
}
func (a *Auth) ValidateRequest(r *http.Request) error {
	// ensure we can process this request
	if r.TLS == nil || r.TLS.VerifiedChains == nil {
		return errors.New("no cert chain detected")
	}

	// TODO: Figure out if having multiple validated peer leaf certs is possible. For now, only validate
	// one cert, and make sure it matches the first peer certificate
	if r.TLS.PeerCertificates != nil {
		if !bytes.Equal(r.TLS.PeerCertificates[0].Raw, r.TLS.VerifiedChains[0][0].Raw) {
			return errors.New("first peer certificate not first verified chain leaf")
		}
	}

	return nil
}
func (a *Auth) Process(w http.ResponseWriter, r *http.Request) error {
	if err := a.ValidateRequest(r); err != nil {
		return err
	}

	// Validate OU
	if len(a.opt.AllowedOUs) > 0 {
		err := a.ValidateOU(r.TLS.VerifiedChains[0][0])
		if err != nil {
			a.authErrHandler.ServeHTTP(w, r)
			return err
		}
	}

	// Validate CN
	if len(a.opt.AllowedCNs) > 0 {
		err := a.ValidateCN(r.TLS.VerifiedChains[0][0])
		if err != nil {
			a.authErrHandler.ServeHTTP(w, r)
			return err
		}
	}
	return nil
}
func (a *Auth) ValidateCN(verifiedCert *x509.Certificate) error {
	var failed []string

	for _, cn := range a.opt.AllowedCNs {
		if cn == verifiedCert.Subject.CommonName {
			return nil
		}
		failed = append(failed, verifiedCert.Subject.CommonName)
	}
	return fmt.Errorf("cert failed CN validation for %v, Allowed: %v", failed, a.opt.AllowedCNs)
}
func (a *Auth) ValidateOU(verifiedCert *x509.Certificate) error {
	var failed []string

	for _, ou := range a.opt.AllowedOUs {
		for _, clientOU := range verifiedCert.Subject.OrganizationalUnit {
			if ou == clientOU {
				return nil
			}
			failed = append(failed, clientOU)
		}
	}
	return fmt.Errorf("cert failed OU validation for %v, Allowed: %v", failed, a.opt.AllowedOUs)
}
func KeyLen(x uint64) int {
	n := 1
	if x >= 1<<32 {
		x >>= 32
		n += 4
	}
	if x >= 1<<16 {
		x >>= 16
		n += 2
	}
	if x >= 1<<8 {
		x >>= 8
		n += 1
	}
	return n
}
func DefaultConfig() Config {
	newClientConfig := vaultclient.DefaultConfig()
	newClientConfig.Address = "http://127.0.0.1:8200"
	newVaultClient, err := vaultclient.NewClient(newClientConfig)
	if err != nil {
		panic(err)
	}

	newConfig := Config{
		// Dependencies.
		VaultClient: newVaultClient,
	}

	return newConfig
}
func New(config Config) (spec.CertSigner, error) {
	newCertSigner := &certSigner{
		Config: config,
	}

	// Dependencies.
	if newCertSigner.VaultClient == nil {
		return nil, microerror.Maskf(invalidConfigError, "Vault client must not be empty")
	}

	return newCertSigner, nil
}
func New(config Config) (spec.VaultFactory, error) {
	newVaultFactory := &vaultFactory{
		Config: config,
	}

	// Dependencies.
	if newVaultFactory.Address == "" {
		return nil, microerror.Maskf(invalidConfigError, "Vault address must not be empty")
	}
	if newVaultFactory.AdminToken == "" {
		return nil, microerror.Maskf(invalidConfigError, "Vault admin token must not be empty")
	}

	return newVaultFactory, nil
}
func DefaultServiceConfig() ServiceConfig {
	newClientConfig := vaultclient.DefaultConfig()
	newClientConfig.Address = "http://127.0.0.1:8200"
	newVaultClient, err := vaultclient.NewClient(newClientConfig)
	if err != nil {
		panic(err)
	}

	newConfig := ServiceConfig{
		// Dependencies.
		VaultClient: newVaultClient,
	}

	return newConfig
}
func NewService(config ServiceConfig) (Service, error) {
	// Dependencies.
	if config.VaultClient == nil {
		return nil, microerror.Maskf(invalidConfigError, "Vault client must not be empty")
	}

	newService := &service{
		ServiceConfig: config,
	}

	return newService, nil
}
func (s *service) Delete(clusterID string) error {
	// Create a client for the system backend configured with the Vault token
	// used for the current cluster's PKI backend.
	sysBackend := s.VaultClient.Sys()

	// Unmount the PKI backend, if it exists.
	mounted, err := s.IsMounted(clusterID)
	if err != nil {
		return microerror.Mask(err)
	}
	if mounted {
		err = sysBackend.Unmount(s.MountPKIPath(clusterID))
		if err != nil {
			return microerror.Mask(err)
		}
	}

	return nil
}
func IsNoVaultHandlerDefined(err error) bool {
	cause := microerror.Cause(err)

	if cause != nil && strings.Contains(cause.Error(), "no handler for route") {
		return true
	}

	return false
}
func New(config Config) (Service, error) {
	// Dependencies.
	if config.VaultClient == nil {
		return nil, microerror.Maskf(invalidConfigError, "Vault client must not be empty")
	}

	if config.PKIMountpoint == "" {
		return nil, microerror.Maskf(invalidConfigError, "PKIMountpoint must not be empty")
	}

	service := &service{
		vaultClient:   config.VaultClient,
		pkiMountpoint: config.PKIMountpoint,
	}

	return service, nil
}
func (s *service) Create(params CreateParams) error {
	logicalStore := s.vaultClient.Logical()

	data := map[string]interface{}{
		"allowed_domains":    params.AllowedDomains,
		"allow_subdomains":   params.AllowSubdomains,
		"ttl":                params.TTL,
		"allow_bare_domains": params.AllowBareDomains,
		"organization":       params.Organizations,
	}

	_, err := logicalStore.Write(fmt.Sprintf("%s/roles/%s", s.pkiMountpoint, params.Name), data)
	if err != nil {
		return microerror.Mask(err)
	}
	return nil
}
func CreateJob() Config {
	return Config{
		LockProvider:               nil,
		RuntimeProcessor:           nil,
		ResultProcessor:            nil,
		RuntimeProcessingFrequency: 200 * time.Millisecond,
		SummaryBuffer:              1,
	}
}
func (config *Config) Run() {
	err := config.ensureLock()
	if err != nil {
		panic(err)
	}
	err = config.runWorker()
	if err != nil {
		panic(err)
	}
}
func newWatcher(dir_notify bool, initpaths ...string) (w *Watcher) {
	w = new(Watcher)
	w.auto_watch = dir_notify
	w.paths = make(map[string]*watchItem, 0)

	var paths []string
	for _, path := range initpaths {
		matches, err := filepath.Glob(path)
		if err != nil {
			continue
		}
		paths = append(paths, matches...)
	}
	if dir_notify {
		w.syncAddPaths(paths...)
	} else {
		for _, path := range paths {
			w.paths[path] = watchPath(path)
		}
	}
	return
}
func (w *Watcher) Start() <-chan *Notification {
	if w.notify_chan != nil {
		return w.notify_chan
	}
	if w.auto_watch {
		w.add_chan = make(chan *watchItem, NotificationBufLen)
		go w.watchItemListener()
	}
	w.notify_chan = make(chan *Notification, NotificationBufLen)
	go w.watch(w.notify_chan)
	return w.notify_chan
}
func (w *Watcher) Stop() {
	if w.notify_chan != nil {
		close(w.notify_chan)
	}

	if w.add_chan != nil {
		close(w.add_chan)
	}
}
func (w *Watcher) Active() bool {
	return w.paths != nil && len(w.paths) > 0
}
func (w *Watcher) Add(inpaths ...string) {
	var paths []string
	for _, path := range inpaths {
		matches, err := filepath.Glob(path)
		if err != nil {
			continue
		}
		paths = append(paths, matches...)
	}
	if w.auto_watch && w.notify_chan != nil {
		for _, path := range paths {
			wi := watchPath(path)
			w.addPaths(wi)
		}
	} else if w.auto_watch {
		w.syncAddPaths(paths...)
	} else {
		for _, path := range paths {
			w.paths[path] = watchPath(path)
		}
	}
}
func (w *Watcher) watch(sndch chan<- *Notification) {
	defer func() {
		recover()
	}()
	for {
		<-time.After(WatchDelay)
		for _, wi := range w.paths {
			if wi.Update() && w.shouldNotify(wi) {
				sndch <- wi.Notification()
			}

			if wi.LastEvent == NOEXIST && w.auto_watch {
				delete(w.paths, wi.Path)
			}

			if len(w.paths) == 0 {
				w.Stop()
			}
		}
	}
}
func (w *Watcher) Watching() (paths []string) {
	paths = make([]string, 0)
	for path, _ := range w.paths {
		paths = append(paths, path)
	}
	return
}
func (w *Watcher) State() (state []Notification) {
	state = make([]Notification, 0)
	if w.paths == nil {
		return
	}
	for _, wi := range w.paths {
		state = append(state, *wi.Notification())
	}
	return
}
func Store(r *http.Request, err error) {
	errptr, ok := r.Context().Value(errorKey).(*error)
	if !ok {
		panic("hatpear: request not configured to store errors")
	}
	// check err after checking context to fail fast if unconfigured
	if err != nil {
		*errptr = err
	}
}
func Get(r *http.Request) error {
	errptr, ok := r.Context().Value(errorKey).(*error)
	if !ok {
		return nil
	}
	return *errptr
}
func Catch(h func(w http.ResponseWriter, r *http.Request, err error)) Middleware {
	return func(next http.Handler) http.Handler {
		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
			var err error
			ctx := context.WithValue(r.Context(), errorKey, &err)

			next.ServeHTTP(w, r.WithContext(ctx))
			if err != nil {
				h(w, r, err)
			}
		})
	}
}
func Try(h Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		err := h.ServeHTTP(w, r)
		Store(r, err)
	})
}
func Recover() Middleware {
	return func(next http.Handler) http.Handler {
		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
			defer func() {
				if v := recover(); v != nil {
					Store(r, PanicError{
						value: v,
						stack: stack(1),
					})
				}
			}()
			next.ServeHTTP(w, r)
		})
	}
}
func main() {
	req, _ := http.NewRequest("GET", "http://localhost:7070/sync", nil)
	req.Header.Set("Accept", "text/event-stream")
	resp, err := http.DefaultClient.Do(req)
	if err != nil {
		log.Fatalf("request: %s", err)
	}
	r := resp.Body

	i := 0
	buff := make([]byte, 32*1024)
	for {
		n, err := r.Read(buff)
		if err != nil {
			break
		}
		i++
		log.Printf("#%d: %s", i, sizestr.ToString(int64(n)))
	}

	r.Close()
	log.Printf("closed")
}
func SyncHandler(gostruct interface{}) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if conn, err := Sync(gostruct, w, r); err != nil {
			log.Printf("[velox] sync handler error: %s", err)
		} else {
			conn.Wait()
		}
	})
}
func (c *conn) connect(w http.ResponseWriter, r *http.Request) error {
	//choose transport
	if r.Header.Get("Accept") == "text/event-stream" {
		c.transport = &eventSourceTransport{writeTimeout: c.state.WriteTimeout}
	} else if r.Header.Get("Upgrade") == "websocket" {
		c.transport = &websocketsTransport{writeTimeout: c.state.WriteTimeout}
	} else {
		return fmt.Errorf("Invalid sync request")
	}
	//non-blocking connect to client over set transport
	if err := c.transport.connect(w, r); err != nil {
		return err
	}
	//initial ping
	if err := c.send(&update{Ping: true}); err != nil {
		return fmt.Errorf("Failed to send initial event")
	}
	//successfully connected
	c.connected = true
	c.waiter.Add(1)
	//while connected, ping loop (every 25s, browser timesout after 30s)
	go func() {
		for {
			select {
			case <-time.After(c.state.PingInterval):
				if err := c.send(&update{Ping: true}); err != nil {
					goto disconnected
				}
			case <-c.connectedCh:
				goto disconnected
			}
		}
	disconnected:
		c.connected = false
		c.Close()
		//unblock waiters
		c.waiter.Done()
	}()
	//non-blocking wait on connection
	go func() {
		if err := c.transport.wait(); err != nil {
			//log error?
		}
		close(c.connectedCh)
	}()
	//now connected, consumer can connection.Wait()
	return nil
}
func (c *conn) send(upd *update) error {
	c.sendingMut.Lock()
	defer c.sendingMut.Unlock()
	//send (transports responsiblity to enforce timeouts)
	return c.transport.send(upd)
}
func (s *State) NumConnections() int {
	s.connMut.Lock()
	n := len(s.conns)
	s.connMut.Unlock()
	return n
}
func (s *State) Push() bool {
	//attempt to mark state as 'pushing'
	if atomic.CompareAndSwapUint32(&s.push.ing, 0, 1) {
		go s.gopush()
		return true
	}
	//if already pushing, mark queued
	atomic.StoreUint32(&s.push.queued, 1)
	return false
}
func (s *State) gopush() {
	s.push.mut.Lock()
	t0 := time.Now()
	//queue cleanup
	defer func() {
		//measure time passed, ensure we wait at least Throttle time
		tdelta := time.Now().Sub(t0)
		if t := s.Throttle - tdelta; t > 0 {
			time.Sleep(t)
		}
		//push complete
		s.push.mut.Unlock()
		atomic.StoreUint32(&s.push.ing, 0)
		//if queued, auto-push again
		if atomic.CompareAndSwapUint32(&s.push.queued, 1, 0) {
			s.Push()
		}
	}()
	//calculate new json state
	l, hasLock := s.gostruct.(sync.Locker)
	if hasLock {
		l.Lock()
	}
	newBytes, err := json.Marshal(s.gostruct)
	if hasLock {
		l.Unlock()
	}
	if err != nil {
		log.Printf("velox: marshal failed: %s", err)
		return
	}
	//if changed, then calculate change set
	if !bytes.Equal(s.data.bytes, newBytes) {
		//calculate change set from last version
		ops, _ := jsonpatch.CreatePatch(s.data.bytes, newBytes)
		if len(s.data.bytes) > 0 && len(ops) > 0 {
			//changes! bump version
			s.data.mut.Lock()
			s.data.delta, _ = json.Marshal(ops)
			s.data.bytes = newBytes
			s.data.version++
			s.data.mut.Unlock()
		}
	}
	//send this new change to each subscriber
	s.connMut.Lock()
	for _, c := range s.conns {
		if c.version != s.data.version {
			go c.push()
		}
	}
	s.connMut.Unlock()
	//defered cleanup()
}
func NewOutForward(configServers []*ConfigServer) (*OutForward, error) {
	loggers := make([]*fluent.Fluent, len(configServers))
	for i, server := range configServers {
		logger, err := fluent.New(fluent.Config{Server: server.Address()})
		if err != nil {
			log.Println("[warning]", err)
		} else {
			log.Println("[info] Server", server.Address(), "connected")
		}
		loggers[i] = logger
		logger.Send([]byte{})
	}
	return &OutForward{
		loggers: loggers,
		sent:    0,
	}, nil
}
func (t *InTail) Run(c *Context) {
	c.InputProcess.Add(1)
	defer c.InputProcess.Done()

	t.messageCh = c.MessageCh
	t.monitorCh = c.MonitorCh

	c.StartProcess.Done()

	if t.eventCh == nil {
		err := t.TailStdin(c)
		if err != nil {
			if _, ok := err.(Signal); ok {
				log.Println("[info]", err)
			} else {
				log.Println("[error]", err)
			}
			return
		}
	}

	log.Println("[info] Trying trail file", t.filename)
	f, err := t.newTrailFile(SEEK_TAIL, c)
	if err != nil {
		if _, ok := err.(Signal); ok {
			log.Println("[info]", err)
		} else {
			log.Println("[error]", err)
		}
		return
	}
	for {
		for {
			err := t.watchFileEvent(f, c)
			if err != nil {
				if _, ok := err.(Signal); ok {
					log.Println("[info]", err)
					return
				} else {
					log.Println("[warning]", err)
					break
				}
			}
		}
		// re open file
		var err error
		f, err = t.newTrailFile(SEEK_HEAD, c)
		if err != nil {
			if _, ok := err.(Signal); ok {
				log.Println("[info]", err)
			} else {
				log.Println("[error]", err)
			}
			return
		}
	}
}
func New(config Config) (f *Fluent, err error) {
	if config.Server == "" {
		config.Server = defaultServer
	}
	if config.Timeout == 0 {
		config.Timeout = defaultTimeout
	}
	if config.RetryWait == 0 {
		config.RetryWait = defaultRetryWait
	}
	if config.MaxRetry == 0 {
		config.MaxRetry = defaultMaxRetry
	}
	f = &Fluent{
		Config:          config,
		reconnecting:    false,
		cancelReconnect: make(chan bool),
	}
	err = f.connect()
	return
}
func (f *Fluent) Close() (err error) {
	if f.conn != nil {
		f.mu.Lock()
		defer f.mu.Unlock()
	} else {
		return
	}
	if f.conn != nil {
		f.conn.Close()
		f.conn = nil
	}
	return
}
func (f *Fluent) IsReconnecting() bool {
	f.mu.Lock()
	defer f.mu.Unlock()
	return f.reconnecting
}
func (f *Fluent) connect() (err error) {
	host, port, err := net.SplitHostPort(f.Server)
	if err != nil {
		return err
	}
	addrs, err := net.LookupHost(host)
	if err != nil || len(addrs) == 0 {
		return err
	}
	// for DNS round robin
	n := Rand.Intn(len(addrs))
	addr := addrs[n]
	var format string
	if strings.Contains(addr, ":") {
		// v6
		format = "[%s]:%s"
	} else {
		// v4
		format = "%s:%s"
	}
	resolved := fmt.Sprintf(format, addr, port)
	log.Printf("[info] Connect to %s (%s)", f.Server, resolved)
	f.conn, err = net.DialTimeout("tcp", resolved, f.Config.Timeout)
	f.recordError(err)
	return
}
func Notification(title, message string) GNotifier {
	config := &Config{title, message, 5000, ""}
	n := &notifier{Config: config}
	return n
}
func NullNotification(title, message string) GNotifier {
	config := &Config{title, message, 5000, ""}
	n := &nullNotifier{Config: config}
	return n
}
func New(opts ...Option) *Identity {
	c := &configuration{}

	for _, opt := range opts {
		option(opt)(c)
	}

	return c.generate()
}
func (id *Identity) Issue(opts ...Option) *Identity {
	opts = append(opts, Issuer(id))
	return New(opts...)
}
func Subject(value pkix.Name) Option {
	return func(c *configuration) {
		c.subject = &value
	}
}
func PrivateKey(value crypto.Signer) Option {
	return func(c *configuration) {
		c.priv = &value
	}
}
func NotBefore(value time.Time) Option {
	return func(c *configuration) {
		c.notBefore = &value
	}
}
func NotAfter(value time.Time) Option {
	return func(c *configuration) {
		c.notAfter = &value
	}
}
func IssuingCertificateURL(value ...string) Option {
	return func(c *configuration) {
		c.issuingCertificateURL = append(c.issuingCertificateURL, value...)
	}
}
func OCSPServer(value ...string) Option {
	return func(c *configuration) {
		c.ocspServer = append(c.ocspServer, value...)
	}
}
func New(apiKey string) (*TelegramBotAPI, error) {
	toReturn := TelegramBotAPI{
		Updates:  make(chan BotUpdate),
		baseURIs: createEndpoints(fmt.Sprintf(apiBaseURI, apiKey)),
		closed:   make(chan struct{}),
		c:        newClient(fmt.Sprintf(apiBaseURI, apiKey)),
		updateC:  newClient(fmt.Sprintf(apiBaseURI, apiKey)),
	}
	user, err := toReturn.GetMe()
	if err != nil {
		return nil, err
	}
	toReturn.ID = user.User.ID
	toReturn.Name = user.User.FirstName
	toReturn.Username = *user.User.Username

	err = toReturn.removeWebhook()
	if err != nil {
		return nil, err
	}

	toReturn.wg.Add(1)
	go toReturn.updateLoop()

	return &toReturn, nil
}
func NewWithWebhook(apiKey, webhookURL, certificate string) (*TelegramBotAPI, http.HandlerFunc, error) {
	toReturn := TelegramBotAPI{
		Updates:  make(chan BotUpdate),
		baseURIs: createEndpoints(fmt.Sprintf(apiBaseURI, apiKey)),
		closed:   make(chan struct{}),
		c:        newClient(fmt.Sprintf(apiBaseURI, apiKey)),
		updateC:  newClient(fmt.Sprintf(apiBaseURI, apiKey)),
	}
	user, err := toReturn.GetMe()
	if err != nil {
		return nil, nil, err
	}
	toReturn.ID = user.User.ID
	toReturn.Name = user.User.FirstName
	toReturn.Username = *user.User.Username

	file, err := os.Open(certificate)
	if err != nil {
		return nil, nil, err
	}

	err = toReturn.setWebhook(webhookURL, certificate, file)
	if err != nil {
		return nil, nil, err
	}

	updateFunc := func(w http.ResponseWriter, r *http.Request) {
		bytes, err := ioutil.ReadAll(r.Body)
		if err != nil {
			toReturn.Updates <- BotUpdate{err: err}
			return
		}

		update := &Update{}
		err = json.Unmarshal(bytes, update)
		if err != nil {
			toReturn.Updates <- BotUpdate{err: err}
			return
		}

		toReturn.Updates <- BotUpdate{update: *update}
	}

	return &toReturn, updateFunc, nil
}
func (api *TelegramBotAPI) Close() {
	select {
	case <-api.closed:
		return
	default:
	}
	close(api.closed)
	api.wg.Wait()
}
func (api *TelegramBotAPI) GetMe() (*UserResponse, error) {
	resp := &UserResponse{}
	_, err := api.c.get(getMe, resp)

	if err != nil {
		return nil, err
	}
	err = check(&resp.baseResponse)
	if err != nil {
		return nil, err
	}
	return resp, nil
}
func RunBot(apiKey string, bot BotFunc, name, description string) {
	closing := make(chan struct{})

	fmt.Printf("%s: %s\n", name, description)
	fmt.Println("Starting...")

	api, err := tbotapi.New(apiKey)
	if err != nil {
		log.Fatal(err)
	}

	// Just to show its working.
	fmt.Printf("User ID: %d\n", api.ID)
	fmt.Printf("Bot Name: %s\n", api.Name)
	fmt.Printf("Bot Username: %s\n", api.Username)

	closed := make(chan struct{})
	wg := &sync.WaitGroup{}

	wg.Add(1)
	go func() {
		defer wg.Done()
		for {
			select {
			case <-closed:
				return
			case update := <-api.Updates:
				if update.Error() != nil {
					// TODO handle this properly
					fmt.Printf("Update error: %s\n", update.Error())
					continue
				}

				bot(update.Update(), api)
			}
		}
	}()

	// Ensure a clean shutdown.
	shutdown := make(chan os.Signal)
	signal.Notify(shutdown, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-shutdown
		close(closing)
	}()

	fmt.Println("Bot started. Press CTRL-C to close...")

	// Wait for the signal.
	<-closing
	fmt.Println("Closing...")

	// Always close the API first, let it clean up the update loop.
	// This might take a while.
	api.Close()
	close(closed)
	wg.Wait()
}
func RunBotOnWebhook(apiKey string, bot BotFunc, name, description, webhookHost string, webhookPort uint16, pubkey, privkey string) {
	closing := make(chan struct{})

	fmt.Printf("%s: %s\n", name, description)
	fmt.Println("Starting...")
	u := url.URL{
		Host:   webhookHost + ":" + fmt.Sprint(webhookPort),
		Scheme: "https",
		Path:   apiKey,
	}

	api, handler, err := tbotapi.NewWithWebhook(apiKey, u.String(), pubkey)
	if err != nil {
		log.Fatal(err)
	}

	// Just to show its working.
	fmt.Printf("User ID: %d\n", api.ID)
	fmt.Printf("Bot Name: %s\n", api.Name)
	fmt.Printf("Bot Username: %s\n", api.Username)

	closed := make(chan struct{})
	wg := &sync.WaitGroup{}

	wg.Add(1)
	go func() {
		defer wg.Done()
		for {
			select {
			case <-closed:
				return
			case update := <-api.Updates:
				if update.Error() != nil {
					// TODO handle this properly
					fmt.Printf("Update error: %s\n", update.Error())
					continue
				}

				bot(update.Update(), api)
			}
		}
	}()

	http.HandleFunc("/"+apiKey, handler)

	fmt.Println("Starting webhook...")
	go func() {
		log.Fatal(http.ListenAndServeTLS("0.0.0.0:"+fmt.Sprint(webhookPort), pubkey, privkey, nil))
	}()

	// Ensure a clean shutdown.
	shutdown := make(chan os.Signal)
	signal.Notify(shutdown, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-shutdown
		close(closing)
	}()

	fmt.Println("Bot started. Press CTRL-C to close...")

	// Wait for the signal.
	<-closing
	fmt.Println("Closing...")

	// Always close the API first.
	api.Close()
	close(closed)
	wg.Wait()
}
func (api *TelegramBotAPI) NewOutgoingMessage(recipient Recipient, text string) *OutgoingMessage {
	return &OutgoingMessage{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		Text:      text,
		ParseMode: ModeDefault,
	}
}
func (api *TelegramBotAPI) NewOutgoingLocation(recipient Recipient, latitude, longitude float32) *OutgoingLocation {
	return &OutgoingLocation{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		Latitude:  latitude,
		Longitude: longitude,
	}
}
func (api *TelegramBotAPI) NewOutgoingVenue(recipient Recipient, latitude, longitude float32, title, address string) *OutgoingVenue {
	return &OutgoingVenue{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		Latitude:  latitude,
		Longitude: longitude,
		Title:     title,
		Address:   address,
	}
}
func (api *TelegramBotAPI) NewOutgoingVideo(recipient Recipient, fileName string, reader io.Reader) *OutgoingVideo {
	return &OutgoingVideo{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileName: fileName,
			r:        reader,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingVideoResend(recipient Recipient, fileID string) *OutgoingVideo {
	return &OutgoingVideo{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileID: fileID,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingPhoto(recipient Recipient, fileName string, reader io.Reader) *OutgoingPhoto {
	return &OutgoingPhoto{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileName: fileName,
			r:        reader,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingPhotoResend(recipient Recipient, fileID string) *OutgoingPhoto {
	return &OutgoingPhoto{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileID: fileID,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingSticker(recipient Recipient, fileName string, reader io.Reader) *OutgoingSticker {
	return &OutgoingSticker{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileName: fileName,
			r:        reader,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingStickerResend(recipient Recipient, fileID string) *OutgoingSticker {
	return &OutgoingSticker{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileID: fileID,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingVoice(recipient Recipient, fileName string, reader io.Reader) *OutgoingVoice {
	return &OutgoingVoice{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileName: fileName,
			r:        reader,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingVoiceResend(recipient Recipient, fileID string) *OutgoingVoice {
	return &OutgoingVoice{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileID: fileID,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingAudio(recipient Recipient, fileName string, reader io.Reader) *OutgoingAudio {
	return &OutgoingAudio{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileName: fileName,
			r:        reader,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingAudioResend(recipient Recipient, fileID string) *OutgoingAudio {
	return &OutgoingAudio{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileID: fileID,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingDocument(recipient Recipient, fileName string, reader io.Reader) *OutgoingDocument {
	return &OutgoingDocument{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileName: fileName,
			r:        reader,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingDocumentResend(recipient Recipient, fileID string) *OutgoingDocument {
	return &OutgoingDocument{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		outgoingFileBase: outgoingFileBase{
			fileID: fileID,
		},
	}
}
func (api *TelegramBotAPI) NewOutgoingForward(recipient Recipient, origin Chat, messageID int) *OutgoingForward {
	return &OutgoingForward{
		outgoingMessageBase: outgoingMessageBase{
			outgoingBase: outgoingBase{
				api:       api,
				Recipient: recipient,
			},
		},
		FromChatID: NewRecipientFromChat(origin),
		MessageID:  messageID,
	}
}
func (api *TelegramBotAPI) NewOutgoingChatAction(recipient Recipient, action ChatAction) *OutgoingChatAction {
	return &OutgoingChatAction{
		outgoingBase: outgoingBase{
			api:       api,
			Recipient: recipient,
		},
		Action: action,
	}
}
func (api *TelegramBotAPI) NewOutgoingUserProfilePhotosRequest(userID int) *OutgoingUserProfilePhotosRequest {
	return &OutgoingUserProfilePhotosRequest{
		api:    api,
		UserID: userID,
	}
}
func (api *TelegramBotAPI) NewOutgoingKickChatMember(chat Recipient, userID int) *OutgoingKickChatMember {
	return &OutgoingKickChatMember{
		api:       api,
		Recipient: chat,
		UserID:    userID,
	}
}
func (api *TelegramBotAPI) NewOutgoingUnbanChatMember(chat Recipient, userID int) *OutgoingUnbanChatMember {
	return &OutgoingUnbanChatMember{
		api:       api,
		Recipient: chat,
		UserID:    userID,
	}
}
func (api *TelegramBotAPI) NewOutgoingCallbackQueryResponse(queryID string) *OutgoingCallbackQueryResponse {
	return &OutgoingCallbackQueryResponse{
		api:             api,
		CallbackQueryID: queryID,
	}
}
func (api *TelegramBotAPI) NewInlineQueryAnswer(queryID string, results []InlineQueryResult) *InlineQueryAnswer {
	return &InlineQueryAnswer{
		api:     api,
		QueryID: queryID,
		Results: results,
	}
}
func (m *Message) Type() MessageType {
	if m.Text != nil {
		return TextMessage
	} else if m.Audio != nil {
		return AudioMessage
	} else if m.Document != nil {
		return DocumentMessage
	} else if m.Photo != nil {
		return PhotoMessage
	} else if m.Sticker != nil {
		return StickerMessage
	} else if m.Video != nil {
		return VideoMessage
	} else if m.Voice != nil {
		return VoiceMessage
	} else if m.Contact != nil {
		return ContactMessage
	} else if m.Location != nil {
		return LocationMessage
	} else if m.NewChatMember != nil {
		return NewChatMember
	} else if m.LeftChatMember != nil {
		return LeftChatMember
	} else if m.NewChatTitle != nil {
		return NewChatTitle
	} else if m.NewChatPhoto != nil {
		return NewChatPhoto
	} else if m.DeleteChatPhoto {
		return DeletedChatPhoto
	} else if m.GroupChatCreated {
		return GroupChatCreated
	} else if m.SupergroupChatCreated {
		return SupergroupChatCreated
	} else if m.ChannelChatCreated {
		return ChannelChatCreated
	} else if m.MigrateToChatID != nil {
		return MigrationToSupergroup
	} else if m.MigrateFromChatID != nil {
		return MigrationFromGroup
	} else if m.Venue != nil {
		return VenueMessage
	} else if m.PinnedMessage != nil {
		return PinnedMessage
	}

	return UnknownMessage
}
func (u *Update) Type() UpdateType {
	if u.Message != nil {
		return MessageUpdate
	} else if u.InlineQuery != nil {
		return InlineQueryUpdate
	} else if u.ChosenInlineResult != nil {
		return ChosenInlineResultUpdate
	}
	return UnknownUpdate
}
func (r Recipient) MarshalJSON() ([]byte, error) {
	toReturn := ""

	if r.isChannel() {
		toReturn = fmt.Sprintf("\"%s\"", *r.ChannelID)
	} else {
		toReturn = fmt.Sprintf("%d", *r.ChatID)
	}

	return []byte(toReturn), nil
}
func (ow *outgoingSetWebhook) querystring() querystring {
	toReturn := make(map[string]string)

	if ow.URL != "" {
		toReturn["url"] = ow.URL
	}

	return querystring(toReturn)
}
func (op *outgoingBase) getBaseQueryString() querystring {
	toReturn := map[string]string{}
	if op.Recipient.isChannel() {
		//Channel
		toReturn["chat_id"] = fmt.Sprint(*op.Recipient.ChannelID)
	} else {
		toReturn["chat_id"] = fmt.Sprint(*op.Recipient.ChatID)
	}

	return querystring(toReturn)
}
func (op *outgoingMessageBase) getBaseQueryString() querystring {
	toReturn := map[string]string{}
	if op.Recipient.isChannel() {
		//Channel.
		toReturn["chat_id"] = fmt.Sprint(*op.Recipient.ChannelID)
	} else {
		toReturn["chat_id"] = fmt.Sprint(*op.Recipient.ChatID)
	}

	if op.replyToMessageIDSet {
		toReturn["reply_to_message_id"] = fmt.Sprint(op.ReplyToMessageID)
	}

	if op.replyMarkupSet {
		b, err := json.Marshal(op.ReplyMarkup)
		if err != nil {
			panic(err)
		}
		toReturn["reply_markup"] = string(b)
	}

	if op.DisableNotification {
		toReturn["disable_notification"] = fmt.Sprint(op.DisableNotification)
	}

	return querystring(toReturn)
}
func (oa *OutgoingAudio) querystring() querystring {
	toReturn := map[string]string(oa.getBaseQueryString())

	if oa.Duration != 0 {
		toReturn["duration"] = fmt.Sprint(oa.Duration)
	}

	if oa.Performer != "" {
		toReturn["performer"] = oa.Performer
	}

	if oa.Title != "" {
		toReturn["title"] = oa.Title
	}

	return querystring(toReturn)
}
func (op *OutgoingPhoto) querystring() querystring {
	toReturn := map[string]string(op.getBaseQueryString())

	if op.Caption != "" {
		toReturn["caption"] = op.Caption
	}

	return querystring(toReturn)
}
func (op *OutgoingUserProfilePhotosRequest) querystring() querystring {
	toReturn := map[string]string{}
	toReturn["user_id"] = fmt.Sprint(op.UserID)

	if op.Offset != 0 {
		toReturn["offset"] = fmt.Sprint(op.Offset)
	}

	if op.Limit != 0 {
		toReturn["limit"] = fmt.Sprint(op.Limit)
	}

	return querystring(toReturn)
}
func (ov *OutgoingVideo) querystring() querystring {
	toReturn := map[string]string(ov.getBaseQueryString())

	if ov.Caption != "" {
		toReturn["caption"] = ov.Caption
	}

	if ov.Duration != 0 {
		toReturn["duration"] = fmt.Sprint(ov.Duration)
	}

	return querystring(toReturn)
}
func (ov *OutgoingVoice) querystring() querystring {
	toReturn := map[string]string(ov.getBaseQueryString())

	if ov.Duration != 0 {
		toReturn["duration"] = fmt.Sprint(ov.Duration)
	}

	return querystring(toReturn)
}
func NewInlineQueryResultArticle(id, title, text string) *InlineQueryResultArticle {
	return &InlineQueryResultArticle{
		InlineQueryResultBase: InlineQueryResultBase{
			Type: ArticleResult,
			ID:   id,
		},
		Title: title,
		Text:  text,
	}
}
func NewInlineQueryResultPhoto(id, photoURL, thumbURL string) *InlineQueryResultPhoto {
	return &InlineQueryResultPhoto{
		InlineQueryResultBase: InlineQueryResultBase{
			Type: PhotoResult,
			ID:   id,
		},
		PhotoURL: photoURL,
		ThumbURL: thumbURL,
	}
}
func NewInlineQueryResultGif(id, gifURL, thumbURL string) *InlineQueryResultGif {
	return &InlineQueryResultGif{
		InlineQueryResultBase: InlineQueryResultBase{
			Type: GifResult,
			ID:   id,
		},
		GifURL:   gifURL,
		ThumbURL: thumbURL,
	}
}
func NewInlineQueryResultMpeg4Gif(id, mpeg4URL, thumbURL string) *InlineQueryResultMpeg4Gif {
	return &InlineQueryResultMpeg4Gif{
		InlineQueryResultBase: InlineQueryResultBase{
			Type: PhotoResult,
			ID:   id,
		},
		Mpeg4URL: mpeg4URL,
		ThumbURL: thumbURL,
	}
}
func NewInlineQueryResultVideo(id, videoURL, thumbURL, title, text string, mimeType MIMEType) *InlineQueryResultVideo {
	return &InlineQueryResultVideo{
		InlineQueryResultBase: InlineQueryResultBase{
			Type: PhotoResult,
			ID:   id,
		},
		VideoURL: videoURL,
		MIMEType: mimeType,
		ThumbURL: thumbURL,
		Title:    title,
		Text:     text,
	}
}
func (op *OutgoingUserProfilePhotosRequest) Send() (*UserProfilePhotosResponse, error) {
	resp := &UserProfilePhotosResponse{}
	_, err := op.api.c.postJSON(getUserProfilePhotos, resp, op)

	if err != nil {
		return nil, err
	}

	err = check(&resp.baseResponse)
	if err != nil {
		return nil, err
	}
	return resp, nil
}
func (oc *OutgoingChatAction) Send() error {
	resp := &baseResponse{}
	_, err := oc.api.c.postJSON(sendChatAction, resp, oc)

	if err != nil {
		return err
	}

	return check(resp)
}
func (ia *InlineQueryAnswer) Send() error {
	resp := &baseResponse{}
	_, err := ia.api.c.postJSON(answerInlineQuery, resp, ia)

	if err != nil {
		return err
	}

	return check(resp)
}
func (kr *OutgoingKickChatMember) Send() error {
	resp := &baseResponse{}
	_, err := kr.api.c.postJSON(kickChatMember, resp, kr)

	if err != nil {
		return err
	}

	return check(resp)
}
func (ub *OutgoingUnbanChatMember) Send() error {
	resp := &baseResponse{}
	_, err := ub.api.c.postJSON(unbanChatMember, resp, ub)

	if err != nil {
		return err
	}

	return check(resp)
}
func (cbr *OutgoingCallbackQueryResponse) Send() error {
	resp := &baseResponse{}
	_, err := cbr.api.c.postJSON(answerCallbackQuery, resp, cbr)

	if err != nil {
		return err
	}

	return check(resp)
}
func NewAPIClient(cfg *Configuration) *APIClient {
	if cfg.HTTPClient == nil {
		cfg.HTTPClient = http.DefaultClient
	}

	c := &APIClient{}
	c.cfg = cfg
	c.common.client = c

	// API Services
	c.ServiceProviderConfigApi = (*ServiceProviderConfigApiService)(&c.common)
	c.UserApi = (*UserApiService)(&c.common)

	return c
}
func (ts *TimeStamp) BeforeInsert() error {
	n := now()
	ts.CreatedAt = n
	ts.UpdatedAt = n
	return nil
}
func ColumnName(d Dialect, tname, cname string) string {
	if cname != "*" {
		cname = d.Quote(cname)
	}
	if tname == "" {
		return cname
	}
	return fmt.Sprintf("%s.%s", d.Quote(tname), cname)
}
func New(dialect Dialect, dsn string) (*DB, error) {
	db, err := sql.Open(dialect.Name(), dsn)
	if err != nil {
		return nil, err
	}
	return &DB{db: db, dialect: dialect, logger: defaultLogger}, nil
}
func (db *DB) From(arg interface{}) *From {
	t := reflect.Indirect(reflect.ValueOf(arg)).Type()
	if t.Kind() != reflect.Struct {
		panic(fmt.Errorf("From: argument must be struct (or that pointer) type, got %v", t))
	}
	return &From{TableName: db.tableName(t)}
}
func (db *DB) Where(cond interface{}, args ...interface{}) *Condition {
	return newCondition(db).Where(cond, args...)
}
func (db *DB) OrderBy(table interface{}, column interface{}, order ...interface{}) *Condition {
	return newCondition(db).OrderBy(table, column, order...)
}
func (db *DB) Limit(lim int) *Condition {
	return newCondition(db).Limit(lim)
}
func (db *DB) Offset(offset int) *Condition {
	return newCondition(db).Offset(offset)
}
func (db *DB) Join(table interface{}) *JoinCondition {
	return (&JoinCondition{db: db}).Join(table)
}
func (db *DB) Count(column ...interface{}) *Function {
	switch len(column) {
	case 0, 1:
		// do nothing.
	default:
		panic(fmt.Errorf("Count: a number of argument must be 0 or 1, got %v", len(column)))
	}
	return &Function{
		Name: "COUNT",
		Args: column,
	}
}
func (db *DB) Update(obj interface{}) (affected int64, err error) {
	rv, rtype, tableName, err := db.tableValueOf("Update", obj)
	if err != nil {
		return -1, err
	}
	if hook, ok := obj.(BeforeUpdater); ok {
		if err := hook.BeforeUpdate(); err != nil {
			return -1, err
		}
	}
	fieldIndexes := db.collectFieldIndexes(rtype, nil)
	pkIdx := db.findPKIndex(rtype, nil)
	if len(pkIdx) < 1 {
		return -1, fmt.Errorf(`Update: fields of struct doesn't have primary key: "pk" struct tag must be specified for update`)
	}
	sets := make([]string, len(fieldIndexes))
	var args []interface{}
	for i, index := range fieldIndexes {
		col := db.columnFromTag(rtype.FieldByIndex(index))
		sets[i] = fmt.Sprintf("%s = %s", db.dialect.Quote(col), db.dialect.PlaceHolder(i))
		args = append(args, rv.FieldByIndex(index).Interface())
	}
	query := fmt.Sprintf("UPDATE %s SET %s WHERE %s = %s",
		db.dialect.Quote(tableName),
		strings.Join(sets, ", "),
		db.dialect.Quote(db.columnFromTag(rtype.FieldByIndex(pkIdx))),
		db.dialect.PlaceHolder(len(fieldIndexes)))
	args = append(args, rv.FieldByIndex(pkIdx).Interface())
	stmt, err := db.prepare(query, args...)
	if err != nil {
		return -1, err
	}
	defer stmt.Close()
	result, err := stmt.Exec(args...)
	if err != nil {
		return -1, err
	}
	affected, _ = result.RowsAffected()
	if hook, ok := obj.(AfterUpdater); ok {
		if err := hook.AfterUpdate(); err != nil {
			return affected, err
		}
	}
	return affected, nil
}
func (db *DB) Delete(obj interface{}) (affected int64, err error) {
	objs, rtype, tableName, err := db.tableObjs("Delete", obj)
	if err != nil {
		return -1, err
	}
	if len(objs) < 1 {
		return 0, nil
	}
	for _, obj := range objs {
		if hook, ok := obj.(BeforeDeleter); ok {
			if err := hook.BeforeDelete(); err != nil {
				return -1, err
			}
		}
	}
	pkIdx := db.findPKIndex(rtype, nil)
	if len(pkIdx) < 1 {
		return -1, fmt.Errorf(`Delete: fields of struct doesn't have primary key: "pk" struct tag must be specified for delete`)
	}
	var args []interface{}
	for _, obj := range objs {
		rv := reflect.Indirect(reflect.ValueOf(obj))
		args = append(args, rv.FieldByIndex(pkIdx).Interface())
	}
	holders := make([]string, len(args))
	for i := 0; i < len(holders); i++ {
		holders[i] = db.dialect.PlaceHolder(i)
	}
	query := fmt.Sprintf("DELETE FROM %s WHERE %s IN (%s)",
		db.dialect.Quote(tableName),
		db.dialect.Quote(db.columnFromTag(rtype.FieldByIndex(pkIdx))),
		strings.Join(holders, ", "))
	stmt, err := db.prepare(query, args...)
	if err != nil {
		return -1, err
	}
	defer stmt.Close()
	result, err := stmt.Exec(args...)
	if err != nil {
		return -1, err
	}
	affected, _ = result.RowsAffected()
	for _, obj := range objs {
		if hook, ok := obj.(AfterDeleter); ok {
			if err := hook.AfterDelete(); err != nil {
				return affected, err
			}
		}
	}
	return affected, nil
}
func (db *DB) Begin() error {
	tx, err := db.db.Begin()
	if err != nil {
		return err
	}
	db.m.Lock()
	defer db.m.Unlock()
	db.tx = tx
	return nil
}
func (db *DB) Commit() error {
	db.m.Lock()
	defer db.m.Unlock()
	if db.tx == nil {
		return ErrTxDone
	}
	err := db.tx.Commit()
	db.tx = nil
	return err
}
func (db *DB) Quote(s string) string {
	return db.dialect.Quote(s)
}
func (db *DB) SetLogOutput(w io.Writer) {
	if w == nil {
		db.logger = defaultLogger
	} else {
		db.logger = &templateLogger{w: w, t: defaultLoggerTemplate}
	}
}
func (db *DB) selectToSlice(rows *sql.Rows, t reflect.Type) (reflect.Value, error) {
	columns, err := rows.Columns()
	if err != nil {
		return reflect.Value{}, err
	}
	t = t.Elem()
	ptrN := 0
	for ; t.Kind() == reflect.Ptr; ptrN++ {
		t = t.Elem()
	}
	fieldIndexes := make([][]int, len(columns))
	for i, column := range columns {
		index := db.fieldIndexByName(t, column, nil)
		if len(index) < 1 {
			return reflect.Value{}, fmt.Errorf("`%v` field isn't defined in %v or embedded struct", stringutil.ToUpperCamelCase(column), t)
		}
		fieldIndexes[i] = index
	}
	dest := make([]interface{}, len(columns))
	var result []reflect.Value
	for rows.Next() {
		v := reflect.New(t).Elem()
		for i, index := range fieldIndexes {
			field := v.FieldByIndex(index)
			dest[i] = field.Addr().Interface()
		}
		if err := rows.Scan(dest...); err != nil {
			return reflect.Value{}, err
		}
		result = append(result, v)
	}
	if err := rows.Err(); err != nil {
		return reflect.Value{}, err
	}
	for i := 0; i < ptrN; i++ {
		t = reflect.PtrTo(t)
	}
	slice := reflect.MakeSlice(reflect.SliceOf(t), len(result), len(result))
	for i, v := range result {
		for j := 0; j < ptrN; j++ {
			v = v.Addr()
		}
		slice.Index(i).Set(v)
	}
	return slice, nil
}
func (db *DB) selectToValue(rows *sql.Rows, t reflect.Type) (reflect.Value, error) {
	ptrN := 0
	for ; t.Kind() == reflect.Ptr; ptrN++ {
		t = t.Elem()
	}
	dest := reflect.New(t).Elem()
	if rows.Next() {
		if err := rows.Scan(dest.Addr().Interface()); err != nil {
			return reflect.Value{}, err
		}
	}
	for i := 0; i < ptrN; i++ {
		dest = dest.Addr()
	}
	return dest, nil
}
func (db *DB) fieldIndexByName(t reflect.Type, name string, index []int) []int {
	for i := 0; i < t.NumField(); i++ {
		field := t.Field(i)
		if candidate := db.columnFromTag(field); candidate == name {
			return append(index, i)
		}
		if field.Anonymous {
			if idx := db.fieldIndexByName(field.Type, name, append(index, i)); len(idx) > 0 {
				return append(index, idx...)
			}
		}
	}
	return nil
}
func (db *DB) columns(tableName string, columns []interface{}) string {
	if len(columns) == 0 {
		return ColumnName(db.dialect, tableName, "*")
	}
	names := make([]string, len(columns))
	for i, col := range columns {
		switch c := col.(type) {
		case Raw:
			names[i] = fmt.Sprint(*c)
		case string:
			names[i] = ColumnName(db.dialect, tableName, c)
		case *Distinct:
			names[i] = fmt.Sprintf("DISTINCT %s", db.columns(tableName, ToInterfaceSlice(c.columns)))
		default:
			panic(fmt.Errorf("column name must be string, Raw or *Distinct, got %T", c))
		}
	}
	return strings.Join(names, ", ")
}
func (db *DB) tagsFromField(field *reflect.StructField) (options []string) {
	if db.hasSkipTag(field) {
		return nil
	}
	for _, tag := range strings.Split(field.Tag.Get(dbTag), ",") {
		if t := strings.ToLower(strings.TrimSpace(tag)); t != "" {
			options = append(options, t)
		}
	}
	return options
}
func (db *DB) hasSkipTag(field *reflect.StructField) bool {
	if field.Tag.Get(dbTag) == skipTag {
		return true
	}
	return false
}
func (db *DB) hasPKTag(field *reflect.StructField) bool {
	for _, tag := range db.tagsFromField(field) {
		if tag == "pk" {
			return true
		}
	}
	return false
}
func (db *DB) isAutoIncrementable(field *reflect.StructField) bool {
	switch field.Type.Kind() {
	case reflect.Int, reflect.Int16, reflect.Int32, reflect.Int64,
		reflect.Uint, reflect.Uint16, reflect.Uint32, reflect.Uint64:
		return true
	}
	return false
}
func (db *DB) collectFieldIndexes(typ reflect.Type, index []int) (indexes [][]int) {
	for i := 0; i < typ.NumField(); i++ {
		field := typ.Field(i)
		if IsUnexportedField(field) {
			continue
		}
		if !(db.hasSkipTag(&field) || (db.hasPKTag(&field) && db.isAutoIncrementable(&field))) {
			tmp := make([]int, len(index)+1)
			copy(tmp, index)
			tmp[len(tmp)-1] = i
			if field.Anonymous {
				indexes = append(indexes, db.collectFieldIndexes(field.Type, tmp)...)
			} else {
				indexes = append(indexes, tmp)
			}
		}
	}
	return indexes
}
func (db *DB) findPKIndex(typ reflect.Type, index []int) []int {
	for i := 0; i < typ.NumField(); i++ {
		field := typ.Field(i)
		if IsUnexportedField(field) {
			continue
		}
		if field.Anonymous {
			if idx := db.findPKIndex(field.Type, append(index, i)); idx != nil {
				return append(index, idx...)
			}
			continue
		}
		if db.hasPKTag(&field) {
			return append(index, i)
		}
	}
	return nil
}
func (db *DB) sizeFromTag(field *reflect.StructField) (size uint64, err error) {
	if s := field.Tag.Get(dbSizeTag); s != "" {
		size, err = strconv.ParseUint(s, 10, 64)
	}
	return size, err
}
func (db *DB) columnFromTag(field reflect.StructField) string {
	col := field.Tag.Get(dbColumnTag)
	if col == "" {
		return stringutil.ToSnakeCase(field.Name)
	}
	return col
}
func (db *DB) defaultFromTag(field *reflect.StructField) (string, error) {
	def := field.Tag.Get(dbDefaultTag)
	if def == "" {
		return "", nil
	}
	switch field.Type.Kind() {
	case reflect.Bool:
		b, err := strconv.ParseBool(def)
		if err != nil {
			return "", err
		}
		return fmt.Sprintf("DEFAULT %v", db.dialect.FormatBool(b)), nil
	}
	return fmt.Sprintf("DEFAULT %v", def), nil
}
func (c *Condition) Where(cond interface{}, args ...interface{}) *Condition {
	return c.appendQueryByCondOrExpr("Where", 0, Where, cond, args...)
}
func (c *Condition) And(cond interface{}, args ...interface{}) *Condition {
	return c.appendQueryByCondOrExpr("And", 100, And, cond, args...)
}
func (c *Condition) Or(cond interface{}, args ...interface{}) *Condition {
	return c.appendQueryByCondOrExpr("Or", 100, Or, cond, args...)
}
func (c *Condition) Like(arg string) *Condition {
	return c.appendQuery(100, Like, arg)
}
func (c *Condition) Between(from, to interface{}) *Condition {
	return c.appendQuery(100, Between, &between{from, to})
}
func (c *Condition) OrderBy(table, col interface{}, order ...interface{}) *Condition {
	order = append([]interface{}{table, col}, order...)
	orderbys := make([]orderBy, 0, 1)
	for len(order) > 0 {
		o, rest := order[0], order[1:]
		if _, ok := o.(string); ok {
			if len(rest) < 1 {
				panic(fmt.Errorf("OrderBy: few arguments"))
			}
			// OrderBy("column", genmai.DESC)
			orderbys = append(orderbys, c.orderBy(nil, o, rest[0]))
			order = rest[1:]
			continue
		}
		if len(rest) < 2 {
			panic(fmt.Errorf("OrderBy: few arguments"))
		}
		// OrderBy(tbl{}, "column", genmai.DESC)
		orderbys = append(orderbys, c.orderBy(o, rest[0], rest[1]))
		order = rest[2:]
	}
	return c.appendQuery(300, OrderBy, orderbys)
}
func (c *Condition) Limit(lim int) *Condition {
	return c.appendQuery(500, Limit, lim)
}
func (c *Condition) Offset(offset int) *Condition {
	return c.appendQuery(700, Offset, offset)
}
func (l *templateLogger) SetFormat(format string) error {
	l.m.Lock()
	defer l.m.Unlock()
	t, err := template.New("genmai").Parse(format)
	if err != nil {
		return err
	}
	l.t = t
	return nil
}
func (l *templateLogger) Print(start time.Time, query string, args ...interface{}) error {
	if len(args) > 0 {
		values := make([]string, len(args))
		for i, arg := range args {
			values[i] = fmt.Sprintf("%#v", arg)
		}
		query = fmt.Sprintf("%v; [%v]", query, strings.Join(values, ", "))
	} else {
		query = fmt.Sprintf("%s;", query)
	}
	data := map[string]interface{}{
		"time":     start,
		"duration": fmt.Sprintf("%.2fms", now().Sub(start).Seconds()*float64(time.Microsecond)),
		"query":    query,
	}
	var buf bytes.Buffer
	if err := l.t.Execute(&buf, data); err != nil {
		return err
	}
	l.m.Lock()
	defer l.m.Unlock()
	if _, err := fmt.Fprintln(l.w, strings.TrimSuffix(buf.String(), "\n")); err != nil {
		return err
	}
	return nil
}
func (l *nullLogger) Print(start time.Time, query string, args ...interface{}) error {
	return nil
}
func (d *MySQLDialect) Quote(s string) string {
	return fmt.Sprintf("`%s`", strings.Replace(s, "`", "``", -1))
}
func (d *PostgresDialect) SQLType(v interface{}, autoIncrement bool, size uint64) (name string, allowNull bool) {
	switch v.(type) {
	case bool:
		return "boolean", false
	case *bool, sql.NullBool:
		return "boolean", true
	case int8, int16, uint8, uint16:
		return d.smallint(autoIncrement), false
	case *int8, *int16, *uint8, *uint16:
		return d.smallint(autoIncrement), true
	case int, int32, uint, uint32:
		return d.integer(autoIncrement), false
	case *int, *int32, *uint, *uint32:
		return d.integer(autoIncrement), true
	case int64, uint64:
		return d.bigint(autoIncrement), false
	case *int64, *uint64, sql.NullInt64:
		return d.bigint(autoIncrement), true
	case string:
		return d.varchar(size), false
	case *string, sql.NullString:
		return d.varchar(size), true
	case []byte:
		return "bytea", true
	case time.Time:
		return "timestamp with time zone", false
	case *time.Time:
		return "timestamp with time zone", true
	case Rat:
		return fmt.Sprintf("numeric(%d, %d)", decimalPrecision, decimalScale), false
	case *Rat:
		return fmt.Sprintf("numeric(%d, %d)", decimalPrecision, decimalScale), true
	case Float32, Float64:
		return "double precision", false
	case *Float32, *Float64:
		return "double precision", true
	case float32, *float32, float64, *float64, sql.NullFloat64:
		panic(ErrUsingFloatType)
	}
	panic(fmt.Errorf("PostgresDialect: unsupported SQL type: %T", v))
}
func New(file *os.File) Archive {
	if filepath.Ext(file.Name()) == ".zip" {
		return zip.New(file)
	}
	return tar.New(file)
}
func (h *Host) Disconnect() {
	// TODO(c4milo): Return an error to the user given that this error
	// may be thrown due to insufficient hardware resources
	if h.handle == C.VIX_E_CANCELLED {
		return
	}

	if &h.handle != nil &&
		h.handle != C.VIX_INVALID_HANDLE {

		C.VixHost_Disconnect(h.handle)
		h.handle = C.VIX_INVALID_HANDLE
	}
}
func (v *VM) nextNetworkAdapterID(vmx map[string]string) int {
	var nextID int
	prefix := "ethernet"

	for key := range vmx {
		if strings.HasPrefix(key, prefix) {
			ethN := strings.Split(key, ".")[0]
			number, _ := strconv.Atoi(strings.Split(ethN, prefix)[1])

			// If ethN is not present, its id is recycled
			if vmx[ethN+".present"] == "FALSE" {
				return number
			}

			if number > nextID {
				nextID = number
			}
		}
	}

	nextID++

	return nextID
}
func (v *VM) totalNetworkAdapters(vmx map[string]string) int {
	var total int
	prefix := "ethernet"

	for key := range vmx {
		if strings.HasPrefix(key, prefix) {
			ethN := strings.Split(key, ".")[0]
			number, _ := strconv.Atoi(strings.Split(ethN, prefix)[1])

			if number > total {
				total = number
			}
		}
	}

	return total
}
func (v *VM) RemoveAllNetworkAdapters() error {
	vmxPath, err := v.VmxPath()
	if err != nil {
		return err
	}

	vmx, err := readVmx(vmxPath)
	if err != nil {
		return err
	}

	for key := range vmx {
		if strings.HasPrefix(key, "ethernet") {
			delete(vmx, key)
		}
	}

	return writeVmx(vmxPath, vmx)
}
func (v *VM) RemoveNetworkAdapter(adapter *NetworkAdapter) error {
	isVMRunning, err := v.IsRunning()
	if err != nil {
		return err
	}

	if isVMRunning {
		return &Error{
			Operation: "vm.RemoveNetworkAdapter",
			Code:      100000,
			Text:      "The VM has to be powered off in order to change its vmx settings",
		}
	}

	vmxPath, err := v.VmxPath()
	if err != nil {
		return err
	}

	vmx, err := readVmx(vmxPath)
	if err != nil {
		return err
	}

	device := "ethernet" + adapter.ID

	for key := range vmx {
		if strings.HasPrefix(key, device) {
			delete(vmx, key)
		}
	}

	vmx[device+".present"] = "FALSE"

	err = writeVmx(vmxPath, vmx)
	if err != nil {
		return err
	}

	return nil
}
func (v *VM) NetworkAdapters() ([]*NetworkAdapter, error) {
	vmxPath, err := v.VmxPath()
	if err != nil {
		return nil, err
	}

	vmx, err := readVmx(vmxPath)
	if err != nil {
		return nil, err
	}

	var adapters []*NetworkAdapter
	// VMX ethernet adapters seem to not be zero based
	for i := 1; i <= v.totalNetworkAdapters(vmx); i++ {
		id := strconv.Itoa(i)
		prefix := "ethernet" + id

		if vmx[prefix+".present"] == "FALSE" {
			continue
		}

		wakeOnPckRcv, _ := strconv.ParseBool(vmx[prefix+".wakeOnPcktRcv"])
		lnkStateProp, _ := strconv.ParseBool(vmx[prefix+".linkStatePropagation.enable"])
		present, _ := strconv.ParseBool(vmx[prefix+".present"])
		startConnected, _ := strconv.ParseBool(vmx[prefix+".startConnected"])
		address, _ := net.ParseMAC(vmx[prefix+".address"])
		genAddress, _ := net.ParseMAC(vmx[prefix+".generatedAddress"])
		vswitch, _ := GetVSwitch(vmx[prefix+".vnet"])

		adapter := &NetworkAdapter{
			ID:                        id,
			present:                   present,
			ConnType:                  NetworkType(vmx[prefix+".connectionType"]),
			Vdevice:                   VNetDevice(vmx[prefix+".virtualDev"]),
			WakeOnPcktRcv:             wakeOnPckRcv,
			LinkStatePropagation:      lnkStateProp,
			MacAddrType:               MacAddressType(vmx[prefix+".addressType"]),
			MacAddress:                address,
			VSwitch:                   vswitch,
			StartConnected:            startConnected,
			GeneratedMacAddress:       genAddress,
			GeneratedMacAddressOffset: vmx[prefix+".generatedAddressOffset"],
			PciSlotNumber:             vmx[prefix+".pciSlotNumber"],
		}

		adapters = append(adapters, adapter)
	}

	return adapters, nil
}
func newArchive(path string, password *string) (*Archive, error) {
	err := detect7zCached()
	if err != nil {
		return nil, err
	}
	cmd := exec.Command("7z", "l", "-slt", "-sccUTF-8", path)
	out, err := cmd.CombinedOutput()
	if err != nil {
		return nil, err
	}
	entries, err := parse7zListOutput(out)
	if err != nil {
		return nil, err
	}
	return &Archive{
		Path:    path,
		Entries: entries,
		password: password,
	}, nil
}
func (a *Archive) GetFileReader(name string) (io.ReadCloser, error) {
	found := false
	for _, e := range a.Entries {
		if e.Path == name {
			found = true
			break
		}
	}
	if !found {
		return nil, errors.New("file not in the archive")
	}

	params := []string{"x", "-so"}
	if a.password != nil {
		params = append(params, fmt.Sprintf("-p%s", *a.password))
	}
	params = append(params, a.Path, name)

	cmd := exec.Command("7z", params...)
	stdout, err := cmd.StdoutPipe()
	rc := &readCloser{
		rc:  stdout,
		cmd: cmd,
	}
	err = cmd.Start()
	if err != nil {
		stdout.Close()
		return nil, err
	}
	return rc, nil
}
func (a *Archive) ExtractToWriter(dst io.Writer, name string) error {
	r, err := a.GetFileReader(name)
	if err != nil {
		return err
	}
	_, err = io.Copy(dst, r)
	err2 := r.Close()
	if err != nil {
		return err
	}
	return err2
}
func (a *Archive) ExtractToFile(dstPath string, name string) error {
	f, err := os.Create(dstPath)
	if err != nil {
		return err
	}
	defer f.Close()
	return a.ExtractToWriter(f, name)
}
func (g *Guest) SharedFoldersParentDir() (string, error) {
	var err C.VixError = C.VIX_OK
	var path *C.char

	err = C.get_property(g.handle,
		C.VIX_PROPERTY_GUEST_SHAREDFOLDERS_SHARES_PATH,
		unsafe.Pointer(&path))

	defer C.Vix_FreeBuffer(unsafe.Pointer(path))

	if C.VIX_OK != err {
		return "", &Error{
			Operation: "guest.SharedFoldersParentDir",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return C.GoString(path), nil
}
func (s *Snapshot) Name() (string, error) {
	var err C.VixError = C.VIX_OK
	var name *C.char

	err = C.get_property(s.handle,
		C.VIX_PROPERTY_SNAPSHOT_DISPLAYNAME,
		unsafe.Pointer(&name))

	defer C.Vix_FreeBuffer(unsafe.Pointer(name))

	if C.VIX_OK != err {
		return "", &Error{
			Operation: "snapshot.Name",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return C.GoString(name), nil
}
func (s *Snapshot) Description() (string, error) {
	var err C.VixError = C.VIX_OK
	var desc *C.char

	err = C.get_property(s.handle,
		C.VIX_PROPERTY_SNAPSHOT_DESCRIPTION,
		unsafe.Pointer(&desc))

	defer C.Vix_FreeBuffer(unsafe.Pointer(desc))

	if C.VIX_OK != err {
		return "", &Error{
			Operation: "snapshot.Description",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return C.GoString(desc), nil
}
func cleanupSnapshot(s *Snapshot) {
	if s.handle != C.VIX_INVALID_HANDLE {
		C.Vix_ReleaseHandle(s.handle)
		s.handle = C.VIX_INVALID_HANDLE
	}
}
func BusTypeFromID(ID string) vmx.BusType {
	var bus vmx.BusType
	switch {
	case strings.HasPrefix(ID, string(vmx.IDE)):
		bus = vmx.IDE
	case strings.HasPrefix(ID, string(vmx.SCSI)):
		bus = vmx.SCSI
	case strings.HasPrefix(ID, string(vmx.SATA)):
		bus = vmx.SATA
	}

	return bus
}
func (vmxfile *VMXFile) Read() error {
	data, err := ioutil.ReadFile(vmxfile.path)
	if err != nil {
		return err
	}

	model := new(vmx.VirtualMachine)

	err = vmx.Unmarshal(data, model)
	if err != nil {
		return err
	}

	vmxfile.model = model

	return nil
}
func (vmxfile *VMXFile) Write() error {
	file, err := os.Create(vmxfile.path)
	if err != nil {
		return err
	}
	defer file.Close()

	data, err := vmx.Marshal(vmxfile.model)
	if err != nil {
		return err
	}

	_, err = file.Write(data)
	if err != nil {
		return err
	}

	return nil
}
func NewVirtualMachine(handle C.VixHandle, vmxpath string) (*VM, error) {
	vmxfile := &VMXFile{
		path: vmxpath,
	}

	// Loads VMX file in memory
	err := vmxfile.Read()
	if err != nil {
		return nil, err
	}

	vm := &VM{
		handle:  handle,
		vmxfile: vmxfile,
	}

	runtime.SetFinalizer(vm, cleanupVM)
	return vm, nil
}
func (v *VM) Vcpus() (uint8, error) {
	var err C.VixError = C.VIX_OK
	vcpus := C.VIX_PROPERTY_NONE

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_NUM_VCPUS,
		unsafe.Pointer(&vcpus))

	if C.VIX_OK != err {
		return 0, &Error{
			Operation: "vm.Vcpus",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return uint8(vcpus), nil
}
func (v *VM) VmxPath() (string, error) {
	var err C.VixError = C.VIX_OK
	var path *C.char

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_VMX_PATHNAME,
		unsafe.Pointer(&path))

	defer C.Vix_FreeBuffer(unsafe.Pointer(path))

	if C.VIX_OK != err {
		return "", &Error{
			Operation: "vm.VmxPath",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return C.GoString(path), nil
}
func (v *VM) MemorySize() (uint, error) {
	var err C.VixError = C.VIX_OK
	memsize := C.VIX_PROPERTY_NONE

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_MEMORY_SIZE,
		unsafe.Pointer(&memsize))

	if C.VIX_OK != err {
		return 0, &Error{
			Operation: "vm.MemorySize",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return uint(memsize), nil
}
func (v *VM) ReadOnly() (bool, error) {
	var err C.VixError = C.VIX_OK
	readonly := C.VIX_PROPERTY_NONE

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_READ_ONLY,
		unsafe.Pointer(&readonly))

	if C.VIX_OK != err {
		return false, &Error{
			Operation: "vm.ReadOnly",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	if readonly == 0 {
		return false, nil
	}

	return true, nil
}
func (v *VM) InVMTeam() (bool, error) {
	var err C.VixError = C.VIX_OK
	inTeam := C.VIX_PROPERTY_NONE

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_IN_VMTEAM,
		unsafe.Pointer(&inTeam))

	if C.VIX_OK != err {
		return false, &Error{
			Operation: "vm.InVmTeam",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	if inTeam == 0 {
		return false, nil
	}

	return true, nil
}
func (v *VM) PowerState() (VMPowerState, error) {
	var err C.VixError = C.VIX_OK
	var state C.VixPowerState = 0x0

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_POWER_STATE,
		unsafe.Pointer(&state))

	if C.VIX_OK != err {
		return VMPowerState(0x0), &Error{
			Operation: "vm.PowerState",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return VMPowerState(state), nil
}
func (v *VM) ToolsState() (GuestToolsState, error) {
	var err C.VixError = C.VIX_OK
	state := C.VIX_TOOLSSTATE_UNKNOWN

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_TOOLS_STATE,
		unsafe.Pointer(&state))

	if C.VIX_OK != err {
		return TOOLSSTATE_UNKNOWN, &Error{
			Operation: "vm.ToolsState",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return GuestToolsState(state), nil
}
func (v *VM) IsRunning() (bool, error) {
	var err C.VixError = C.VIX_OK
	running := C.VIX_PROPERTY_NONE

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_IS_RUNNING,
		unsafe.Pointer(&running))

	if C.VIX_OK != err {
		return false, &Error{
			Operation: "vm.IsRunning",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	if running == 0 {
		return false, nil
	}

	return true, nil
}
func (v *VM) GuestOS() (string, error) {
	var err C.VixError = C.VIX_OK
	var os *C.char

	err = C.get_property(v.handle,
		C.VIX_PROPERTY_VM_GUESTOS,
		unsafe.Pointer(&os))

	defer C.Vix_FreeBuffer(unsafe.Pointer(os))

	if C.VIX_OK != err {
		return "", &Error{
			Operation: "vm.GuestOS",
			Code:      int(err & 0xFFFF),
			Text:      C.GoString(C.Vix_GetErrorText(err, nil)),
		}
	}

	return C.GoString(os), nil
}
func cleanupVM(v *VM) {
	if v.handle != C.VIX_INVALID_HANDLE {
		C.Vix_ReleaseHandle(v.handle)
		v.handle = C.VIX_INVALID_HANDLE
	}
}
func (v *VM) updateVMX(updateFunc func(model *vmx.VirtualMachine) error) error {
	isVMRunning, err := v.IsRunning()
	if err != nil {
		return err
	}

	if isVMRunning {
		return &Error{
			Operation: "vm.updateVMX",
			Code:      100000,
			Text:      "The VM has to be powered off in order to change its vmx settings",
		}
	}

	err = v.vmxfile.Read()
	if err != nil {
		return &Error{
			Operation: "vm.updateVMX",
			Code:      300001,
			Text:      fmt.Sprintf("Error reading vmx file: %s", err),
		}
	}

	err = updateFunc(v.vmxfile.model)
	if err != nil {
		return &Error{
			Operation: "vm.updateVMX",
			Code:      300002,
			Text:      fmt.Sprintf("Error changing vmx value: %s", err),
		}
	}

	err = v.vmxfile.Write()
	if err != nil {
		return &Error{
			Operation: "vm.updateVMX",
			Code:      300003,
			Text:      fmt.Sprintf("Error writing vmx file: %s", err),
		}
	}

	return nil
}
func (v *VM) SetMemorySize(size uint) error {
	if size == 0 {
		size = 4
	}

	// Makes sure memory size is divisible by 4, otherwise VMware is going to
	// silently fail, cancelling vix operations.
	if size%4 != 0 {
		size = uint(math.Floor(float64((size / 4) * 4)))
	}

	return v.updateVMX(func(model *vmx.VirtualMachine) error {
		model.Memsize = size
		return nil
	})
}
func (v *VM) SetNumberVcpus(vcpus uint) error {
	if vcpus < 1 {
		vcpus = 1
	}

	return v.updateVMX(func(model *vmx.VirtualMachine) error {
		model.NumvCPUs = vcpus
		return nil
	})
}
func (v *VM) SetDisplayName(name string) error {
	return v.updateVMX(func(model *vmx.VirtualMachine) error {
		model.DisplayName = name
		return nil
	})
}
func (v *VM) SetAnnotation(text string) error {
	return v.updateVMX(func(model *vmx.VirtualMachine) error {
		model.Annotation = text
		return nil
	})
}
func (v *VM) SetVirtualHwVersion(version string) error {
	return v.updateVMX(func(model *vmx.VirtualMachine) error {
		version, err := strconv.ParseInt(version, 10, 32)
		if err != nil {
			return err
		}
		model.Vhardware.Compat = "hosted"
		model.Vhardware.Version = int(version)
		return nil
	})
}
func (e *Error) Error() string {
	return fmt.Sprintf("VIX Error: %s, code: %d, operation: %s", e.Text, e.Code, e.Operation)
}
func Errorf(message string, a ...interface{}) error {
	return wrap(fmt.Errorf(message, a...))
}
func (t *traceableError) Error() string {
	str := t.err.Error()
	for _, frame := range t.stack {
		str += fmt.Sprintf("\n  at %s", frame.string())
	}
	return str
}
func (s *stackFrame) string() string {
	return fmt.Sprintf("%s (%s:%d)", s.function, s.file, s.line)
}
func newStackFrame(pc uintptr) *stackFrame {
	fn := runtime.FuncForPC(pc)
	file, line := fn.FileLine(pc)
	packagePath, funcSignature := parseFuncName(fn.Name())
	_, fileName := filepath.Split(file)

	return &stackFrame{
		file:     filepath.Join(packagePath, fileName),
		line:     line,
		function: funcSignature,
	}
}
func captureStack(skip, maxDepth int) []*stackFrame {
	pcs := make([]uintptr, maxDepth)
	count := runtime.Callers(skip+1, pcs)

	frames := make([]*stackFrame, count)
	for i, pc := range pcs[0:count] {
		frames[i] = newStackFrame(pc)
	}

	return frames
}
func parseFuncName(fnName string) (packagePath, signature string) {
	regEx := regexp.MustCompile("([^\\(]*)\\.(.*)")
	parts := regEx.FindStringSubmatch(fnName)
	packagePath = parts[1]
	signature = parts[2]
	return
}
func Stack(err interface{}) {
	stack := make([]byte, 64<<10)
	stack = stack[:runtime.Stack(stack, false)]

	log.Printf("%v\n%s", err, stack)
}
func StackWithCaller(err interface{}) {
	stack := make([]byte, 64<<10)
	stack = stack[:runtime.Stack(stack, false)]

	if pack, ok := callerPackage(); ok {
		log.Printf("%s: %v\n%s", pack, err, stack)
	} else {
		log.Printf("%v\n%s", err, stack)
	}
}
func (w responseWriterBinder) Write(p []byte) (int, error) {
	for _, f := range w.before {
		f(p)
	}
	return w.Writer.Write(p)
}
func ResponseStatus(w http.ResponseWriter) int {
	return int(httpResponseStruct(reflect.ValueOf(w)).FieldByName("status").Int())
}
func httpResponseStruct(v reflect.Value) reflect.Value {
	if v.Kind() == reflect.Ptr {
		v = v.Elem()
	}

	if v.Type().String() == "http.response" {
		return v
	}

	return httpResponseStruct(v.FieldByName("ResponseWriter").Elem())
}
func SetDetectedContentType(w http.ResponseWriter, p []byte) string {
	ct := w.Header().Get("Content-Type")
	if ct == "" {
		ct = http.DetectContentType(p)
		w.Header().Set("Content-Type", ct)
	}
	return ct
}
func (e *ServerError) New(message string) *ServerError {
	e.HTTPCode = http.StatusInternalServerError
	e.Errno = 0
	e.Message = message
	return e
}
func (e *DBError) New(dbName string, message string) *DBError {
	e.HTTPCode = http.StatusInternalServerError
	e.Errno = 0
	e.Message = message
	e.DBName = dbName
	return e
}
func (e *ValidationError) New(message string) *ValidationError {
	e.HTTPCode = http.StatusBadRequest
	e.Errno = 0
	e.Message = message
	return e
}
func (e *NotFoundError) New(message string) *NotFoundError {
	e.HTTPCode = http.StatusNotFound
	e.Errno = 0
	e.Message = message
	return e
}
func (c *Controller) StrLength(fieldName string, p interface{}, n int) string {
	if p == nil {
		p = ""
	}
	v, ok := p.(string)
	if ok == false {
		panic((&ValidationError{}).New(fieldName + "长度应该为" + strconv.Itoa(n)))
	}
	b := c.Validate.Length(v, n)
	if b == false {
		panic((&ValidationError{}).New(fieldName + "长度应该为" + strconv.Itoa(n)))
	}
	return v
}
func (c *Controller) StrLenIn(fieldName string, p interface{}, l ...int) string {
	if p == nil {
		p = ""
	}
	v, ok := p.(string)
	if ok == false {
		panic((&ValidationError{}).New(fieldName + "格式错误"))
	}
	length := utf8.RuneCountInString(v)
	b := false
	for i := 0; i < len(l); i++ {
		if l[i] == length {
			b = true
		}
	}
	if b == false {
		panic((&ValidationError{}).New(fieldName + "值的长度应该在" + strings.Replace(strings.Trim(fmt.Sprint(l), "[]"), " ", ",", -1) + "中"))
	}
	return v
}
func (c *Controller) StrIn(fieldName string, p interface{}, l ...string) string {
	if p == nil {
		p = ""
	}
	v, ok := p.(string)
	if ok == false {
		panic((&ValidationError{}).New(fieldName + "格式错误"))
	}
	b := false
	for i := 0; i < len(l); i++ {
		if l[i] == v {
			b = true
		}
	}
	if b == false {
		panic((&ValidationError{}).New(fieldName + "值应该在" + strings.Replace(strings.Trim(fmt.Sprint(l), "[]"), " ", ",", -1) + "中"))
	}
	return v
}
func (c *Controller) GetEmail(fieldName string, p interface{}) string {
	if p == nil {
		p = ""
	}
	v, ok := p.(string)
	if ok == false {
		panic((&ValidationError{}).New(fieldName + "格式错误"))
	}
	b := c.Validate.Email(v)
	if b == false {
		panic((&ValidationError{}).New(fieldName + "格式错误"))
	}
	return v
}
func MostSpecificType(types []string) (string, error) {
	if len(types) == 0 {
		return "", errors.New("no types supplied")
	}
	sorted, err := SortTypes(types)
	if err != nil {
		return "", err
	}
	return sorted[len(sorted)-1], nil
}
func FullTypeHierarchy(highestLevelType string) []string {
	var typeHierarchy []string
	t := strings.Split(highestLevelType, "/")
	typeToCheck := t[len(t)-1]
	for {
		typeHierarchy = append(typeHierarchy, typeToCheck)
		parentType := ParentType(typeToCheck)
		if parentType != "" {
			typeToCheck = parentType
		} else {
			return TypeURIs(typeHierarchy)
		}
	}

}
func SortTypes(types []string) ([]string, error) {
	ts := &typeSorter{types: make([]string, len(types))}
	copy(ts.types, types)
	sort.Sort(ts)
	if ts.invalid {
		return types, ErrNotHierarchy
	}
	return ts.types, nil
}
func (rs *redisStore) Delete(key string) error {
	delete(rs.Values, key)
	err := provider.refresh(rs)
	return err
}
func (rp *redisProvider) Set(key string, values map[string]string) (*redisStore, error) {
	rs := &redisStore{SID: key, Values: values}
	err := provider.refresh(rs)
	return rs, err
}
func (rp *redisProvider) refresh(rs *redisStore) error {
	var err error
	redisPool.Exec(func(c *redis.Client) {
		err = c.HMSet(rs.SID, rs.Values).Err()
		if err != nil {
			return
		}
		err = c.Expire(rs.SID, sessExpire).Err()
	})
	return nil
}
func (rp *redisProvider) Get(sid string) (*redisStore, error) {
	var rs = &redisStore{}
	var val map[string]string
	var err error
	redisPool.Exec(func(c *redis.Client) {
		val, err = c.HGetAll(sid).Result()
		rs.Values = val
	})
	return rs, err
}
func (rp *redisProvider) Destroy(sid string) error {
	var err error
	redisPool.Exec(func(c *redis.Client) {
		err = c.Del(sid).Err()
	})
	return err
}
func (rp *redisProvider) UpExpire(sid string) error {
	var err error
	redisPool.Exec(func(c *redis.Client) {
		err = c.Expire(sid, sessExpire).Err()
	})
	return err
}
func (hs *HandlersStack) Use(h RouterHandler) {
	hs.Handlers = append(hs.Handlers, h)
}
func (hs *HandlersStack) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	// Get a context for the request from ctxPool.
	c := getContext(w, r)

	// Set some "good practice" default headers.
	c.ResponseWriter.Header().Set("Cache-Control", "no-cache")
	c.ResponseWriter.Header().Set("Content-Type", "application/json")
	c.ResponseWriter.Header().Set("Connection", "keep-alive")
	c.ResponseWriter.Header().Set("Vary", "Accept-Encoding")
	//c.ResponseWriter.Header().Set("Access-Control-Allow-Origin", "*")
	c.ResponseWriter.Header().Set("Access-Control-Allow-Headers", "X-Requested-With")
	c.ResponseWriter.Header().Set("Access-Control-Allow-Methods", "PUT,POST,GET,DELETE,OPTIONS")

	// Always recover form panics.
	defer c.Recover()

	// Enter the handlers stack.
	c.Next()

	// Respnose data
	// if c.written == false {
	// 	c.Fail(errors.New("not written"))
	// }
	// Put the context to ctxPool
	putContext(c)
}
func (group *RouterGroup) Use(middleware ...RouterHandler) IRoutes {
	group.Handlers = append(group.Handlers, middleware...)
	return group.returnObj()
}
func (group *RouterGroup) Group(relativePath string, handlers ...RouterHandler) *RouterGroup {
	return &RouterGroup{
		Handlers: group.combineHandlers(handlers),
		basePath: group.calculateAbsolutePath(relativePath),
		engine:   group.engine,
	}
}
func Run() {
	for _, f := range beforeRun {
		f()
	}

	// parse command line params.
	if OpenCommandLine {
		flag.StringVar(&Address, "address", ":8080", "-address=:8080")
		flag.BoolVar(&Production, "production", false, "-production=false")
		flag.Parse()
	}

	log.Warnln(fmt.Sprintf("Serving %s with pid %d. Production is %t.", Address, os.Getpid(), Production))

	// set default router.
	Use(Routers.handlers)

	// set graceful server.
	srv := &graceful.Server{
		ListenLimit: ListenLimit,
		ConnState: func(conn net.Conn, state http.ConnState) {
			// conn has a new state
		},
		Server: &http.Server{
			Addr:           Address,
			Handler:        defaultHandlersStack,
			ReadTimeout:    ReadTimeout,
			WriteTimeout:   WriteTimeout,
			IdleTimeout:    IdleTimeout,
			MaxHeaderBytes: MaxHeaderBytes,
		},
	}
	err := srv.ListenAndServe()

	if err != nil {
		log.Fatalln(err)
	}
	log.Warnln("Server stoped.")

}
func create() *Engine {
	engine := &Engine{
		RouterGroup: RouterGroup{
			Handlers: nil,
			basePath: "/",
			root:     true,
		},
		trees: make(methodTrees, 0, 9),
	}
	engine.RouterGroup.engine = engine
	return engine
}
func (ctx *Context) Redirect(url string, code int) {
	http.Redirect(ctx.ResponseWriter, ctx.Request, url, code)
}
func (ctx *Context) Ok(data interface{}) {
	if ctx.written == true {
		log.WithFields(log.Fields{"path": ctx.Request.URL.Path}).Warnln("Context.Success: request has been writed")
		return
	}
	ctx.written = true
	var json = jsoniter.ConfigCompatibleWithStandardLibrary
	b, _ := json.Marshal(&ResFormat{Ok: true, Data: data})
	ctx.ResponseWriter.WriteHeader(http.StatusOK)
	ctx.ResponseWriter.Write(b)
}
func (ctx *Context) Fail(err error) {
	if err == nil {
		log.WithFields(log.Fields{"path": ctx.Request.URL.Path}).Warnln("Context.Fail: err is nil")
		ctx.ResponseWriter.WriteHeader(http.StatusInternalServerError)
		ctx.ResponseWriter.Write(nil)
		return
	}

	if ctx.written == true {
		log.WithFields(log.Fields{"path": ctx.Request.URL.Path}).Warnln("Context.Fail: request has been writed")
		return
	}

	errno := 0
	errCore, ok := err.(ICoreError)
	if ok == true {
		errno = errCore.GetErrno()
	}
	ctx.written = true
	if Production == false {
		log.WithFields(log.Fields{"path": ctx.Request.URL.Path}).Warnln(err.Error())
	} else if _, ok := err.(*ServerError); ok == true {
		log.WithFields(log.Fields{"path": ctx.Request.URL.Path}).Warnln(err.Error())
	}

	var json = jsoniter.ConfigCompatibleWithStandardLibrary
	b, _ := json.Marshal(&ResFormat{Ok: false, Message: err.Error(), Errno: errno})

	coreErr, ok := err.(ICoreError)
	if ok == true {
		ctx.ResponseWriter.WriteHeader(coreErr.GetHTTPCode())
	} else {
		ctx.ResponseWriter.WriteHeader(http.StatusInternalServerError)
	}
	ctx.ResponseWriter.Write(b)
}
func (ctx *Context) ResStatus(code int) (int, error) {
	if ctx.written == true {
		return 0, errors.New("Context.ResStatus: request has been writed")
	}
	ctx.written = true
	ctx.ResponseWriter.WriteHeader(code)
	return fmt.Fprint(ctx.ResponseWriter, http.StatusText(code))
}
func (ctx *Context) Next() {
	// Call the next handler only if there is one and the response hasn't been written.
	if !ctx.Written() && ctx.index < len(ctx.handlersStack.Handlers)-1 {
		ctx.index++
		ctx.handlersStack.Handlers[ctx.index](ctx)
	}
}
func (ctx *Context) GetSession() IStore {
	store := ctx.Data["session"]
	if store == nil {
		return nil
	}
	st, ok := store.(IStore)
	if ok == false {
		return nil
	}
	return st
}
func (ctx *Context) GetBodyJSON() {
	var reqJSON map[string]interface{}
	body, _ := ioutil.ReadAll(ctx.Request.Body)
	defer ctx.Request.Body.Close()
	cType := ctx.Request.Header.Get("Content-Type")
	a := strings.Split(cType, ";")
	if a[0] == "application/x-www-form-urlencoded" {
		reqJSON = make(map[string]interface{})
		reqStr := string(body)
		reqArr := strings.Split(reqStr, "&")
		for _, v := range reqArr {
			param := strings.Split(v, "=")
			reqJSON[param[0]], _ = url.QueryUnescape(param[1])
		}
	} else {
		json.Unmarshal(body, &reqJSON)
	}
	ctx.BodyJSON = reqJSON
}
func (ctx *Context) SetSession(key string, values map[string]string) error {
	sid := ctx.genSid(key)
	values["Sid"] = sid
	timestamp := strconv.FormatInt(time.Now().Unix(), 10)
	token := ctx.genSid(key + timestamp)
	values["Token"] = token
	store, err := provider.Set(sid, values)
	if err != nil {
		return err
	}
	cookie := httpCookie
	cookie.Value = sid
	ctx.Data["session"] = store

	respCookie := ctx.ResponseWriter.Header().Get("Set-Cookie")
	if strings.HasPrefix(respCookie, cookie.Name) {
		ctx.ResponseWriter.Header().Del("Set-Cookie")
	}
	http.SetCookie(ctx.ResponseWriter, &cookie)
	return nil
}
func (ctx *Context) FreshSession(key string) error {
	err := provider.UpExpire(key)
	if err != nil {
		return err
	}
	return nil
}
func (ctx *Context) DeleteSession() error {
	sid := ctx.Data["Sid"].(string)
	ctx.Data["session"] = nil
	provider.Destroy(sid)
	cookie := httpCookie
	cookie.MaxAge = -1
	http.SetCookie(ctx.ResponseWriter, &cookie)
	return nil
}
func (w contextWriter) Write(p []byte) (int, error) {
	w.context.written = true
	return w.ResponseWriter.Write(p)
}
func (w contextWriter) WriteHeader(code int) {
	w.context.written = true
	w.ResponseWriter.WriteHeader(code)
}
func New(pemPath string, options ...func(*Client) error) *Client {
	c := Client{
		pemPath:        pemPath,
		RequestTimeout: defaultRequestTimeout,
	}

	c.PrintDebug = false

	for _, option := range options {
		err := option(&c)
		if err != nil {
			return nil
		}
	}

	return &c
}
func (c *Client) setAllowLargeResults(shouldAllow bool, tempTableName string, flattenResults bool) error {
	c.allowLargeResults = shouldAllow
	c.tempTableName = tempTableName
	c.flattenResults = flattenResults
	return nil
}
func (c *Client) connect() (*bigquery.Service, error) {
	if c.token != nil {
		if !c.token.Valid() && c.service != nil {
			return c.service, nil
		}
	}

	// generate auth token and create service object
	//authScope := bigquery.BigqueryScope
	pemKeyBytes, err := ioutil.ReadFile(c.pemPath)
	if err != nil {
		panic(err)
	}

	t, err := google.JWTConfigFromJSON(
		pemKeyBytes,
		"https://www.googleapis.com/auth/bigquery")
	//t := jwt.NewToken(c.accountEmailAddress, bigquery.BigqueryScope, pemKeyBytes)
	client := t.Client(oauth2.NoContext)

	service, err := bigquery.New(client)
	if err != nil {
		return nil, err
	}

	c.service = service
	return service, nil
}
func (c *Client) InsertRow(projectID, datasetID, tableID string, rowData map[string]interface{}) error {
	service, err := c.connect()
	if err != nil {
		return err
	}

	insertRequest := buildBigQueryInsertRequest([]map[string]interface{}{rowData})

	result, err := service.Tabledata.InsertAll(projectID, datasetID, tableID, insertRequest).Do()
	if err != nil {
		c.printDebug("Error inserting row: ", err)
		return err
	}

	if len(result.InsertErrors) > 0 {
		return errors.New("Error inserting row")
	}

	return nil
}
func (c *Client) AsyncQuery(pageSize int, dataset, project, queryStr string, dataChan chan Data) {
	c.pagedQuery(pageSize, dataset, project, queryStr, dataChan)
}
func (c *Client) Query(dataset, project, queryStr string) ([][]interface{}, []string, error) {
	return c.pagedQuery(defaultPageSize, dataset, project, queryStr, nil)
}
func (c *Client) stdPagedQuery(service *bigquery.Service, pageSize int, dataset, project, queryStr string, dataChan chan Data) ([][]interface{}, []string, error) {
	c.printDebug("std paged query")
	datasetRef := &bigquery.DatasetReference{
		DatasetId: dataset,
		ProjectId: project,
	}

	query := &bigquery.QueryRequest{
		DefaultDataset: datasetRef,
		MaxResults:     int64(pageSize),
		Kind:           "json",
		Query:          queryStr,
	}

	qr, err := service.Jobs.Query(project, query).Do()

	// extract the initial rows that have already been returned with the Query
	headers, rows := c.headersAndRows(qr.Schema, qr.Rows)

	if err != nil {
		c.printDebug("Error loading query: ", err)
		if dataChan != nil {
			dataChan <- Data{Err: err}
		}

		return nil, nil, err
	}

	return c.processPagedQuery(qr.JobReference, qr.PageToken, dataChan, headers, rows)
}
func (c *Client) largeDataPagedQuery(service *bigquery.Service, pageSize int, dataset, project, queryStr string, dataChan chan Data) ([][]interface{}, []string, error) {
	c.printDebug("largeDataPagedQuery starting")
	ts := time.Now()
	// start query
	tableRef := bigquery.TableReference{DatasetId: dataset, ProjectId: project, TableId: c.tempTableName}
	jobConfigQuery := bigquery.JobConfigurationQuery{}

	datasetRef := &bigquery.DatasetReference{
		DatasetId: dataset,
		ProjectId: project,
	}

	jobConfigQuery.AllowLargeResults = true
	jobConfigQuery.Query = queryStr
	jobConfigQuery.DestinationTable = &tableRef
	jobConfigQuery.DefaultDataset = datasetRef
	if !c.flattenResults {
		c.printDebug("setting FlattenResults to false")
		// need a pointer to bool
		f := false
		jobConfigQuery.FlattenResults = &f
	}
	jobConfigQuery.WriteDisposition = "WRITE_TRUNCATE"
	jobConfigQuery.CreateDisposition = "CREATE_IF_NEEDED"

	jobConfig := bigquery.JobConfiguration{}

	jobConfig.Query = &jobConfigQuery

	job := bigquery.Job{}
	job.Configuration = &jobConfig

	jobInsert := service.Jobs.Insert(project, &job)
	runningJob, jerr := jobInsert.Do()

	if jerr != nil {
		c.printDebug("Error inserting job!", jerr)
		if dataChan != nil {
			dataChan <- Data{Err: jerr}
		}
		return nil, nil, jerr
	}

	var qr *bigquery.GetQueryResultsResponse
	var rows [][]interface{}
	var headers []string
	var err error

	// Periodically, job references are not created, but errors are also not thrown.
	// In this scenario, retry up to 5 times to get a job reference before giving up.
	for i := 1; ; i++ {
		r := service.Jobs.GetQueryResults(project, runningJob.JobReference.JobId)
		r.TimeoutMs(c.RequestTimeout)
		qr, err = r.Do()

		headers, rows = c.headersAndRows(qr.Schema, qr.Rows)

		if i >= maxRequestRetry || qr.JobReference != nil || err != nil {
			if i > 1 {
				c.printDebug("Took %v tries to get a job reference", i)
			}
			break
		}
	}

	if err == nil && qr.JobReference == nil {
		err = fmt.Errorf("missing job reference")
	}

	if err != nil {
		c.printDebug("Error loading query: ", err)
		if dataChan != nil {
			dataChan <- Data{Err: err}
		}
		return nil, nil, err
	}

	rows, headers, err = c.processPagedQuery(qr.JobReference, qr.PageToken, dataChan, headers, rows)
	c.printDebug("largeDataPagedQuery completed in ", time.Now().Sub(ts).Seconds(), "s")

	return rows, headers, err
}
func (c *Client) pagedQuery(pageSize int, dataset, project, queryStr string, dataChan chan Data) ([][]interface{}, []string, error) {
	// connect to service
	service, err := c.connect()
	if err != nil {
		if dataChan != nil {
			dataChan <- Data{Err: err}
		}
		return nil, nil, err
	}

	if c.allowLargeResults && len(c.tempTableName) > 0 {
		return c.largeDataPagedQuery(service, pageSize, dataset, project, queryStr, dataChan)
	}

	return c.stdPagedQuery(service, pageSize, dataset, project, queryStr, dataChan)
}
func (c *Client) pageOverJob(rowCount int, jobRef *bigquery.JobReference, pageToken string, resultChan chan [][]interface{}, headersChan chan []string) error {
	service, err := c.connect()
	if err != nil {
		return err
	}

	qrc := service.Jobs.GetQueryResults(jobRef.ProjectId, jobRef.JobId)
	if len(pageToken) > 0 {
		qrc.PageToken(pageToken)
	}

	qr, err := qrc.Do()
	if err != nil {
		c.printDebug("Error loading additional data: ", err)
		close(resultChan)
		return err
	}

	if qr.JobComplete {
		c.printDebug("qr.JobComplete")
		headers, rows := c.headersAndRows(qr.Schema, qr.Rows)
		if headersChan != nil {
			headersChan <- headers
			close(headersChan)
		}

		// send back the rows we got
		c.printDebug("sending rows")
		resultChan <- rows
		rowCount = rowCount + len(rows)
		c.printDebug("Total rows: ", rowCount)
	}

	if qr.TotalRows > uint64(rowCount) || !qr.JobComplete {
		c.printDebug("!qr.JobComplete")
		if qr.JobReference == nil {
			c.pageOverJob(rowCount, jobRef, pageToken, resultChan, headersChan)
		} else {
			c.pageOverJob(rowCount, qr.JobReference, qr.PageToken, resultChan, nil)
		}
	} else {
		close(resultChan)
		return nil
	}

	return nil
}
func (c *Client) Count(dataset, project, datasetTable string) int64 {
	qstr := fmt.Sprintf("select count(*) from [%s]", datasetTable)
	res, err := c.SyncQuery(dataset, project, qstr, 1)
	if err == nil {
		if len(res) > 0 {
			val, _ := strconv.ParseInt(res[0][0].(string), 10, 64)
			return val
		}
	}
	return 0
}
func work(args ...interface{}) interface{} {
	url := args[0].(string)
	depth := args[1].(int)
	fetcher := args[2].(Fetcher)
	if depth <= 0 {
		return crawlResult{}
	}
	body, urls, err := fetcher.Fetch(url)
	return crawlResult{body, urls, err}
}
func (pool *Pool) subworker(job *Job) {
	defer func() {
		if err := recover(); err != nil {
			log.Println("panic while running job:", err)
			job.Result = nil
			job.Err = fmt.Errorf(err.(string))
		}
	}()
	job.Result = job.F(job.Args...)
}
func (pool *Pool) worker(worker_id uint) {
	job_pipe := make(chan *Job)
WORKER_LOOP:
	for {
		pool.job_wanted_pipe <- job_pipe
		job := <-job_pipe
		if job == nil {
			time.Sleep(pool.interval * time.Millisecond)
		} else {
			job.Worker_id = worker_id
			pool.subworker(job)
			pool.done_pipe <- job
		}
		select {
		case <-pool.worker_kill_pipe:
			break WORKER_LOOP
		default:
		}
	}
	pool.worker_wg.Done()
}
func (pool *Pool) supervisor() {
SUPERVISOR_LOOP:
	for {
		select {
		// new job
		case job := <-pool.add_pipe:
			pool.jobs_ready_to_run.PushBack(job)
			pool.num_jobs_submitted++
			job.added <- true
		// send jobs to the workers
		case job_pipe := <-pool.job_wanted_pipe:
			element := pool.jobs_ready_to_run.Front()
			var job *Job = nil
			if element != nil {
				job = element.Value.(*Job)
				pool.num_jobs_running++
				pool.jobs_ready_to_run.Remove(element)
			}
			job_pipe <- job
		// job completed
		case job := <-pool.done_pipe:
			pool.num_jobs_running--
			pool.jobs_completed.PushBack(job)
			pool.num_jobs_completed++
		// wait for job
		case result_pipe := <-pool.result_wanted_pipe:
			close_pipe := false
			job := (*Job)(nil)
			element := pool.jobs_completed.Front()
			if element != nil {
				job = element.Value.(*Job)
				pool.jobs_completed.Remove(element)
			} else {
				if pool.num_jobs_running == 0 && pool.num_jobs_completed == pool.num_jobs_submitted {
					close_pipe = true
				}
			}
			if close_pipe {
				close(result_pipe)
			} else {
				result_pipe <- job
			}
		// is the pool working or just lazing on a Sunday afternoon?
		case working_pipe := <-pool.working_wanted_pipe:
			working := true
			if pool.jobs_ready_to_run.Len() == 0 && pool.num_jobs_running == 0 {
				working = false
			}
			working_pipe <- working
		// stats
		case stats_pipe := <-pool.stats_wanted_pipe:
			pool_stats := stats{pool.num_jobs_submitted, pool.num_jobs_running, pool.num_jobs_completed}
			stats_pipe <- pool_stats
		// stopping
		case <-pool.supervisor_kill_pipe:
			break SUPERVISOR_LOOP
		}
	}
	pool.supervisor_wg.Done()
}
func (pool *Pool) Run() {
	if pool.workers_started {
		panic("trying to start a pool that's already running")
	}
	for i := uint(0); i < uint(pool.num_workers); i++ {
		pool.worker_wg.Add(1)
		go pool.worker(i)
	}
	pool.workers_started = true
	// handle the supervisor
	if !pool.supervisor_started {
		pool.startSupervisor()
	}
}
func (pool *Pool) Add(f func(...interface{}) interface{}, args ...interface{}) {
	job := &Job{f, args, nil, nil, make(chan bool), 0, pool.getNextJobId()}
	pool.add_pipe <- job
	<-job.added
}
func (pool *Pool) Wait() {
	working_pipe := make(chan bool)
	for {
		pool.working_wanted_pipe <- working_pipe
		if !<-working_pipe {
			break
		}
		time.Sleep(pool.interval * time.Millisecond)
	}
}
func (pool *Pool) Results() (res []*Job) {
	res = make([]*Job, pool.jobs_completed.Len())
	i := 0
	for e := pool.jobs_completed.Front(); e != nil; e = e.Next() {
		res[i] = e.Value.(*Job)
		i++
	}
	pool.jobs_completed = list.New()
	return
}
func (pool *Pool) WaitForJob() *Job {
	result_pipe := make(chan *Job)
	var job *Job
	var ok bool
	for {
		pool.result_wanted_pipe <- result_pipe
		job, ok = <-result_pipe
		if !ok {
			// no more results available
			return nil
		}
		if job == (*Job)(nil) {
			// no result available right now but there are jobs running
			time.Sleep(pool.interval * time.Millisecond)
		} else {
			break
		}
	}
	return job
}
func (pool *Pool) Status() stats {
	stats_pipe := make(chan stats)
	if pool.supervisor_started {
		pool.stats_wanted_pipe <- stats_pipe
		return <-stats_pipe
	}
	// the supervisor wasn't started so we return a zeroed structure
	return stats{}
}
func WrapHTTPHandlerFunc(f http.HandlerFunc) HandlerFunc {
	newF := func(ctx *Context) error {
		f(ctx.Response, ctx.Request)
		return nil
	}
	return newF
}
func WebSocketHandlerFunc(f func(ws *websocket.Conn)) HandlerFunc {
	h := websocket.Handler(f)
	return WrapHTTPHandlerFunc(h.ServeHTTP)
}
func StaticFile(filename string, contentType string) staticFile {
	if contentType == "" {
		contentType = mime.TypeByExtension(path.Ext(filename))
	}
	header := make(http.Header)
	header.Set("Content-Type", contentType)
	return staticFile{filename, header}
}
func PreloadFile(filename string, contentType string) (preloadFile, error) {
	body, err := ioutil.ReadFile(filename)
	if err != nil {
		return preloadFile{}, err
	}
	if contentType == "" {
		contentType = mime.TypeByExtension(path.Ext(filename))
	}
	header := make(http.Header)
	header.Set("Content-Type", contentType)
	return preloadFile{body, header}, nil
}
func InitHtmlTemplates(pattern string) (err error) {
	htmlTemp.Template, err = html.ParseGlob(pattern)
	return
}
func InitTextTemplates(pattern string) (err error) {
	textTemp.Template, err = text.ParseGlob(pattern)
	return nil
}
func Html(name, contentType, charSet string) template {
	if htmlTemp.Template == nil {
		panic("Function `InitHtmlTemplates` should be called first.")
	}
	if contentType == "" {
		contentType = ContentTypeHTML
	}
	if charSet == "" {
		charSet = CharSetUTF8
	}
	header := make(http.Header)
	header.Set("Content-Type",
		fmt.Sprintf("%s; charset=%s", contentType, charSet))
	return template{&htmlTemp, name, header}
}
func Text(name, contentType, charSet string) template {
	if textTemp.Template == nil {
		panic("Function `InitTextTemplates` should be called first.")
	}
	if contentType == "" {
		contentType = ContentTypePlain
	}
	if charSet == "" {
		charSet = CharSetUTF8
	}
	header := make(http.Header)
	header.Set("Content-Type",
		fmt.Sprintf("%s; charset=%s", contentType, charSet))
	return template{&textTemp, name, header}
}
func InitWatcher(pattern string, f func(string) error, ef func(error)) (err error) {
	if err = f(pattern); err != nil {
		return
	}
	if watcher.Watcher == nil {
		watcher.Watcher, err = fsnotify.NewWatcher()
		if err != nil {
			return
		}
		watcher.closer = make(chan bool)
	}
	go func() {
		atomic.AddUint32(&watcher.count, 1)
		for {
			select {
			case <-watcher.Events:
				if err := f(pattern); err != nil {
					ef(err)
				}
			case err := <-watcher.Errors:
				if ef != nil {
					ef(err)
				}
			case <-watcher.closer:
				break
			}
		}
	}()

	var matches []string
	matches, err = filepath.Glob(pattern)
	if err != nil {
		return
	}
	for _, v := range matches {
		if err = watcher.Add(v); err != nil {
			return
		}
	}
	return
}
func CloseWatcher() error {
	for i := uint32(0); i < watcher.count; i++ {
		watcher.closer <- true
	}
	return watcher.Close()
}
func (rs *Routers) Find(path string) (url.Values, HandlerFunc, view.View) {
	defer rs.RUnlock()
	rs.RLock()
	if s, ok := rs.s[path]; ok {
		return nil, s.h, s.v
	}
	for e := rs.l.Front(); e != nil; e = e.Next() {
		s := e.Value.(struct {
			r router.Router
			v view.View
			h HandlerFunc
		})
		if params, ok := s.r.Match(path); ok {
			return params, s.h, s.v
		}
	}
	return nil, nil, nil
}
func (rs *Routers) Add(r router.Router, h HandlerFunc, v view.View) {
	defer rs.Unlock()
	rs.Lock()
	s := struct {
		r router.Router
		v view.View
		h HandlerFunc
	}{r, v, h}
	// simple will full-match the path
	if sr, ok := r.(*router.Base); ok {
		rs.s[sr.Path] = s
		return
	}
	rs.l.PushFront(s)
}
func NewRouters() *Routers {
	return &Routers{
		s: make(map[string]struct {
			r router.Router
			v view.View
			h HandlerFunc
		}),
		l: list.New(),
	}
}
func NewServerMux() (mux *ServerMux) {
	nf := struct {
		View    view.View
		Handler HandlerFunc
	}{view.Simple(view.ContentTypePlain, view.CharSetUTF8), defaultNotFound}
	return &ServerMux{NewRouters(), nil, nil, nil, nf}
}
func (mux *ServerMux) err(err error) {
	if mux.ErrorHandle != nil {
		mux.ErrorHandle(err)
	}
}
func (mux *ServerMux) HandleFunc(r router.Router, h HandlerFunc, v view.View) {
	mux.routers.Add(r, h, v)
}
func (mux *ServerMux) handleError(ctx *Context, err error) bool {
	if err == nil {
		return false
	}
	if e, ok := err.(Error); ok {
		ctx.Response.Status = e.Status
		ctx.Response.Data = e
		return true
	}
	if ctx.Response.Status == http.StatusOK {
		ctx.Response.Status = http.StatusInternalServerError
	}
	ctx.Response.Data = err.Error()
	mux.err(err)
	return true
}
func (ctx *Context) Redirect(code int, url string) {
	ctx.Response.Status = code
	ctx.Response.Data = url
}
func (mux *ServerMux) InitPProf(prefix string) {
	if prefix == "" {
		prefix = "/debug/pprof"
	}
	mux.HandleFunc(router.Wildcard(fmt.Sprintf("%s/*", prefix)),
		WrapHTTPHandlerFunc(pprofIndex(prefix)), nil)
	mux.HandleFunc(router.Simple(fmt.Sprintf("%s/cmdline", prefix)),
		WrapHTTPHandlerFunc(http.HandlerFunc(pprof.Cmdline)), nil)
	mux.HandleFunc(router.Simple(fmt.Sprintf("%s/profile", prefix)),
		WrapHTTPHandlerFunc(http.HandlerFunc(pprof.Profile)), nil)
	mux.HandleFunc(router.Simple(fmt.Sprintf("%s/symbol", prefix)),
		WrapHTTPHandlerFunc(http.HandlerFunc(pprof.Symbol)), nil)
}
func (ctx *Context) StartSession(f session.FactoryFunc) (err error) {
	ctx.Session, err = f(ctx.Response, ctx.Request)
	return
}
func combinations(list []int, select_num, buf int) (c chan []int) {
	c = make(chan []int, buf)
	go func() {
		defer close(c)
		switch {
		case select_num == 0:
			c <- []int{}
		case select_num == len(list):
			c <- list
		case len(list) < select_num:
			return
		default:
			for i := 0; i < len(list); i++ {
				for sub_comb := range combinations(list[i+1:], select_num-1, buf) {
					c <- append([]int{list[i]}, sub_comb...)
				}
			}
		}
	}()
	return
}
func repeated_combinations(list []int, select_num, buf int) (c chan []int) {
	c = make(chan []int, buf)
	go func() {
		defer close(c)
		if select_num == 1 {
			for v := range list {
				c <- []int{v}
			}
			return
		}
		for i := 0; i < len(list); i++ {
			for sub_comb := range repeated_combinations(list[i:], select_num-1, buf) {
				c <- append([]int{list[i]}, sub_comb...)
			}
		}
	}()
	return
}
func permutations(list []int, select_num, buf int) (c chan []int) {
	c = make(chan []int, buf)
	go func() {
		defer close(c)
		switch select_num {
		case 1:
			for _, v := range list {
				c <- []int{v}
			}
			return
		case 0:
			return
		case len(list):
			for i := 0; i < len(list); i++ {
				top, sub_list := pop(list, i)
				for perm := range permutations(sub_list, select_num-1, buf) {
					c <- append([]int{top}, perm...)
				}
			}
		default:
			for comb := range combinations(list, select_num, buf) {
				for perm := range permutations(comb, select_num, buf) {
					c <- perm
				}
			}
		}
	}()
	return
}
func repeated_permutations(list []int, select_num, buf int) (c chan []int) {
	c = make(chan []int, buf)
	go func() {
		defer close(c)
		switch select_num {
		case 1:
			for _, v := range list {
				c <- []int{v}
			}
		default:
			for i := 0; i < len(list); i++ {
				for perm := range repeated_permutations(list, select_num-1, buf) {
					c <- append([]int{list[i]}, perm...)
				}
			}
		}
	}()
	return
}
func gformat(format string, args map[string]interface{}) (string, []interface{}) {
	// holder for new format string - capacity as length of provided string
	// should be enough not to resize during appending, since expected length
	// of new format is smaller then provided one (names are removed)
	var new_format = make([]rune, 0, len(format))

	// flag that indicates if current place in format string in inside { }
	var in_format = false

	// flag that indicates if current place is format string in inside { } and after :
	var in_args = false

	var previousChar rune

	// temp slice for holding name in current format
	var current_name_runes = make([]rune, 0, 10)

	// temp slice for holding args in current format
	var current_args_runes = make([]rune, 0, 10)

	var new_format_params []interface{}

	for i, ch := range format {
		if i > 0 {
			previousChar = rune(format[i-1])
		}
		switch ch {
		case '{':
			if in_format && previousChar == '{' {
				in_format = false
				new_format = append(new_format, ch)
				break
			}
			in_format = true
		case '}':
			if !in_format {
				if previousChar == '}' {
					new_format = append(new_format, ch)
					break
				}
				// what to do if not in_format and only single } appears?
				break
			}
			if in_format {
				if len(current_args_runes) > 0 {
					// append formatting arguments to new_format directly
					new_format = append(new_format, current_args_runes...)
				} else {
					// if no arguments are supplied, use default ones
					new_format = append(new_format, defaultFormat...)
				}
				// reset format args for new iteration
				current_args_runes = current_args_runes[0:0]
			}

			var name string
			if len(current_name_runes) == 0 {
				name = "EMPTY_PLACEHOLDER"
			} else {
				name = string(current_name_runes)
			}
			// reset name runes for next iteration
			current_name_runes = current_name_runes[0:0]

			// get value from provided args and append it to new_format_args
			val, ok := args[name]
			if !ok {
				val = fmt.Sprintf("%%MISSING=%s", name)
			}
			new_format_params = append(new_format_params, val)

			// reset flags
			in_format = false
			in_args = false
		case ':':
			if in_format {
				in_args = true
			}
		default:
			if in_format {
				if in_args {
					current_args_runes = append(current_args_runes, ch)
				} else {
					current_name_runes = append(current_name_runes, ch)
				}
			} else {
				new_format = append(new_format, ch)
			}
		}
	}
	return string(new_format), new_format_params
}
func Errorm(format string, args map[string]interface{}) error {
	f, a := gformat(format, args)
	return fmt.Errorf(f, a...)
}
func Fprintm(w io.Writer, format string, args map[string]interface{}) (n int, err error) {
	f, a := gformat(format, args)
	return fmt.Fprintf(w, f, a...)
}
func Printm(format string, args map[string]interface{}) (n int, err error) {
	f, a := gformat(format, args)
	return fmt.Printf(f, a...)
}
func Sprintm(format string, args map[string]interface{}) string {
	f, a := gformat(format, args)
	return fmt.Sprintf(f, a...)
}
func (p *PasswordStrengthRequirements) Validate(password string) (bool, string) {
	reqs := MakeRequirements(password)
	if p.MaximumTotalLength > 0 && reqs.MaximumTotalLength > p.MaximumTotalLength {
		return false, "password is too long"
	}
	if reqs.MinimumTotalLength < p.MinimumTotalLength {
		return false, "password is too short"
	}
	if reqs.Digits < p.Digits {
		return false, "password has too few digits"
	}
	if reqs.Punctuation < p.Punctuation {
		return false, "password has too few punctuation characters"
	}
	if reqs.Uppercase < p.Uppercase {
		return false, "password has too few uppercase characters"
	}
	return true, ""
}
func MakeRequirements(password string) PasswordStrengthRequirements {
	pwd := []byte(password)
	reqs := PasswordStrengthRequirements{}
	reqs.MaximumTotalLength = len(password)
	reqs.MinimumTotalLength = len(password)
	for i := range pwd {
		switch {
		case unicode.IsDigit(rune(pwd[i])):
			reqs.Digits++
		case unicode.IsUpper(rune(pwd[i])):
			reqs.Uppercase++
		case unicode.IsPunct(rune(pwd[i])):
			reqs.Punctuation++
		}
	}
	return reqs
}
func (p *PasswordStrengthRequirements) sanityCheck() (bool, string) {
	if p.MaximumTotalLength == 0 {
		return true, ""
	}
	if p.MaximumTotalLength < p.MinimumTotalLength {
		return false, "maximum total length is less than minimum total length"
	}
	if p.MaximumTotalLength < p.Digits {
		return false, "maximum required digits is more than maximum total length"
	}
	if p.MaximumTotalLength < p.Punctuation {
		return false, "maximum required punctuation is more than maximum total length"
	}
	if p.MaximumTotalLength < p.Uppercase {
		return false, "maximum required uppercase characters is more than maximum total length"
	}
	if p.MaximumTotalLength < p.Digits+p.Uppercase+p.Punctuation {
		return false, "maximum required digits + uppercase + punctuation is more than maximum total length"
	}
	return true, ""
}
func (g Garbler) password(req PasswordStrengthRequirements) (string, error) {
	//Step 1: Figure out settings
	letters := 0
	mustGarble := 0
	switch {
	case req.MaximumTotalLength > 0 && req.MaximumTotalLength > 6:
		letters = req.MaximumTotalLength - req.Digits - req.Punctuation
	case req.MaximumTotalLength > 0 && req.MaximumTotalLength <= 6:
		letters = req.MaximumTotalLength - req.Punctuation
		mustGarble = req.Digits
	case req.MinimumTotalLength > req.Digits+req.Punctuation+6:
		letters = req.MinimumTotalLength - req.Digits - req.Punctuation
	default:
		letters = req.MinimumTotalLength
	}
	if req.Uppercase > letters {
		letters = req.Uppercase
	}
	password := g.garbledSequence(letters, mustGarble)
	password = g.uppercase(password, req.Uppercase)
	password = g.addNums(password, req.Digits-mustGarble)
	password = g.punctuate(password, req.Punctuation)
	return password, nil
}
func NewPassword(reqs *PasswordStrengthRequirements) (string, error) {
	if reqs == nil {
		reqs = &Medium
	}
	if ok, problems := reqs.sanityCheck(); !ok {
		return "", errors.New("requirements failed validation: " + problems)
	}
	e := Garbler{}
	return e.password(*reqs)
}
func NewPasswords(reqs *PasswordStrengthRequirements, n int) ([]string, error) {
	var err error
	if reqs == nil {
		reqs = &Medium
	}
	if ok, problems := reqs.sanityCheck(); !ok {
		return nil, errors.New("requirements failed validation: " + problems)
	}
	e := Garbler{}
	passes := make([]string, n, n)
	for i := 0; i < n; i++ {
		passes[i], err = e.password(*reqs)
		if err != nil {
			return nil, err
		}
	}
	return passes, nil
}
func (g Garbler) addNums(p string, numDigits int) string {
	if numDigits <= 0 {
		return p
	}
	ret := p
	remaining := numDigits
	for remaining > 10 {
		ret += fmt.Sprintf("%d", pow(10, 9)+randInt(pow(10, 10)-pow(10, 9)))
		remaining -= 10
	}
	ret += fmt.Sprintf("%d", pow(10, remaining-1)+randInt(pow(10, remaining)-pow(10, remaining-1)))

	return ret
}
func (g Garbler) punctuate(p string, numPunc int) string {
	if numPunc <= 0 {
		return p
	}
	ret := p
	for i := 0; i < numPunc; i++ {
		if i%2 == 0 {
			ret += string(Punctuation[randInt(len(Punctuation))])
		} else {
			ret = string(Punctuation[randInt(len(Punctuation))]) + ret
		}
	}
	return ret
}
func deprecated_init() {
	// if piping from stdin we can just exit
	// and use the default Stdin value
	stat, _ := os.Stdin.Stat()
	if (stat.Mode() & os.ModeCharDevice) == 0 {
		return
	}

	// check for params after the double dash
	// in the command string
	for i, argv := range os.Args {
		if argv == "--" {
			arg := os.Args[i+1]
			buf := bytes.NewBufferString(arg)
			Stdin = NewParamSet(buf)
			return
		}
	}

	// else use the first variable in the list
	if len(os.Args) > 1 {
		buf := bytes.NewBufferString(os.Args[1])
		Stdin = NewParamSet(buf)
	}
}
func (p ParamSet) Param(name string, value interface{}) {
	p.params[name] = value
}
func (p ParamSet) Parse() error {
	raw := map[string]json.RawMessage{}
	err := json.NewDecoder(p.reader).Decode(&raw)
	if err != nil {
		return err
	}

	for key, val := range p.params {
		data, ok := raw[key]
		if !ok {
			continue
		}
		err := json.Unmarshal(data, val)
		if err != nil {
			return fmt.Errorf("Unable to unarmshal %s. %s", key, err)
		}
	}

	return nil
}
func (p ParamSet) Unmarshal(v interface{}) error {
	return json.NewDecoder(p.reader).Decode(v)
}
func GetDefaultHTTPClient(timeout time.Duration) IHTTPClient {
	client := http.Client{
		Timeout: timeout,
	}
	return IHTTPClient(&client)
}
func (p Pushy) DeviceInfo(deviceID string) (*DeviceInfo, *Error, error) {
	url := fmt.Sprintf("%s/devices/%s?api_key=%s", p.APIEndpoint, deviceID, p.APIToken)
	var errResponse *Error
	var info *DeviceInfo
	err := get(p.httpClient, url, &info, &errResponse)
	return info, errResponse, err
}
func (p *Pushy) DevicePresence(deviceID ...string) (*DevicePresenceResponse, *Error, error) {
	url := fmt.Sprintf("%s/devices/presence?api_key=%s", p.APIEndpoint, p.APIToken)
	var devicePresenceResponse *DevicePresenceResponse
	var pushyErr *Error
	err := post(p.httpClient, url, DevicePresenceRequest{Tokens: deviceID}, &devicePresenceResponse, &pushyErr)
	return devicePresenceResponse, pushyErr, err
}
func (p *Pushy) NotificationStatus(pushID string) (*NotificationStatus, *Error, error) {
	url := fmt.Sprintf("%s/pushes/%s?api_key=%s", p.APIEndpoint, pushID, p.APIToken)
	var errResponse *Error
	var status *NotificationStatus
	err := get(p.httpClient, url, &status, &errResponse)
	return status, errResponse, err
}
func (p *Pushy) DeleteNotification(pushID string) (*SimpleSuccess, *Error, error) {
	url := fmt.Sprintf("%s/pushes/%s?api_key=%s", p.APIEndpoint, pushID, p.APIToken)
	var success *SimpleSuccess
	var pushyErr *Error
	err := del(p.httpClient, url, &success, &pushyErr)
	return success, pushyErr, err
}
func (p *Pushy) NotifyDevice(request SendNotificationRequest) (*NotificationResponse, *Error, error) {
	url := fmt.Sprintf("%s/push?api_key=%s", p.APIEndpoint, p.APIToken)
	var success *NotificationResponse
	var pushyErr *Error
	err := post(p.httpClient, url, request, &success, &pushyErr)
	return success, pushyErr, err
}
func Assert(t Tester, b bool, message ...interface{}) {
	if !b {
		pc, file, line, _ := runtime.Caller(1)
		caller_func_info := runtime.FuncForPC(pc)

		error_string := fmt.Sprintf("\n\rASSERT:\tfunc (%s) 0x%x\n\r\tFile %s:%d",
			caller_func_info.Name(),
			pc,
			file,
			line)

		if len(message) > 0 {
			error_string += fmt.Sprintf("\n\r\tInfo: %+v", message)
		}

		t.Errorf(error_string)
		t.FailNow()
	}
}
func CreateFile(filename string, size int64) error {

	buf := make([]byte, size)

	// Create the file store some data
	fp, err := os.Create(filename)
	if err != nil {
		return err
	}

	// Write the buffer
	_, err = fp.Write(buf)

	// Cleanup
	fp.Close()

	return err
}
func (f *FormErrors) AddError(e string) {
	f.Errors = append(f.Errors, e)
}
func (f *FormErrors) AddFieldError(field, e string) {
	if f.FieldErrors == nil {
		f.FieldErrors = map[string][]string{}
	}
	if _, ok := f.FieldErrors[field]; !ok {
		f.FieldErrors[field] = []string{}
	}
	f.FieldErrors[field] = append(f.FieldErrors[field], e)
}
func (f FormErrors) HasErrors() bool {
	if len(f.Errors) > 0 {
		return true
	}
	for _, v := range f.FieldErrors {
		if len(v) > 0 {
			return true
		}
	}
	return false
}
func NewError(e string) FormErrors {
	errors := FormErrors{}
	errors.AddError(e)
	return errors
}
func NewFieldError(field, e string) FormErrors {
	errors := FormErrors{}
	errors.AddFieldError(field, e)
	return errors
}
func ChainHandlers(handlers ...func(http.Handler) http.Handler) (h http.Handler) {
	for i := len(handlers) - 1; i >= 0; i-- {
		h = handlers[i](h)
	}
	return
}
func FinalHandler(h http.Handler) func(http.Handler) http.Handler {
	return func(_ http.Handler) http.Handler {
		return h
	}
}
func (s MD5Hasher) Hash(reader io.Reader) (string, error) {
	hash := md5.New()
	if _, err := io.Copy(hash, reader); err != nil {
		return "", err
	}
	h := hash.Sum(nil)
	if len(h) < s.HashLength {
		return "", nil
	}
	return strings.TrimRight(hex.EncodeToString(h)[:s.HashLength], "="), nil
}
func (s MD5Hasher) IsHash(h string) bool {
	if len(h) != s.HashLength {
		return false
	}
	var found bool
	for _, c := range h {
		found = false
		for _, m := range hexChars {
			if c == m {
				found = true
				break
			}
		}
		if !found {
			return false
		}
	}
	return true
}
func WithBaseDir(dir string) Option {
	return func(o *Options) {
		o.fileFindFunc = func(f string) string {
			return filepath.Join(dir, f)
		}
	}
}
func WithFileFindFunc(fn func(filename string) string) Option {
	return func(o *Options) { o.fileFindFunc = fn }
}
func WithTemplateFromFiles(name string, files ...string) Option {
	return func(o *Options) { o.files[name] = files }
}
func WithTemplatesFromFiles(ts map[string][]string) Option {
	return func(o *Options) {
		for name, files := range ts {
			o.files[name] = files
		}
	}
}
func WithTemplateFromStrings(name string, strings ...string) Option {
	return func(o *Options) { o.strings[name] = strings }
}
func WithTemplatesFromStrings(ts map[string][]string) Option {
	return func(o *Options) {
		for name, strings := range ts {
			o.strings[name] = strings
		}
	}
}
func WithFunction(name string, fn interface{}) Option {
	return func(o *Options) { o.functions[name] = fn }
}
func WithFunctions(fns template.FuncMap) Option {
	return func(o *Options) {
		for name, fn := range fns {
			o.functions[name] = fn
		}
	}
}
func WithDelims(open, close string) Option {
	return func(o *Options) {
		o.delimOpen = open
		o.delimClose = close
	}
}
func New(opts ...Option) (t *Templates, err error) {
	functions := template.FuncMap{}
	for name, fn := range defaultFunctions {
		functions[name] = fn
	}
	o := &Options{
		fileFindFunc: func(f string) string {
			return f
		},
		fileReadFunc: ioutil.ReadFile,
		files:        map[string][]string{},
		functions:    functions,
		delimOpen:    "{{",
		delimClose:   "}}",
		logf:         log.Printf,
	}
	for _, opt := range opts {
		opt(o)
	}

	t = &Templates{
		templates:   map[string]*template.Template{},
		contentType: o.contentType,
		logf:        o.logf,
	}
	for name, strings := range o.strings {
		tpl, err := parseStrings(template.New("").Funcs(o.functions).Delims(o.delimOpen, o.delimClose), strings...)
		if err != nil {
			return nil, err
		}
		t.templates[name] = tpl
	}
	for name, files := range o.files {
		fs := []string{}
		for _, f := range files {
			fs = append(fs, o.fileFindFunc(f))
		}
		tpl, err := parseFiles(o.fileReadFunc, template.New("").Funcs(o.functions).Delims(o.delimOpen, o.delimClose), fs...)
		if err != nil {
			return nil, err
		}
		t.templates[name] = tpl
	}
	return
}
func (t Templates) RespondWithStatus(w http.ResponseWriter, name string, data interface{}, status int) {
	buf := bytes.Buffer{}
	tpl, ok := t.templates[name]
	if !ok {
		panic(&Error{Err: ErrUnknownTemplate, Template: name})
	}
	if err := tpl.Execute(&buf, data); err != nil {
		panic(err)
	}
	if t.contentType != "" {
		w.Header().Set("Content-Type", t.contentType)
	}
	if status > 0 {
		w.WriteHeader(status)
	}
	if _, err := buf.WriteTo(w); err != nil {
		t.logf("respond %q: %v", name, err)
	}
}
func (t Templates) RespondTemplate(w http.ResponseWriter, name, templateName string, data interface{}) {
	t.RespondTemplateWithStatus(w, name, templateName, data, 0)
}
func (t Templates) Respond(w http.ResponseWriter, name string, data interface{}) {
	t.RespondWithStatus(w, name, data, 0)
}
func (t Templates) RenderTemplate(name, templateName string, data interface{}) (s string, err error) {
	buf := bytes.Buffer{}
	tpl, ok := t.templates[name]
	if !ok {
		return "", &Error{Err: ErrUnknownTemplate, Template: name}
	}
	if err := tpl.ExecuteTemplate(&buf, templateName, data); err != nil {
		return "", err
	}
	return buf.String(), nil
}
func New(handler http.Handler, opts ...Option) (s *Server) {
	o := &Options{}
	for _, opt := range opts {
		opt(o)
	}
	s = &Server{
		Server: &h2quic.Server{
			Server: &http.Server{
				Handler:   handler,
				TLSConfig: o.tlsConfig,
			},
		},
	}
	return
}
func (s *Server) ServeUDP(conn *net.UDPConn) (err error) {
	s.Server.Server.Addr = conn.LocalAddr().String()
	return s.Server.Serve(conn)
}
func (s *Server) Shutdown(_ context.Context) (err error) {
	return s.Server.Close()
}
func (s *Server) QuicHeadersHandler(h http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		s.SetQuicHeaders(w.Header())
		h.ServeHTTP(w, r)
	})
}
func GetRequestIPs(r *http.Request) string {
	ip, _, err := net.SplitHostPort(r.RemoteAddr)
	if err != nil {
		ip = r.RemoteAddr
	}
	ips := []string{ip}
	xfr := r.Header.Get("X-Forwarded-For")
	if xfr != "" {
		ips = append(ips, xfr)
	}
	xri := r.Header.Get("X-Real-Ip")
	if xri != "" {
		ips = append(ips, xri)
	}
	return strings.Join(ips, ", ")
}
func DomainRedirectHandler(h http.Handler, domain, httpsPort string) http.Handler {
	if domain == "" && httpsPort == "" {
		return h
	}

	scheme := "http"
	port := ""
	if httpsPort != "" {
		if _, err := strconv.Atoi(httpsPort); err == nil {
			scheme = "https"
			port = httpsPort
		}
		if _, p, err := net.SplitHostPort(httpsPort); err == nil {
			scheme = "https"
			port = p
		}
	}
	if port == "443" {
		port = ""
	}
	var altDomain string
	if strings.HasPrefix("www.", domain) {
		altDomain = strings.TrimPrefix(domain, "www.")
	} else {
		altDomain = "www." + domain
	}
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		d, p, err := net.SplitHostPort(r.Host)
		if err != nil {
			d = r.Host
		}
		rs := r.URL.Scheme
		if fs := r.Header.Get("X-Forwarded-Proto"); fs != "" {
			rs = strings.ToLower(fs)
		}
		s := scheme
		if rs == "https" {
			s = "https"
		}
		if d == domain && rs == s {
			h.ServeHTTP(w, r)
			return
		}
		switch {
		case s == "http" && p == "80":
			p = ""
		case s == "https" && p == "443":
			p = ""
		case port != "":
			p = ":" + port
		case p != "":
			p = ":" + p
		}
		if d == altDomain {
			http.Redirect(w, r, strings.Join([]string{s, "://", domain, p, r.RequestURI}, ""), http.StatusMovedPermanently)
			return
		}
		http.Redirect(w, r, strings.Join([]string{s, "://", domain, p, r.RequestURI}, ""), http.StatusFound)
	})
}
func New(opts ...Option) (s *Servers) {
	s = &Servers{
		logger:  stdLogger{},
		recover: func() {},
	}
	for _, opt := range opts {
		opt(s)
	}
	return
}
func (s *Servers) Add(name, address string, srv Server) {
	s.mu.Lock()
	s.servers = append(s.servers, &server{
		Server:  srv,
		name:    name,
		address: address,
	})
	s.mu.Unlock()
}
func (s *Servers) TCPAddr(name string) (a *net.TCPAddr) {
	s.mu.Lock()
	defer s.mu.Unlock()

	for _, srv := range s.servers {
		if srv.name == name {
			return srv.tcpAddr
		}
	}
	return nil
}
func (s *Servers) UDPAddr(name string) (a *net.UDPAddr) {
	s.mu.Lock()
	defer s.mu.Unlock()

	for _, srv := range s.servers {
		if srv.name == name {
			return srv.udpAddr
		}
	}
	return nil
}
func (s *Servers) Close() {
	wg := &sync.WaitGroup{}
	for _, srv := range s.servers {
		wg.Add(1)
		go func(srv *server) {
			defer s.recover()
			defer wg.Done()

			s.logger.Infof("%s closing", srv.label())
			if err := srv.Close(); err != nil {
				s.logger.Errorf("%s close: %v", srv.label(), err)
			}
		}(srv)
	}
	wg.Wait()
	return
}
func (s *Servers) Shutdown(ctx context.Context) {
	wg := &sync.WaitGroup{}
	for _, srv := range s.servers {
		wg.Add(1)
		go func(srv *server) {
			defer s.recover()
			defer wg.Done()

			s.logger.Infof("%s shutting down", srv.label())
			if err := srv.Shutdown(ctx); err != nil {
				s.logger.Errorf("%s shutdown: %v", srv.label(), err)
			}
		}(srv)
	}
	wg.Wait()
	return
}
func (l TLSListener) Accept() (net.Conn, error) {
	c, err := l.AcceptTCP()
	if err != nil {
		return nil, err
	}
	c.SetKeepAlive(true)
	c.SetKeepAlivePeriod(3 * time.Minute)

	b := make([]byte, 1)
	_, err = c.Read(b)
	if err != nil {
		c.Close()
		if err != io.EOF {
			return nil, err
		}
	}

	con := &conn{
		Conn: c,
		b:    b[0],
		e:    err,
		f:    true,
	}

	if b[0] == 22 {
		return tls.Server(con, l.TLSConfig), nil
	}

	return con, nil
}
func NewStaticFilesHandler(h http.Handler, prefix string, fs http.FileSystem) http.Handler {
	fileserver := http.StripPrefix(prefix, http.FileServer(fs))
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		filename := strings.TrimPrefix(r.URL.Path, prefix)
		_, err := fs.Open(filename)
		if err != nil {
			h.ServeHTTP(w, r)
			return
		}
		fileserver.ServeHTTP(w, r)
	})
}
func (h AuthHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	valid, entity, err := h.authenticate(r)
	if err != nil {
		h.error(w, r, err)
		return
	}
	if h.PostAuthFunc != nil {
		rr, err := h.PostAuthFunc(w, r, valid, entity)
		if err != nil {
			h.error(w, r, err)
			return
		}
		if rr != nil {
			r = rr
		}
	}
	if !valid {
		h.unauthorized(w, r)
		return
	}

	if h.Handler != nil {
		h.Handler.ServeHTTP(w, r)
	}
}
func (o Options) MarshalJSON() ([]byte, error) {
	return json.Marshal(optionsJSON{
		Timeout:             marshal.Duration(o.Timeout),
		KeepAlive:           marshal.Duration(o.KeepAlive),
		TLSHandshakeTimeout: marshal.Duration(o.TLSHandshakeTimeout),
		TLSSkipVerify:       o.TLSSkipVerify,
		RetryTimeMax:        marshal.Duration(o.RetryTimeMax),
		RetrySleepMax:       marshal.Duration(o.RetrySleepMax),
		RetrySleepBase:      marshal.Duration(o.RetrySleepBase),
	})
}
func (o *Options) UnmarshalJSON(data []byte) error {
	v := &optionsJSON{}
	if err := json.Unmarshal(data, v); err != nil {
		return err
	}
	*o = Options{
		Timeout:             v.Timeout.Duration(),
		KeepAlive:           v.KeepAlive.Duration(),
		TLSHandshakeTimeout: v.TLSHandshakeTimeout.Duration(),
		TLSSkipVerify:       v.TLSSkipVerify,
		RetryTimeMax:        v.RetryTimeMax.Duration(),
		RetrySleepMax:       v.RetrySleepMax.Duration(),
		RetrySleepBase:      v.RetrySleepBase.Duration(),
	}
	return nil
}
func (o Options) MarshalYAML() (interface{}, error) {
	return optionsJSON{
		Timeout:             marshal.Duration(o.Timeout),
		KeepAlive:           marshal.Duration(o.KeepAlive),
		TLSHandshakeTimeout: marshal.Duration(o.TLSHandshakeTimeout),
		TLSSkipVerify:       o.TLSSkipVerify,
		RetryTimeMax:        marshal.Duration(o.RetryTimeMax),
		RetrySleepMax:       marshal.Duration(o.RetrySleepMax),
		RetrySleepBase:      marshal.Duration(o.RetrySleepBase),
	}, nil
}
func (o *Options) UnmarshalYAML(unmarshal func(interface{}) error) error {
	v := &optionsJSON{}
	if err := unmarshal(v); err != nil {
		return err
	}
	*o = Options{
		Timeout:             v.Timeout.Duration(),
		KeepAlive:           v.KeepAlive.Duration(),
		TLSHandshakeTimeout: v.TLSHandshakeTimeout.Duration(),
		TLSSkipVerify:       v.TLSSkipVerify,
		RetryTimeMax:        v.RetryTimeMax.Duration(),
		RetrySleepMax:       v.RetrySleepMax.Duration(),
		RetrySleepBase:      v.RetrySleepBase.Duration(),
	}
	return nil
}
func NewHandler(h http.Handler, logger *logging.Logger) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		startTime := time.Now()
		rl := &responseLogger{w, 0, 0}
		h.ServeHTTP(rl, r)
		referrer := r.Referer()
		if referrer == "" {
			referrer = "-"
		}
		userAgent := r.UserAgent()
		if userAgent == "" {
			userAgent = "-"
		}
		ips := []string{}
		xfr := r.Header.Get("X-Forwarded-For")
		if xfr != "" {
			ips = append(ips, xfr)
		}
		xri := r.Header.Get("X-Real-Ip")
		if xri != "" {
			ips = append(ips, xri)
		}
		xips := "-"
		if len(ips) > 0 {
			xips = strings.Join(ips, ", ")
		}
		var level logging.Level
		switch {
		case rl.status >= 500:
			level = logging.ERROR
		case rl.status >= 400:
			level = logging.WARNING
		case rl.status >= 300:
			level = logging.INFO
		case rl.status >= 200:
			level = logging.INFO
		default:
			level = logging.DEBUG
		}
		logger.Logf(level, "%s \"%s\" \"%v %s %v\" %d %d %f \"%s\" \"%s\"", r.RemoteAddr, xips, r.Method, r.RequestURI, r.Proto, rl.status, rl.size, time.Since(startTime).Seconds(), referrer, userAgent)
	})
}
func WithPanicResponse(body, contentType string) Option {
	return func(o *Handler) {
		o.panicBody = body
		o.panicContentType = contentType
	}
}
func WithPanicResponseHandler(h http.Handler) Option {
	return func(o *Handler) { o.panicResponseHandler = h }
}
func New(handler http.Handler, options ...Option) (h *Handler) {
	h = &Handler{
		handler: handler,
		logf:    log.Printf,
	}
	for _, option := range options {
		option(h)
	}
	return
}
func (h Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	defer func() {
		if err := recover(); err != nil {
			debugMsg := fmt.Sprintf(
				"%s\n\n%#v\n\n%#v",
				debug.Stack(),
				r.URL,
				r.Header,
			)
			if h.label != "" {
				debugMsg = h.label + "\n\n" + debugMsg
			}
			h.logf("http recovery handler: %s %s: %s\n%s", r.Method, r.URL.String(), err, debugMsg)

			if h.notifier != nil {
				go func() {
					defer func() {
						if err := recover(); err != nil {
							h.logf("http recovery handler: notify panic: %v", err)
						}
					}()

					if err := h.notifier.Notify(
						fmt.Sprint(
							"Panic ",
							r.Method,
							" ",
							r.URL.String(),
							": ", err,
						),
						debugMsg,
					); err != nil {
						h.logf("http recovery handler: notify: %v", err)
					}
				}()
			}

			if h.panicResponseHandler != nil {
				h.panicResponseHandler.ServeHTTP(w, r)
				return
			}

			if h.panicContentType != "" {
				w.Header().Set("Content-Type", h.panicContentType)
			}
			w.WriteHeader(http.StatusInternalServerError)
			if h.panicBody != "" {
				fmt.Fprintln(w, h.panicBody)
			}
		}
	}()

	h.handler.ServeHTTP(w, r)
}
func NewContextFunc(m map[string]interface{}) func(string) interface{} {
	return func(key string) interface{} {
		if value, ok := m[key]; ok {
			return value
		}
		return nil
	}
}
func NewMapErrorRegistry(errors map[int]error, handlers map[int]func(body []byte) error) *MapErrorRegistry {
	if errors == nil {
		errors = map[int]error{}
	}
	if handlers == nil {
		handlers = map[int]func(body []byte) error{}
	}
	return &MapErrorRegistry{
		errors:   errors,
		handlers: handlers,
	}
}
func (r *MapErrorRegistry) AddError(code int, err error) error {
	if _, ok := r.errors[code]; ok {
		return ErrErrorAlreadyRegistered
	}
	if _, ok := r.handlers[code]; ok {
		return ErrErrorAlreadyRegistered
	}
	r.errors[code] = err
	return nil
}
func (r *MapErrorRegistry) AddMessageError(code int, message string) (*Error, error) {
	if _, ok := r.errors[code]; ok {
		return nil, ErrErrorAlreadyRegistered
	}
	if _, ok := r.handlers[code]; ok {
		return nil, ErrErrorAlreadyRegistered
	}
	err := &Error{
		Message: message,
		Code:    code,
	}
	r.errors[code] = err
	return err, nil
}
func (r *MapErrorRegistry) MustAddError(code int, err error) {
	if e := r.AddError(code, err); e != nil {
		panic(e)
	}
}
func (r *MapErrorRegistry) MustAddMessageError(code int, message string) *Error {
	err, e := r.AddMessageError(code, message)
	if e != nil {
		panic(e)
	}
	return err
}
func (r *MapErrorRegistry) AddHandler(code int, handler func(body []byte) error) error {
	if _, ok := r.errors[code]; ok {
		return ErrErrorAlreadyRegistered
	}
	if _, ok := r.handlers[code]; ok {
		return ErrErrorAlreadyRegistered
	}
	r.handlers[code] = handler
	return nil
}
func (r *MapErrorRegistry) MustAddHandler(code int, handler func(body []byte) error) {
	if err := r.AddHandler(code, handler); err != nil {
		panic(err)
	}
}
func (r MapErrorRegistry) Handler(code int) func(body []byte) error {
	return r.handlers[code]
}
func New(endpoint string, errorRegistry ErrorRegistry) *Client {
	return &Client{
		Endpoint:      endpoint,
		ErrorRegistry: errorRegistry,
		KeyHeader:     DefaultKeyHeader,
		HTTPClient:    http.DefaultClient,
	}
}
func (c Client) Request(method, path string, query url.Values, body io.Reader, accept []string) (resp *http.Response, err error) {
	return c.RequestContext(nil, method, path, query, body, accept)
}
func (c Client) JSONContext(ctx context.Context, method, path string, query url.Values, body io.Reader, response interface{}) (err error) {
	resp, err := c.RequestContext(ctx, method, path, query, body, []string{"application/json"})
	if err != nil {
		return
	}
	defer func() {
		io.Copy(ioutil.Discard, resp.Body)
		resp.Body.Close()
	}()

	if response != nil {
		if resp.ContentLength == 0 {
			return errors.New("empty response body")
		}
		contentType := resp.Header.Get("Content-Type")
		if !strings.Contains(contentType, "application/json") {
			return fmt.Errorf("unsupported content type: %s", contentType)
		}
		var body []byte
		body, err = ioutil.ReadAll(resp.Body)
		if err != nil {
			return
		}
		if err = JSONUnmarshal(body, &response); err != nil {
			return
		}
	}

	return
}
func (c Client) StreamContext(ctx context.Context, method, path string, query url.Values, body io.Reader, accept []string) (data io.ReadCloser, contentType string, err error) {
	resp, err := c.RequestContext(ctx, method, path, query, body, accept)
	if err != nil {
		return
	}

	contentType = resp.Header.Get("Content-Type")
	data = resp.Body
	return
}
func (c Client) Stream(method, path string, query url.Values, body io.Reader, accept []string) (data io.ReadCloser, contentType string, err error) {
	return c.StreamContext(nil, method, path, query, body, accept)
}
func JSONUnmarshal(data []byte, v interface{}) error {
	if err := json.Unmarshal(data, v); err != nil {
		switch e := err.(type) {
		case *json.SyntaxError:
			line, col := getLineColFromOffset(data, e.Offset)
			return fmt.Errorf("json %s, line: %d, column: %d", e, line, col)
		case *json.UnmarshalTypeError:
			line, col := getLineColFromOffset(data, e.Offset)
			return fmt.Errorf("expected json %s value but got %s, line: %d, column: %d", e.Type, e.Value, line, col)
		}
		return err
	}
	return nil
}
func (s *Server) ServeTCP(ln net.Listener) (err error) {
	if l, ok := ln.(*net.TCPListener); ok {
		ln = tcpKeepAliveListener{TCPListener: l}
	}
	if s.TLSConfig != nil {
		ln = tls.NewListener(ln, s.TLSConfig)
	}

	err = s.Server.Serve(ln)
	if err == http.ErrServerClosed {
		return nil
	}
	return
}
func (s *Server) ServeTCP(ln net.Listener) (err error) {
	return s.Server.Serve(ln)
}
func (s *Server) Shutdown(ctx context.Context) (err error) {
	s.Server.GracefulStop()
	return
}
func HandleMethods(methods map[string]http.Handler, body string, contentType string, w http.ResponseWriter, r *http.Request) {
	if handler, ok := methods[r.Method]; ok {
		handler.ServeHTTP(w, r)
	} else {
		allow := []string{}
		for k := range methods {
			allow = append(allow, k)
		}
		sort.Strings(allow)
		w.Header().Set("Allow", strings.Join(allow, ", "))
		if r.Method == "OPTIONS" {
			w.WriteHeader(http.StatusOK)
		} else {
			w.Header().Set("Content-Type", contentType)
			w.WriteHeader(http.StatusMethodNotAllowed)
			fmt.Fprintln(w, body)
		}
	}
}
func NewSetHeadersHandler(h http.Handler, headers map[string]string) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		for header, value := range headers {
			w.Header().Set(header, value)
		}
		h.ServeHTTP(w, r)
	})
}
func New(root, dir string, options *Options) *Server {
	if options == nil {
		options = &Options{}
	}
	return &Server{
		Options: *options,

		root: root,
		dir:  dir,

		hashes: map[string]string{},
		mu:     &sync.RWMutex{},
	}
}
func (s *Server) HashedPath(p string) (string, error) {
	if s.Hasher == nil {
		return path.Join(s.root, p), nil
	}
	h, cont, err := s.hash(p)
	if err != nil {
		if cont {
			h, _, err = s.hashFromFilename(p)
		}
		if err != nil {
			return "", err
		}
	}
	return path.Join(s.root, s.hashedPath(p, h)), nil
}
func New(options ...Option) (s *Service) {
	s = &Service{
		logger: stdLogger{},
	}
	for _, option := range options {
		option(s)
	}
	if s.store == nil {
		s.store = NewMemoryStore()
	}
	return
}
func (s Service) HTMLHandler(h http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		on, err := s.store.Status()
		if err != nil {
			s.logger.Errorf("maintenance status: %v", err)
		}
		if on || err != nil {
			if s.HTML.Handler != nil {
				s.HTML.Handler.ServeHTTP(w, r)
				return
			}
			w.Header().Set("Content-Type", HTMLContentType)
			w.WriteHeader(http.StatusServiceUnavailable)
			fmt.Fprintln(w, s.HTML.Body)
			return
		}
		h.ServeHTTP(w, r)
	})
}
func (s Service) Status() (on bool, err error) {
	return s.store.Status()
}
func (s Service) StatusHandler(w http.ResponseWriter, r *http.Request) {
	on, err := s.store.Status()
	if err != nil {
		s.logger.Errorf("maintenance status: %s", err)
		jsonInternalServerErrorResponse(w)
		return
	}
	jsonStatusResponse(w, on)
}
func (s Service) OnHandler(w http.ResponseWriter, r *http.Request) {
	changed, err := s.store.On()
	if err != nil {
		s.logger.Errorf("maintenance on: %s", err)
		jsonInternalServerErrorResponse(w)
		return
	}
	if changed {
		s.logger.Infof("maintenance on")
		jsonCreatedResponse(w)
		return
	}
	jsonOKResponse(w)
}
func (s Service) OffHandler(w http.ResponseWriter, r *http.Request) {
	changed, err := s.store.Off()
	if err != nil {
		s.logger.Errorf("maintenance off: %s", err)
		jsonInternalServerErrorResponse(w)
		return
	}
	if changed {
		s.logger.Infof("maintenance off")
	}
	jsonOKResponse(w)
}
func (this *PostIRCMessageRequest) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *PostArtifactRequest) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *PostArtifactResponse) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *HookChangedMessage) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *TriggerHookRequest) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *TriggerHookResponse) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *TriggerHookResponse) UnmarshalJSON(data []byte) error {
	if this == nil {
		return errors.New("TriggerHookResponse: UnmarshalJSON on nil pointer")
	}
	*this = append((*this)[0:0], data...)
	return nil
}
func (this *LaunchInfo) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *Var) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *Var1) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *Var3) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (t Time) MarshalJSON() ([]byte, error) {
	if y := time.Time(t).Year(); y < 0 || y >= 10000 {
		// RFC 3339 is clear that years are 4 digits exactly.
		// See golang.org/issue/4556#c15 for more discussion.
		return nil, errors.New("queue.Time.MarshalJSON: year outside of range [0,9999]")
	}
	return []byte(`"` + t.String() + `"`), nil
}
func (t *Time) UnmarshalJSON(data []byte) (err error) {
	// Fractional seconds are handled implicitly by Parse.
	x := new(time.Time)
	*x, err = time.Parse(`"`+time.RFC3339+`"`, string(data))
	*t = Time(*x)
	return
}
func (rws *ReadWriteSeeker) Write(p []byte) (n int, err error) {
	minCap := rws.pos + len(p)
	if minCap > cap(rws.buf) { // Make sure buf has enough capacity:
		buf2 := make([]byte, len(rws.buf), minCap+len(p)) // add some extra
		copy(buf2, rws.buf)
		rws.buf = buf2
	}
	if minCap > len(rws.buf) {
		rws.buf = rws.buf[:minCap]
	}
	copy(rws.buf[rws.pos:], p)
	rws.pos += len(p)
	return len(p), nil
}
func (rws *ReadWriteSeeker) Seek(offset int64, whence int) (int64, error) {
	newPos, offs := 0, int(offset)
	switch whence {
	case io.SeekStart:
		newPos = offs
	case io.SeekCurrent:
		newPos = rws.pos + offs
	case io.SeekEnd:
		newPos = len(rws.buf) + offs
	}
	if newPos < 0 {
		return 0, errors.New("negative result pos")
	}
	rws.pos = newPos
	return int64(newPos), nil
}
func (rws *ReadWriteSeeker) Read(b []byte) (n int, err error) {
	if rws.pos >= len(rws.buf) {
		return 0, io.EOF
	}
	n = copy(b, rws.buf[rws.pos:])
	rws.pos += n
	return
}
func (this *LaunchSpecsResponse) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (this *RegionLaunchSpec) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func (apiDefs APIDefinitions) GenerateCode(goOutputDir, modelData string, downloaded time.Time) {
	downloadedTime = downloaded
	for i := range apiDefs {
		apiDefs[i].PackageName = "tc" + strings.ToLower(apiDefs[i].Data.Name())
		// Used throughout docs, and also methods that use the class, we need a
		// variable name to be used when referencing the go type. It should not
		// clash with either the package name or the go type of the principle
		// member of the package (e.g. awsprovisioner.AwsProvisioner). We'll
		// lowercase the name (e.g. awsProvisioner) and if that clashes with
		// either package or principle member, we'll just use my<Name>. This
		// results in e.g. `var myQueue queue.Queue`, but `var awsProvisioner
		// awsprovisioner.AwsProvisioner`.
		apiDefs[i].ExampleVarName = strings.ToLower(string(apiDefs[i].Data.Name()[0])) + apiDefs[i].Data.Name()[1:]
		if apiDefs[i].ExampleVarName == apiDefs[i].Data.Name() || apiDefs[i].ExampleVarName == apiDefs[i].PackageName {
			apiDefs[i].ExampleVarName = "my" + apiDefs[i].Data.Name()
		}
		apiDefs[i].PackagePath = filepath.Join(goOutputDir, apiDefs[i].PackageName)
		err = os.MkdirAll(apiDefs[i].PackagePath, 0755)
		exitOnFail(err)

		fmt.Printf("Generating go types for %s\n", apiDefs[i].PackageName)
		job := &jsonschema2go.Job{
			Package:              apiDefs[i].PackageName,
			URLs:                 apiDefs[i].schemaURLs,
			ExportTypes:          true,
			TypeNameBlacklist:    apiDefs[i].members,
			DisableNestedStructs: true,
		}
		result, err := job.Execute()
		exitOnFail(err)

		apiDefs[i].schemas = result.SchemaSet
		typesSourceFile := filepath.Join(apiDefs[i].PackagePath, "types.go")
		FormatSourceAndSave(typesSourceFile, result.SourceCode)

		fmt.Printf("Generating functions and methods for %s\n", job.Package)
		content := `
// The following code is AUTO-GENERATED. Please DO NOT edit.
// To update this generated code, run the following command:
// in the /codegenerator/model subdirectory of this project,
// making sure that ` + "`${GOPATH}/bin` is in your `PATH`" + `:
//
// go install && go generate
//
// This package was generated from the schema defined at
// ` + apiDefs[i].URL + `

`
		content += apiDefs[i].generateAPICode()
		sourceFile := filepath.Join(apiDefs[i].PackagePath, apiDefs[i].PackageName+".go")
		FormatSourceAndSave(sourceFile, []byte(content))
	}

	content := "Generated: " + strconv.FormatInt(downloadedTime.Unix(), 10) + "\n"
	content += "The following file is an auto-generated static dump of the API models at time of code generation.\n"
	content += "It is provided here for reference purposes, but is not used by any code.\n"
	content += "\n"
	for i := range apiDefs {
		content += text.Underline(apiDefs[i].URL)
		content += apiDefs[i].Data.String() + "\n\n"
		for _, url := range apiDefs[i].schemas.SortedSanitizedURLs() {
			content += text.Underline(url)
			content += apiDefs[i].schemas.SubSchema(url).String() + "\n\n"
		}
	}
	exitOnFail(ioutil.WriteFile(modelData, []byte(content), 0644))
}
func (entry *APIEntry) postPopulate(apiDef *APIDefinition) {
	if x := &entry.Parent.apiDef.schemaURLs; entry.Input != "" {
		entry.InputURL = tcurls.Schema(tcclient.RootURLFromEnvVars(), entry.Parent.ServiceName, entry.Input)
		*x = append(*x, entry.InputURL)
	}
	if x := &entry.Parent.apiDef.schemaURLs; entry.Output != "" {
		entry.OutputURL = tcurls.Schema(tcclient.RootURLFromEnvVars(), entry.Parent.ServiceName, entry.Output)
		*x = append(*x, entry.OutputURL)
	}
}
func (permaCreds *Credentials) CreateTemporaryCredentials(duration time.Duration, scopes ...string) (tempCreds *Credentials, err error) {
	return permaCreds.CreateNamedTemporaryCredentials("", duration, scopes...)
}
func setURL(client *Client, route string, query url.Values) (u *url.URL, err error) {
	URL := client.BaseURL
	// See https://bugzil.la/1484702
	// Avoid double separator; routes must start with `/`, so baseURL shouldn't
	// end with `/`.
	if strings.HasSuffix(URL, "/") {
		URL = URL[:len(URL)-1]
	}
	URL += route
	u, err = url.Parse(URL)
	if err != nil {
		return nil, fmt.Errorf("Cannot parse url: '%v', is BaseURL (%v) set correctly?\n%v\n", URL, client.BaseURL, err)
	}
	if query != nil {
		u.RawQuery = query.Encode()
	}
	return
}
func (c *Credentials) SignRequest(req *http.Request) (err error) {
	// s, err := c.SignHeader(req.Method, req.URL.String(), hash)
	// req.Header.Set("Authorization", s)
	// return err

	credentials := &hawk.Credentials{
		ID:   c.ClientID,
		Key:  c.AccessToken,
		Hash: sha256.New,
	}
	reqAuth := hawk.NewRequestAuth(req, credentials, 0)
	reqAuth.Ext, err = getExtHeader(c)
	if err != nil {
		return fmt.Errorf("Internal error: was not able to generate hawk ext header from provided credentials:\n%s\n%s", c, err)
	}
	req.Header.Set("Authorization", reqAuth.RequestHeader())
	return nil
}
func (client *Client) APICall(payload interface{}, method, route string, result interface{}, query url.Values) (interface{}, *CallSummary, error) {
	rawPayload := []byte{}
	var err error
	if reflect.ValueOf(payload).IsValid() && !reflect.ValueOf(payload).IsNil() {
		rawPayload, err = json.Marshal(payload)
		if err != nil {
			cs := &CallSummary{
				HTTPRequestObject: payload,
			}
			return result,
				cs,
				&APICallException{
					CallSummary: cs,
					RootCause:   err,
				}
		}
	}
	callSummary, err := client.Request(rawPayload, method, route, query)
	callSummary.HTTPRequestObject = payload
	if err != nil {
		// If context failed during this request, then we should just return that error
		if client.Context != nil && client.Context.Err() != nil {
			return result, callSummary, client.Context.Err()
		}
		return result,
			callSummary,
			&APICallException{
				CallSummary: callSummary,
				RootCause:   err,
			}
	}
	// if result is passed in as nil, it means the API defines no response body
	// json
	if reflect.ValueOf(result).IsValid() && !reflect.ValueOf(result).IsNil() {
		err = json.Unmarshal([]byte(callSummary.HTTPResponseBody), &result)
	}

	if err != nil {
		return result,
			callSummary,
			&APICallException{
				CallSummary: callSummary,
				RootCause:   err,
			}
	}
	return result, callSummary, nil
}
func (client *Client) SignedURL(route string, query url.Values, duration time.Duration) (u *url.URL, err error) {
	u, err = setURL(client, route, query)
	if err != nil {
		return
	}
	credentials := &hawk.Credentials{
		ID:   client.Credentials.ClientID,
		Key:  client.Credentials.AccessToken,
		Hash: sha256.New,
	}
	reqAuth, err := hawk.NewURLAuth(u.String(), credentials, duration)
	if err != nil {
		return
	}
	reqAuth.Ext, err = getExtHeader(client.Credentials)
	if err != nil {
		return
	}
	bewitSignature := reqAuth.Bewit()
	if query == nil {
		query = url.Values{}
	}
	query.Set("bewit", bewitSignature)
	u.RawQuery = query.Encode()
	return
}
func (this *HawkSignatureAuthenticationResponse) MarshalJSON() ([]byte, error) {
	x := json.RawMessage(*this)
	return (&x).MarshalJSON()
}
func bmw256(input []byte) []byte {
	b := new()
	buf := make([]byte, 64)
	copy(buf, input)
	buf[len(input)] = 0x80
	bitLen := uint64(len(input)) << 3
	binary.LittleEndian.PutUint64(buf[56:], bitLen)
	for i := 0; i < 16; i++ {
		b.m[i] = binary.LittleEndian.Uint32(buf[i*4:])
	}
	b.compress(b.m)
	b.h, b.h2 = b.h2, b.h
	copy(b.h, final)
	b.compress(b.h2)
	output := make([]byte, 32)
	outlen := len(output) >> 2
	for i := 0; i < outlen; i++ {
		j := 16 - outlen + i
		binary.LittleEndian.PutUint32(output[4*i:], b.h[j])
	}
	return output
}
func NewCubeHash() *CubeHash {
	c := &CubeHash{}
	c.x0 = iv[0]
	c.x1 = iv[1]
	c.x2 = iv[2]
	c.x3 = iv[3]
	c.x4 = iv[4]
	c.x5 = iv[5]
	c.x6 = iv[6]
	c.x7 = iv[7]
	c.x8 = iv[8]
	c.x9 = iv[9]
	c.xa = iv[10]
	c.xb = iv[11]
	c.xc = iv[12]
	c.xd = iv[13]
	c.xe = iv[14]
	c.xf = iv[15]
	c.xg = iv[16]
	c.xh = iv[17]
	c.xi = iv[18]
	c.xj = iv[19]
	c.xk = iv[20]
	c.xl = iv[21]
	c.xm = iv[22]
	c.xn = iv[23]
	c.xo = iv[24]
	c.xp = iv[25]
	c.xq = iv[26]
	c.xr = iv[27]
	c.xs = iv[28]
	c.xt = iv[29]
	c.xu = iv[30]
	c.xv = iv[31]

	return c
}
func cubehash256(data []byte) []byte {
	c := NewCubeHash()
	buf := make([]byte, 32)
	buf[0] = 0x80
	c.inputBlock(data)
	c.sixteenRounds()
	c.inputBlock(buf)
	c.sixteenRounds()
	c.xv ^= 1
	for j := 0; j < 10; j++ {
		c.sixteenRounds()
	}
	out := make([]byte, 32)
	binary.LittleEndian.PutUint32(out[0:], c.x0)
	binary.LittleEndian.PutUint32(out[4:], c.x1)
	binary.LittleEndian.PutUint32(out[8:], c.x2)
	binary.LittleEndian.PutUint32(out[12:], c.x3)
	binary.LittleEndian.PutUint32(out[16:], c.x4)
	binary.LittleEndian.PutUint32(out[20:], c.x5)
	binary.LittleEndian.PutUint32(out[24:], c.x6)
	binary.LittleEndian.PutUint32(out[28:], c.x7)
	return out
}
func Sum(data []byte) ([]byte, error) {
	blake := blake256.New()
	if _, err := blake.Write(data); err != nil {
		return nil, err
	}
	resultBlake := blake.Sum(nil)

	keccak := sha3.NewKeccak256()
	if _, err := keccak.Write(resultBlake); err != nil {
		return nil, err
	}
	resultkeccak := keccak.Sum(nil)

	resultcube := cubehash256(resultkeccak)
	lyra2result := make([]byte, 32)
	lyra2(lyra2result, resultcube, resultcube, 1, 4, 4)
	var skeinresult [32]byte
	skein.Sum256(&skeinresult, lyra2result, nil)
	resultcube2 := cubehash256(skeinresult[:])
	resultbmw := bmw256(resultcube2)
	return resultbmw, nil
}
func squeeze(state []uint64, out []byte) {
	tmp := make([]byte, blockLenBytes)
	for j := 0; j < len(out)/blockLenBytes+1; j++ {
		for i := 0; i < blockLenInt64; i++ {
			binary.LittleEndian.PutUint64(tmp[i*8:], state[i])
		}
		copy(out[j*blockLenBytes:], tmp) //be care in case of len(out[i:])<len(tmp)
		blake2bLyra(state)
	}
}
func reducedSqueezeRow0(state []uint64, rowOut []uint64, nCols int) {
	ptr := (nCols - 1) * blockLenInt64
	//M[row][C-1-col] = H.reduced_squeeze()
	for i := 0; i < nCols; i++ {
		ptrWord := rowOut[ptr:] //In Lyra2: pointer to M[0][C-1]
		ptrWord[0] = state[0]
		ptrWord[1] = state[1]
		ptrWord[2] = state[2]
		ptrWord[3] = state[3]
		ptrWord[4] = state[4]
		ptrWord[5] = state[5]
		ptrWord[6] = state[6]
		ptrWord[7] = state[7]
		ptrWord[8] = state[8]
		ptrWord[9] = state[9]
		ptrWord[10] = state[10]
		ptrWord[11] = state[11]

		//Goes to next block (column) that will receive the squeezed data
		ptr -= blockLenInt64

		//Applies the reduced-round transformation f to the sponge's state
		reducedBlake2bLyra(state)
	}
}
func reducedDuplexRow1(state []uint64, rowIn []uint64, rowOut []uint64, nCols int) {
	ptrIn := 0
	ptrOut := (nCols - 1) * blockLenInt64

	for i := 0; i < nCols; i++ {
		ptrWordIn := rowIn[ptrIn:]    //In Lyra2: pointer to prev
		ptrWordOut := rowOut[ptrOut:] //In Lyra2: pointer to row
		//Absorbing "M[prev][col]"
		state[0] ^= (ptrWordIn[0])
		state[1] ^= (ptrWordIn[1])
		state[2] ^= (ptrWordIn[2])
		state[3] ^= (ptrWordIn[3])
		state[4] ^= (ptrWordIn[4])
		state[5] ^= (ptrWordIn[5])
		state[6] ^= (ptrWordIn[6])
		state[7] ^= (ptrWordIn[7])
		state[8] ^= (ptrWordIn[8])
		state[9] ^= (ptrWordIn[9])
		state[10] ^= (ptrWordIn[10])
		state[11] ^= (ptrWordIn[11])

		//Applies the reduced-round transformation f to the sponge's state
		reducedBlake2bLyra(state)

		//M[row][C-1-col] = M[prev][col] XOR rand
		ptrWordOut[0] = ptrWordIn[0] ^ state[0]
		ptrWordOut[1] = ptrWordIn[1] ^ state[1]
		ptrWordOut[2] = ptrWordIn[2] ^ state[2]
		ptrWordOut[3] = ptrWordIn[3] ^ state[3]
		ptrWordOut[4] = ptrWordIn[4] ^ state[4]
		ptrWordOut[5] = ptrWordIn[5] ^ state[5]
		ptrWordOut[6] = ptrWordIn[6] ^ state[6]
		ptrWordOut[7] = ptrWordIn[7] ^ state[7]
		ptrWordOut[8] = ptrWordIn[8] ^ state[8]
		ptrWordOut[9] = ptrWordIn[9] ^ state[9]
		ptrWordOut[10] = ptrWordIn[10] ^ state[10]
		ptrWordOut[11] = ptrWordIn[11] ^ state[11]

		//Input: next column (i.e., next block in sequence)
		ptrIn += blockLenInt64
		//Output: goes to previous column
		ptrOut -= blockLenInt64
	}
}
func NewReaderByteCodeLoader(p parser.Parser, c compiler.Compiler) *ReaderByteCodeLoader {
	return &ReaderByteCodeLoader{NewFlags(), p, c}
}
func (l *ReaderByteCodeLoader) LoadReader(name string, rdr io.Reader) (*vm.ByteCode, error) {
	ast, err := l.Parser.ParseReader(name, rdr)
	if err != nil {
		return nil, err
	}

	if l.ShouldDumpAST() {
		fmt.Fprintf(os.Stderr, "AST:\n%s\n", ast)
	}

	bc, err := l.Compiler.Compile(ast)
	if err != nil {
		return nil, err
	}

	return bc, nil
}
func NewV3(namespace *UUID, name []byte) *UUID {
	uuid := newByHash(md5.New(), namespace, name)
	uuid[6] = (uuid[6] & 0x0f) | 0x30
	return uuid
}
func txLiteral(st *State) {
	st.sa = st.CurrentOp().Arg()
	st.Advance()
}
func txFetchSymbol(st *State) {
	// Need to handle local vars?
	key := st.CurrentOp().Arg()
	vars := st.Vars()
	if v, ok := vars.Get(key); ok {
		st.sa = v
	} else {
		st.sa = nil
	}
	st.Advance()
}
func txMarkRaw(st *State) {
	if reflect.ValueOf(st.sa).Type() != rawStringType {
		st.sa = rawString(interfaceToString(st.sa))
	}
	st.Advance()
}
func txUnmarkRaw(st *State) {
	if reflect.ValueOf(st.sa).Type() == rawStringType {
		st.sa = string(interfaceToString(st.sa))
	}
	st.Advance()
}
func txPrint(st *State) {
	arg := st.sa
	if arg == nil {
		st.Warnf("Use of nil to print\n")
	} else if reflect.ValueOf(st.sa).Type() != rawStringType {
		st.AppendOutputString(html.EscapeString(interfaceToString(arg)))
	} else {
		st.AppendOutputString(interfaceToString(arg))
	}
	st.Advance()
}
func txPrintRaw(st *State) {
	// XXX TODO: mark_raw handling
	arg := st.sa
	if arg == nil {
		st.Warnf("Use of nil to print\n")
	} else {
		st.AppendOutputString(interfaceToString(arg))
	}
	st.Advance()
}
func NewLoopVar(idx int, array reflect.Value) *LoopVar {
	lv := &LoopVar{
		Index:    idx,
		Count:    idx + 1,
		Body:     array,
		Size:     array.Len(),
		MaxIndex: array.Len() - 1,
		PeekNext: nil,
		PeekPrev: nil,
		IsFirst:  false,
		IsLast:   false,
	}
	return lv
}
func txMakeArray(st *State) {
	start := st.CurrentMark() // start
	end := st.StackTip()      // end

	if end <= start {
		panic(fmt.Sprintf("MakeArray: list start (%d) >= end (%d)", start, end))
	}

	list := make([]interface{}, end-start+1)
	for i := end; i >= start; i-- {
		list[i-start] = st.StackPop()
	}
	st.sa = list
	st.Advance()
}
func txFunCallOmni(st *State) {
	t := reflect.ValueOf(st.sa)
	switch t.Kind() {
	case reflect.Int:
		// If it's an int, assume that it's a MACRO, which points to
		// the location in the bytecode that contains the macro code
		txMacroCall(st)
	case reflect.Func:
		txFunCall(st)
	default:
		st.Warnf("Unknown variable as function call: %s\n", st.sa)
		st.sa = nil
		st.Advance()
	}
}
func (f *Flags) DumpAST(b bool) {
	if b {
		f.flags |= MaskDumpAST
	} else {
		f.flags &= ^MaskDumpAST
	}
}
func (f *Flags) DumpByteCode(b bool) {
	if b {
		f.flags |= MaskDumpByteCode
	} else {
		f.flags &= ^MaskDumpByteCode
	}
}
func (o *NaiveOptimizer) Optimize(bc *vm.ByteCode) error {
	for i := 0; i < bc.Len(); i++ {
		op := bc.Get(i)
		if op == nil {
			return errors.New("failed to fetch op '" + op.String() + "'")
		}
		switch op.Type() {
		case vm.TXOPLiteral:
			if i+1 < bc.Len() && bc.Get(i+1).Type() == vm.TXOPPrintRaw {
				bc.OpList[i] = vm.NewOp(vm.TXOPPrintRawConst, op.ArgString())
				bc.OpList[i+1] = vm.NewOp(vm.TXOPNoop)
				i++
			}
		}
	}
	return nil
}
func NewFuncDepot(namespace string) *FuncDepot {
	return &FuncDepot{namespace, make(map[string]reflect.Value)}
}
func (fc *FuncDepot) Get(key string) (reflect.Value, bool) {
	f, ok := fc.depot[key]
	return f, ok
}
func (fc *FuncDepot) Set(key string, v interface{}) {
	fc.depot[key] = reflect.ValueOf(v)
}
func NewFileTemplateFetcher(paths []string) (*FileTemplateFetcher, error) {
	l := &FileTemplateFetcher{
		Paths: make([]string, len(paths)),
	}
	for k, v := range paths {
		abs, err := filepath.Abs(v)
		if err != nil {
			return nil, err
		}
		l.Paths[k] = abs
	}
	return l, nil
}
func (s *FileSource) LastModified() (time.Time, error) {
	// Calling os.Stat() for *every* Render of the same source is a waste
	// Only call os.Stat() if we haven't done so in the last 1 second
	if time.Since(s.LastStat) < time.Second {
		// A-ha! it's not that long ago we calculated this value, just return
		// the same thing as our last call
		return s.LastStatResult.ModTime(), nil
	}

	// If we got here, our previous check was too old or this is the first
	// time we're checking for os.Stat()
	fi, err := os.Stat(s.Path)
	if err != nil {
		return time.Time{}, err
	}

	// Save these for later...
	s.LastStat = time.Now()
	s.LastStatResult = fi

	return s.LastStatResult.ModTime(), nil
}
func (s *FileSource) Reader() (io.Reader, error) {
	fh, err := os.Open(s.Path)
	if err != nil {
		return nil, err
	}
	return fh, nil
}
func (s *FileSource) Bytes() ([]byte, error) {
	rdr, err := s.Reader()
	if err != nil {
		return nil, err
	}
	return ioutil.ReadAll(rdr)
}
func NewState() *State {
	st := &State{
		opidx:      0,
		pc:         NewByteCode(),
		stack:      stack.New(5),
		markstack:  stack.New(5),
		framestack: stack.New(5),
		frames:     stack.New(5),
		vars:       make(Vars),
		warn:       os.Stderr,
		MaxLoopCount: 1000,
	}

	st.Pushmark()
	st.PushFrame()
	return st
}
func (st *State) PushFrame() *frame.Frame {
	f := frame.New(st.framestack)
	st.frames.Push(f)
	f.SetMark(st.frames.Size())
	return f
}
func (st *State) PopFrame() *frame.Frame {
	x := st.frames.Pop()
	if x == nil {
		return nil
	}
	f := x.(*frame.Frame)
	for i := st.framestack.Size(); i > f.Mark(); i-- {
		st.framestack.Pop()
	}
	return f
}
func (st *State) CurrentFrame() *frame.Frame {
	x, err := st.frames.Top()
	if err != nil {
		return nil
	}
	return x.(*frame.Frame)
}
func (st *State) Warnf(format string, args ...interface{}) {
	st.warn.Write([]byte(fmt.Sprintf(format, args...)))
}
func (st *State) AppendOutputString(o string) {
	st.output.Write([]byte(o))
}
func (st *State) Popmark() int {
	x := st.markstack.Pop()
	return x.(int)
}
func (st *State) CurrentMark() int {
	x, err := st.markstack.Top()
	if err != nil {
		x = 0
	}
	return x.(int)
}
func (st *State) LoadByteCode(key string) (*ByteCode, error) {
	return st.Loader.Load(key)
}
func (st *State) Reset() {
	st.opidx = 0
	st.sa = nil
	st.sb = nil
	st.stack.Reset()
	st.markstack.Reset()
	st.frames.Reset()
	st.framestack.Reset()

	st.Pushmark()
	st.PushFrame()
}
func (f *Frame) DeclareVar(v interface{}) int {
	f.stack.Push(v)
	return f.stack.Size() - 1
}
func (f *Frame) GetLvar(i int) (interface{}, error) {
	v, err := f.stack.Get(i)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get local variable at "+strconv.Itoa(i+f.mark))
	}
	return v, nil
}
func (f *Frame) SetLvar(i int, v interface{}) {
	f.stack.Set(i, v)
}
func NewByteCode() *ByteCode {
	return &ByteCode{
		GeneratedOn: time.Now(),
		Name:        "",
		OpList:      nil,
		Version:     1.0,
	}
}
func (b *ByteCode) Append(op Op) {
	b.OpList = append(b.OpList, op)
}
func (b *ByteCode) AppendOp(o OpType, args ...interface{}) Op {
	x := NewOp(o, args...)
	b.Append(x)
	return x
}
func (b *ByteCode) String() string {
	buf := rbpool.Get()
	defer rbpool.Release(buf)

	fmt.Fprintf(buf,
		"// Bytecode for '%s'\n// Generated On: %s\n",
		b.Name,
		b.GeneratedOn,
	)
	for k, v := range b.OpList {
		fmt.Fprintf(buf, "%03d. %s\n", k+1, v)
	}
	return buf.String()
}
func NewCachedByteCodeLoader(
	cache Cache,
	cacheLevel CacheStrategy,
	fetcher TemplateFetcher,
	parser parser.Parser,
	compiler compiler.Compiler,
) *CachedByteCodeLoader {
	return &CachedByteCodeLoader{
		NewStringByteCodeLoader(parser, compiler),
		NewReaderByteCodeLoader(parser, compiler),
		fetcher,
		[]Cache{MemoryCache{}, cache},
		cacheLevel,
	}
}
func (l *CachedByteCodeLoader) Load(key string) (bc *vm.ByteCode, err error) {
	defer func() {
		if bc != nil && err == nil && l.ShouldDumpByteCode() {
			fmt.Fprintf(os.Stderr, "%s\n", bc.String())
		}
	}()

	var source TemplateSource
	if l.CacheLevel > CacheNone {
		var entity *CacheEntity
		for _, cache := range l.Caches {
			entity, err = cache.Get(key)
			if err == nil {
				break
			}
		}

		if err == nil {
			if l.CacheLevel == CacheNoVerify {
				return entity.ByteCode, nil
			}

			t, err := entity.Source.LastModified()
			if err != nil {
				return nil, errors.Wrap(err, "failed to get last-modified from source")
			}

			if t.Before(entity.ByteCode.GeneratedOn) {
				return entity.ByteCode, nil
			}

			// ByteCode validation failed, but we can still re-use source
			source = entity.Source
		}
	}

	if source == nil {
		source, err = l.Fetcher.FetchTemplate(key)
		if err != nil {
			return nil, errors.Wrap(err, "failed to fetch template")
		}
	}

	rdr, err := source.Reader()
	if err != nil {
		return nil, errors.Wrap(err, "failed to get the reader")
	}

	bc, err = l.LoadReader(key, rdr)
	if err != nil {
		return nil, errors.Wrap(err, "failed to read byte code")
	}

	entity := &CacheEntity{bc, source}
	for _, cache := range l.Caches {
		cache.Set(key, entity)
	}

	return bc, nil
}
func NewFileCache(dir string) (*FileCache, error) {
	f := &FileCache{dir}
	return f, nil
}
func (c *FileCache) GetCachePath(key string) string {
	// What's the best, portable way to remove make an absolute path into
	// a relative path?
	key = filepath.Clean(key)
	key = strings.TrimPrefix(key, "/")
	return filepath.Join(c.Dir, key)
}
func (c *FileCache) Get(key string) (*CacheEntity, error) {
	path := c.GetCachePath(key)

	// Need to avoid race condition
	file, err := os.Open(path)
	if err != nil {
		return nil, errors.Wrap(err, "failed to open cache file '"+path+"'")
	}
	defer file.Close()

	var entity CacheEntity
	dec := gob.NewDecoder(file)
	if err = dec.Decode(&entity); err != nil {
		return nil, errors.Wrap(err, "failed to gob decode from cache file '"+path+"'")
	}

	return &entity, nil
}
func (c *FileCache) Set(key string, entity *CacheEntity) error {
	path := c.GetCachePath(key)
	if err := os.MkdirAll(filepath.Dir(path), 0777); err != nil {
		return errors.Wrap(err, "failed to create directory for cache file")
	}

	// Need to avoid race condition
	file, err := os.OpenFile(path, os.O_WRONLY|os.O_CREATE, 0666)
	if err != nil {
		return errors.Wrap(err, "failed to open/create a cache file")
	}
	defer file.Close()

	f := bufio.NewWriter(file)
	defer f.Flush()
	enc := gob.NewEncoder(f)
	if err = enc.Encode(entity); err != nil {
		return errors.Wrap(err, "failed to encode Entity via gob")
	}

	return nil
}
func (c *FileCache) Delete(key string) error {
	return errors.Wrap(os.Remove(c.GetCachePath(key)), "failed to remove file cache file")
}
func (c MemoryCache) Get(key string) (*CacheEntity, error) {
	bc, ok := c[key]
	if !ok {
		return nil, errors.New("cache miss")
	}
	return bc, nil
}
func (c MemoryCache) Set(key string, bc *CacheEntity) error {
	c[key] = bc
	return nil
}
func (c MemoryCache) Delete(key string) error {
	delete(c, key)
	return nil
}
func NewStringLexer(template string) *parser.Lexer {
	l := parser.NewStringLexer(template, SymbolSet)
	l.SetTagStart("[%")
	l.SetTagEnd("%]")

	return l
}
func NewReaderLexer(rdr io.Reader) *parser.Lexer {
	l := parser.NewReaderLexer(rdr, SymbolSet)
	l.SetTagStart("[%")
	l.SetTagEnd("%]")

	return l
}
func NewV4() *UUID {
	buf := make([]byte, 16)
	rand.Read(buf)
	buf[6] = (buf[6] & 0x0f) | 0x40
	var uuid UUID
	copy(uuid[:], buf[:])
	uuid.variantRFC4122()
	return &uuid
}
func (p *Kolonish) Parse(name string, template []byte) (*parser.AST, error) {
	return p.ParseString(name, string(template))
}
func (p *Kolonish) ParseReader(name string, rdr io.Reader) (*parser.AST, error) {
	b := parser.NewBuilder()
	lex := NewReaderLexer(rdr)
	return b.Parse(name, lex)
}
func (ast *AST) Visit() <-chan node.Node {
	c := make(chan node.Node)
	go func() {
		defer close(c)
		ast.Root.Visit(c)
	}()
	return c
}
func (ast *AST) String() string {
	buf := rbpool.Get()
	defer rbpool.Release(buf)

	c := ast.Visit()
	k := 0
	for v := range c {
		k++
		fmt.Fprintf(buf, "%03d. %s\n", k, v)
	}
	return buf.String()
}
func (vm *VM) Run(bc *ByteCode, vars Vars, output io.Writer) {
	if !vm.IsSupportedByteCodeVersion(bc) {
		panic(fmt.Sprintf(
			"error: ByteCode version %f no supported",
			bc.Version,
		))
	}

	st := vm.st

	if _, ok := output.(*bufio.Writer); !ok {
		output = bufio.NewWriter(output)
		defer output.(*bufio.Writer).Flush()
	}
	st.Reset()
	st.pc = bc
	st.output = output
	newvars := Vars(rvpool.Get())
	defer rvpool.Release(newvars)
	defer newvars.Reset()

	st.vars = newvars
	if fc := vm.functions; fc != nil {
		for k, v := range vm.functions {
			st.vars[k] = v
		}
	}

	if vars != nil {
		for k, v := range vars {
			st.vars[k] = v
		}
	}
	st.Loader = vm.Loader

	// This is the main loop
	for op := st.CurrentOp(); op.Type() != TXOPEnd; op = st.CurrentOp() {
		op.Call(st)
	}
}
func DefaultParser(tx *Xslate, args Args) error {
	syntax, ok := args.Get("Syntax")
	if !ok {
		syntax = "TTerse"
	}

	switch syntax {
	case "TTerse":
		tx.Parser = tterse.New()
	case "Kolon", "Kolonish":
		tx.Parser = kolonish.New()
	default:
		return errors.New("sytanx '" + syntax.(string) + "' is not available")
	}
	return nil
}
func DefaultLoader(tx *Xslate, args Args) error {
	var tmp interface{}

	tmp, ok := args.Get("CacheDir")
	if !ok {
		tmp, _ = ioutil.TempDir("", "go-xslate-cache-")

	}
	cacheDir := tmp.(string)

	tmp, ok = args.Get("LoadPaths")
	if !ok {
		cwd, _ := os.Getwd()
		tmp = []string{cwd}
	}
	paths := tmp.([]string)

	cache, err := loader.NewFileCache(cacheDir)
	if err != nil {
		return err
	}
	fetcher, err := loader.NewFileTemplateFetcher(paths)
	if err != nil {
		return err
	}

	tmp, ok = args.Get("CacheLevel")
	if !ok {
		tmp = 1
	}
	cacheLevel := tmp.(int)
	tx.Loader = loader.NewCachedByteCodeLoader(cache, loader.CacheStrategy(cacheLevel), fetcher, tx.Parser, tx.Compiler)
	return nil
}
func DefaultVM(tx *Xslate, args Args) error {
	dvm := vm.NewVM()
	dvm.Loader = tx.Loader
	tx.VM = dvm
	return nil
}
func (args Args) Get(key string) (interface{}, bool) {
	ret, ok := args[key]
	return ret, ok
}
func NewHTTPSource(r *http.Response) (*HTTPSource, error) {
	body, err := ioutil.ReadAll(r.Body)
	if err != nil {
		return nil, err
	}

	s := &HTTPSource{
		bytes.NewBuffer(body),
		time.Time{},
	}

	if lastmodStr := r.Header.Get("Last-Modified"); lastmodStr != "" {
		t, err := time.Parse(http.TimeFormat, lastmodStr)
		if err != nil {
			fmt.Printf("failed to parse: %s\n", err)
			t = time.Now()
		}
		s.LastModifiedTime = t
	} else {
		s.LastModifiedTime = time.Now()
	}

	return s, nil
}
func NewStringByteCodeLoader(p parser.Parser, c compiler.Compiler) *StringByteCodeLoader {
	return &StringByteCodeLoader{NewFlags(), p, c}
}
func (l *StringByteCodeLoader) LoadString(name string, template string) (*vm.ByteCode, error) {
	ast, err := l.Parser.ParseString(name, template)
	if err != nil {
		return nil, err
	}

	if l.ShouldDumpAST() {
		fmt.Fprintf(os.Stderr, "AST:\n%s\n", ast)
	}

	bc, err := l.Compiler.Compile(ast)
	if err != nil {
		return nil, err
	}

	if l.ShouldDumpByteCode() {
		fmt.Fprintf(os.Stderr, "ByteCode:\n%s\n", bc)
	}

	return bc, nil
}
func (v Vars) Get(k interface{}) (interface{}, bool) {
	key, ok := k.(string)
	if !ok {
		key = fmt.Sprintf("%s", k)
	}
	x, ok := v[key]
	return x, ok
}
func NewOp(o OpType, args ...interface{}) Op {
	h := optypeToHandler(o)

	var arg interface{}
	if len(args) > 0 {
		arg = args[0]
	}

	return &op{
		OpType:    o,
		OpHandler: h,
		uArg:      arg,
	}
}
func (o op) MarshalBinary() ([]byte, error) {
	buf := rbpool.Get()
	defer rbpool.Release(buf)

	// Write the code/opcode
	if err := binary.Write(buf, binary.LittleEndian, int64(o.OpType)); err != nil {
		return nil, errors.Wrap(err, "failed to marshal op to binary")
	}

	// If this has args, we need to encode the args
	tArg := reflect.TypeOf(o.uArg)
	hasArg := tArg != nil
	if hasArg {
		binary.Write(buf, binary.LittleEndian, int8(1))
	} else {
		binary.Write(buf, binary.LittleEndian, int8(0))
	}

	if hasArg {
		switch tArg.Kind() {
		case reflect.Int:
			binary.Write(buf, binary.LittleEndian, int64(2))
			binary.Write(buf, binary.LittleEndian, int64(o.uArg.(int)))
		case reflect.Int64:
			binary.Write(buf, binary.LittleEndian, int64(2))
			binary.Write(buf, binary.LittleEndian, int64(o.uArg.(int64)))
		case reflect.Slice:
			if tArg.Elem().Kind() != reflect.Uint8 {
				panic("Slice of what?")
			}
			binary.Write(buf, binary.LittleEndian, int64(5))
			binary.Write(buf, binary.LittleEndian, int64(len(o.uArg.([]byte))))
			for _, v := range o.uArg.([]byte) {
				binary.Write(buf, binary.LittleEndian, v)
			}
		case reflect.String:
			binary.Write(buf, binary.LittleEndian, int64(6))
			binary.Write(buf, binary.LittleEndian, int64(len(o.uArg.(string))))
			for _, v := range []byte(o.uArg.(string)) {
				binary.Write(buf, binary.LittleEndian, v)
			}
		default:
			panic("Unknown type " + tArg.String())
		}
	}

	v := o.comment
	hasComment := v != ""
	if hasComment {
		binary.Write(buf, binary.LittleEndian, int8(1))
		binary.Write(buf, binary.LittleEndian, v)
	} else {
		binary.Write(buf, binary.LittleEndian, int8(0))
	}

	return buf.Bytes(), nil
}
func (o *op) UnmarshalBinary(data []byte) error {
	buf := bytes.NewReader(data)

	var t int64
	if err := binary.Read(buf, binary.LittleEndian, &t); err != nil {
		return errors.Wrap(err, "optype check failed during UnmarshalBinary")
	}

	o.OpType = OpType(t)
	o.OpHandler = optypeToHandler(o.OpType)

	var hasArg int8
	if err := binary.Read(buf, binary.LittleEndian, &hasArg); err != nil {
		return errors.Wrap(err, "hasArg check failed during UnmarshalBinary")
	}

	if hasArg == 1 {
		var tArg int64
		if err := binary.Read(buf, binary.LittleEndian, &tArg); err != nil {
			return errors.Wrap(err, "failed to read argument from buffer during UnmarshalBinary")
		}

		switch tArg {
		case 2:
			var i int64
			if err := binary.Read(buf, binary.LittleEndian, &i); err != nil {
				return errors.Wrap(err, "failed to read integer argument during UnmarshalBinary")
			}
			o.uArg = i
		case 5:
			var l int64
			if err := binary.Read(buf, binary.LittleEndian, &l); err != nil {
				return errors.Wrap(err, "failed to read length argument during UnmarshalBinary")
			}

			b := make([]byte, l)
			for i := int64(0); i < l; i++ {
				if err := binary.Read(buf, binary.LittleEndian, &b[i]); err != nil {
					return errors.Wrap(err, "failed to read bytes from buffer during UnmarshalBinary")
				}
			}
			o.uArg = b
		default:
			panic(fmt.Sprintf("Unknown tArg: %d", tArg))
		}
	}

	var hasComment int8
	if err := binary.Read(buf, binary.LittleEndian, &hasComment); err != nil {
		return errors.Wrap(err, "hasComment check failed during UnmarshalBinary")
	}

	if hasComment == 1 {
		if err := binary.Read(buf, binary.LittleEndian, &o.comment); err != nil {
			return errors.Wrap(err, "failed to read comment bytes during UnmarshalBinary")
		}
	}

	return nil
}
func (o op) ArgInt() int {
	v := interfaceToNumeric(o.uArg)
	return int(v.Int())
}
func (o op) ArgString() string {
	// In most cases we do this because it's a sring
	if v, ok := o.uArg.(string); ok {
		return v
	}
	return interfaceToString(o.uArg)
}
func (ctx *context) AppendOp(o vm.OpType, args ...interface{}) vm.Op {
	return ctx.ByteCode.AppendOp(o, args...)
}
func (c *BasicCompiler) Compile(ast *parser.AST) (*vm.ByteCode, error) {
	ctx := &context{
		ByteCode: vm.NewByteCode(),
	}
	for _, n := range ast.Root.Nodes {
		compile(ctx, n)
	}

	// When we're done compiling, always append an END op
	ctx.ByteCode.AppendOp(vm.TXOPEnd)

	opt := &NaiveOptimizer{}
	opt.Optimize(ctx.ByteCode)

	ctx.ByteCode.Name = ast.Name
	return ctx.ByteCode, nil
}
func NewV5(namespaceUUID *UUID, name []byte) *UUID {
	uuid := newByHash(sha1.New(), namespaceUUID, name)
	uuid[6] = (uuid[6] & 0x0f) | 0x50
	return uuid
}
func (list LexSymbolList) Sort() LexSymbolList {
	sorter := LexSymbolSorter{
		list: list,
	}
	sort.Sort(sorter)
	return sorter.list
}
func (s LexSymbolSorter) Less(i, j int) bool {
	return s.list[i].Priority > s.list[j].Priority
}
func (s LexSymbolSorter) Swap(i, j int) {
	s.list[i], s.list[j] = s.list[j], s.list[i]
}
func (l *LexSymbolSet) Copy() *LexSymbolSet {
	c := NewLexSymbolSet()
	for k, v := range l.Map {
		c.Map[k] = LexSymbol{v.Name, v.Type, v.Priority}
	}
	return c
}
func (l *LexSymbolSet) Set(name string, typ lex.ItemType, prio ...float32) {
	var x float32
	if len(prio) < 1 {
		x = 1.0
	} else {
		x = prio[0]
	}
	l.Map[name] = LexSymbol{name, typ, x}
	l.SortedList = nil // reset
}
func (l *LexSymbolSet) GetSortedList() LexSymbolList {
	// Because symbols are parsed automatically in a loop, we need to make
	// sure that we search starting with the longest term (e.g., "INCLUDE"
	// must come before "IN")
	// However, simply sorting the symbols using alphabetical sort then
	// max-length forces us to make more comparisons than necessary.
	// To get the best of both world, we allow passing a floating point
	// "priority" parameter to sort the symbols
	if l.SortedList != nil {
		return l.SortedList
	}

	num := len(l.Map)
	list := make(LexSymbolList, num)
	i := 0
	for _, v := range l.Map {
		list[i] = v
		i++
	}
	l.SortedList = list.Sort()

	return l.SortedList
}
func (s *Stack) Top() (interface{}, error) {
	if len(*s) == 0 {
		return nil, errors.New("nothing on stack")
	}
	return (*s)[len(*s)-1], nil
}
func (s *Stack) Resize(size int) {
	newl := make([]interface{}, len(*s), size)
	copy(newl, *s)
	*s = newl
}
func (s *Stack) Extend(extendBy int) {
	s.Resize(s.Size() + extendBy)
}
func (s *Stack) Grow(min int) {
	// Automatically grow the stack to some long-enough length
	if min <= s.BufferSize() {
		// we have enough
		return
	}

	s.Resize(calcNewSize(min))
}
func (s *Stack) Get(i int) (interface{}, error) {
	if i < 0 || i >= len(*s) {
		return nil, errors.New(strconv.Itoa(i) + " is out of range")
	}

	return (*s)[i], nil
}
func (s *Stack) Set(i int, v interface{}) error {
	if i < 0 {
		return errors.New("invalid index into stack")
	}

	if i >= s.BufferSize() {
		s.Resize(calcNewSize(i))
	}

	for len(*s) < i + 1 {
		*s = append(*s, nil)
	}

	(*s)[i] = v
	return nil
}
func (s *Stack) Push(v interface{}) {
	if len(*s) >= s.BufferSize() {
		s.Resize(calcNewSize(cap(*s)))
	}

	*s = append(*s, v)
}
func (s *Stack) Pop() interface{} {
	l := len(*s)
	if l == 0 {
		return nil
	}

	v := (*s)[l-1]
	*s = (*s)[:l-1]
	return v
}
func (s *Stack) String() string {
	buf := bytes.Buffer{}
	for k, v := range *s {
		fmt.Fprintf(&buf, "%03d: %q\n", k, v)
	}
	return buf.String()
}
func GetHostIPs() ([]net.IP, error) {
	ifaces, err := net.Interfaces()
	if err != nil {
		return nil, err
	}

	var ips []net.IP
	for _, iface := range ifaces {
		if strings.HasPrefix(iface.Name, "docker") {
			continue
		}
		addrs, err := iface.Addrs()
		if err != nil {
			continue
		}
		for _, addr := range addrs {
			if ipnet, ok := addr.(*net.IPNet); ok {
				ips = append(ips, ipnet.IP)
			}
		}
	}

	return ips, nil
}
func GetPrivateHostIPs() ([]net.IP, error) {
	ips, err := GetHostIPs()
	if err != nil {
		return nil, err
	}

	var privateIPs []net.IP
	for _, ip := range ips {
		// skip loopback, non-IPv4 and non-private addresses
		if ip.IsLoopback() || ip.To4() == nil || !IsPrivate(ip) {
			continue
		}
		privateIPs = append(privateIPs, ip)
	}

	return privateIPs, nil
}
func IsPrivate(ip net.IP) bool {
	for _, ipnet := range privateNets {
		if ipnet.Contains(ip) {
			return true
		}
	}
	return false
}
func Environ() []string {
	s := make([]string, 0)

	FlagSet.VisitAll(func(f *flag.Flag) {
		if value, ok := getenv(f.Name); ok {
			s = append(s, flagAsEnv(f.Name)+"="+value)
		}
	})

	return s
}
func getenv(name string) (s string, ok bool) {
	m := make(map[string]bool)

	for _, keyVal := range os.Environ() {
		split := strings.Split(keyVal, "=")
		m[split[0]] = true
	}

	name = flagAsEnv(name)
	if _, ok = m[name]; ok {
		s = os.Getenv(name)
	}

	return
}
func flagAsEnv(name string) string {
	name = strings.ToUpper(EnvPrefix + name)
	name = strings.Replace(name, ".", "_", -1)
	name = strings.Replace(name, "-", "_", -1)
	return name
}
func NewPolicy() Policy {
	p := Policy{
		MinLength:    6,
		MaxLength:    16,
		MinLowers:    0,
		MinUppers:    0,
		MinDigits:    0,
		MinSpclChars: 0,
		LowerPool:    "abcdefghijklmnopqrstuvwxyz",
		UpperPool:    "ABCDEFGHIJKLMNOPQRSTUVWXYZ",
		DigitPool:    "0123456789",
		SpclCharPool: "!@#$%^&*()-_=+,.?/:;{}[]~",
	}
	return p
}
func CreateRandom(bs []byte, length int) []byte {
	filled := make([]byte, length)
	max := len(bs)

	for i := 0; i < length; i++ {
		Shuffle(bs)
		filled[i] = bs[random(0, max)]
	}

	return filled
}
func Shuffle(bs []byte) {
	n := len(bs)
	for i := n - 1; i > 0; i-- {
		rand.Seed(time.Now().UnixNano())
		j := rand.Intn(i + 1)
		bs[i], bs[j] = bs[j], bs[i]
	}
}
func Generate(p Policy) (string, error) {

	// Character length based policies should not be negative
	if p.MinLength < 0 || p.MaxLength < 0 || p.MinUppers < 0 ||
		p.MinLowers < 0 || p.MinDigits < 0 || p.MinSpclChars < 0 {
		return "", ErrNegativeLengthNotAllowed
	}

	collectiveMinLength := p.MinUppers + p.MinLowers + p.MinDigits + p.MinSpclChars

	// Min length is the collective min length
	if collectiveMinLength > p.MinLength {
		p.MinLength = collectiveMinLength
	}

	// Max length should be greater than collective minimun length
	if p.MinLength > p.MaxLength {
		return "", ErrMaxLengthExceeded
	}

	if p.MaxLength == 0 {
		return "", nil
	}

	capsAlpha := []byte(p.UpperPool)
	smallAlpha := []byte(p.LowerPool)
	digits := []byte(p.DigitPool)
	spclChars := []byte(p.SpclCharPool)
	allChars := []byte(p.UpperPool + p.LowerPool + p.DigitPool + p.SpclCharPool)

	passwd := CreateRandom(capsAlpha, p.MinUppers)

	passwd = append(passwd, CreateRandom(smallAlpha, p.MinLowers)...)
	passwd = append(passwd, CreateRandom(digits, p.MinDigits)...)
	passwd = append(passwd, CreateRandom(spclChars, p.MinSpclChars)...)

	passLen := len(passwd)

	if passLen < p.MaxLength {
		randLength := random(p.MinLength, p.MaxLength)
		passwd = append(passwd, CreateRandom(allChars, randLength-passLen)...)
	}

	Shuffle(passwd)

	return string(passwd), nil
}
func ExecutableFolder() (string, error) {
	p, err := Executable()
	if err != nil {
		return "", err
	}
	folder, _ := filepath.Split(p)
	return folder, nil
}
func Ignore(ignore ...func(error) bool) Option {
	return func(s *Sentinel) error {
		s.Lock()
		defer s.Unlock()

		if s.started {
			return ErrAlreadyStarted
		}

		s.ignoreErrors = append(s.ignoreErrors, ignore...)
		return nil
	}
}
func Sigs(sigs ...os.Signal) Option {
	return func(s *Sentinel) error {
		s.Lock()
		defer s.Unlock()

		if s.started {
			return ErrAlreadyStarted
		}

		s.shutdownSigs = sigs
		return nil
	}
}
func Logf(f func(string, ...interface{})) Option {
	return func(s *Sentinel) error {
		s.logf = f
		return nil
	}
}
func Errorf(f func(string, ...interface{})) Option {
	return func(s *Sentinel) error {
		s.errf = f
		return nil
	}
}
func New(opts ...Option) (*Sentinel, error) {
	s := &Sentinel{
		shutdownDuration: DefaultShutdownDuration,
		logf:             func(string, ...interface{}) {},
	}

	var err error

	// apply options
	for _, o := range opts {
		if err = o(s); err != nil {
			return nil, err
		}
	}

	// ensure sigs set
	if s.shutdownSigs == nil {
		s.shutdownSigs = []os.Signal{os.Interrupt}
	}

	// ensure errf set
	if s.errf == nil {
		s.errf = func(str string, v ...interface{}) {
			s.logf("ERROR: "+str, v...)
		}
	}

	return s, nil
}
func (s *Sentinel) Run(ctxt context.Context) error {
	s.Lock()
	if s.started {
		defer s.Unlock()
		return ErrAlreadyStarted
	}
	s.started = true
	s.Unlock()

	eg, ctxt := errgroup.WithContext(ctxt)

	// add servers
	for _, f := range s.serverFuncs {
		eg.Go(func(f func(context.Context) error) func() error {
			return func() error {
				return f(ctxt)
			}
		}(f))
	}

	// add shutdown
	eg.Go(func() func() error {
		s.sig = make(chan os.Signal, 1)
		signal.Notify(s.sig, s.shutdownSigs...)
		return func() error {
			s.logf("received signal: %v", <-s.sig)
			return s.Shutdown()
		}
	}())

	if err := eg.Wait(); err != nil && !s.ShutdownIgnore(err) {
		return err
	}

	return nil
}
func (s *Sentinel) Shutdown() error {
	var firstErr error
	for i, f := range s.shutdownFuncs {
		ctxt, cancel := context.WithTimeout(context.Background(), s.shutdownDuration)
		defer cancel()
		if err := f(ctxt); err != nil {
			s.errf("could not shutdown %d: %v", i, err)
			if firstErr == nil {
				firstErr = err
			}
		}
	}
	return firstErr
}
func (s *Sentinel) ShutdownIgnore(err error) bool {
	if err == nil {
		return true
	}
	for _, f := range s.ignoreErrors {
		if z := f(err); z {
			return true
		}
	}
	return false
}
func (s *Sentinel) Register(server, shutdown interface{}, ignore ...func(error) bool) error {
	// add server and shutdown funcs
	var err error
	s.serverFuncs, err = convertAndAppendContextFuncs(s.serverFuncs, server)
	if err != nil {
		return err
	}
	s.shutdownFuncs, err = convertAndAppendContextFuncs(s.shutdownFuncs, shutdown)
	if err != nil {
		return err

	}
	s.ignoreErrors = append(s.ignoreErrors, ignore...)
	return nil
}
func (s *Sentinel) Mux(listener net.Listener, opts ...netmux.Option) (*netmux.Netmux, error) {
	s.Lock()
	defer s.Unlock()

	if s.started {
		return nil, ErrAlreadyStarted
	}

	// create connection mux
	mux, err := netmux.New(listener, opts...)
	if err != nil {
		return nil, err
	}

	// register server + shutdown
	if err = s.Register(mux, mux, IgnoreListenerClosed, IgnoreNetOpError); err != nil {
		return nil, err
	}

	return mux, nil
}
func (s *Sentinel) HTTP(listener net.Listener, handler http.Handler, opts ...ServerOption) error {
	s.Lock()
	defer s.Unlock()

	if s.started {
		return ErrAlreadyStarted
	}

	var err error

	// create server and apply options
	server := &http.Server{
		Handler: handler,
	}
	for _, o := range opts {
		if err = o(server); err != nil {
			return err
		}
	}

	// register server
	return s.Register(func() error {
		return server.Serve(listener)
	}, server.Shutdown, IgnoreServerClosed, IgnoreNetOpError)
}
func IgnoreError(err error) func(error) bool {
	return func(e error) bool {
		return err == e
	}
}
func IgnoreNetOpError(err error) bool {
	if opErr, ok := err.(*net.OpError); ok {
		return opErr.Err.Error() == "use of closed network connection"
	}
	return false
}
func convertAndAppendContextFuncs(o []func(context.Context) error, v ...interface{}) ([]func(context.Context) error, error) {
	for _, z := range v {
		var t func(context.Context) error
		switch f := z.(type) {
		case func(context.Context) error:
			t = f

		case func():
			t = func(context.Context) error {
				f()
				return nil
			}

		case func() error:
			t = func(context.Context) error {
				return f()
			}
		}

		if t == nil {
			return nil, ErrInvalidType
		}

		o = append(o, t)
	}
	return o, nil
}
func router(apiData []byte, services map[string]Service, healthHandler func(http.ResponseWriter, *http.Request)) *mux.Router {
	m := mux.NewRouter()

	gtgChecker := make([]gtg.StatusChecker, 0)

	for path, service := range services {
		handlers := httpHandlers{service}
		m.HandleFunc(fmt.Sprintf("/%s/__count", path), handlers.countHandler).Methods("GET")
		m.HandleFunc(fmt.Sprintf("/%s/__ids", path), handlers.idsHandler).Methods("GET")
		m.HandleFunc(fmt.Sprintf("/%s/{uuid}", path), handlers.getHandler).Methods("GET")
		m.HandleFunc(fmt.Sprintf("/%s/{uuid}", path), handlers.putHandler).Methods("PUT")
		m.HandleFunc(fmt.Sprintf("/%s/{uuid}", path), handlers.deleteHandler).Methods("DELETE")
		gtgChecker = append(gtgChecker, func() gtg.Status {
			if err := service.Check(); err != nil {
				return gtg.Status{GoodToGo: false, Message: err.Error()}
			}

			return gtg.Status{GoodToGo: true}
		})
	}

	if apiData != nil && len(apiData) != 0 {
		endpoint, err := api.NewAPIEndpointForYAML(apiData)
		if err != nil {
			log.Warn("Failed to serve API endpoint, please check whether the OpenAPI file is valid")
		} else {
			m.HandleFunc(api.DefaultPath, endpoint.ServeHTTP)
		}
	}

	m.HandleFunc("/__health", healthHandler)
	// The top one of these feels more correct, but the lower one matches what we have in Dropwizard,
	// so it's what apps expect currently
	m.HandleFunc(status.PingPath, status.PingHandler)
	m.HandleFunc(status.PingPathDW, status.PingHandler)

	// The top one of these feels more correct, but the lower one matches what we have in Dropwizard,
	// so it's what apps expect currently same as ping, the content of build-info needs more definition
	m.HandleFunc(status.BuildInfoPath, status.BuildInfoHandler)
	m.HandleFunc(status.BuildInfoPathDW, status.BuildInfoHandler)

	m.HandleFunc(status.GTGPath, status.NewGoodToGoHandler(gtg.FailFastParallelCheck(gtgChecker)))

	return m
}
func buildInfoHandler(w http.ResponseWriter, req *http.Request) {
	fmt.Fprintf(w, "build-info")
}
func (_ JsonEncoder) Encode(v ...interface{}) ([]byte, error) {
	var data interface{} = v
	var result interface{}

	if v == nil {
		// So that empty results produces `[]` and not `null`
		data = []interface{}{}
	} else if len(v) == 1 {
		data = v[0]
	}

	t := reflect.TypeOf(data)

	if t.Kind() == reflect.Ptr {
		t = t.Elem()
	}

	if t.Kind() == reflect.Struct {
		result = copyStruct(reflect.ValueOf(data), t).Interface()
	} else {
		result = data
	}

	b, err := json.Marshal(result)

	return b, err
}
func Json(jsonStruct interface{}, ifacePtr ...interface{}) martini.Handler {
	return func(context martini.Context, req *http.Request) {
		ensureNotPointer(jsonStruct)
		jsonStruct := reflect.New(reflect.TypeOf(jsonStruct))
		errors := newErrors()

		if req.Body != nil {
			defer req.Body.Close()
		}

		if err := json.NewDecoder(req.Body).Decode(jsonStruct.Interface()); err != nil {
			errors.Overall[DeserializationError] = err.Error()
		}

		validateAndMap(jsonStruct, context, errors, ifacePtr...)
	}
}
func validateAndMap(obj reflect.Value, context martini.Context, errors *Errors, ifacePtr ...interface{}) {
	context.Invoke(Validate(obj.Interface()))
	errors.combine(getErrors(context))
	context.Map(*errors)
	context.Map(obj.Elem().Interface())
	if len(ifacePtr) > 0 {
		context.MapTo(obj.Elem().Interface(), ifacePtr[0])
	}
}
func (self Errors) Count() int {
	return len(self.Overall) + len(self.Fields)
}
func (o *Options) Header(origin string) (headers map[string]string) {
	headers = make(map[string]string)
	// if origin is not alowed, don't extend the headers
	// with CORS headers.
	if !o.AllowAllOrigins && !o.IsOriginAllowed(origin) {
		return
	}

	// add allow origin
	if o.AllowAllOrigins {
		headers[headerAllowOrigin] = "*"
	} else {
		headers[headerAllowOrigin] = origin
	}

	// add allow credentials
	headers[headerAllowCredentials] = strconv.FormatBool(o.AllowCredentials)

	// add allow methods
	if len(o.AllowMethods) > 0 {
		headers[headerAllowMethods] = strings.Join(o.AllowMethods, ",")
	}

	// add allow headers
	if len(o.AllowHeaders) > 0 {
		// TODO: Add default headers
		headers[headerAllowHeaders] = strings.Join(o.AllowHeaders, ",")
	}

	// add exposed header
	if len(o.ExposeHeaders) > 0 {
		headers[headerExposeHeaders] = strings.Join(o.ExposeHeaders, ",")
	}
	// add a max age header
	if o.MaxAge > time.Duration(0) {
		headers[headerMaxAge] = strconv.FormatInt(int64(o.MaxAge/time.Second), 10)
	}
	return
}
func (o *Options) PreflightHeader(origin, rMethod, rHeaders string) (headers map[string]string) {
	headers = make(map[string]string)
	if !o.AllowAllOrigins && !o.IsOriginAllowed(origin) {
		return
	}
	// verify if requested method is allowed
	// TODO: Too many for loops
	for _, method := range o.AllowMethods {
		if method == rMethod {
			headers[headerAllowMethods] = strings.Join(o.AllowMethods, ",")
			break
		}
	}

	// verify if requested headers are allowed
	var allowed []string
	for _, rHeader := range strings.Split(rHeaders, ",") {
	lookupLoop:
		for _, allowedHeader := range o.AllowHeaders {
			if rHeader == allowedHeader {
				allowed = append(allowed, rHeader)
				break lookupLoop
			}
		}
	}

	// add allowed headers
	if len(allowed) > 0 {
		headers[headerAllowHeaders] = strings.Join(allowed, ",")
	}

	// add exposed headers
	if len(o.ExposeHeaders) > 0 {
		headers[headerExposeHeaders] = strings.Join(o.ExposeHeaders, ",")
	}
	// add a max age header
	if o.MaxAge > time.Duration(0) {
		headers[headerMaxAge] = strconv.FormatInt(int64(o.MaxAge/time.Second), 10)
	}
	return
}
func (o *Options) IsOriginAllowed(origin string) (allowed bool) {
	for _, pattern := range o.AllowOrigins {
		allowed, _ = regexp.MatchString(pattern, origin)
		if allowed {
			return
		}
	}
	return
}
func Allow(opts *Options) http.HandlerFunc {
	return func(res http.ResponseWriter, req *http.Request) {
		var (
			origin           = req.Header.Get(headerOrigin)
			requestedMethod  = req.Header.Get(headerRequestMethod)
			requestedHeaders = req.Header.Get(headerRequestHeaders)
			// additional headers to be added
			// to the response.
			headers map[string]string
		)

		if req.Method == "OPTIONS" &&
			(requestedMethod != "" || requestedHeaders != "") {
			// TODO: if preflight, respond with exact headers if allowed
			headers = opts.PreflightHeader(origin, requestedMethod, requestedHeaders)
		} else {
			headers = opts.Header(origin)
		}

		for key, value := range headers {
			res.Header().Set(key, value)
		}
	}
}
func Renderer(options ...Options) martini.Handler {
	opt := prepareOptions(options)
	cs := prepareCharset(opt.Charset)
	t := compile(opt)
	return func(res http.ResponseWriter, req *http.Request, c martini.Context) {
		// recompile for easy development
		if martini.Env == martini.Dev {
			t = compile(opt)
		}
		tc, _ := t.Clone()
		c.MapTo(&renderer{res, req, tc, opt, cs}, (*Render)(nil))
	}
}
func (al AcceptLanguages) String() string {
	output := bytes.NewBufferString("")
	for i, language := range al {
		output.WriteString(fmt.Sprintf("%s (%1.1f)", language.Language, language.Quality))
		if i != len(al)-1 {
			output.WriteString(", ")
		}
	}

	if output.Len() == 0 {
		output.WriteString("[]")
	}

	return output.String()
}
func Languages() martini.Handler {
	return func(context martini.Context, request *http.Request) {
		header := request.Header.Get(acceptLanguageHeader)
		if header != "" {
			acceptLanguageHeaderValues := strings.Split(header, ",")
			acceptLanguages := make(AcceptLanguages, len(acceptLanguageHeaderValues))

			for i, languageRange := range acceptLanguageHeaderValues {
				// Check if a given range is qualified or not
				if qualifiedRange := strings.Split(languageRange, ";q="); len(qualifiedRange) == 2 {
					quality, error := strconv.ParseFloat(qualifiedRange[1], 32)
					if error != nil {
						// When the quality is unparseable, assume it's 1
						acceptLanguages[i] = AcceptLanguage{trimLanguage(qualifiedRange[0]), 1}
					} else {
						acceptLanguages[i] = AcceptLanguage{trimLanguage(qualifiedRange[0]), float32(quality)}
					}
				} else {
					acceptLanguages[i] = AcceptLanguage{trimLanguage(languageRange), 1}
				}
			}

			sort.Sort(acceptLanguages)
			context.Map(acceptLanguages)
		} else {
			// If we have no Accept-Language header just map an empty slice
			context.Map(make(AcceptLanguages, 0))
		}
	}
}
func Prefix(prefix string) martini.Handler {
	return func(w http.ResponseWriter, r *http.Request) {
		if prefix == "" {
			return
		}
		if p := strings.TrimPrefix(r.URL.Path, prefix); len(p) < len(r.URL.Path) {
			r.URL.Path = p
		} else {
			http.NotFound(w, r)
		}
	}
}
func Basic(username string, password string) http.HandlerFunc {
	var siteAuth = base64.StdEncoding.EncodeToString([]byte(username + ":" + password))
	return func(res http.ResponseWriter, req *http.Request) {
		auth := req.Header.Get("Authorization")
		if !SecureCompare(auth, "Basic "+siteAuth) {
			res.Header().Set("WWW-Authenticate", "Basic realm=\"Authorization Required\"")
			http.Error(res, "Not Authorized", http.StatusUnauthorized)
		}
	}
}
func UpdateUser(s sessions.Session, user User) error {
	s.Set(SessionKey, user.UniqueId())
	return nil
}
func (u *MyUserModel) GetById(id interface{}) error {
	err := dbmap.SelectOne(u, "SELECT * FROM users WHERE id = $1", id)
	if err != nil {
		return err
	}

	return nil
}
func AddressToAccountId(address string) (result xdr.AccountId, err error) {

	bytes, err := strkey.Decode(strkey.VersionByteAccountID, address)

	if err != nil {
		return
	}

	var raw xdr.Uint256
	copy(raw[:], bytes)
	pk, err := xdr.NewPublicKey(xdr.CryptoKeyTypeKeyTypeEd25519, raw)
	if err != nil {
		return
	}

	result = xdr.AccountId(pk)

	return
}
func (m Asset) MutateChangeTrust(o *xdr.ChangeTrustOp) (err error) {
	if m.Native {
		return errors.New("Native asset not allowed")
	}

	o.Line, err = m.ToXdrObject()
	return
}
func (m Limit) MutateChangeTrust(o *xdr.ChangeTrustOp) (err error) {
	o.Limit, err = amount.Parse(string(m))
	return
}
func Trust(code, issuer string, args ...interface{}) (result ChangeTrustBuilder) {
	mutators := []interface{}{
		CreditAsset(code, issuer),
	}

	limitSet := false

	for _, mut := range args {
		mutators = append(mutators, mut)
		_, isLimit := mut.(Limit)
		if isLimit {
			limitSet = true
		}
	}

	if !limitSet {
		mutators = append(mutators, MaxLimit)
	}

	return ChangeTrust(mutators...)
}
func RemoveTrust(code, issuer string, args ...interface{}) (result ChangeTrustBuilder) {
	mutators := []interface{}{
		CreditAsset(code, issuer),
		Limit("0"),
	}

	for _, mut := range args {
		mutators = append(mutators, mut)
	}

	return ChangeTrust(mutators...)
}
func (m CreditAmount) MutatePayment(o interface{}) (err error) {
	switch o := o.(type) {
	default:
		err = errors.New("Unexpected operation type")
	case *xdr.PaymentOp:
		o.Amount, err = amount.Parse(m.Amount)
		if err != nil {
			return
		}

		o.Asset, err = createAlphaNumAsset(m.Code, m.Issuer)
	case *xdr.PathPaymentOp:
		o.DestAmount, err = amount.Parse(m.Amount)
		if err != nil {
			return
		}

		o.DestAsset, err = createAlphaNumAsset(m.Code, m.Issuer)
	}
	return
}
func (m Destination) MutatePayment(o interface{}) error {
	switch o := o.(type) {
	default:
		return errors.New("Unexpected operation type")
	case *xdr.PaymentOp:
		return setAccountId(m.AddressOrSeed, &o.Destination)
	case *xdr.PathPaymentOp:
		return setAccountId(m.AddressOrSeed, &o.Destination)
	}
	return nil
}
func (m NativeAmount) MutatePayment(o interface{}) (err error) {
	switch o := o.(type) {
	default:
		err = errors.New("Unexpected operation type")
	case *xdr.PaymentOp:
		o.Amount, err = amount.Parse(m.Amount)
		if err != nil {
			return
		}

		o.Asset, err = xdr.NewAsset(xdr.AssetTypeAssetTypeNative, nil)
	case *xdr.PathPaymentOp:
		o.DestAmount, err = amount.Parse(m.Amount)
		if err != nil {
			return
		}

		o.DestAsset, err = xdr.NewAsset(xdr.AssetTypeAssetTypeNative, nil)
	}
	return
}
func (m PayWithPath) MutatePayment(o interface{}) (err error) {
	var pathPaymentOp *xdr.PathPaymentOp
	var ok bool
	if pathPaymentOp, ok = o.(*xdr.PathPaymentOp); !ok {
		return errors.New("Unexpected operation type")
	}

	// MaxAmount
	pathPaymentOp.SendMax, err = amount.Parse(m.MaxAmount)
	if err != nil {
		return
	}

	// Path
	var path []xdr.Asset
	var xdrAsset xdr.Asset

	for _, asset := range m.Path {
		xdrAsset, err = asset.ToXdrObject()
		if err != nil {
			return err
		}

		path = append(path, xdrAsset)
	}

	pathPaymentOp.Path = path

	// Asset
	pathPaymentOp.SendAsset, err = m.Asset.ToXdrObject()
	return
}
func (m Destination) MutateAccountMerge(o *AccountMergeBuilder) error {
	return setAccountId(m.AddressOrSeed, &o.Destination)
}
func MustParse(v string) xdr.Int64 {
	ret, err := Parse(v)
	if err != nil {
		panic(err)
	}
	return ret
}
func Parse(v string) (xdr.Int64, error) {
	var f, o, r big.Rat

	_, ok := f.SetString(v)
	if !ok {
		return xdr.Int64(0), fmt.Errorf("cannot parse amount: %s", v)
	}

	o.SetInt64(One)
	r.Mul(&f, &o)

	is := r.FloatString(0)
	i, err := strconv.ParseInt(is, 10, 64)
	if err != nil {
		return xdr.Int64(0), err
	}
	return xdr.Int64(i), nil
}
func String(v xdr.Int64) string {
	var f, o, r big.Rat

	f.SetInt64(int64(v))
	o.SetInt64(One)
	r.Quo(&f, &o)

	return r.FloatString(7)
}
func CreateOffer(rate Rate, amount Amount) (result ManageOfferBuilder) {
	return ManageOffer(false, rate, amount)
}
func CreatePassiveOffer(rate Rate, amount Amount) (result ManageOfferBuilder) {
	return ManageOffer(true, rate, amount)
}
func UpdateOffer(rate Rate, amount Amount, offerID OfferID) (result ManageOfferBuilder) {
	return ManageOffer(false, rate, amount, offerID)
}
func DeleteOffer(rate Rate, offerID OfferID) (result ManageOfferBuilder) {
	return ManageOffer(false, rate, Amount("0"), offerID)
}
func ManageOffer(passiveOffer bool, muts ...interface{}) (result ManageOfferBuilder) {
	result.PassiveOffer = passiveOffer
	result.Mutate(muts...)
	return
}
func (b *ManageOfferBuilder) Mutate(muts ...interface{}) {
	for _, m := range muts {
		var err error
		switch mut := m.(type) {
		case ManageOfferMutator:
			if b.PassiveOffer {
				err = mut.MutateManageOffer(&b.PO)
			} else {
				err = mut.MutateManageOffer(&b.MO)
			}
		case OperationMutator:
			err = mut.MutateOperation(&b.O)
		default:
			err = errors.New("Mutator type not allowed")
		}

		if err != nil {
			b.Err = err
			return
		}
	}
}
func (m Amount) MutateManageOffer(o interface{}) (err error) {
	switch o := o.(type) {
	default:
		err = errors.New("Unexpected operation type")
	case *xdr.ManageOfferOp:
		o.Amount, err = amount.Parse(string(m))
	case *xdr.CreatePassiveOfferOp:
		o.Amount, err = amount.Parse(string(m))
	}
	return
}
func (m OfferID) MutateManageOffer(o interface{}) (err error) {
	switch o := o.(type) {
	default:
		err = errors.New("Unexpected operation type")
	case *xdr.ManageOfferOp:
		o.OfferId = xdr.Uint64(m)
	}
	return
}
func (m Rate) MutateManageOffer(o interface{}) (err error) {
	switch o := o.(type) {
	default:
		err = errors.New("Unexpected operation type")
	case *xdr.ManageOfferOp:
		o.Selling, err = m.Selling.ToXdrObject()
		if err != nil {
			return
		}

		o.Buying, err = m.Buying.ToXdrObject()
		if err != nil {
			return
		}

		o.Price, err = price.Parse(string(m.Price))
	case *xdr.CreatePassiveOfferOp:
		o.Selling, err = m.Selling.ToXdrObject()
		if err != nil {
			return
		}

		o.Buying, err = m.Buying.ToXdrObject()
		if err != nil {
			return
		}

		o.Price, err = price.Parse(string(m.Price))
	}
	return
}
func (m SourceAccount) MutateOperation(o *xdr.Operation) error {
	o.SourceAccount = &xdr.AccountId{}
	return setAccountId(m.AddressOrSeed, o.SourceAccount)
}
func (p *Price) String() string {
	return big.NewRat(int64(p.N), int64(p.D)).FloatString(7)
}
func Transaction(muts ...TransactionMutator) (result *TransactionBuilder) {
	result = &TransactionBuilder{}
	result.Mutate(muts...)
	result.Mutate(Defaults{})
	return
}
func (b *TransactionBuilder) Mutate(muts ...TransactionMutator) {
	if b.TX == nil {
		b.TX = &xdr.Transaction{}
	}

	for _, m := range muts {
		err := m.MutateTransaction(b)
		if err != nil {
			b.Err = err
			return
		}
	}
}
func (b *TransactionBuilder) Hash() ([32]byte, error) {
	var txBytes bytes.Buffer

	_, err := fmt.Fprintf(&txBytes, "%s", b.NetworkID)
	if err != nil {
		return [32]byte{}, err
	}

	_, err = xdr.Marshal(&txBytes, xdr.EnvelopeTypeEnvelopeTypeTx)
	if err != nil {
		return [32]byte{}, err
	}

	_, err = xdr.Marshal(&txBytes, b.TX)
	if err != nil {
		return [32]byte{}, err
	}

	return hash.Hash(txBytes.Bytes()), nil
}
func (b *TransactionBuilder) HashHex() (string, error) {
	hash, err := b.Hash()
	if err != nil {
		return "", err
	}

	return hex.EncodeToString(hash[:]), nil
}
func (b *TransactionBuilder) Sign(signers ...string) (result TransactionEnvelopeBuilder) {
	result.Mutate(b)

	for _, s := range signers {
		result.Mutate(Sign{s})
	}

	return
}
func (m AllowTrustBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeAllowTrust, m.AT)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m ChangeTrustBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeChangeTrust, m.CT)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m CreateAccountBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeCreateAccount, m.CA)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m Defaults) MutateTransaction(o *TransactionBuilder) error {

	if o.TX.Fee == 0 {
		o.TX.Fee = xdr.Uint32(100 * len(o.TX.Operations))
	}

	if o.NetworkID == [32]byte{} {
		o.NetworkID = DefaultNetwork.ID()
	}
	return nil
}
func (m InflationBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeInflation, nil)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m ManageDataBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeManageData, m.MD)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m ManageOfferBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	if m.PassiveOffer {
		m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeCreatePassiveOffer, m.PO)
		o.TX.Operations = append(o.TX.Operations, m.O)
	} else {
		m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeManageOffer, m.MO)
		o.TX.Operations = append(o.TX.Operations, m.O)
	}
	return m.Err
}
func (m MemoHash) MutateTransaction(o *TransactionBuilder) (err error) {
	o.TX.Memo, err = xdr.NewMemo(xdr.MemoTypeMemoHash, m.Value)
	return
}
func (m MemoID) MutateTransaction(o *TransactionBuilder) (err error) {
	o.TX.Memo, err = xdr.NewMemo(xdr.MemoTypeMemoId, xdr.Uint64(m.Value))
	return
}
func (m MemoReturn) MutateTransaction(o *TransactionBuilder) (err error) {
	o.TX.Memo, err = xdr.NewMemo(xdr.MemoTypeMemoReturn, m.Value)
	return
}
func (m MemoText) MutateTransaction(o *TransactionBuilder) (err error) {

	if len([]byte(m.Value)) > MemoTextMaxLength {
		err = errors.New("Memo too long; over 28 bytes")
		return
	}

	o.TX.Memo, err = xdr.NewMemo(xdr.MemoTypeMemoText, m.Value)
	return
}
func (m Network) MutateTransaction(o *TransactionBuilder) error {
	o.NetworkID = m.ID()
	return nil
}
func (m PaymentBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	if m.PathPayment {
		m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypePathPayment, m.PP)
		o.TX.Operations = append(o.TX.Operations, m.O)
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypePayment, m.P)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m SetOptionsBuilder) MutateTransaction(o *TransactionBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	m.O.Body, m.Err = xdr.NewOperationBody(xdr.OperationTypeSetOptions, m.SO)
	o.TX.Operations = append(o.TX.Operations, m.O)
	return m.Err
}
func (m Sequence) MutateTransaction(o *TransactionBuilder) error {
	o.TX.SeqNum = xdr.SequenceNumber(m.Sequence)
	return nil
}
func (m SourceAccount) MutateTransaction(o *TransactionBuilder) error {
	return setAccountId(m.AddressOrSeed, &o.TX.SourceAccount)
}
func (t *Int64) Scan(src interface{}) error {
	val, ok := src.(int64)
	if !ok {
		return errors.New("Invalid value for xdr.Int64")
	}

	*t = Int64(val)
	return nil
}
func (b *Bundle) InitialState(key xdr.LedgerKey) (*xdr.LedgerEntry, error) {
	all := b.Changes(key)

	if len(all) == 0 {
		return nil, ErrMetaNotFound
	}

	first := all[0]

	if first.Type != xdr.LedgerEntryChangeTypeLedgerEntryState {
		return nil, nil
	}

	result := first.MustState()

	return &result, nil
}
func (b *Bundle) Changes(target xdr.LedgerKey) (ret []xdr.LedgerEntryChange) {
	return b.changes(target, math.MaxInt32)
}
func (b *Bundle) StateAfter(key xdr.LedgerKey, opidx int) (*xdr.LedgerEntry, error) {
	all := b.changes(key, opidx)

	if len(all) == 0 {
		return nil, ErrMetaNotFound
	}

	change := all[len(all)-1]

	switch change.Type {
	case xdr.LedgerEntryChangeTypeLedgerEntryCreated:
		entry := change.MustCreated()
		return &entry, nil
	case xdr.LedgerEntryChangeTypeLedgerEntryRemoved:
		return nil, nil
	case xdr.LedgerEntryChangeTypeLedgerEntryUpdated:
		entry := change.MustUpdated()
		return &entry, nil
	case xdr.LedgerEntryChangeTypeLedgerEntryState:
		// scott: stellar-core should not emit a lone state entry, and we are
		// retrieving changes from the end of the collection.  If this situation
		// occurs, it means that I didn't understand something correctly or there is
		// a bug in stellar-core.
		panic(fmt.Errorf("Unexpected 'state' entry"))
	default:
		panic(fmt.Errorf("Unknown change type: %v", change.Type))
	}
}
func (b *Bundle) changes(target xdr.LedgerKey, maxOp int) (ret []xdr.LedgerEntryChange) {
	for _, change := range b.FeeMeta {
		key := change.LedgerKey()

		if !key.Equals(target) {
			continue
		}

		ret = append(ret, change)
	}

	for i, op := range b.TransactionMeta.MustOperations() {
		if i > maxOp {
			break
		}

		for _, change := range op.Changes {
			key := change.LedgerKey()

			if !key.Equals(target) {
				continue
			}

			ret = append(ret, change)
		}
	}

	return
}
func MustDecode(expected VersionByte, src string) []byte {
	d, err := Decode(expected, src)
	if err != nil {
		panic(err)
	}
	return d
}
func Encode(version VersionByte, src []byte) (string, error) {
	if err := checkValidVersionByte(version); err != nil {
		return "", err
	}

	var raw bytes.Buffer

	// write version byte
	if err := binary.Write(&raw, binary.LittleEndian, version); err != nil {
		return "", err
	}

	// write payload
	if _, err := raw.Write(src); err != nil {
		return "", err
	}

	// calculate and write checksum
	checksum := crc16.Checksum(raw.Bytes())
	if _, err := raw.Write(checksum); err != nil {
		return "", err
	}

	result := base32.StdEncoding.EncodeToString(raw.Bytes())
	return result, nil
}
func MustEncode(version VersionByte, src []byte) string {
	e, err := Encode(version, src)
	if err != nil {
		panic(err)
	}
	return e
}
func checkValidVersionByte(version VersionByte) error {
	if version == VersionByteAccountID {
		return nil
	}
	if version == VersionByteSeed {
		return nil
	}

	return ErrInvalidVersionByte
}
func Checksum(data []byte) []byte {
	var crc uint16
	var out bytes.Buffer
	for _, b := range data {
		crc = ((crc << 8) & 0xffff) ^ crc16tab[((crc>>8)^uint16(b))&0x00FF]
	}

	err := binary.Write(&out, binary.LittleEndian, crc)
	if err != nil {
		panic(err)
	}

	return out.Bytes()
}
func Validate(data []byte, expected []byte) error {

	actual := Checksum(data)

	// validate the provided checksum against the calculated
	if !bytes.Equal(actual, expected) {
		return ErrInvalidChecksum
	}

	return nil
}
func (change *LedgerEntryChange) LedgerKey() LedgerKey {
	switch change.Type {
	case LedgerEntryChangeTypeLedgerEntryCreated:
		change := change.MustCreated()
		return change.LedgerKey()
	case LedgerEntryChangeTypeLedgerEntryRemoved:
		return change.MustRemoved()
	case LedgerEntryChangeTypeLedgerEntryUpdated:
		change := change.MustUpdated()
		return change.LedgerKey()
	case LedgerEntryChangeTypeLedgerEntryState:
		change := change.MustState()
		return change.LedgerKey()
	default:
		panic(fmt.Errorf("Unknown change type: %v", change.Type))
	}
}
func checkPlausible() {
	for _, r := range prefix {
		if !strings.ContainsRune(alphabet, r) {
			fmt.Printf("Invalid prefix: %s is not in the base32 alphabet\n", strconv.QuoteRune(r))
			os.Exit(1)
		}
	}
}
func (aid *AccountId) Address() string {
	if aid == nil {
		return ""
	}

	switch aid.Type {
	case CryptoKeyTypeKeyTypeEd25519:
		ed := aid.MustEd25519()
		raw := make([]byte, 32)
		copy(raw, ed[:])
		return strkey.MustEncode(strkey.VersionByteAccountID, raw)
	default:
		panic(fmt.Errorf("Unknown account id type: %v", aid.Type))
	}
}
func (aid *AccountId) Equals(other AccountId) bool {
	if aid.Type != other.Type {
		return false
	}

	switch aid.Type {
	case CryptoKeyTypeKeyTypeEd25519:
		l := aid.MustEd25519()
		r := other.MustEd25519()
		return l == r
	default:
		panic(fmt.Errorf("Unknown account id type: %v", aid.Type))
	}
}
func (aid *AccountId) SetAddress(address string) error {
	if aid == nil {
		return nil
	}

	raw, err := strkey.Decode(strkey.VersionByteAccountID, address)
	if err != nil {
		return err
	}

	if len(raw) != 32 {
		return errors.New("invalid address")
	}

	var ui Uint256
	copy(ui[:], raw)

	*aid, err = NewAccountId(CryptoKeyTypeKeyTypeEd25519, ui)

	return err
}
func (a Asset) ToXdrObject() (xdr.Asset, error) {
	if a.Native {
		return xdr.NewAsset(xdr.AssetTypeAssetTypeNative, nil)
	}

	var issuer xdr.AccountId
	err := setAccountId(a.Issuer, &issuer)
	if err != nil {
		return xdr.Asset{}, err
	}

	length := len(a.Code)
	switch {
	case length >= 1 && length <= 4:
		var codeArray [4]byte
		byteArray := []byte(a.Code)
		copy(codeArray[:], byteArray[0:length])
		asset := xdr.AssetAlphaNum4{codeArray, issuer}
		return xdr.NewAsset(xdr.AssetTypeAssetTypeCreditAlphanum4, asset)
	case length >= 5 && length <= 12:
		var codeArray [12]byte
		byteArray := []byte(a.Code)
		copy(codeArray[:], byteArray[0:length])
		asset := xdr.AssetAlphaNum12{codeArray, issuer}
		return xdr.NewAsset(xdr.AssetTypeAssetTypeCreditAlphanum12, asset)
	default:
		return xdr.Asset{}, errors.New("Asset code length is invalid")
	}
}
func (pathSend PayWithPath) Through(asset Asset) PayWithPath {
	pathSend.Path = append(pathSend.Path, asset)
	return pathSend
}
func PayWith(sendAsset Asset, maxAmount string) PayWithPath {
	return PayWithPath{
		Asset:     sendAsset,
		MaxAmount: maxAmount,
	}
}
func continuedFraction(price string) (xdrPrice xdr.Price, err error) {
	number := &big.Rat{}
	maxInt32 := &big.Rat{}
	zero := &big.Rat{}
	one := &big.Rat{}

	_, ok := number.SetString(price)
	if !ok {
		return xdrPrice, fmt.Errorf("cannot parse price: %s", price)
	}

	maxInt32.SetInt64(int64(math.MaxInt32))
	zero.SetInt64(int64(0))
	one.SetInt64(int64(1))

	fractions := [][2]*big.Rat{
		{zero, one},
		{one, zero},
	}

	i := 2
	for {
		if number.Cmp(maxInt32) == 1 {
			break
		}

		f := &big.Rat{}
		h := &big.Rat{}
		k := &big.Rat{}

		a := floor(number)
		f.Sub(number, a)
		h.Mul(a, fractions[i-1][0])
		h.Add(h, fractions[i-2][0])
		k.Mul(a, fractions[i-1][1])
		k.Add(k, fractions[i-2][1])

		if h.Cmp(maxInt32) == 1 || k.Cmp(maxInt32) == 1 {
			break
		}

		fractions = append(fractions, [2]*big.Rat{h, k})
		if f.Cmp(zero) == 0 {
			break
		}
		number.Quo(one, f)
		i++
	}

	n, d := fractions[len(fractions)-1][0], fractions[len(fractions)-1][1]

	if n.Cmp(zero) == 0 || d.Cmp(zero) == 0 {
		return xdrPrice, errors.New("Couldn't find approximation")
	}

	return xdr.Price{
		N: xdr.Int32(n.Num().Int64()),
		D: xdr.Int32(d.Num().Int64()),
	}, nil
}
func (b *TransactionEnvelopeBuilder) Mutate(muts ...TransactionEnvelopeMutator) {
	b.Init()

	for _, m := range muts {
		err := m.MutateTransactionEnvelope(b)
		if err != nil {
			b.Err = err
			return
		}
	}
}
func (b *TransactionEnvelopeBuilder) MutateTX(muts ...TransactionMutator) {
	b.Init()

	if b.Err != nil {
		return
	}

	b.child.Mutate(muts...)
	b.Err = b.child.Err
}
func (b *TransactionEnvelopeBuilder) Bytes() ([]byte, error) {
	if b.Err != nil {
		return nil, b.Err
	}

	var txBytes bytes.Buffer
	_, err := xdr.Marshal(&txBytes, b.E)
	if err != nil {
		return nil, err
	}

	return txBytes.Bytes(), nil
}
func (b *TransactionEnvelopeBuilder) Base64() (string, error) {
	bs, err := b.Bytes()
	return base64.StdEncoding.EncodeToString(bs), err
}
func (m *TransactionBuilder) MutateTransactionEnvelope(txe *TransactionEnvelopeBuilder) error {
	if m.Err != nil {
		return m.Err
	}

	txe.E.Tx = *m.TX
	newChild := *m
	txe.child = &newChild
	m.TX = &txe.E.Tx
	return nil
}
func (m HomeDomain) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	if len(m) > 32 {
		return errors.New("HomeDomain is too long")
	}

	value := xdr.String32(m)
	o.HomeDomain = &value
	return
}
func (m InflationDest) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	o.InflationDest = &xdr.AccountId{}
	err = setAccountId(string(m), o.InflationDest)
	return
}
func (m MasterWeight) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	val := xdr.Uint32(m)
	o.MasterWeight = &val
	return
}
func (m Signer) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	var signer xdr.Signer
	signer.Weight = xdr.Uint32(m.Weight)
	err = setAccountId(m.PublicKey, &signer.PubKey)
	o.Signer = &signer
	return
}
func SetThresholds(low, medium, high uint32) Thresholds {
	return Thresholds{
		Low:    &low,
		Medium: &medium,
		High:   &high,
	}
}
func (m Thresholds) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	if m.Low != nil {
		val := xdr.Uint32(*m.Low)
		o.LowThreshold = &val
	}

	if m.Medium != nil {
		val := xdr.Uint32(*m.Medium)
		o.MedThreshold = &val
	}

	if m.High != nil {
		val := xdr.Uint32(*m.High)
		o.HighThreshold = &val
	}

	return
}
func (m SetFlag) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	if !isFlagValid(xdr.AccountFlags(m)) {
		return errors.New("Unknown flag in SetFlag mutator")
	}

	var val xdr.Uint32
	if o.SetFlags == nil {
		val = xdr.Uint32(m)
	} else {
		val = xdr.Uint32(m) | *o.SetFlags
	}
	o.SetFlags = &val
	return
}
func (m ClearFlag) MutateSetOptions(o *xdr.SetOptionsOp) (err error) {
	if !isFlagValid(xdr.AccountFlags(m)) {
		return errors.New("Unknown flag in SetFlag mutator")
	}

	var val xdr.Uint32
	if o.ClearFlags == nil {
		val = xdr.Uint32(m)
	} else {
		val = xdr.Uint32(m) | *o.ClearFlags
	}
	o.ClearFlags = &val
	return
}
func (m Destination) MutateCreateAccount(o *xdr.CreateAccountOp) error {
	return setAccountId(m.AddressOrSeed, &o.Destination)
}
func (m NativeAmount) MutateCreateAccount(o *xdr.CreateAccountOp) (err error) {
	o.StartingBalance, err = amount.Parse(m.Amount)
	return
}
func Random() (*Full, error) {
	var rawSeed [32]byte

	_, err := io.ReadFull(rand.Reader, rawSeed[:])
	if err != nil {
		return nil, err
	}

	kp, err := FromRawSeed(rawSeed)

	if err != nil {
		return nil, err
	}

	return kp, nil
}
func Master(networkPassphrase string) KP {
	kp, err := FromRawSeed(network.ID(networkPassphrase))

	if err != nil {
		panic(err)
	}

	return kp
}
func Parse(addressOrSeed string) (KP, error) {
	_, err := strkey.Decode(strkey.VersionByteAccountID, addressOrSeed)
	if err == nil {
		return &FromAddress{addressOrSeed}, nil
	}

	if err != strkey.ErrInvalidVersionByte {
		return nil, err
	}

	_, err = strkey.Decode(strkey.VersionByteSeed, addressOrSeed)
	if err == nil {
		return &Full{addressOrSeed}, nil
	}

	return nil, err
}
func MustParse(addressOrSeed string) KP {
	kp, err := Parse(addressOrSeed)
	if err != nil {
		panic(err)
	}

	return kp
}
func (m Authorize) MutateAllowTrust(o *xdr.AllowTrustOp) error {
	o.Authorize = m.Value
	return nil
}
func (m AllowTrustAsset) MutateAllowTrust(o *xdr.AllowTrustOp) (err error) {
	length := len(m.Code)

	switch {
	case length >= 1 && length <= 4:
		var code [4]byte
		byteArray := []byte(m.Code)
		copy(code[:], byteArray[0:length])
		o.Asset, err = xdr.NewAllowTrustOpAsset(xdr.AssetTypeAssetTypeCreditAlphanum4, code)
	case length >= 5 && length <= 12:
		var code [12]byte
		byteArray := []byte(m.Code)
		copy(code[:], byteArray[0:length])
		o.Asset, err = xdr.NewAllowTrustOpAsset(xdr.AssetTypeAssetTypeCreditAlphanum12, code)
	default:
		err = errors.New("Asset code length is invalid")
	}

	return
}
func (m Trustor) MutateAllowTrust(o *xdr.AllowTrustOp) error {
	return setAccountId(m.Address, &o.Trustor)
}
func (a AllowTrustOpAsset) ToAsset(issuer AccountId) (ret Asset) {
	var err error

	switch a.Type {
	case AssetTypeAssetTypeCreditAlphanum4:

		ret, err = NewAsset(AssetTypeAssetTypeCreditAlphanum4, AssetAlphaNum4{
			AssetCode: a.MustAssetCode4(),
			Issuer:    issuer,
		})
	case AssetTypeAssetTypeCreditAlphanum12:
		ret, err = NewAsset(AssetTypeAssetTypeCreditAlphanum12, AssetAlphaNum12{
			AssetCode: a.MustAssetCode12(),
			Issuer:    issuer,
		})
	default:
		err = fmt.Errorf("Unexpected type for AllowTrustOpAsset: %d", a.Type)
	}

	if err != nil {
		panic(err)
	}
	return
}
func (a *Asset) SetNative() error {
	newa, err := NewAsset(AssetTypeAssetTypeNative, nil)
	if err != nil {
		return err
	}
	*a = newa
	return nil
}
func (a Asset) String() string {
	var t, c, i string

	a.MustExtract(&t, &c, &i)

	if a.Type == AssetTypeAssetTypeNative {
		return t
	}

	return fmt.Sprintf("%s/%s/%s", t, c, i)
}
func (a Asset) Equals(other Asset) bool {
	if a.Type != other.Type {
		return false
	}
	switch a.Type {
	case AssetTypeAssetTypeNative:
		return true
	case AssetTypeAssetTypeCreditAlphanum4:
		l := a.MustAlphaNum4()
		r := other.MustAlphaNum4()
		return l.AssetCode == r.AssetCode && l.Issuer.Equals(r.Issuer)
	case AssetTypeAssetTypeCreditAlphanum12:
		l := a.MustAlphaNum12()
		r := other.MustAlphaNum12()
		return l.AssetCode == r.AssetCode && l.Issuer.Equals(r.Issuer)
	default:
		panic(fmt.Errorf("Unknown asset type: %v", a.Type))
	}
}
func (a Asset) MustExtract(typ interface{}, code interface{}, issuer interface{}) {
	err := a.Extract(typ, code, issuer)

	if err != nil {
		panic(err)
	}
}
func Unmarshal(r io.Reader, v interface{}) (int, error) {
	// delegate to xdr package's Unmarshal
	return xdr.Unmarshal(r, v)
}
func Marshal(w io.Writer, v interface{}) (int, error) {
	// delegate to xdr package's Marshal
	return xdr.Marshal(w, v)
}
func (e CryptoKeyType) ValidEnum(v int32) bool {
	_, ok := cryptoKeyTypeMap[v]
	return ok
}
func NewPublicKey(aType CryptoKeyType, value interface{}) (result PublicKey, err error) {
	result.Type = aType
	switch CryptoKeyType(aType) {
	case CryptoKeyTypeKeyTypeEd25519:
		tv, ok := value.(Uint256)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint256")
			return
		}
		result.Ed25519 = &tv
	}
	return
}
func (u PublicKey) MustEd25519() Uint256 {
	val, ok := u.GetEd25519()

	if !ok {
		panic("arm Ed25519 is not set")
	}

	return val
}
func NewNodeId(aType CryptoKeyType, value interface{}) (result NodeId, err error) {
	u, err := NewPublicKey(aType, value)
	result = NodeId(u)
	return
}
func NewAccountId(aType CryptoKeyType, value interface{}) (result AccountId, err error) {
	u, err := NewPublicKey(aType, value)
	result = AccountId(u)
	return
}
func (e AssetType) ValidEnum(v int32) bool {
	_, ok := assetTypeMap[v]
	return ok
}
func (u Asset) ArmForSwitch(sw int32) (string, bool) {
	switch AssetType(sw) {
	case AssetTypeAssetTypeNative:
		return "", true
	case AssetTypeAssetTypeCreditAlphanum4:
		return "AlphaNum4", true
	case AssetTypeAssetTypeCreditAlphanum12:
		return "AlphaNum12", true
	}
	return "-", false
}
func NewAsset(aType AssetType, value interface{}) (result Asset, err error) {
	result.Type = aType
	switch AssetType(aType) {
	case AssetTypeAssetTypeNative:
		// void
	case AssetTypeAssetTypeCreditAlphanum4:
		tv, ok := value.(AssetAlphaNum4)
		if !ok {
			err = fmt.Errorf("invalid value, must be AssetAlphaNum4")
			return
		}
		result.AlphaNum4 = &tv
	case AssetTypeAssetTypeCreditAlphanum12:
		tv, ok := value.(AssetAlphaNum12)
		if !ok {
			err = fmt.Errorf("invalid value, must be AssetAlphaNum12")
			return
		}
		result.AlphaNum12 = &tv
	}
	return
}
func (u Asset) MustAlphaNum4() AssetAlphaNum4 {
	val, ok := u.GetAlphaNum4()

	if !ok {
		panic("arm AlphaNum4 is not set")
	}

	return val
}
func (u Asset) GetAlphaNum4() (result AssetAlphaNum4, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AlphaNum4" {
		result = *u.AlphaNum4
		ok = true
	}

	return
}
func (u Asset) MustAlphaNum12() AssetAlphaNum12 {
	val, ok := u.GetAlphaNum12()

	if !ok {
		panic("arm AlphaNum12 is not set")
	}

	return val
}
func (u Asset) GetAlphaNum12() (result AssetAlphaNum12, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AlphaNum12" {
		result = *u.AlphaNum12
		ok = true
	}

	return
}
func (e ThresholdIndexes) ValidEnum(v int32) bool {
	_, ok := thresholdIndexesMap[v]
	return ok
}
func (e LedgerEntryType) ValidEnum(v int32) bool {
	_, ok := ledgerEntryTypeMap[v]
	return ok
}
func (e AccountFlags) ValidEnum(v int32) bool {
	_, ok := accountFlagsMap[v]
	return ok
}
func NewAccountEntryExt(v int32, value interface{}) (result AccountEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func (e TrustLineFlags) ValidEnum(v int32) bool {
	_, ok := trustLineFlagsMap[v]
	return ok
}
func NewTrustLineEntryExt(v int32, value interface{}) (result TrustLineEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func (e OfferEntryFlags) ValidEnum(v int32) bool {
	_, ok := offerEntryFlagsMap[v]
	return ok
}
func NewOfferEntryExt(v int32, value interface{}) (result OfferEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func NewDataEntryExt(v int32, value interface{}) (result DataEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func NewLedgerEntryData(aType LedgerEntryType, value interface{}) (result LedgerEntryData, err error) {
	result.Type = aType
	switch LedgerEntryType(aType) {
	case LedgerEntryTypeAccount:
		tv, ok := value.(AccountEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be AccountEntry")
			return
		}
		result.Account = &tv
	case LedgerEntryTypeTrustline:
		tv, ok := value.(TrustLineEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be TrustLineEntry")
			return
		}
		result.TrustLine = &tv
	case LedgerEntryTypeOffer:
		tv, ok := value.(OfferEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be OfferEntry")
			return
		}
		result.Offer = &tv
	case LedgerEntryTypeData:
		tv, ok := value.(DataEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be DataEntry")
			return
		}
		result.Data = &tv
	}
	return
}
func NewLedgerEntryExt(v int32, value interface{}) (result LedgerEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func (e EnvelopeType) ValidEnum(v int32) bool {
	_, ok := envelopeTypeMap[v]
	return ok
}
func (e OperationType) ValidEnum(v int32) bool {
	_, ok := operationTypeMap[v]
	return ok
}
func (u AllowTrustOpAsset) ArmForSwitch(sw int32) (string, bool) {
	switch AssetType(sw) {
	case AssetTypeAssetTypeCreditAlphanum4:
		return "AssetCode4", true
	case AssetTypeAssetTypeCreditAlphanum12:
		return "AssetCode12", true
	}
	return "-", false
}
func NewAllowTrustOpAsset(aType AssetType, value interface{}) (result AllowTrustOpAsset, err error) {
	result.Type = aType
	switch AssetType(aType) {
	case AssetTypeAssetTypeCreditAlphanum4:
		tv, ok := value.([4]byte)
		if !ok {
			err = fmt.Errorf("invalid value, must be [4]byte")
			return
		}
		result.AssetCode4 = &tv
	case AssetTypeAssetTypeCreditAlphanum12:
		tv, ok := value.([12]byte)
		if !ok {
			err = fmt.Errorf("invalid value, must be [12]byte")
			return
		}
		result.AssetCode12 = &tv
	}
	return
}
func (u AllowTrustOpAsset) MustAssetCode4() [4]byte {
	val, ok := u.GetAssetCode4()

	if !ok {
		panic("arm AssetCode4 is not set")
	}

	return val
}
func (u AllowTrustOpAsset) GetAssetCode4() (result [4]byte, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AssetCode4" {
		result = *u.AssetCode4
		ok = true
	}

	return
}
func (u AllowTrustOpAsset) MustAssetCode12() [12]byte {
	val, ok := u.GetAssetCode12()

	if !ok {
		panic("arm AssetCode12 is not set")
	}

	return val
}
func (u AllowTrustOpAsset) GetAssetCode12() (result [12]byte, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AssetCode12" {
		result = *u.AssetCode12
		ok = true
	}

	return
}
func NewOperationBody(aType OperationType, value interface{}) (result OperationBody, err error) {
	result.Type = aType
	switch OperationType(aType) {
	case OperationTypeCreateAccount:
		tv, ok := value.(CreateAccountOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be CreateAccountOp")
			return
		}
		result.CreateAccountOp = &tv
	case OperationTypePayment:
		tv, ok := value.(PaymentOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be PaymentOp")
			return
		}
		result.PaymentOp = &tv
	case OperationTypePathPayment:
		tv, ok := value.(PathPaymentOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be PathPaymentOp")
			return
		}
		result.PathPaymentOp = &tv
	case OperationTypeManageOffer:
		tv, ok := value.(ManageOfferOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be ManageOfferOp")
			return
		}
		result.ManageOfferOp = &tv
	case OperationTypeCreatePassiveOffer:
		tv, ok := value.(CreatePassiveOfferOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be CreatePassiveOfferOp")
			return
		}
		result.CreatePassiveOfferOp = &tv
	case OperationTypeSetOptions:
		tv, ok := value.(SetOptionsOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be SetOptionsOp")
			return
		}
		result.SetOptionsOp = &tv
	case OperationTypeChangeTrust:
		tv, ok := value.(ChangeTrustOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be ChangeTrustOp")
			return
		}
		result.ChangeTrustOp = &tv
	case OperationTypeAllowTrust:
		tv, ok := value.(AllowTrustOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be AllowTrustOp")
			return
		}
		result.AllowTrustOp = &tv
	case OperationTypeAccountMerge:
		tv, ok := value.(AccountId)
		if !ok {
			err = fmt.Errorf("invalid value, must be AccountId")
			return
		}
		result.Destination = &tv
	case OperationTypeInflation:
		// void
	case OperationTypeManageData:
		tv, ok := value.(ManageDataOp)
		if !ok {
			err = fmt.Errorf("invalid value, must be ManageDataOp")
			return
		}
		result.ManageDataOp = &tv
	}
	return
}
func (u OperationBody) MustCreateAccountOp() CreateAccountOp {
	val, ok := u.GetCreateAccountOp()

	if !ok {
		panic("arm CreateAccountOp is not set")
	}

	return val
}
func (u OperationBody) GetCreateAccountOp() (result CreateAccountOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "CreateAccountOp" {
		result = *u.CreateAccountOp
		ok = true
	}

	return
}
func (u OperationBody) MustPaymentOp() PaymentOp {
	val, ok := u.GetPaymentOp()

	if !ok {
		panic("arm PaymentOp is not set")
	}

	return val
}
func (u OperationBody) GetPaymentOp() (result PaymentOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "PaymentOp" {
		result = *u.PaymentOp
		ok = true
	}

	return
}
func (u OperationBody) MustPathPaymentOp() PathPaymentOp {
	val, ok := u.GetPathPaymentOp()

	if !ok {
		panic("arm PathPaymentOp is not set")
	}

	return val
}
func (u OperationBody) GetPathPaymentOp() (result PathPaymentOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "PathPaymentOp" {
		result = *u.PathPaymentOp
		ok = true
	}

	return
}
func (u OperationBody) MustManageOfferOp() ManageOfferOp {
	val, ok := u.GetManageOfferOp()

	if !ok {
		panic("arm ManageOfferOp is not set")
	}

	return val
}
func (u OperationBody) GetManageOfferOp() (result ManageOfferOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "ManageOfferOp" {
		result = *u.ManageOfferOp
		ok = true
	}

	return
}
func (u OperationBody) MustCreatePassiveOfferOp() CreatePassiveOfferOp {
	val, ok := u.GetCreatePassiveOfferOp()

	if !ok {
		panic("arm CreatePassiveOfferOp is not set")
	}

	return val
}
func (u OperationBody) GetCreatePassiveOfferOp() (result CreatePassiveOfferOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "CreatePassiveOfferOp" {
		result = *u.CreatePassiveOfferOp
		ok = true
	}

	return
}
func (u OperationBody) MustSetOptionsOp() SetOptionsOp {
	val, ok := u.GetSetOptionsOp()

	if !ok {
		panic("arm SetOptionsOp is not set")
	}

	return val
}
func (u OperationBody) GetSetOptionsOp() (result SetOptionsOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "SetOptionsOp" {
		result = *u.SetOptionsOp
		ok = true
	}

	return
}
func (u OperationBody) MustChangeTrustOp() ChangeTrustOp {
	val, ok := u.GetChangeTrustOp()

	if !ok {
		panic("arm ChangeTrustOp is not set")
	}

	return val
}
func (u OperationBody) GetChangeTrustOp() (result ChangeTrustOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "ChangeTrustOp" {
		result = *u.ChangeTrustOp
		ok = true
	}

	return
}
func (u OperationBody) MustAllowTrustOp() AllowTrustOp {
	val, ok := u.GetAllowTrustOp()

	if !ok {
		panic("arm AllowTrustOp is not set")
	}

	return val
}
func (u OperationBody) GetAllowTrustOp() (result AllowTrustOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AllowTrustOp" {
		result = *u.AllowTrustOp
		ok = true
	}

	return
}
func (u OperationBody) MustDestination() AccountId {
	val, ok := u.GetDestination()

	if !ok {
		panic("arm Destination is not set")
	}

	return val
}
func (u OperationBody) GetDestination() (result AccountId, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Destination" {
		result = *u.Destination
		ok = true
	}

	return
}
func (u OperationBody) MustManageDataOp() ManageDataOp {
	val, ok := u.GetManageDataOp()

	if !ok {
		panic("arm ManageDataOp is not set")
	}

	return val
}
func (u OperationBody) GetManageDataOp() (result ManageDataOp, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "ManageDataOp" {
		result = *u.ManageDataOp
		ok = true
	}

	return
}
func (e MemoType) ValidEnum(v int32) bool {
	_, ok := memoTypeMap[v]
	return ok
}
func (u Memo) ArmForSwitch(sw int32) (string, bool) {
	switch MemoType(sw) {
	case MemoTypeMemoNone:
		return "", true
	case MemoTypeMemoText:
		return "Text", true
	case MemoTypeMemoId:
		return "Id", true
	case MemoTypeMemoHash:
		return "Hash", true
	case MemoTypeMemoReturn:
		return "RetHash", true
	}
	return "-", false
}
func NewMemo(aType MemoType, value interface{}) (result Memo, err error) {
	result.Type = aType
	switch MemoType(aType) {
	case MemoTypeMemoNone:
		// void
	case MemoTypeMemoText:
		tv, ok := value.(string)
		if !ok {
			err = fmt.Errorf("invalid value, must be string")
			return
		}
		result.Text = &tv
	case MemoTypeMemoId:
		tv, ok := value.(Uint64)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint64")
			return
		}
		result.Id = &tv
	case MemoTypeMemoHash:
		tv, ok := value.(Hash)
		if !ok {
			err = fmt.Errorf("invalid value, must be Hash")
			return
		}
		result.Hash = &tv
	case MemoTypeMemoReturn:
		tv, ok := value.(Hash)
		if !ok {
			err = fmt.Errorf("invalid value, must be Hash")
			return
		}
		result.RetHash = &tv
	}
	return
}
func (u Memo) MustText() string {
	val, ok := u.GetText()

	if !ok {
		panic("arm Text is not set")
	}

	return val
}
func (u Memo) GetText() (result string, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Text" {
		result = *u.Text
		ok = true
	}

	return
}
func (u Memo) MustId() Uint64 {
	val, ok := u.GetId()

	if !ok {
		panic("arm Id is not set")
	}

	return val
}
func (u Memo) GetId() (result Uint64, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Id" {
		result = *u.Id
		ok = true
	}

	return
}
func (u Memo) MustHash() Hash {
	val, ok := u.GetHash()

	if !ok {
		panic("arm Hash is not set")
	}

	return val
}
func (u Memo) GetHash() (result Hash, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Hash" {
		result = *u.Hash
		ok = true
	}

	return
}
func (u Memo) MustRetHash() Hash {
	val, ok := u.GetRetHash()

	if !ok {
		panic("arm RetHash is not set")
	}

	return val
}
func NewTransactionExt(v int32, value interface{}) (result TransactionExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func (e CreateAccountResultCode) ValidEnum(v int32) bool {
	_, ok := createAccountResultCodeMap[v]
	return ok
}
func (u CreateAccountResult) ArmForSwitch(sw int32) (string, bool) {
	switch CreateAccountResultCode(sw) {
	case CreateAccountResultCodeCreateAccountSuccess:
		return "", true
	default:
		return "", true
	}
}
func NewCreateAccountResult(code CreateAccountResultCode, value interface{}) (result CreateAccountResult, err error) {
	result.Code = code
	switch CreateAccountResultCode(code) {
	case CreateAccountResultCodeCreateAccountSuccess:
		// void
	default:
		// void
	}
	return
}
func (e PaymentResultCode) ValidEnum(v int32) bool {
	_, ok := paymentResultCodeMap[v]
	return ok
}
func (u PaymentResult) ArmForSwitch(sw int32) (string, bool) {
	switch PaymentResultCode(sw) {
	case PaymentResultCodePaymentSuccess:
		return "", true
	default:
		return "", true
	}
}
func NewPaymentResult(code PaymentResultCode, value interface{}) (result PaymentResult, err error) {
	result.Code = code
	switch PaymentResultCode(code) {
	case PaymentResultCodePaymentSuccess:
		// void
	default:
		// void
	}
	return
}
func (e PathPaymentResultCode) ValidEnum(v int32) bool {
	_, ok := pathPaymentResultCodeMap[v]
	return ok
}
func (u PathPaymentResult) ArmForSwitch(sw int32) (string, bool) {
	switch PathPaymentResultCode(sw) {
	case PathPaymentResultCodePathPaymentSuccess:
		return "Success", true
	case PathPaymentResultCodePathPaymentNoIssuer:
		return "NoIssuer", true
	default:
		return "", true
	}
}
func NewPathPaymentResult(code PathPaymentResultCode, value interface{}) (result PathPaymentResult, err error) {
	result.Code = code
	switch PathPaymentResultCode(code) {
	case PathPaymentResultCodePathPaymentSuccess:
		tv, ok := value.(PathPaymentResultSuccess)
		if !ok {
			err = fmt.Errorf("invalid value, must be PathPaymentResultSuccess")
			return
		}
		result.Success = &tv
	case PathPaymentResultCodePathPaymentNoIssuer:
		tv, ok := value.(Asset)
		if !ok {
			err = fmt.Errorf("invalid value, must be Asset")
			return
		}
		result.NoIssuer = &tv
	default:
		// void
	}
	return
}
func (u PathPaymentResult) MustNoIssuer() Asset {
	val, ok := u.GetNoIssuer()

	if !ok {
		panic("arm NoIssuer is not set")
	}

	return val
}
func (u PathPaymentResult) GetNoIssuer() (result Asset, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Code))

	if armName == "NoIssuer" {
		result = *u.NoIssuer
		ok = true
	}

	return
}
func (e ManageOfferResultCode) ValidEnum(v int32) bool {
	_, ok := manageOfferResultCodeMap[v]
	return ok
}
func (e ManageOfferEffect) ValidEnum(v int32) bool {
	_, ok := manageOfferEffectMap[v]
	return ok
}
func (u ManageOfferSuccessResultOffer) ArmForSwitch(sw int32) (string, bool) {
	switch ManageOfferEffect(sw) {
	case ManageOfferEffectManageOfferCreated:
		return "Offer", true
	case ManageOfferEffectManageOfferUpdated:
		return "Offer", true
	default:
		return "", true
	}
}
func NewManageOfferSuccessResultOffer(effect ManageOfferEffect, value interface{}) (result ManageOfferSuccessResultOffer, err error) {
	result.Effect = effect
	switch ManageOfferEffect(effect) {
	case ManageOfferEffectManageOfferCreated:
		tv, ok := value.(OfferEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be OfferEntry")
			return
		}
		result.Offer = &tv
	case ManageOfferEffectManageOfferUpdated:
		tv, ok := value.(OfferEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be OfferEntry")
			return
		}
		result.Offer = &tv
	default:
		// void
	}
	return
}
func (u ManageOfferResult) ArmForSwitch(sw int32) (string, bool) {
	switch ManageOfferResultCode(sw) {
	case ManageOfferResultCodeManageOfferSuccess:
		return "Success", true
	default:
		return "", true
	}
}
func NewManageOfferResult(code ManageOfferResultCode, value interface{}) (result ManageOfferResult, err error) {
	result.Code = code
	switch ManageOfferResultCode(code) {
	case ManageOfferResultCodeManageOfferSuccess:
		tv, ok := value.(ManageOfferSuccessResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be ManageOfferSuccessResult")
			return
		}
		result.Success = &tv
	default:
		// void
	}
	return
}
func (e SetOptionsResultCode) ValidEnum(v int32) bool {
	_, ok := setOptionsResultCodeMap[v]
	return ok
}
func (u SetOptionsResult) ArmForSwitch(sw int32) (string, bool) {
	switch SetOptionsResultCode(sw) {
	case SetOptionsResultCodeSetOptionsSuccess:
		return "", true
	default:
		return "", true
	}
}
func NewSetOptionsResult(code SetOptionsResultCode, value interface{}) (result SetOptionsResult, err error) {
	result.Code = code
	switch SetOptionsResultCode(code) {
	case SetOptionsResultCodeSetOptionsSuccess:
		// void
	default:
		// void
	}
	return
}
func (e ChangeTrustResultCode) ValidEnum(v int32) bool {
	_, ok := changeTrustResultCodeMap[v]
	return ok
}
func (u ChangeTrustResult) ArmForSwitch(sw int32) (string, bool) {
	switch ChangeTrustResultCode(sw) {
	case ChangeTrustResultCodeChangeTrustSuccess:
		return "", true
	default:
		return "", true
	}
}
func NewChangeTrustResult(code ChangeTrustResultCode, value interface{}) (result ChangeTrustResult, err error) {
	result.Code = code
	switch ChangeTrustResultCode(code) {
	case ChangeTrustResultCodeChangeTrustSuccess:
		// void
	default:
		// void
	}
	return
}
func (e AllowTrustResultCode) ValidEnum(v int32) bool {
	_, ok := allowTrustResultCodeMap[v]
	return ok
}
func (u AllowTrustResult) ArmForSwitch(sw int32) (string, bool) {
	switch AllowTrustResultCode(sw) {
	case AllowTrustResultCodeAllowTrustSuccess:
		return "", true
	default:
		return "", true
	}
}
func NewAllowTrustResult(code AllowTrustResultCode, value interface{}) (result AllowTrustResult, err error) {
	result.Code = code
	switch AllowTrustResultCode(code) {
	case AllowTrustResultCodeAllowTrustSuccess:
		// void
	default:
		// void
	}
	return
}
func (e AccountMergeResultCode) ValidEnum(v int32) bool {
	_, ok := accountMergeResultCodeMap[v]
	return ok
}
func (u AccountMergeResult) ArmForSwitch(sw int32) (string, bool) {
	switch AccountMergeResultCode(sw) {
	case AccountMergeResultCodeAccountMergeSuccess:
		return "SourceAccountBalance", true
	default:
		return "", true
	}
}
func NewAccountMergeResult(code AccountMergeResultCode, value interface{}) (result AccountMergeResult, err error) {
	result.Code = code
	switch AccountMergeResultCode(code) {
	case AccountMergeResultCodeAccountMergeSuccess:
		tv, ok := value.(Int64)
		if !ok {
			err = fmt.Errorf("invalid value, must be Int64")
			return
		}
		result.SourceAccountBalance = &tv
	default:
		// void
	}
	return
}
func (u AccountMergeResult) MustSourceAccountBalance() Int64 {
	val, ok := u.GetSourceAccountBalance()

	if !ok {
		panic("arm SourceAccountBalance is not set")
	}

	return val
}
func (u AccountMergeResult) GetSourceAccountBalance() (result Int64, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Code))

	if armName == "SourceAccountBalance" {
		result = *u.SourceAccountBalance
		ok = true
	}

	return
}
func (e InflationResultCode) ValidEnum(v int32) bool {
	_, ok := inflationResultCodeMap[v]
	return ok
}
func (u InflationResult) ArmForSwitch(sw int32) (string, bool) {
	switch InflationResultCode(sw) {
	case InflationResultCodeInflationSuccess:
		return "Payouts", true
	default:
		return "", true
	}
}
func NewInflationResult(code InflationResultCode, value interface{}) (result InflationResult, err error) {
	result.Code = code
	switch InflationResultCode(code) {
	case InflationResultCodeInflationSuccess:
		tv, ok := value.([]InflationPayout)
		if !ok {
			err = fmt.Errorf("invalid value, must be []InflationPayout")
			return
		}
		result.Payouts = &tv
	default:
		// void
	}
	return
}
func (u InflationResult) MustPayouts() []InflationPayout {
	val, ok := u.GetPayouts()

	if !ok {
		panic("arm Payouts is not set")
	}

	return val
}
func (u InflationResult) GetPayouts() (result []InflationPayout, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Code))

	if armName == "Payouts" {
		result = *u.Payouts
		ok = true
	}

	return
}
func (e ManageDataResultCode) ValidEnum(v int32) bool {
	_, ok := manageDataResultCodeMap[v]
	return ok
}
func (u ManageDataResult) ArmForSwitch(sw int32) (string, bool) {
	switch ManageDataResultCode(sw) {
	case ManageDataResultCodeManageDataSuccess:
		return "", true
	default:
		return "", true
	}
}
func NewManageDataResult(code ManageDataResultCode, value interface{}) (result ManageDataResult, err error) {
	result.Code = code
	switch ManageDataResultCode(code) {
	case ManageDataResultCodeManageDataSuccess:
		// void
	default:
		// void
	}
	return
}
func (e OperationResultCode) ValidEnum(v int32) bool {
	_, ok := operationResultCodeMap[v]
	return ok
}
func (u OperationResultTr) ArmForSwitch(sw int32) (string, bool) {
	switch OperationType(sw) {
	case OperationTypeCreateAccount:
		return "CreateAccountResult", true
	case OperationTypePayment:
		return "PaymentResult", true
	case OperationTypePathPayment:
		return "PathPaymentResult", true
	case OperationTypeManageOffer:
		return "ManageOfferResult", true
	case OperationTypeCreatePassiveOffer:
		return "CreatePassiveOfferResult", true
	case OperationTypeSetOptions:
		return "SetOptionsResult", true
	case OperationTypeChangeTrust:
		return "ChangeTrustResult", true
	case OperationTypeAllowTrust:
		return "AllowTrustResult", true
	case OperationTypeAccountMerge:
		return "AccountMergeResult", true
	case OperationTypeInflation:
		return "InflationResult", true
	case OperationTypeManageData:
		return "ManageDataResult", true
	}
	return "-", false
}
func NewOperationResultTr(aType OperationType, value interface{}) (result OperationResultTr, err error) {
	result.Type = aType
	switch OperationType(aType) {
	case OperationTypeCreateAccount:
		tv, ok := value.(CreateAccountResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be CreateAccountResult")
			return
		}
		result.CreateAccountResult = &tv
	case OperationTypePayment:
		tv, ok := value.(PaymentResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be PaymentResult")
			return
		}
		result.PaymentResult = &tv
	case OperationTypePathPayment:
		tv, ok := value.(PathPaymentResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be PathPaymentResult")
			return
		}
		result.PathPaymentResult = &tv
	case OperationTypeManageOffer:
		tv, ok := value.(ManageOfferResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be ManageOfferResult")
			return
		}
		result.ManageOfferResult = &tv
	case OperationTypeCreatePassiveOffer:
		tv, ok := value.(ManageOfferResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be ManageOfferResult")
			return
		}
		result.CreatePassiveOfferResult = &tv
	case OperationTypeSetOptions:
		tv, ok := value.(SetOptionsResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be SetOptionsResult")
			return
		}
		result.SetOptionsResult = &tv
	case OperationTypeChangeTrust:
		tv, ok := value.(ChangeTrustResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be ChangeTrustResult")
			return
		}
		result.ChangeTrustResult = &tv
	case OperationTypeAllowTrust:
		tv, ok := value.(AllowTrustResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be AllowTrustResult")
			return
		}
		result.AllowTrustResult = &tv
	case OperationTypeAccountMerge:
		tv, ok := value.(AccountMergeResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be AccountMergeResult")
			return
		}
		result.AccountMergeResult = &tv
	case OperationTypeInflation:
		tv, ok := value.(InflationResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be InflationResult")
			return
		}
		result.InflationResult = &tv
	case OperationTypeManageData:
		tv, ok := value.(ManageDataResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be ManageDataResult")
			return
		}
		result.ManageDataResult = &tv
	}
	return
}
func (u OperationResultTr) MustCreateAccountResult() CreateAccountResult {
	val, ok := u.GetCreateAccountResult()

	if !ok {
		panic("arm CreateAccountResult is not set")
	}

	return val
}
func (u OperationResultTr) GetCreateAccountResult() (result CreateAccountResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "CreateAccountResult" {
		result = *u.CreateAccountResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustPaymentResult() PaymentResult {
	val, ok := u.GetPaymentResult()

	if !ok {
		panic("arm PaymentResult is not set")
	}

	return val
}
func (u OperationResultTr) GetPaymentResult() (result PaymentResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "PaymentResult" {
		result = *u.PaymentResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustPathPaymentResult() PathPaymentResult {
	val, ok := u.GetPathPaymentResult()

	if !ok {
		panic("arm PathPaymentResult is not set")
	}

	return val
}
func (u OperationResultTr) GetPathPaymentResult() (result PathPaymentResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "PathPaymentResult" {
		result = *u.PathPaymentResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustManageOfferResult() ManageOfferResult {
	val, ok := u.GetManageOfferResult()

	if !ok {
		panic("arm ManageOfferResult is not set")
	}

	return val
}
func (u OperationResultTr) GetManageOfferResult() (result ManageOfferResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "ManageOfferResult" {
		result = *u.ManageOfferResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustCreatePassiveOfferResult() ManageOfferResult {
	val, ok := u.GetCreatePassiveOfferResult()

	if !ok {
		panic("arm CreatePassiveOfferResult is not set")
	}

	return val
}
func (u OperationResultTr) MustSetOptionsResult() SetOptionsResult {
	val, ok := u.GetSetOptionsResult()

	if !ok {
		panic("arm SetOptionsResult is not set")
	}

	return val
}
func (u OperationResultTr) GetSetOptionsResult() (result SetOptionsResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "SetOptionsResult" {
		result = *u.SetOptionsResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustChangeTrustResult() ChangeTrustResult {
	val, ok := u.GetChangeTrustResult()

	if !ok {
		panic("arm ChangeTrustResult is not set")
	}

	return val
}
func (u OperationResultTr) GetChangeTrustResult() (result ChangeTrustResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "ChangeTrustResult" {
		result = *u.ChangeTrustResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustAllowTrustResult() AllowTrustResult {
	val, ok := u.GetAllowTrustResult()

	if !ok {
		panic("arm AllowTrustResult is not set")
	}

	return val
}
func (u OperationResultTr) GetAllowTrustResult() (result AllowTrustResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AllowTrustResult" {
		result = *u.AllowTrustResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustAccountMergeResult() AccountMergeResult {
	val, ok := u.GetAccountMergeResult()

	if !ok {
		panic("arm AccountMergeResult is not set")
	}

	return val
}
func (u OperationResultTr) GetAccountMergeResult() (result AccountMergeResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "AccountMergeResult" {
		result = *u.AccountMergeResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustInflationResult() InflationResult {
	val, ok := u.GetInflationResult()

	if !ok {
		panic("arm InflationResult is not set")
	}

	return val
}
func (u OperationResultTr) GetInflationResult() (result InflationResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "InflationResult" {
		result = *u.InflationResult
		ok = true
	}

	return
}
func (u OperationResultTr) MustManageDataResult() ManageDataResult {
	val, ok := u.GetManageDataResult()

	if !ok {
		panic("arm ManageDataResult is not set")
	}

	return val
}
func (u OperationResultTr) GetManageDataResult() (result ManageDataResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "ManageDataResult" {
		result = *u.ManageDataResult
		ok = true
	}

	return
}
func (u OperationResult) ArmForSwitch(sw int32) (string, bool) {
	switch OperationResultCode(sw) {
	case OperationResultCodeOpInner:
		return "Tr", true
	default:
		return "", true
	}
}
func NewOperationResult(code OperationResultCode, value interface{}) (result OperationResult, err error) {
	result.Code = code
	switch OperationResultCode(code) {
	case OperationResultCodeOpInner:
		tv, ok := value.(OperationResultTr)
		if !ok {
			err = fmt.Errorf("invalid value, must be OperationResultTr")
			return
		}
		result.Tr = &tv
	default:
		// void
	}
	return
}
func (u OperationResult) MustTr() OperationResultTr {
	val, ok := u.GetTr()

	if !ok {
		panic("arm Tr is not set")
	}

	return val
}
func (u OperationResult) GetTr() (result OperationResultTr, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Code))

	if armName == "Tr" {
		result = *u.Tr
		ok = true
	}

	return
}
func (e TransactionResultCode) ValidEnum(v int32) bool {
	_, ok := transactionResultCodeMap[v]
	return ok
}
func (u TransactionResultResult) ArmForSwitch(sw int32) (string, bool) {
	switch TransactionResultCode(sw) {
	case TransactionResultCodeTxSuccess:
		return "Results", true
	case TransactionResultCodeTxFailed:
		return "Results", true
	default:
		return "", true
	}
}
func NewTransactionResultResult(code TransactionResultCode, value interface{}) (result TransactionResultResult, err error) {
	result.Code = code
	switch TransactionResultCode(code) {
	case TransactionResultCodeTxSuccess:
		tv, ok := value.([]OperationResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be []OperationResult")
			return
		}
		result.Results = &tv
	case TransactionResultCodeTxFailed:
		tv, ok := value.([]OperationResult)
		if !ok {
			err = fmt.Errorf("invalid value, must be []OperationResult")
			return
		}
		result.Results = &tv
	default:
		// void
	}
	return
}
func (u TransactionResultResult) MustResults() []OperationResult {
	val, ok := u.GetResults()

	if !ok {
		panic("arm Results is not set")
	}

	return val
}
func (u TransactionResultResult) GetResults() (result []OperationResult, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Code))

	if armName == "Results" {
		result = *u.Results
		ok = true
	}

	return
}
func NewTransactionResultExt(v int32, value interface{}) (result TransactionResultExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func NewStellarValueExt(v int32, value interface{}) (result StellarValueExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func NewLedgerHeaderExt(v int32, value interface{}) (result LedgerHeaderExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func (e LedgerUpgradeType) ValidEnum(v int32) bool {
	_, ok := ledgerUpgradeTypeMap[v]
	return ok
}
func (u LedgerUpgrade) ArmForSwitch(sw int32) (string, bool) {
	switch LedgerUpgradeType(sw) {
	case LedgerUpgradeTypeLedgerUpgradeVersion:
		return "NewLedgerVersion", true
	case LedgerUpgradeTypeLedgerUpgradeBaseFee:
		return "NewBaseFee", true
	case LedgerUpgradeTypeLedgerUpgradeMaxTxSetSize:
		return "NewMaxTxSetSize", true
	}
	return "-", false
}
func NewLedgerUpgrade(aType LedgerUpgradeType, value interface{}) (result LedgerUpgrade, err error) {
	result.Type = aType
	switch LedgerUpgradeType(aType) {
	case LedgerUpgradeTypeLedgerUpgradeVersion:
		tv, ok := value.(Uint32)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint32")
			return
		}
		result.NewLedgerVersion = &tv
	case LedgerUpgradeTypeLedgerUpgradeBaseFee:
		tv, ok := value.(Uint32)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint32")
			return
		}
		result.NewBaseFee = &tv
	case LedgerUpgradeTypeLedgerUpgradeMaxTxSetSize:
		tv, ok := value.(Uint32)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint32")
			return
		}
		result.NewMaxTxSetSize = &tv
	}
	return
}
func (u LedgerUpgrade) MustNewLedgerVersion() Uint32 {
	val, ok := u.GetNewLedgerVersion()

	if !ok {
		panic("arm NewLedgerVersion is not set")
	}

	return val
}
func (u LedgerUpgrade) GetNewLedgerVersion() (result Uint32, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "NewLedgerVersion" {
		result = *u.NewLedgerVersion
		ok = true
	}

	return
}
func (u LedgerUpgrade) MustNewBaseFee() Uint32 {
	val, ok := u.GetNewBaseFee()

	if !ok {
		panic("arm NewBaseFee is not set")
	}

	return val
}
func (u LedgerUpgrade) GetNewBaseFee() (result Uint32, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "NewBaseFee" {
		result = *u.NewBaseFee
		ok = true
	}

	return
}
func (u LedgerUpgrade) MustNewMaxTxSetSize() Uint32 {
	val, ok := u.GetNewMaxTxSetSize()

	if !ok {
		panic("arm NewMaxTxSetSize is not set")
	}

	return val
}
func (u LedgerUpgrade) GetNewMaxTxSetSize() (result Uint32, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "NewMaxTxSetSize" {
		result = *u.NewMaxTxSetSize
		ok = true
	}

	return
}
func (u LedgerKey) ArmForSwitch(sw int32) (string, bool) {
	switch LedgerEntryType(sw) {
	case LedgerEntryTypeAccount:
		return "Account", true
	case LedgerEntryTypeTrustline:
		return "TrustLine", true
	case LedgerEntryTypeOffer:
		return "Offer", true
	case LedgerEntryTypeData:
		return "Data", true
	}
	return "-", false
}
func NewLedgerKey(aType LedgerEntryType, value interface{}) (result LedgerKey, err error) {
	result.Type = aType
	switch LedgerEntryType(aType) {
	case LedgerEntryTypeAccount:
		tv, ok := value.(LedgerKeyAccount)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerKeyAccount")
			return
		}
		result.Account = &tv
	case LedgerEntryTypeTrustline:
		tv, ok := value.(LedgerKeyTrustLine)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerKeyTrustLine")
			return
		}
		result.TrustLine = &tv
	case LedgerEntryTypeOffer:
		tv, ok := value.(LedgerKeyOffer)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerKeyOffer")
			return
		}
		result.Offer = &tv
	case LedgerEntryTypeData:
		tv, ok := value.(LedgerKeyData)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerKeyData")
			return
		}
		result.Data = &tv
	}
	return
}
func (e BucketEntryType) ValidEnum(v int32) bool {
	_, ok := bucketEntryTypeMap[v]
	return ok
}
func (u BucketEntry) ArmForSwitch(sw int32) (string, bool) {
	switch BucketEntryType(sw) {
	case BucketEntryTypeLiveentry:
		return "LiveEntry", true
	case BucketEntryTypeDeadentry:
		return "DeadEntry", true
	}
	return "-", false
}
func NewBucketEntry(aType BucketEntryType, value interface{}) (result BucketEntry, err error) {
	result.Type = aType
	switch BucketEntryType(aType) {
	case BucketEntryTypeLiveentry:
		tv, ok := value.(LedgerEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerEntry")
			return
		}
		result.LiveEntry = &tv
	case BucketEntryTypeDeadentry:
		tv, ok := value.(LedgerKey)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerKey")
			return
		}
		result.DeadEntry = &tv
	}
	return
}
func (u BucketEntry) MustLiveEntry() LedgerEntry {
	val, ok := u.GetLiveEntry()

	if !ok {
		panic("arm LiveEntry is not set")
	}

	return val
}
func (u BucketEntry) GetLiveEntry() (result LedgerEntry, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "LiveEntry" {
		result = *u.LiveEntry
		ok = true
	}

	return
}
func (u BucketEntry) MustDeadEntry() LedgerKey {
	val, ok := u.GetDeadEntry()

	if !ok {
		panic("arm DeadEntry is not set")
	}

	return val
}
func (u BucketEntry) GetDeadEntry() (result LedgerKey, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "DeadEntry" {
		result = *u.DeadEntry
		ok = true
	}

	return
}
func NewTransactionHistoryEntryExt(v int32, value interface{}) (result TransactionHistoryEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func (u TransactionHistoryResultEntryExt) ArmForSwitch(sw int32) (string, bool) {
	switch int32(sw) {
	case 0:
		return "", true
	}
	return "-", false
}
func NewTransactionHistoryResultEntryExt(v int32, value interface{}) (result TransactionHistoryResultEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func NewLedgerHeaderHistoryEntryExt(v int32, value interface{}) (result LedgerHeaderHistoryEntryExt, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		// void
	}
	return
}
func NewScpHistoryEntry(v int32, value interface{}) (result ScpHistoryEntry, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		tv, ok := value.(ScpHistoryEntryV0)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpHistoryEntryV0")
			return
		}
		result.V0 = &tv
	}
	return
}
func (e LedgerEntryChangeType) ValidEnum(v int32) bool {
	_, ok := ledgerEntryChangeTypeMap[v]
	return ok
}
func (u LedgerEntryChange) ArmForSwitch(sw int32) (string, bool) {
	switch LedgerEntryChangeType(sw) {
	case LedgerEntryChangeTypeLedgerEntryCreated:
		return "Created", true
	case LedgerEntryChangeTypeLedgerEntryUpdated:
		return "Updated", true
	case LedgerEntryChangeTypeLedgerEntryRemoved:
		return "Removed", true
	case LedgerEntryChangeTypeLedgerEntryState:
		return "State", true
	}
	return "-", false
}
func NewLedgerEntryChange(aType LedgerEntryChangeType, value interface{}) (result LedgerEntryChange, err error) {
	result.Type = aType
	switch LedgerEntryChangeType(aType) {
	case LedgerEntryChangeTypeLedgerEntryCreated:
		tv, ok := value.(LedgerEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerEntry")
			return
		}
		result.Created = &tv
	case LedgerEntryChangeTypeLedgerEntryUpdated:
		tv, ok := value.(LedgerEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerEntry")
			return
		}
		result.Updated = &tv
	case LedgerEntryChangeTypeLedgerEntryRemoved:
		tv, ok := value.(LedgerKey)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerKey")
			return
		}
		result.Removed = &tv
	case LedgerEntryChangeTypeLedgerEntryState:
		tv, ok := value.(LedgerEntry)
		if !ok {
			err = fmt.Errorf("invalid value, must be LedgerEntry")
			return
		}
		result.State = &tv
	}
	return
}
func (u LedgerEntryChange) MustCreated() LedgerEntry {
	val, ok := u.GetCreated()

	if !ok {
		panic("arm Created is not set")
	}

	return val
}
func (u LedgerEntryChange) GetCreated() (result LedgerEntry, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Created" {
		result = *u.Created
		ok = true
	}

	return
}
func (u LedgerEntryChange) MustUpdated() LedgerEntry {
	val, ok := u.GetUpdated()

	if !ok {
		panic("arm Updated is not set")
	}

	return val
}
func (u LedgerEntryChange) GetUpdated() (result LedgerEntry, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Updated" {
		result = *u.Updated
		ok = true
	}

	return
}
func (u LedgerEntryChange) MustRemoved() LedgerKey {
	val, ok := u.GetRemoved()

	if !ok {
		panic("arm Removed is not set")
	}

	return val
}
func (u LedgerEntryChange) GetRemoved() (result LedgerKey, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Removed" {
		result = *u.Removed
		ok = true
	}

	return
}
func (u LedgerEntryChange) MustState() LedgerEntry {
	val, ok := u.GetState()

	if !ok {
		panic("arm State is not set")
	}

	return val
}
func (u LedgerEntryChange) GetState() (result LedgerEntry, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "State" {
		result = *u.State
		ok = true
	}

	return
}
func NewTransactionMeta(v int32, value interface{}) (result TransactionMeta, err error) {
	result.V = v
	switch int32(v) {
	case 0:
		tv, ok := value.([]OperationMeta)
		if !ok {
			err = fmt.Errorf("invalid value, must be []OperationMeta")
			return
		}
		result.Operations = &tv
	}
	return
}
func (u TransactionMeta) MustOperations() []OperationMeta {
	val, ok := u.GetOperations()

	if !ok {
		panic("arm Operations is not set")
	}

	return val
}
func (u TransactionMeta) GetOperations() (result []OperationMeta, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.V))

	if armName == "Operations" {
		result = *u.Operations
		ok = true
	}

	return
}
func (e ErrorCode) ValidEnum(v int32) bool {
	_, ok := errorCodeMap[v]
	return ok
}
func (e IpAddrType) ValidEnum(v int32) bool {
	_, ok := ipAddrTypeMap[v]
	return ok
}
func (u PeerAddressIp) ArmForSwitch(sw int32) (string, bool) {
	switch IpAddrType(sw) {
	case IpAddrTypeIPv4:
		return "Ipv4", true
	case IpAddrTypeIPv6:
		return "Ipv6", true
	}
	return "-", false
}
func NewPeerAddressIp(aType IpAddrType, value interface{}) (result PeerAddressIp, err error) {
	result.Type = aType
	switch IpAddrType(aType) {
	case IpAddrTypeIPv4:
		tv, ok := value.([4]byte)
		if !ok {
			err = fmt.Errorf("invalid value, must be [4]byte")
			return
		}
		result.Ipv4 = &tv
	case IpAddrTypeIPv6:
		tv, ok := value.([16]byte)
		if !ok {
			err = fmt.Errorf("invalid value, must be [16]byte")
			return
		}
		result.Ipv6 = &tv
	}
	return
}
func (u PeerAddressIp) MustIpv4() [4]byte {
	val, ok := u.GetIpv4()

	if !ok {
		panic("arm Ipv4 is not set")
	}

	return val
}
func (u PeerAddressIp) GetIpv4() (result [4]byte, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Ipv4" {
		result = *u.Ipv4
		ok = true
	}

	return
}
func (u PeerAddressIp) MustIpv6() [16]byte {
	val, ok := u.GetIpv6()

	if !ok {
		panic("arm Ipv6 is not set")
	}

	return val
}
func (u PeerAddressIp) GetIpv6() (result [16]byte, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Ipv6" {
		result = *u.Ipv6
		ok = true
	}

	return
}
func (e MessageType) ValidEnum(v int32) bool {
	_, ok := messageTypeMap[v]
	return ok
}
func (u StellarMessage) ArmForSwitch(sw int32) (string, bool) {
	switch MessageType(sw) {
	case MessageTypeErrorMsg:
		return "Error", true
	case MessageTypeHello:
		return "Hello", true
	case MessageTypeAuth:
		return "Auth", true
	case MessageTypeDontHave:
		return "DontHave", true
	case MessageTypeGetPeers:
		return "", true
	case MessageTypePeers:
		return "Peers", true
	case MessageTypeGetTxSet:
		return "TxSetHash", true
	case MessageTypeTxSet:
		return "TxSet", true
	case MessageTypeTransaction:
		return "Transaction", true
	case MessageTypeGetScpQuorumset:
		return "QSetHash", true
	case MessageTypeScpQuorumset:
		return "QSet", true
	case MessageTypeScpMessage:
		return "Envelope", true
	case MessageTypeGetScpState:
		return "GetScpLedgerSeq", true
	}
	return "-", false
}
func NewStellarMessage(aType MessageType, value interface{}) (result StellarMessage, err error) {
	result.Type = aType
	switch MessageType(aType) {
	case MessageTypeErrorMsg:
		tv, ok := value.(Error)
		if !ok {
			err = fmt.Errorf("invalid value, must be Error")
			return
		}
		result.Error = &tv
	case MessageTypeHello:
		tv, ok := value.(Hello)
		if !ok {
			err = fmt.Errorf("invalid value, must be Hello")
			return
		}
		result.Hello = &tv
	case MessageTypeAuth:
		tv, ok := value.(Auth)
		if !ok {
			err = fmt.Errorf("invalid value, must be Auth")
			return
		}
		result.Auth = &tv
	case MessageTypeDontHave:
		tv, ok := value.(DontHave)
		if !ok {
			err = fmt.Errorf("invalid value, must be DontHave")
			return
		}
		result.DontHave = &tv
	case MessageTypeGetPeers:
		// void
	case MessageTypePeers:
		tv, ok := value.([]PeerAddress)
		if !ok {
			err = fmt.Errorf("invalid value, must be []PeerAddress")
			return
		}
		result.Peers = &tv
	case MessageTypeGetTxSet:
		tv, ok := value.(Uint256)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint256")
			return
		}
		result.TxSetHash = &tv
	case MessageTypeTxSet:
		tv, ok := value.(TransactionSet)
		if !ok {
			err = fmt.Errorf("invalid value, must be TransactionSet")
			return
		}
		result.TxSet = &tv
	case MessageTypeTransaction:
		tv, ok := value.(TransactionEnvelope)
		if !ok {
			err = fmt.Errorf("invalid value, must be TransactionEnvelope")
			return
		}
		result.Transaction = &tv
	case MessageTypeGetScpQuorumset:
		tv, ok := value.(Uint256)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint256")
			return
		}
		result.QSetHash = &tv
	case MessageTypeScpQuorumset:
		tv, ok := value.(ScpQuorumSet)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpQuorumSet")
			return
		}
		result.QSet = &tv
	case MessageTypeScpMessage:
		tv, ok := value.(ScpEnvelope)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpEnvelope")
			return
		}
		result.Envelope = &tv
	case MessageTypeGetScpState:
		tv, ok := value.(Uint32)
		if !ok {
			err = fmt.Errorf("invalid value, must be Uint32")
			return
		}
		result.GetScpLedgerSeq = &tv
	}
	return
}
func (u StellarMessage) MustError() Error {
	val, ok := u.GetError()

	if !ok {
		panic("arm Error is not set")
	}

	return val
}
func (u StellarMessage) GetError() (result Error, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Error" {
		result = *u.Error
		ok = true
	}

	return
}
func (u StellarMessage) MustHello() Hello {
	val, ok := u.GetHello()

	if !ok {
		panic("arm Hello is not set")
	}

	return val
}
func (u StellarMessage) GetHello() (result Hello, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Hello" {
		result = *u.Hello
		ok = true
	}

	return
}
func (u StellarMessage) MustAuth() Auth {
	val, ok := u.GetAuth()

	if !ok {
		panic("arm Auth is not set")
	}

	return val
}
func (u StellarMessage) GetAuth() (result Auth, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Auth" {
		result = *u.Auth
		ok = true
	}

	return
}
func (u StellarMessage) MustDontHave() DontHave {
	val, ok := u.GetDontHave()

	if !ok {
		panic("arm DontHave is not set")
	}

	return val
}
func (u StellarMessage) GetDontHave() (result DontHave, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "DontHave" {
		result = *u.DontHave
		ok = true
	}

	return
}
func (u StellarMessage) MustPeers() []PeerAddress {
	val, ok := u.GetPeers()

	if !ok {
		panic("arm Peers is not set")
	}

	return val
}
func (u StellarMessage) GetPeers() (result []PeerAddress, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Peers" {
		result = *u.Peers
		ok = true
	}

	return
}
func (u StellarMessage) MustTxSetHash() Uint256 {
	val, ok := u.GetTxSetHash()

	if !ok {
		panic("arm TxSetHash is not set")
	}

	return val
}
func (u StellarMessage) GetTxSetHash() (result Uint256, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "TxSetHash" {
		result = *u.TxSetHash
		ok = true
	}

	return
}
func (u StellarMessage) MustTxSet() TransactionSet {
	val, ok := u.GetTxSet()

	if !ok {
		panic("arm TxSet is not set")
	}

	return val
}
func (u StellarMessage) GetTxSet() (result TransactionSet, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "TxSet" {
		result = *u.TxSet
		ok = true
	}

	return
}
func (u StellarMessage) MustTransaction() TransactionEnvelope {
	val, ok := u.GetTransaction()

	if !ok {
		panic("arm Transaction is not set")
	}

	return val
}
func (u StellarMessage) GetTransaction() (result TransactionEnvelope, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Transaction" {
		result = *u.Transaction
		ok = true
	}

	return
}
func (u StellarMessage) MustQSetHash() Uint256 {
	val, ok := u.GetQSetHash()

	if !ok {
		panic("arm QSetHash is not set")
	}

	return val
}
func (u StellarMessage) GetQSetHash() (result Uint256, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "QSetHash" {
		result = *u.QSetHash
		ok = true
	}

	return
}
func (u StellarMessage) MustQSet() ScpQuorumSet {
	val, ok := u.GetQSet()

	if !ok {
		panic("arm QSet is not set")
	}

	return val
}
func (u StellarMessage) GetQSet() (result ScpQuorumSet, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "QSet" {
		result = *u.QSet
		ok = true
	}

	return
}
func (u StellarMessage) MustEnvelope() ScpEnvelope {
	val, ok := u.GetEnvelope()

	if !ok {
		panic("arm Envelope is not set")
	}

	return val
}
func (u StellarMessage) GetEnvelope() (result ScpEnvelope, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Envelope" {
		result = *u.Envelope
		ok = true
	}

	return
}
func (u StellarMessage) MustGetScpLedgerSeq() Uint32 {
	val, ok := u.GetGetScpLedgerSeq()

	if !ok {
		panic("arm GetScpLedgerSeq is not set")
	}

	return val
}
func (u StellarMessage) GetGetScpLedgerSeq() (result Uint32, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "GetScpLedgerSeq" {
		result = *u.GetScpLedgerSeq
		ok = true
	}

	return
}
func (u AuthenticatedMessage) ArmForSwitch(sw int32) (string, bool) {
	switch Uint32(sw) {
	case 0:
		return "V0", true
	}
	return "-", false
}
func NewAuthenticatedMessage(v Uint32, value interface{}) (result AuthenticatedMessage, err error) {
	result.V = v
	switch Uint32(v) {
	case 0:
		tv, ok := value.(AuthenticatedMessageV0)
		if !ok {
			err = fmt.Errorf("invalid value, must be AuthenticatedMessageV0")
			return
		}
		result.V0 = &tv
	}
	return
}
func (e ScpStatementType) ValidEnum(v int32) bool {
	_, ok := scpStatementTypeMap[v]
	return ok
}
func (u ScpStatementPledges) ArmForSwitch(sw int32) (string, bool) {
	switch ScpStatementType(sw) {
	case ScpStatementTypeScpStPrepare:
		return "Prepare", true
	case ScpStatementTypeScpStConfirm:
		return "Confirm", true
	case ScpStatementTypeScpStExternalize:
		return "Externalize", true
	case ScpStatementTypeScpStNominate:
		return "Nominate", true
	}
	return "-", false
}
func NewScpStatementPledges(aType ScpStatementType, value interface{}) (result ScpStatementPledges, err error) {
	result.Type = aType
	switch ScpStatementType(aType) {
	case ScpStatementTypeScpStPrepare:
		tv, ok := value.(ScpStatementPrepare)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpStatementPrepare")
			return
		}
		result.Prepare = &tv
	case ScpStatementTypeScpStConfirm:
		tv, ok := value.(ScpStatementConfirm)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpStatementConfirm")
			return
		}
		result.Confirm = &tv
	case ScpStatementTypeScpStExternalize:
		tv, ok := value.(ScpStatementExternalize)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpStatementExternalize")
			return
		}
		result.Externalize = &tv
	case ScpStatementTypeScpStNominate:
		tv, ok := value.(ScpNomination)
		if !ok {
			err = fmt.Errorf("invalid value, must be ScpNomination")
			return
		}
		result.Nominate = &tv
	}
	return
}
func (u ScpStatementPledges) MustPrepare() ScpStatementPrepare {
	val, ok := u.GetPrepare()

	if !ok {
		panic("arm Prepare is not set")
	}

	return val
}
func (u ScpStatementPledges) GetPrepare() (result ScpStatementPrepare, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Prepare" {
		result = *u.Prepare
		ok = true
	}

	return
}
func (u ScpStatementPledges) MustConfirm() ScpStatementConfirm {
	val, ok := u.GetConfirm()

	if !ok {
		panic("arm Confirm is not set")
	}

	return val
}
func (u ScpStatementPledges) GetConfirm() (result ScpStatementConfirm, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Confirm" {
		result = *u.Confirm
		ok = true
	}

	return
}
func (u ScpStatementPledges) MustExternalize() ScpStatementExternalize {
	val, ok := u.GetExternalize()

	if !ok {
		panic("arm Externalize is not set")
	}

	return val
}
func (u ScpStatementPledges) GetExternalize() (result ScpStatementExternalize, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Externalize" {
		result = *u.Externalize
		ok = true
	}

	return
}
func (u ScpStatementPledges) MustNominate() ScpNomination {
	val, ok := u.GetNominate()

	if !ok {
		panic("arm Nominate is not set")
	}

	return val
}
func (u ScpStatementPledges) GetNominate() (result ScpNomination, ok bool) {
	armName, _ := u.ArmForSwitch(int32(u.Type))

	if armName == "Nominate" {
		result = *u.Nominate
		ok = true
	}

	return
}
func SafeUnmarshalBase64(data string, dest interface{}) error {
	count := &countWriter{}
	l := len(data)

	b64 := io.TeeReader(strings.NewReader(data), count)
	raw := base64.NewDecoder(base64.StdEncoding, b64)
	_, err := Unmarshal(raw, dest)

	if err != nil {
		return err
	}

	if count.Count != l {
		return fmt.Errorf("input not fully consumed. expected to read: %d, actual: %d", l, count.Count)
	}

	return nil
}
func SafeUnmarshal(data []byte, dest interface{}) error {
	r := bytes.NewReader(data)
	n, err := Unmarshal(r, dest)

	if err != nil {
		return err
	}

	if n != len(data) {
		return fmt.Errorf("input not fully consumed. expected to read: %d, actual: %d", len(data), n)
	}

	return nil
}
func (pr *PathPaymentResult) SendAmount() Int64 {
	s, ok := pr.GetSuccess()
	if !ok {
		return 0
	}

	if len(s.Offers) == 0 {
		return s.Last.Amount
	}

	sa := s.Offers[0].AssetBought
	var ret Int64

	for _, o := range s.Offers {
		if o.AssetBought.String() != sa.String() {
			break
		}
		ret += o.AmountBought
	}

	return ret
}
func (c *Client) LoadAccount(accountID string) (account Account, err error) {
	c.initHTTPClient()
	resp, err := c.Client.Get(c.URL + "/accounts/" + accountID)
	if err != nil {
		return
	}

	err = decodeResponse(resp, &account)
	return
}
func (c *Client) SequenceForAccount(
	accountID string,
) (xdr.SequenceNumber, error) {

	a, err := c.LoadAccount(accountID)
	if err != nil {
		return 0, err
	}

	seq, err := strconv.ParseUint(a.Sequence, 10, 64)
	if err != nil {
		return 0, err
	}

	return xdr.SequenceNumber(seq), nil
}
func (c *Client) SubmitTransaction(transactionEnvelopeXdr string) (response TransactionSuccess, err error) {
	v := url.Values{}
	v.Set("tx", transactionEnvelopeXdr)

	c.initHTTPClient()
	resp, err := c.Client.PostForm(c.URL+"/transactions", v)
	if err != nil {
		return
	}

	err = decodeResponse(resp, &response)
	return
}
func (key *LedgerKey) Equals(other LedgerKey) bool {
	if key.Type != other.Type {
		return false
	}

	switch key.Type {
	case LedgerEntryTypeAccount:
		l := key.MustAccount()
		r := other.MustAccount()
		return l.AccountId.Equals(r.AccountId)
	case LedgerEntryTypeData:
		l := key.MustData()
		r := other.MustData()
		return l.AccountId.Equals(r.AccountId) && l.DataName == r.DataName
	case LedgerEntryTypeOffer:
		l := key.MustOffer()
		r := other.MustOffer()
		return l.SellerId.Equals(r.SellerId) && l.OfferId == r.OfferId
	case LedgerEntryTypeTrustline:
		l := key.MustTrustLine()
		r := other.MustTrustLine()
		return l.AccountId.Equals(r.AccountId) && l.Asset.Equals(r.Asset)
	default:
		panic(fmt.Errorf("Unknown ledger key type: %v", key.Type))
	}
}
func (key *LedgerKey) SetAccount(account AccountId) error {
	data := LedgerKeyAccount{account}
	nkey, err := NewLedgerKey(LedgerEntryTypeAccount, data)
	if err != nil {
		return err
	}

	*key = nkey
	return nil
}
func (key *LedgerKey) SetData(account AccountId, name string) error {
	data := LedgerKeyData{account, String64(name)}
	nkey, err := NewLedgerKey(LedgerEntryTypeData, data)
	if err != nil {
		return err
	}

	*key = nkey
	return nil
}
func (key *LedgerKey) SetOffer(account AccountId, id uint64) error {
	data := LedgerKeyOffer{account, Uint64(id)}
	nkey, err := NewLedgerKey(LedgerEntryTypeOffer, data)
	if err != nil {
		return err
	}

	*key = nkey
	return nil
}
func (key *LedgerKey) SetTrustline(account AccountId, line Asset) error {
	data := LedgerKeyTrustLine{account, line}
	nkey, err := NewLedgerKey(LedgerEntryTypeTrustline, data)
	if err != nil {
		return err
	}

	*key = nkey
	return nil
}
func (b *CHDBuilder) Add(key []byte, value []byte) {
	b.keys = append(b.keys, key)
	b.values = append(b.values, value)
}
func tryHash(hasher *chdHasher, seen map[uint64]bool, keys [][]byte, values [][]byte, indices []uint16, bucket *bucket, ri uint16, r uint64) bool {
	// Track duplicates within this bucket.
	duplicate := make(map[uint64]bool)
	// Make hashes for each entry in the bucket.
	hashes := make([]uint64, len(bucket.keys))
	for i, k := range bucket.keys {
		h := hasher.Table(r, k)
		hashes[i] = h
		if seen[h] {
			return false
		}
		if duplicate[h] {
			return false
		}
		duplicate[h] = true
	}

	// Update seen hashes
	for _, h := range hashes {
		seen[h] = true
	}

	// Add the hash index.
	indices[bucket.index] = ri

	// Update the the hash table.
	for i, h := range hashes {
		keys[h] = bucket.keys[i]
		values[h] = bucket.values[i]
	}
	return true
}
func (h *chdHasher) HashIndexFromKey(b []byte) uint64 {
	return (hasher(b) ^ h.r[0]) % h.buckets
}
func (b *sliceReader) ReadInt() uint64 {
	return uint64(binary.LittleEndian.Uint32(b.Read(4)))
}
func Read(r io.Reader) (*CHD, error) {
	b, err := ioutil.ReadAll(r)
	if err != nil {
		return nil, err
	}
	return Mmap(b)
}
func (c *CHD) Get(key []byte) []byte {
	r0 := c.r[0]
	h := hasher(key) ^ r0
	i := h % uint64(len(c.indices))
	ri := c.indices[i]
	// This can occur if there were unassigned slots in the hash table.
	if ri >= uint16(len(c.r)) {
		return nil
	}
	r := c.r[ri]
	ti := (h ^ r) % uint64(len(c.keys))
	// fmt.Printf("r[0]=%d, h=%d, i=%d, ri=%d, r=%d, ti=%d\n", c.r[0], h, i, ri, r, ti)
	k := c.keys[ti]
	if bytes.Compare(k, key) != 0 {
		return nil
	}
	v := c.values[ti]
	return v
}
func (c *CHD) Iterate() *Iterator {
	if len(c.keys) == 0 {
		return nil
	}
	return &Iterator{c: c}
}
func (c *CHD) Write(w io.Writer) error {
	write := func(nd ...interface{}) error {
		for _, d := range nd {
			if err := binary.Write(w, binary.LittleEndian, d); err != nil {
				return err
			}
		}
		return nil
	}

	data := []interface{}{
		uint32(len(c.r)), c.r,
		uint32(len(c.indices)), c.indices,
		uint32(len(c.keys)),
	}

	if err := write(data...); err != nil {
		return err
	}

	for i := range c.keys {
		k, v := c.keys[i], c.values[i]
		if err := write(uint32(len(k)), uint32(len(v))); err != nil {
			return err
		}
		if _, err := w.Write(k); err != nil {
			return err
		}
		if _, err := w.Write(v); err != nil {
			return err
		}
	}
	return nil
}
func RegisterDatasources(path string) error {
	cs := C.CString(path)
	defer C.free(unsafe.Pointer(cs))
	if C.mapnik_register_datasources(cs) == 0 {
		e := C.GoString(C.mapnik_register_last_error())
		if e != "" {
			return errors.New("registering datasources: " + e)
		}
		return errors.New("error while registering datasources")
	}
	return nil
}
func RegisterFonts(path string) error {
	cs := C.CString(path)
	defer C.free(unsafe.Pointer(cs))
	if C.mapnik_register_fonts(cs) == 0 {
		e := C.GoString(C.mapnik_register_last_error())
		if e != "" {
			return errors.New("registering fonts: " + e)
		}
		return errors.New("error while registering fonts")
	}
	return nil
}
func New() *Map {
	return &Map{
		m:      C.mapnik_map(C.uint(800), C.uint(600)),
		width:  800,
		height: 600,
	}
}
func NewSized(width, height int) *Map {
	return &Map{
		m:      C.mapnik_map(C.uint(width), C.uint(height)),
		width:  width,
		height: height,
	}
}
func (m *Map) Load(stylesheet string) error {
	cs := C.CString(stylesheet)
	defer C.free(unsafe.Pointer(cs))
	if C.mapnik_map_load(m.m, cs) != 0 {
		return m.lastError()
	}
	return nil
}
func (m *Map) Resize(width, height int) {
	C.mapnik_map_resize(m.m, C.uint(width), C.uint(height))
	m.width = width
	m.height = height
}
func (m *Map) Free() {
	C.mapnik_map_free(m.m)
	m.m = nil
}
func (m *Map) SRS() string {
	return C.GoString(C.mapnik_map_get_srs(m.m))
}
func (m *Map) ZoomAll() error {
	if C.mapnik_map_zoom_all(m.m) != 0 {
		return m.lastError()
	}
	return nil
}
func (m *Map) ZoomTo(minx, miny, maxx, maxy float64) {
	bbox := C.mapnik_bbox(C.double(minx), C.double(miny), C.double(maxx), C.double(maxy))
	defer C.mapnik_bbox_free(bbox)
	C.mapnik_map_zoom_to_box(m.m, bbox)
}
func (m *Map) Render(opts RenderOpts) ([]byte, error) {
	scaleFactor := opts.ScaleFactor
	if scaleFactor == 0.0 {
		scaleFactor = 1.0
	}
	i := C.mapnik_map_render_to_image(m.m, C.double(opts.Scale), C.double(scaleFactor))
	if i == nil {
		return nil, m.lastError()
	}
	defer C.mapnik_image_free(i)
	if opts.Format == "raw" {
		size := 0
		raw := C.mapnik_image_to_raw(i, (*C.size_t)(unsafe.Pointer(&size)))
		return C.GoBytes(unsafe.Pointer(raw), C.int(size)), nil
	}
	var format *C.char
	if opts.Format != "" {
		format = C.CString(opts.Format)
	} else {
		format = C.CString("png256")
	}
	b := C.mapnik_image_to_blob(i, format)
	if b == nil {
		return nil, errors.New("mapnik: " + C.GoString(C.mapnik_image_last_error(i)))
	}
	C.free(unsafe.Pointer(format))
	defer C.mapnik_image_blob_free(b)
	return C.GoBytes(unsafe.Pointer(b.ptr), C.int(b.len)), nil
}
func (m *Map) RenderImage(opts RenderOpts) (*image.NRGBA, error) {
	scaleFactor := opts.ScaleFactor
	if scaleFactor == 0.0 {
		scaleFactor = 1.0
	}
	i := C.mapnik_map_render_to_image(m.m, C.double(opts.Scale), C.double(scaleFactor))
	if i == nil {
		return nil, m.lastError()
	}
	defer C.mapnik_image_free(i)
	size := 0
	raw := C.mapnik_image_to_raw(i, (*C.size_t)(unsafe.Pointer(&size)))
	b := C.GoBytes(unsafe.Pointer(raw), C.int(size))
	img := &image.NRGBA{
		Pix:    b,
		Stride: int(m.width * 4),
		Rect:   image.Rect(0, 0, int(m.width), int(m.height)),
	}
	return img, nil
}
func (m *Map) RenderToFile(opts RenderOpts, path string) error {
	scaleFactor := opts.ScaleFactor
	if scaleFactor == 0.0 {
		scaleFactor = 1.0
	}
	cs := C.CString(path)
	defer C.free(unsafe.Pointer(cs))
	var format *C.char
	if opts.Format != "" {
		format = C.CString(opts.Format)
	} else {
		format = C.CString("png256")
	}
	defer C.free(unsafe.Pointer(format))
	if C.mapnik_map_render_to_file(m.m, cs, C.double(opts.Scale), C.double(scaleFactor), format) != 0 {
		return m.lastError()
	}
	return nil
}
func (m *Map) SetBufferSize(s int) {
	C.mapnik_map_set_buffer_size(m.m, C.int(s))
}
func Search(st string, mx time.Duration) ([]SearchResponse, error) {
	conn, err := listenForSearchResponses()
	if err != nil {
		return nil, err
	}
	defer conn.Close()

	searchBytes, broadcastAddr := buildSearchRequest(st, mx)
	// Write search bytes on the wire so all devices can respond
	_, err = conn.WriteTo(searchBytes, broadcastAddr)
	if err != nil {
		return nil, err
	}

	return readSearchResponses(conn, mx)
}
func (p *blockingPool) Get() (net.Conn, error) {
	//in case that pool is closed or pool.conns is set to nil
	conns := p.conns
	if conns == nil {
		return nil, ErrClosed
	}

	select {
	case conn := <-conns:
		if time.Since(conn.start) > p.livetime {
			if conn.Conn != nil {
				conn.Conn.Close()
				conn.Conn = nil
			}
		}
		if conn.Conn == nil {
			var err error
			conn.Conn, err = p.factory()
			if err != nil {
				conn.start = time.Now()
				p.put(conn)
				return nil, err
			}
		}
		conn.unusable = false
		return conn, nil
	case <-time.After(time.Second*p.timeout):
		return nil, ErrTimeout
	}
}
func (p *blockingPool) put(conn *WrappedConn) error {
	//in case that pool is closed and pool.conns is set to nil
	conns := p.conns
	if conns == nil {
		//conn.Conn is possibly nil coz factory() may fail, in which case conn is immediately 
		//put back to the pool
		if conn.Conn != nil {
			conn.Conn.Close()
			conn.Conn = nil
		}
		return ErrClosed
	}

	//if conn is marked unusable, underlying net.Conn is set to nil
	if conn.unusable {
		if conn.Conn != nil {
			conn.Conn.Close()
			conn.Conn = nil
		}
	}

	//It is impossible to block as number of connections is never more than length of channel
	conns <-conn
	return nil
}
func MonoToStereoF32(buf *audio.Float32Buffer) error {
	if buf == nil || buf.Format == nil || buf.Format.NumChannels != 1 {
		return audio.ErrInvalidBuffer
	}
	stereoData := make([]float32, len(buf.Data)*2)
	var j int
	for i := 0; i < len(buf.Data); i++ {
		stereoData[j] = buf.Data[i]
		j++
		stereoData[j] = buf.Data[i]
		j++
	}
	buf.Data = stereoData
	buf.Format.NumChannels = 2
	return nil
}
func BitCrush(buf *audio.FloatBuffer, factor float64) {
	stepSize := crusherStepSize * factor
	for i := 0; i < len(buf.Data); i++ {
		frac, exp := math.Frexp(buf.Data[i])
		frac = signum(frac) * math.Floor(math.Abs(frac)/stepSize+0.5) * stepSize
		buf.Data[i] = math.Ldexp(frac, exp)
	}
}
func NormalizeMax(buf *audio.FloatBuffer) {
	if buf == nil {
		return
	}
	max := 0.0

	for i := 0; i < len(buf.Data); i++ {
		if math.Abs(buf.Data[i]) > max {
			max = math.Abs(buf.Data[i])
		}
	}

	if max != 0.0 {
		for i := 0; i < len(buf.Data); i++ {
			buf.Data[i] /= max
		}
	}
}
func Gain(buf *audio.FloatBuffer, multiplier float64) error {
	if buf == nil {
		return audio.ErrInvalidBuffer
	}

	for i := 0; i < len(buf.Data); i++ {
		buf.Data[i] *= multiplier
	}

	return nil
}
func MonoDownmix(buf *audio.FloatBuffer) error {
	if buf == nil || buf.Format == nil {
		return audio.ErrInvalidBuffer
	}
	nChans := buf.Format.NumChannels
	if nChans < 2 {
		return nil
	}
	nChansF := float64(nChans)

	frameCount := buf.NumFrames()
	newData := make([]float64, frameCount)
	for i := 0; i < frameCount; i++ {
		newData[i] = 0
		for j := 0; j < nChans; j++ {
			newData[i] += buf.Data[i*nChans+j]
		}
		newData[i] /= nChansF
	}
	buf.Data = newData
	buf.Format.NumChannels = 1

	return nil
}
func Quantize(buf *audio.FloatBuffer, bitDepth int) {
	if buf == nil {
		return
	}
	max := math.Pow(2, float64(bitDepth)) - 1

	bufLen := len(buf.Data)
	for i := 0; i < bufLen; i++ {
		buf.Data[i] = round((buf.Data[i]+1)*max)/max - 1.0
	}
}
func PCMScale(buf *audio.FloatBuffer, bitDepth int) error {
	if buf == nil || buf.Format == nil {
		return audio.ErrInvalidBuffer
	}
	factor := math.Pow(2, 8*float64(bitDepth/8)-1)
	for i := 0; i < len(buf.Data); i++ {
		buf.Data[i] *= factor
	}

	return nil
}
func StereoPan(buf *audio.FloatBuffer, pan float64) error {
	if buf == nil || buf.Format == nil || buf.Format.NumChannels != 2 {
		return audio.ErrInvalidBuffer
	}
	if pan < 0 || pan > 1 {
		return errors.New("invalid pan value, should be betwen 0 and 1")
	}
	if pan == 0.5 {
		return nil
	}

	if pan < 0.5 {
		for i := 0; i+2 <= len(buf.Data); i += 2 {
			buf.Data[i+1] *= (pan * 2)
		}
	} else {
		for i := 0; i+2 <= len(buf.Data); i += 2 {
			buf.Data[i] *= ((1 - pan) * 2)
		}
	}

	return nil
}
func (f *Follower) Leader() string {
	f.lock.Lock()
	defer f.lock.Unlock()
	return f.leader
}
func (c *Candidate) IsLeader() bool {
	c.lock.Lock()
	defer c.lock.Unlock()
	return c.leader
}
func NewResponse(clientID, redirectURI, userID, exp, code string) Response {
	return Response{ClientID: clientID, RedirectURI: redirectURI, UserID: userID, Exp: exp, Code: code}
}
func GetRequestID(ctx context.Context) (string, error) {
	if ctx == nil {
		return "", errors.New("nil context")
	}

	reqID, ok := ctx.Value(contextRequestID).(string)
	if !ok {
		return "", errors.New("unexpected type")
	}

	if len(reqID) == 0 {
		return "", errors.New("empty value in context")
	}

	return reqID, nil
}
func (m *monitorableWriter) Log() {
	duration := time.Now().Sub(m.t0)
	if m.Code == 0 {
		m.Code = 200
	}
	if m.opts.Filter != nil && !m.opts.Filter(m.r, m.Code, duration, m.Size) {
		return //skip
	}
	cc := m.colorCode()
	size := ""
	if m.Size > 0 {
		size = sizestr.ToString(m.Size)
	}
	buff := bytes.Buffer{}
	m.opts.formatTmpl.Execute(&buff, &struct {
		*Colors
		Timestamp, Method, Path, CodeColor string
		Code                               int
		Duration, Size, IP                 string
	}{
		m.opts.Colors,
		m.t0.Format(m.opts.TimeFormat), m.method, m.path, cc,
		m.Code,
		fmtDuration(duration), size, m.ip,
	})
	//fmt is threadsafe :)
	fmt.Fprint(m.opts.Writer, buff.String())
}
func SendMailSSL(addr string, a smtp.Auth, from string, to []string, msg []byte) error {
	conn, err := tls.Dial("tcp", addr, &tls.Config{InsecureSkipVerify: true}) //TODO: Not secure
	if err != nil {
		log.Println("Error Dialing", err)
		return err
	}
	h, _, _ := net.SplitHostPort(addr)
	c, err := smtp.NewClient(conn, h)
	if err != nil {
		log.Println("Error SMTP connection", err)
		return err
	}
	defer c.Close()

	if a != nil {
		if ok, _ := c.Extension("AUTH"); ok {
			if err = c.Auth(a); err != nil {
				log.Printf("Authentication error: %v", err)
				return err
			}
		}
	}

	if err = c.Mail(from); err != nil {
		log.Printf("From error: %v", err)
		return err
	}

	for _, addr := range to {
		if err = c.Rcpt(addr); err != nil {
			log.Printf("Recipient error: %v", err)
			return err
		}
	}

	w, err := c.Data()
	if err != nil {
		return err
	}

	w.Write(msg)
	w.Close()

	return c.Quit()
}
func (m *MailService) Send(message string, subject string, from string, to string) (err error) {
	t := []string{to}
	msg := []byte("From: " + from + "\r\n" +
		"To: " + to + "\r\n" +
		"Subject: " + subject + "\r\n" +
		"\r\n" +
		message + "\r\n")

	err = m.SMTP.SendMail(from, t, msg)

	return
}
func WaitForIt(fullConn, host string, port, timeout int) error {
	// fullConn := flag.String("full-connection", "", "full connection")
	// host := flag.String("host", "", "host to connect")
	// port := flag.Int("port", 80, "port to connect")
	// timeout := flag.Int("timeout", 10, "time to wait until port become available")
	// printVersion := flag.Bool("v", false, "show the current version")
	// debug = flag.Bool("debug", false, "enable debug")

	// flag.Parse()

	// if *printVersion {
	// 	fmt.Println("waitforit version " + VERSION)
	// 	return
	// }

	conn := buildConn(host, port, fullConn)
	if conn == nil {
		return errors.New("Invalid connection")
	}

	log.Debug("Waiting " + strconv.Itoa(timeout) + " seconds")
	if err := pingTCP(conn, timeout); err != nil {
		return err
	}

	if conn.Scheme != "http" && conn.Scheme != "https" {
		return nil
	}

	if err := pingHTTP(conn, timeout); err != nil {
		return err
	}

	return nil
}
func Get(source interface{}, environment string, configEnv Environment) (conf interface{}, err error) {
	if filename, ok := source.(string); ok {
		source, err = ioutil.ReadFile(filename)
		if err != nil {
			log.Printf("Fatal: %v", err)
			return
		}
	}

	err = yaml.Unmarshal(source.([]byte), configEnv)
	if err != nil {
		log.Printf("Fatal: bad config : %v", err)
		return
	}

	conf = configEnv.GetEnvironment(environment)
	if conf == nil {
		err = errors.New("No configuration")
		return
	}
	return
}
func initConfig() {
	if cfgFile != "" { // enable ability to specify config file via flag
		Viper.SetConfigFile(cfgFile)
	}

	Viper.SetConfigName("config") // name of config file (without extension)
	Viper.AddConfigPath("$HOME")  // adding home directory as first search path
	Viper.AddConfigPath("./")     // adding local directory as second search path
	Viper.AutomaticEnv()          // read in environment variables that match

	// If a config file is found, read it in.
	if err := Viper.ReadInConfig(); err == nil {
		fmt.Println("Using config file:", Viper.ConfigFileUsed())
	}
}
func WritePluginResultsToDatabase(results map[string]interface{}) {

	// connect to RethinkDB
	session, err := r.Connect(r.ConnectOpts{
		Address:  fmt.Sprintf("%s:28015", utils.Getopt("MALICE_RETHINKDB", "rethink")),
		Timeout:  5 * time.Second,
		Database: "malice",
	})
	if err != nil {
		log.Debug(err)
		return
	}
	defer session.Close()

	res, err := r.Table("samples").Get(results["ID"]).Run(session)
	utils.Assert(err)
	defer res.Close()

	if res.IsNil() {
		// upsert into RethinkDB
		resp, err := r.Table("samples").Insert(results, r.InsertOpts{Conflict: "replace"}).RunWrite(session)
		utils.Assert(err)
		log.Debug(resp)
	} else {
		resp, err := r.Table("samples").Get(results["ID"]).Update(map[string]interface{}{
			"plugins": map[string]interface{}{
				category: map[string]interface{}{
					name: results["Data"],
				},
			},
		}).RunWrite(session)
		utils.Assert(err)

		log.Debug(resp)
	}
}
func NewAuthController(um UserManager, cnf web.Config) *AuthController {
	authController := AuthController{UserManager: um, cnf: cnf}
	return &authController
}
func (c *AuthController) Authenticate(w http.ResponseWriter, r *http.Request) {
	decoder := json.NewDecoder(r.Body)
	var user User
	var data map[string]string
	err := decoder.Decode(&user)
	if err != nil {
		http.Error(w, fmt.Sprintf("%v", err), 400)
		return
	}
	user, _ = c.UserManager.Get(user.Username, user.Password)
	if user.Username != "" {
		token := jwt.New(jwt.GetSigningMethod("HS256"))
		claims := token.Claims.(jwt.MapClaims)

		claims["username"] = user.Username
		claims["email"] = user.Email
		claims["exp"] = time.Now().Add(time.Minute * 10).Unix()
		tokenString, _ := token.SignedString([]byte(c.cnf.Jwt.Key))
		data = map[string]string{
			"id_token": tokenString,
		}
	}
	js, _ := json.Marshal(data)
	w.Write(js)
}
func NewOAuth2Controller(am ApplicationManager, cnf web.Config) *OAuth2Controller {
	oAuth2Controller := OAuth2Controller{cnf: cnf, ApplicationManager: am}
	return &oAuth2Controller
}
func (c *OAuth2Controller) Refresh(w http.ResponseWriter, r *http.Request) {
	grantType := r.URL.Query().Get("grant_type")
	refreshToken := r.URL.Query().Get("refresh_token")
	if strings.Compare(grantType, "refresh_token") != 0 {
		http.Error(w, errors.New("Parameter grant_type is required").Error(), http.StatusBadRequest)
		return
	}
	if strings.Compare(refreshToken, "") == 0 {
		http.Error(w, errors.New("Parameter refreshToken is required").Error(), http.StatusBadRequest)
		return
	}
	token, err := jwt.Parse(refreshToken, func(token *jwt.Token) (interface{}, error) {
		if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
			return nil, fmt.Errorf("Unexpected signing method: %v", token.Header["alg"])
		}
		return []byte(c.cnf.Jwt.Key), nil
	})
	if err == nil && token.Valid {
		token := jwt.New(jwt.GetSigningMethod("HS256"))
		claims := token.Claims.(jwt.MapClaims)
		claims["exp"] = time.Now().Add(time.Hour * 1).Unix()
		tokenString, _ := token.SignedString([]byte(c.cnf.Jwt.Key))

		data := map[string]string{
			"access_token": tokenString,
			"token_type":   "bearer",
			"expires_in":   "3600",
		}
		js, _ := json.Marshal(data)
		w.Write(js)
	}
}
func (c *OAuth2Controller) Token(w http.ResponseWriter, r *http.Request) {
	grantType := r.URL.Query().Get("grant_type")
	code := r.URL.Query().Get("code")
	if strings.Compare(grantType, "authorization_code") != 0 {
		http.Error(w, errors.New("Parameter grant_type is required").Error(), http.StatusBadRequest)
		return
	}
	if strings.Compare(code, "") == 0 {
		http.Error(w, errors.New("Parameter code is required").Error(), http.StatusBadRequest)
		return
	}
	response, err := DecodeOAuth2Code(code, c.cnf.Jwt.Key)
	if err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	i, err := strconv.ParseInt(response.Exp, 10, 64)
	exp := time.Unix(i, 0)
	if exp.After(time.Now()) {
		log.Printf("Code is expired")
	} else {
		token := jwt.New(jwt.GetSigningMethod("HS256"))
		claims := token.Claims.(jwt.MapClaims)
		claims["exp"] = time.Now().Add(time.Hour * 1).Unix()
		tokenString, _ := token.SignedString([]byte(c.cnf.Jwt.Key))

		refreshToken := jwt.New(jwt.GetSigningMethod("HS256"))
		refreshClaims := refreshToken.Claims.(jwt.MapClaims)
		refreshClaims["exp"] = 0
		refreshTokenString, _ := refreshToken.SignedString([]byte(c.cnf.Jwt.Key))
		data := map[string]string{
			"access_token":  tokenString,
			"token_type":    "bearer",
			"refresh_token": refreshTokenString,
			"expires_in":    "3600",
		}
		js, _ := json.Marshal(data)
		w.Write(js)
	}
}
func Logging(next http.Handler, log *logrus.Logger) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		srw := StatusResponseWriter{w, 0, 0}
		start := time.Now()
		next.ServeHTTP(&srw, r)
		end := time.Now()
		latency := end.Sub(start)

		log.WithField("client", r.RemoteAddr).WithField("latency", latency).WithField("length", srw.Length()).WithField("code", srw.Status()).Printf("%s %s %s", r.Method, r.URL, r.Proto)
	})
}
func SingleFile(filename string) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		http.ServeFile(w, r, filename)
	})
}
func (w *StatusResponseWriter) Write(data []byte) (int, error) {
	w.length = len(data)
	return w.ResponseWriter.Write(data)
}
func (w *StatusResponseWriter) Hijack() (net.Conn, *bufio.ReadWriter, error) {
	if hj, ok := w.ResponseWriter.(http.Hijacker); ok {
		return hj.Hijack()
	}
	return nil, nil, errors.New("Not a Hijacker")
}
func NewMongo(filename string, environment string) (*Mongo, error) {
	ctx := context.Background()
	cnf, err := GetMongo(filename, environment)
	if err != nil {
		return nil, err
	}
	var uri string
	if len(cnf.Username) > 0 && len(cnf.Password) > 0 {
		uri = fmt.Sprintf(`mongodb://%s:%s@%s:%d/%s`,
			cnf.Username,
			cnf.Password,
			cnf.Host,
			cnf.Port,
			cnf.Database,
		)
	} else {
		uri = fmt.Sprintf(`mongodb://%s:%d/%s`,
			cnf.Host,
			cnf.Port,
			cnf.Database,
		)
	}
	client, err := mongo.NewClient(uri)
	if err != nil {
		log.Printf("L'URI du serveur MongoDB est incorrect: %s", uri)
		return nil, err
	}
	err = client.Connect(ctx)
	if err != nil {
		log.Print("Impossible d'utiliser ce context")
		return nil, err
	}

	db := client.Database(cnf.Database)

	err = client.Ping(ctx, nil)
	if err != nil {
		log.Printf("Impossible de contacter %v sur le port %d", cnf.Host, cnf.Port)
		return nil, err
	}
	return &Mongo{Client: client, Database: db, context: ctx}, nil
}
func (m *Mongo) Disconnect() error {
	err := m.Client.Disconnect(m.context)
	if err != nil {
		log.Printf("Impossible de fermer la connexion")
		return err
	}
	return nil
}
func New(fields []string) *Table {
	return &Table{
		Fields:     fields,
		Rows:       make([]map[string]string, 0),
		fieldSizes: make(map[string]int),
	}
}
func PrintTable(fields []string, rows []map[string]interface{}) {
	table := New(fields)
	for _, r := range rows {
		table.AddRow(r)
	}
	table.Print()
}
func PrintHorizontal(m map[string]interface{}) {
	table := New([]string{"Key", "Value"})
	rows := mapToRows(m)
	for _, row := range rows {
		table.AddRow(row)
	}
	table.HideHead = true
	table.Print()
}
func PrintRow(fields []string, row map[string]interface{}) {
	table := New(fields)
	table.AddRow(row)
	table.Print()
}
func (t *Table) AddRow(row map[string]interface{}) {
	newRow := make(map[string]string)
	for _, k := range t.Fields {
		v := row[k]
		// If is not nil format
		// else value is empty string
		var val string
		if v == nil {
			val = ""
		} else {
			val = fmt.Sprintf("%v", v)
		}

		newRow[k] = val
	}

	t.calculateSizes(newRow)

	if len(newRow) > 0 {
		t.Rows = append(t.Rows, newRow)
	}
}
func (t *Table) Print() {
	if len(t.Rows) == 0 && t.Footer == nil {
		return
	}

	t.calculateSizes(t.Footer)

	if !t.Markdown {
		t.printDash()
	}

	if !t.HideHead {
		fmt.Println(t.getHead())
		t.printTableDash()
	}

	for _, r := range t.Rows {
		fmt.Println(t.rowString(r))
		if !t.Markdown {
			t.printDash()
		}
	}

	if t.Footer != nil {
		t.printTableDash()
		fmt.Println(t.rowString(t.Footer))
		if !t.Markdown {
			t.printTableDash()
		}
	}
}
func (t *Table) String(title string) string {

	tableString := []string{}

	if len(t.Rows) == 0 && t.Footer == nil {
		return ""
	}

	tableString = append(tableString, "### "+title)

	t.calculateSizes(t.Footer)

	if !t.Markdown {
		// t.printDash()
		tableString = append(tableString, t.stringDash())
	}

	if !t.HideHead {
		tableString = append(tableString, t.getHead())
		tableString = append(tableString, t.stringTableDash())
		// fmt.Println(t.getHead())
		// t.printTableDash()
	}

	for _, r := range t.Rows {
		// fmt.Println(t.rowString(r))
		tableString = append(tableString, t.rowString(r))
		if !t.Markdown {
			tableString = append(tableString, t.stringDash())
		}
	}

	if t.Footer != nil {
		tableString = append(tableString, t.stringTableDash())
		tableString = append(tableString, t.rowString(t.Footer))
		if !t.Markdown {
			tableString = append(tableString, t.stringDash())
		}
	}

	return strings.Join(tableString[:], "\n")
}
func (t *Table) getHead() string {
	s := "|"
	for _, name := range t.Fields {
		s += t.fieldString(name, strings.Title(name)) + "|"
	}
	return s
}
func (t *Table) rowString(row map[string]string) string {
	s := "|"
	for _, name := range t.Fields {
		value := row[name]
		s += t.fieldString(name, value) + "|"
	}
	return s
}
func (t *Table) fieldString(name, value string) string {
	value = fmt.Sprintf(" %s ", value)
	spacesLeft := t.fieldSizes[name] - runewidth.StringWidth(value)
	if spacesLeft > 0 {
		for i := 0; i < spacesLeft; i++ {
			value += " "
		}
	}
	return value
}
func (t *Table) stringTableDash() string {
	if t.Markdown {
		return t.stringMarkdownDash()
	}
	return t.stringDash()
}
func (t *Table) printMarkdownDash() {
	r := make(map[string]string)
	for _, name := range t.Fields {
		r[name] = strings.Repeat("-", t.fieldSizes[name]-2)
	}
	fmt.Println(t.rowString(r))
}
func (t *Table) stringMarkdownDash() string {
	r := make(map[string]string)
	for _, name := range t.Fields {
		r[name] = strings.Repeat("-", t.fieldSizes[name]-2)
	}
	return t.rowString(r)
}
func HashPassword(password []byte, salt []byte) (hash []byte, err error) {
	hash, err = scrypt.Key(password, salt, N, R, P, KEYLENGTH)
	if err != nil {
		return nil, err
	}
	return
}
func EncodeOAuth2Code(clientID, redirectURI, userID, sharedKey string) (code string, err error) {
	rand := RandStringBytesMaskImprSrc(20)
	exp := time.Now().Add(time.Minute * 10).String()
	response := NewResponse(clientID, redirectURI, userID, exp, rand)
	jresponse, err := json.Marshal(response)
	if err != nil {
		log.Printf("Error: %v", err)
	}
	j64response := base64.StdEncoding.EncodeToString(jresponse)
	signer, err := jose.NewSigner(jose.SigningKey{Algorithm: jose.HS512, Key: []byte(sharedKey)}, nil)
	if err != nil {
		log.Printf("Error: %v", err)
	}
	object, err := signer.Sign([]byte(j64response))
	if err != nil {
		log.Printf("Error: %v", err)
	}
	code, err = object.CompactSerialize()
	return
}
func DecodeOAuth2Code(code, sharedKey string) (response Response, err error) {
	object, err := jose.ParseSigned(code)
	if err != nil {
		return
	}
	output, err := object.Verify([]byte(sharedKey))
	if err != nil {
		return
	}
	base64Text := make([]byte, base64.StdEncoding.DecodedLen(len(output)))
	l, err := base64.StdEncoding.Decode(base64Text, output)
	if err != nil {
		return
	}
	response = Response{}
	err = json.Unmarshal(base64Text[:l], &response)
	return
}
func NewServer(filename string, environment string) (server *Server, err error) {
	conf, err := GetConfig(filename, environment)

	logFile, err := os.OpenFile(conf.Log.File+logFilename, os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0666)
	if err != nil {
		log.SetOutput(os.Stderr)
		log.Warningf("Can't open logfile: %v", err)
	} else {
		log.SetOutput(logFile)
	}

	level := log.ErrorLevel
	if strings.Compare(conf.Log.Level, "") != 0 {
		level, _ = log.ParseLevel(conf.Log.Level)
	} else {
		log.Infof("Log Level: %v", level)
	}
	log.SetLevel(level)

	server = &Server{Config: conf, Done: make(chan bool, 1), Error: make(chan error, 1), Server: http.Server{Handler: NewLoggingServeMux(conf)}, quit: make(chan bool), isStarted: false}
	return
}
func (s *Server) Start() (err error) {
	log.Infof("Lunarc is starting on port :%d", s.Config.Port)
	var l net.Listener
	go func() {
		l, err = net.Listen("tcp", fmt.Sprintf(":%d", s.Config.Port))
		if err != nil {
			log.Errorf("Error: %v", err)
			s.Error <- err
			return
		}
		s.isStarted = true
		if len(s.Config.SSL.Certificate) > 0 && len(s.Config.SSL.Key) > 0 {
			err = s.ServeTLS(l, s.Config.SSL.Certificate, s.Config.SSL.Key)
			if err != nil && err != http.ErrServerClosed {
				log.Errorf("%v", err)
				l.Close()
				s.Error <- err
				s.quit <- true
			}
			close(s.quit)
		} else {
			err = s.Serve(l)
			if err != nil && err != http.ErrServerClosed {
				log.Errorf("%v", err)
				s.Error <- err
				s.quit <- true
			}
			close(s.quit)
		}
	}()

	<-s.quit

	if err = s.Shutdown(context.Background()); err != nil {
		log.Errorf("%v", err)
		s.Error <- err
	}

	<-s.quit

	l = nil
	log.Info("Lunarc terminated.")
	s.isStarted = false
	s.Done <- true
	return
}
func (s *Server) Stop() {
	if s.isStarted && s.quit != nil {
		log.Info("Lunarc is stopping...")
		s.quit <- true
	} else {
		log.Info("Lunarc is not running")
		s.Error <- errors.New("Lunarc is not running")
		s.Done <- false
	}
}
func NewLoggingServeMux(conf Config) *LoggingServeMux {
	serveMux := http.NewServeMux()
	return &LoggingServeMux{serveMux, conf}
}
func (mux *LoggingServeMux) Handler(r *http.Request) (h http.Handler, pattern string) {
	return mux.serveMux.Handler(r)
}
func (mux *LoggingServeMux) Handle(pattern string, handler http.Handler) {

	var log = logrus.New()

	logFile, err := os.OpenFile(mux.conf.Log.File+aFilename, os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0666)
	if err != nil {
		log.Out = os.Stderr
		log.Warningf("Can't open logfile: %v", err)
	} else {
		log.Out = logFile
	}
	mux.serveMux.Handle(pattern, Logging(handler, log))
}
func (mux *LoggingServeMux) HandleFunc(pattern string, handler func(http.ResponseWriter, *http.Request)) {
	mux.serveMux.Handle(pattern, http.HandlerFunc(handler))
}
func (db *Database) Init() error {

	// Create URL from host/port
	db.getURL()

	// Test connection to ElasticSearch
	err := db.TestConnection()
	if err != nil {
		return errors.Wrap(err, "failed to connect to database")
	}

	client, err := elastic.NewSimpleClient(
		elastic.SetURL(db.URL),
		elastic.SetBasicAuth(
			utils.Getopts(db.Username, "MALICE_ELASTICSEARCH_USERNAME", ""),
			utils.Getopts(db.Password, "MALICE_ELASTICSEARCH_PASSWORD", ""),
		),
	)
	if err != nil {
		return errors.Wrap(err, "failed to create elasticsearch simple client")
	}

	exists, err := client.IndexExists(db.Index).Do(context.Background())
	if err != nil {
		return errors.Wrap(err, "failed to check if index exists")
	}

	if !exists {
		// Index does not exist yet.
		createIndex, err := client.CreateIndex(db.Index).BodyString(mapping).Do(context.Background())
		if err != nil {
			return errors.Wrapf(err, "failed to create index: %s", db.Index)
		}

		if !createIndex.Acknowledged {
			log.Error("index creation not acknowledged")
		} else {
			log.Debugf("created index %s", db.Index)
		}
	} else {
		log.Debugf("index %s already exists", db.Index)
	}

	return nil
}
func (db *Database) WaitForConnection(ctx context.Context, timeout int) error {

	var err error

	secondsWaited := 0

	connCtx, cancel := context.WithTimeout(ctx, time.Duration(timeout)*time.Second)
	defer cancel()

	log.Debug("===> trying to connect to elasticsearch")
	for {
		// Try to connect to Elasticsearch
		select {
		case <-connCtx.Done():
			return errors.Wrapf(err, "connecting to elasticsearch timed out after %d seconds", secondsWaited)
		default:
			err = db.TestConnection()
			if err == nil {
				log.Debugf("elasticsearch came online after %d seconds", secondsWaited)
				return nil
			}
			// not ready yet
			secondsWaited++
			log.Debug(" * could not connect to elasticsearch (sleeping for 1 second)")
			time.Sleep(1 * time.Second)
		}
	}
}
func (db *Database) StoreFileInfo(sample map[string]interface{}) (elastic.IndexResponse, error) {

	if len(db.Plugins) == 0 {
		return elastic.IndexResponse{}, errors.New("Database.Plugins is empty (you must set this field to use this function)")
	}

	// Test connection to ElasticSearch
	err := db.TestConnection()
	if err != nil {
		return elastic.IndexResponse{}, errors.Wrap(err, "failed to connect to database")
	}

	client, err := elastic.NewSimpleClient(
		elastic.SetURL(db.URL),
		elastic.SetBasicAuth(
			utils.Getopts(db.Username, "MALICE_ELASTICSEARCH_USERNAME", ""),
			utils.Getopts(db.Password, "MALICE_ELASTICSEARCH_PASSWORD", ""),
		),
	)
	if err != nil {
		return elastic.IndexResponse{}, errors.Wrap(err, "failed to create elasticsearch simple client")
	}

	// NOTE: I am not setting ID because I want to be able to re-scan files with updated signatures in the future
	fInfo := map[string]interface{}{
		// "id":      sample.SHA256,
		"file":      sample,
		"plugins":   db.Plugins,
		"scan_date": time.Now().Format(time.RFC3339Nano),
	}

	newScan, err := client.Index().
		Index(db.Index).
		Type(db.Type).
		OpType("index").
		// Id("1").
		BodyJson(fInfo).
		Do(context.Background())
	if err != nil {
		return elastic.IndexResponse{}, errors.Wrap(err, "failed to index file info")
	}

	log.WithFields(log.Fields{
		"id":    newScan.Id,
		"index": newScan.Index,
		"type":  newScan.Type,
	}).Debug("indexed sample")

	return *newScan, nil
}
func (db *Database) StorePluginResults(results database.PluginResults) error {

	// Test connection to ElasticSearch
	err := db.TestConnection()
	if err != nil {
		return errors.Wrap(err, "failed to connect to database")
	}

	client, err := elastic.NewSimpleClient(
		elastic.SetURL(db.URL),
		elastic.SetBasicAuth(
			utils.Getopts(db.Username, "MALICE_ELASTICSEARCH_USERNAME", ""),
			utils.Getopts(db.Password, "MALICE_ELASTICSEARCH_PASSWORD", ""),
		),
	)
	if err != nil {
		return errors.Wrap(err, "failed to create elasticsearch simple client")
	}

	// get sample db record
	getSample, err := client.Get().
		Index(db.Index).
		Type(db.Type).
		Id(results.ID).
		Do(context.Background())
	// ignore 404 not found error
	if err != nil && !elastic.IsNotFound(err) {
		return errors.Wrapf(err, "failed to get sample with id: %s", results.ID)
	}

	if getSample != nil && getSample.Found {
		log.Debugf("got document %s in version %d from index %s, type %s\n", getSample.Id, getSample.Version, getSample.Index, getSample.Type)
		updateScan := map[string]interface{}{
			"scan_date": time.Now().Format(time.RFC3339Nano),
			"plugins": map[string]interface{}{
				results.Category: map[string]interface{}{
					results.Name: results.Data,
				},
			},
		}
		update, err := client.Update().Index(db.Index).Type(db.Type).Id(getSample.Id).
			Doc(updateScan).
			Do(context.Background())
		if err != nil {
			return errors.Wrapf(err, "failed to update sample with id: %s", results.ID)
		}

		log.Debugf("updated version of sample %q is now %d\n", update.Id, update.Version)

	} else {
		// ID not found so create new document with `index` command
		scan := map[string]interface{}{
			"plugins": map[string]interface{}{
				results.Category: map[string]interface{}{
					results.Name: results.Data,
				},
			},
			"scan_date": time.Now().Format(time.RFC3339Nano),
		}

		newScan, err := client.Index().
			Index(db.Index).
			Type(db.Type).
			OpType("index").
			// Id("1").
			BodyJson(scan).
			Do(context.Background())
		if err != nil {
			return errors.Wrapf(err, "failed to create new sample plugin doc with id: %s", results.ID)
		}

		log.WithFields(log.Fields{
			"id":    newScan.Id,
			"index": newScan.Index,
			"type":  newScan.Type,
		}).Debug("indexed sample")
	}

	return nil
}
func CamelCase(src string) string {
	byteSrc := []byte(src)
	chunks := camelingRegex.FindAll(byteSrc, -1)
	for idx, val := range chunks {
		if idx > 0 {
			chunks[idx] = bytes.Title(val)
		}
	}
	return string(bytes.Join(chunks, nil))
}
func Getopt(name, dfault string) string {
	value := os.Getenv(name)
	if value == "" {
		value = dfault
	}
	return value
}
func Getopts(userInput, envVar, dfault string) string {

	if len(strings.TrimSpace(userInput)) > 0 {
		return userInput
	}
	value := os.Getenv(envVar)
	if value == "" {
		value = dfault
	}
	return value
}
func GetSHA256(name string) string {

	dat, err := ioutil.ReadFile(name)
	Assert(err)

	h256 := sha256.New()
	_, err = h256.Write(dat)
	Assert(err)

	return fmt.Sprintf("%x", h256.Sum(nil))
}
func RunCommand(ctx context.Context, cmd string, args ...string) (string, error) {

	var c *exec.Cmd

	if ctx != nil {
		c = exec.CommandContext(ctx, cmd, args...)
	} else {
		c = exec.Command(cmd, args...)
	}

	output, err := c.Output()
	if err != nil {
		return string(output), err
	}

	// check for exec context timeout
	if ctx != nil {
		if ctx.Err() == context.DeadlineExceeded {
			return "", fmt.Errorf("command %s timed out", cmd)
		}
	}

	return string(output), nil
}
func RemoveDuplicates(elements []string) []string {
	// Use map to record duplicates as we find them.
	encountered := map[string]bool{}
	result := []string{}

	for v := range elements {
		if encountered[elements[v]] == true {
			// Do not add duplicate.
		} else {
			// Record this element as an encountered element.
			encountered[elements[v]] = true
			// Append to result slice.
			result = append(result, elements[v])
		}
	}
	// Return the new slice.
	return result
}
func Unzip(archive, target string) error {

	// fmt.Println("Unzip archive ", target)

	reader, err := zip.OpenReader(archive)
	if err != nil {
		return err
	}
	defer reader.Close()

	for _, file := range reader.File {
		filePath := filepath.Join(target, file.Name)

		if file.FileInfo().IsDir() {
			os.MkdirAll(filePath, file.Mode())
			continue
		}
		fileReader, err := file.Open()
		if err != nil {
			return err
		}
		defer fileReader.Close()

		targetFile, err := os.OpenFile(filePath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, file.Mode())
		if err != nil {
			return err
		}
		defer targetFile.Close()

		if _, err := io.Copy(targetFile, fileReader); err != nil {
			return err
		}
	}
	return nil
}
func SliceContainsString(a string, list []string) bool {
	for _, b := range list {
		if strings.Contains(b, a) {
			return true
		}
	}
	return false
}
func NewSMTP(filename string, environment string) (s *SMTP, err error) {
	conf, err := GetSMTP(filename, environment)
	if err != nil {
		return
	}
	auth := smtp.PlainAuth("", conf.Auth.User, conf.Auth.Password, conf.Host)
	f := smtp.SendMail
	if conf.SSL {
		f = SendMailSSL
	}
	s = &SMTP{auth: auth, send: f, addr: fmt.Sprintf("%s:%d", conf.Host, conf.Port)}
	return
}
func (s *SMTP) SendMail(from string, to []string, msg []byte) (err error) {
	err = s.send(s.addr, s.auth, from, to, msg)
	return
}
func (se *SMTPEnvironment) GetEnvironment(environment string) interface{} {
	for env, conf := range se.Env {
		if strings.Compare(environment, env) == 0 {
			return conf
		}
	}
	return nil
}
func GetSMTP(source interface{}, environment string) (smtp Config, err error) {
	var smtpEnvironment SMTPEnvironment
	i, err := config.Get(source, environment, &smtpEnvironment)
	smtp = i.(Config)
	return
}
func GetConfig(source interface{}, environment string) (server Config, err error) {
	var serverEnvironment ServerEnvironment
	i, err := config.Get(source, environment, &serverEnvironment)
	server = i.(Config)
	return
}
func (m *Environment) GetEnvironment(environment string) interface{} {
	for env, conf := range m.Env {
		if strings.Compare(environment, env) == 0 {
			return conf
		}
	}
	return nil
}
func GetMongo(source interface{}, environment string) (mongo Config, err error) {
	var env Environment
	i, err := config.Get(source, environment, &env)
	mongo = i.(Config)
	return
}
func TokenHandler(next http.Handler, cnf web.Config) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		token, err := request.ParseFromRequest(r, request.AuthorizationHeaderExtractor, func(token *jwt.Token) (interface{}, error) {
			if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
				//TODO: On ne passe jamais à l'intérieur
				return nil, fmt.Errorf("Unexpected signing method: %v", token.Header["alg"])
			}
			return []byte(cnf.Jwt.Key), nil
		})
		if err == nil && token.Valid {
			next.ServeHTTP(w, r)
		} else {
			if r.URL.String() == "/" {
				next.ServeHTTP(w, r)
			} else {
				w.WriteHeader(http.StatusUnauthorized)
			}
		}
	})
}
func (c *Conn) Receive() (*InMsg, error) {
	stanza, err := c.xmppConn.Recv()
	if err != nil {
		return nil, err
	}

	if c.debug {
		log.Printf("Incoming raw CCS stanza: %+v\n", stanza)
	}

	chat, ok := stanza.(xmpp.Chat)
	if !ok {
		return nil, nil
	}

	if chat.Type == "error" {
		// todo: once go-xmpp can parse XMPP error messages, return error with XMPP error message (issue: https://github.com/soygul/gcm/issues/14)
		return nil, errors.New("CCS returned an XMPP error (can be a stanza or JSON error or anything else)")
	}

	var m InMsg
	if err = json.Unmarshal([]byte(chat.Other[0]), &m); err != nil { // todo: handle other fields of chat (remote/type/text/other[1,2,..])
		return nil, errors.New("unknow message from CCS")
	}

	switch m.MessageType {
	case "ack":
		return &m, nil // todo: mark message as sent
	case "nack":
		return &m, nil // todo: try and resend the message (after reconnect if problem is about connection draining)
	case "receipt":
		return &m, nil // todo: mark message as delivered and remove from the queue
	case "control":
		return &m, nil // todo: handle connection draining (and any other control message type?)
	case "":
		// acknowledge the incoming ordinary messages as per spec
		ack := &OutMsg{MessageType: "ack", To: m.From, ID: m.ID}
		if _, err = c.Send(ack); err != nil {
			return nil, fmt.Errorf("failed to send ack message to CCS with error: %v", err)
		}
		return &m, nil
	default:
		// unknown message types can be ignored, as per GCM specs
	}
	return &m, nil
}
func (c *Conn) Send(m *OutMsg) (n int, err error) {
	if m.ID == "" {
		if m.ID, err = getMsgID(); err != nil {
			return 0, err
		}
	}

	mb, err := json.Marshal(m)
	if err != nil {
		return 0, err
	}
	ms := string(mb)
	res := fmt.Sprintf(gcmMessageStanza, ms)
	return c.xmppConn.SendOrg(res)
}
func (d pivnetReleaseDiffer) allBoshReleaseNames() []string {
	boshReleaseNamesMap := make(map[string]string)
	var addReleaseNames = func(br map[string]*release.BoshRelease) {
		for brname := range br {
			boshReleaseNamesMap[brname] = brname
		}
	}
	addReleaseNames(d.release1.BoshRelease)
	addReleaseNames(d.release2.BoshRelease)
	var releaseNames []string
	for brname := range boshReleaseNamesMap {
		releaseNames = append(releaseNames, brname)
	}
	return releaseNames
}
func (dj *DeltaJob) AddedProperty(name string, p *enaml.JobManifestProperty) {
	dj.AddedProperties[name] = *p
}
func (dj *DeltaJob) RemovedProperty(name string, p *enaml.JobManifestProperty) {
	dj.RemovedProperties[name] = *p
}
func (r *Result) AddDeltaJob(dj *DeltaJob) {
	r.DeltaJob = append(r.DeltaJob, *dj)
}
func (r *Result) Concat(other *Result) {
	for _, dj := range other.DeltaJob {
		r.DeltaJob = append(r.DeltaJob, dj)
	}
}
func (r *Release) Read(releaseLocation string) (io.ReadCloser, error) {
	local, err := r.Pull(releaseLocation)
	if err != nil {
		return nil, err
	}
	rr, err := os.Open(local)
	if err != nil {
		return nil, err
	}
	return rr, nil
}
func (r *Release) Pull(releaseLocation string) (filename string, err error) {
	u, uerr := url.Parse(releaseLocation)
	if uerr != nil || !(u.Scheme == "http" || u.Scheme == "https") {
		// assume a local file, ensure it exists
		if _, ferr := os.Stat(releaseLocation); os.IsNotExist(ferr) {
			err = fmt.Errorf("Could not pull %s. The file doesn't exist or isn't a valid http(s) URL", releaseLocation)
			return
		}
		filename = releaseLocation
	} else {
		// remote file, ensure its in the local cache
		filename = r.CacheDir + "/" + path.Base(releaseLocation)
		if _, err = os.Stat(filename); os.IsNotExist(err) {
			fmt.Println("Could not find release in local cache. Downloading now.")
			err = r.download(releaseLocation, filename)
		}
	}
	return
}
func BuildJob(jobMeta BoshJobMeta, dest string) error {
	b, err := json.Marshal(jobMeta)

	if err != nil {
		return err
	}
	fmt.Println("building job: ", string(b))
	monitFile, specFile, err := createJobFiles(dest, jobMeta.Name)

	if err != nil {
		return err
	}
	defer monitFile.Close()
	defer specFile.Close()
	err = writeMonitFile(monitFile, jobMeta.Name, jobMeta.PIDFile)

	if err != nil {
		return err
	}
	err = writeSpecFile(specFile, jobMeta.Name, jobMeta.JobProperties, jobMeta.Packages)
	return err
}
func LoadBoshRelease(releaseRepo pull.Release, path string) (release *BoshRelease, err error) {
	var rr io.ReadCloser
	rr, err = releaseRepo.Read(path)
	if err != nil {
		return
	}
	defer func() {
		if cerr := rr.Close(); cerr != nil {
			err = cerr
		}
	}()
	release, err = readBoshRelease(rr)
	return
}
func readBoshRelease(rr io.Reader) (*BoshRelease, error) {
	release := &BoshRelease{
		JobManifests: make(map[string]enaml.JobManifest),
	}
	err := release.readBoshRelease(rr)
	return release, err
}
func (r *BoshRelease) readBoshRelease(rr io.Reader) error {
	w := pkg.NewTgzWalker(rr)
	w.OnMatch("release.MF", func(file pkg.FileEntry) error {
		return decodeYaml(file.Reader, &r.ReleaseManifest)
	})
	w.OnMatch("/jobs/", func(file pkg.FileEntry) error {
		job, jerr := r.readBoshJob(file.Reader)
		if jerr == nil {
			r.JobManifests[job.Name] = job
		}
		return jerr
	})
	err := w.Walk()
	return err
}
func (r *BoshRelease) readBoshJob(jr io.Reader) (enaml.JobManifest, error) {
	var job enaml.JobManifest
	jw := pkg.NewTgzWalker(jr)
	jw.OnMatch("job.MF", func(file pkg.FileEntry) error {
		return decodeYaml(file.Reader, &job)
	})
	err := jw.Walk()
	return job, err
}
func (s *Client) NewRequest(method, url string, body io.Reader) (*http.Request, error) {
	req, err := http.NewRequest(method, url, body)
	if err != nil {
		return nil, err
	}

	setAuth(s, req)
	return req, nil
}
func (s *Client) PushCloudConfig(manifest []byte) error {
	ccm := enaml.NewCloudConfigManifest(manifest)
	req, err := s.newCloudConfigRequest(*ccm)
	if err != nil {
		return err
	}
	res, err := s.http.Do(req)
	if err != nil {
		return err
	}
	defer res.Body.Close()
	if res.StatusCode >= 400 {
		body, err := ioutil.ReadAll(res.Body)
		if err != nil {
			return err
		}
		return fmt.Errorf("%s error pushing cloud config to BOSH: %s", res.Status, string(body))
	}
	return nil
}
func Generate(packagename string, fileBytes []byte, outputDir string) {
	b := preprocessJobManifest(fileBytes)
	objects := make(map[string]map[string]ObjectField)
	var properties []string
	for _, v := range b.recs {
		properties = append(properties, v.Orig)
	}
	for i := 0; i < b.max; i++ {

		for _, v := range b.recs {
			if v.Length-1 >= i {

				var structname = v.StructName(i, packagename, properties)
				var typeName = v.TypeName(i, properties)
				elementName := v.Slice[i]
				attributeName := FormatName(elementName)

				if _, ok := objects[structname]; !ok {
					objects[structname] = make(map[string]ObjectField)
				}

				if previousElement, ok := objects[structname][attributeName]; !ok {
					lo.G.Debug("Adding", attributeName, "to", structname, "with type", typeName)
					objects[structname][attributeName] = ObjectField{
						ElementName:       attributeName,
						ElementType:       typeName,
						ElementAnnotation: createElementAnnotation(elementName),
						Meta:              v.Yaml,
					}
				} else {
					if previousElement.ElementAnnotation != createElementAnnotation(elementName) {
						lo.G.Warning("******** Recommended creating custom yaml marshaller on", structname, "for", attributeName, " ********")
						previousElement.ElementAnnotation = "`yaml:\"-\"`"
						objects[structname][attributeName] = previousElement
					}
				}
			}
		}
	}
	structs := generateStructs(objects, packagename)
	writeStructsToDisk(structs, outputDir)
}
func CreateNewRecord(property string, yaml enaml.JobManifestProperty) (record Record) {
	elementArray := strings.Split(property, ".")
	record = Record{
		Length: len(elementArray),
		Orig:   property,
		Slice:  elementArray,
		Yaml:   yaml,
	}
	return
}
func NewShowCmd(releaseRepo pull.Release, release string) *ShowCmd {
	return &ShowCmd{
		releaseRepo: releaseRepo,
		release:     release,
	}
}
func (s *ShowCmd) All(w io.Writer) error {
	if filepath.Ext(s.release) == ".pivotal" {
		pivnetRelease, err := release.LoadPivnetRelease(s.releaseRepo, s.release)
		if err != nil {
			return err
		}
		for _, br := range pivnetRelease.BoshRelease {
			s.printBoshRelease(w, br)
		}
		return nil
	}
	boshRelease, err := release.LoadBoshRelease(s.releaseRepo, s.release)
	if err != nil {
		return err
	}
	s.printBoshRelease(w, boshRelease)
	return nil
}
func (s *ConcoursePipeline) GetDefaultTaskImageResource() atc.ImageResource {
	return atc.ImageResource{
		Type: s.defaultImageType,
		Source: atc.Source{
			"repository": s.defaultImageRepository,
		},
	}
}
func (s *ConcoursePipeline) AddRawJob(job atc.JobConfig) {
	s.Jobs = append(s.Jobs, job)
}
func (s *ConcoursePipeline) AddGroup(name string, jobs ...string) {
	s.Groups = append(s.Groups, atc.GroupConfig{
		Name: name,
		Jobs: jobs,
	})
}
func (s *ConcoursePipeline) GetResourceByName(name string) *atc.ResourceConfig {
	for i, v := range s.Resources {
		if v.Name == name {
			return &s.Resources[i]
		}
	}
	return nil
}
func (s *ConcoursePipeline) AddRawResource(rawResource atc.ResourceConfig) {
	s.Resources = append(s.Resources, rawResource)
}
func (s *ConcoursePipeline) AddResource(name string, typename string, source map[string]interface{}) {
	s.Resources = append(s.Resources, atc.ResourceConfig{
		Name:   name,
		Type:   typename,
		Source: source,
	})
}
func (s *ConcoursePipeline) AddGithubResource(name string, source map[string]interface{}) {
	s.AddResource(name, GithubResourceName, source)
}
func (s *ConcoursePipeline) AddBoshIOResource(name string, source map[string]interface{}) {
	s.AddResource(name, BoshIOResourceName, source)
}
func (s *ConcoursePipeline) AddBoshDeploymentResource(name string, source map[string]interface{}) {
	s.AddResource(name, BoshDeploymentResourceName, source)
}
func (s *ConcoursePipeline) AddGitResource(name string, source map[string]interface{}) {
	s.AddResource(name, GitResourceName, source)
}
func (d boshReleaseDiffer) allJobNames() []string {
	jobNamesMap := make(map[string]string)
	var addJobNames = func(br *release.BoshRelease) {
		if br != nil {
			for jbname := range br.JobManifests {
				jobNamesMap[jbname] = jbname
			}
		}
	}
	addJobNames(d.release1)
	addJobNames(d.release2)
	var jobNames []string
	for jname := range jobNamesMap {
		jobNames = append(jobNames, jname)
	}
	return jobNames
}
func NewDeploymentManifestFromFile(f *os.File) *DeploymentManifest {
	var b []byte
	fi, _ := f.Stat()

	if fi.Size() > 0 {
		b, _ = ioutil.ReadAll(f)
	}
	return NewDeploymentManifest(b)
}
func NewDeploymentManifest(b []byte) *DeploymentManifest {
	dm := new(DeploymentManifest)
	yaml.Unmarshal(b, dm)
	return dm
}
func (s *DeploymentManifest) AddRemoteRelease(releaseName, ver, url, sha1 string) (err error) {
	s.Releases = append(s.Releases, Release{
		Name:    releaseName,
		URL:     url,
		SHA1:    sha1,
		Version: ver,
	})
	return
}
func (s *DeploymentManifest) AddRemoteStemcell(os, alias, ver, url, sha1 string) {
	s.Stemcells = append(s.Stemcells, Stemcell{
		OS:      os,
		Alias:   alias,
		Version: ver,
		URL:     url,
		SHA1:    sha1,
	})
}
func (s *DeploymentManifest) Tag(key string) string {
	if s.Tags == nil {
		return ""
	}
	return s.Tags[key]
}
func (s *DeploymentManifest) RemoveTag(key string) {
	if s.Tags != nil {
		delete(s.Tags, key)
	}
}
func LoadPivnetRelease(releaseRepo pull.Release, path string) (release *PivnetRelease, err error) {
	release = &PivnetRelease{}
	var localPath string
	localPath, err = releaseRepo.Pull(path)
	if err != nil {
		return
	}
	release = &PivnetRelease{
		BoshRelease: make(map[string]*BoshRelease),
	}
	err = release.readPivnetRelease(localPath)
	return
}
func (r *PivnetRelease) BoshReleaseOrEmpty(name string) *BoshRelease {
	br := r.BoshRelease[name]
	if br == nil {
		br = emptyBoshRelease
	}
	return br
}
func (r *PivnetRelease) readPivnetRelease(path string) error {
	walker := pkg.NewZipWalker(path)
	walker.OnMatch("releases/", func(file pkg.FileEntry) error {
		br, berr := readBoshRelease(file.Reader)
		if berr != nil {
			return berr
		}
		r.BoshRelease[br.ReleaseManifest.Name] = br
		return nil
	})
	return walker.Walk()
}
func decodeYaml(r io.Reader, v interface{}) error {
	bytes, err := ioutil.ReadAll(r)
	if err == nil {
		yaml.Unmarshal(bytes, v)
	}
	return err
}
func NewZipWalker(zipFile string) Walker {
	return zipWalker{
		zipPath:   zipFile,
		callbacks: make(map[*regexp.Regexp]WalkFunc),
	}
}
func NewDiffCmd(releaseRepo pull.Release, release1, release2 string) *DiffCmd {
	return &DiffCmd{
		releaseRepo: releaseRepo,
		release1:    release1,
		release2:    release2,
	}
}
func (s *DiffCmd) All(w io.Writer) error {
	differ, err := diff.New(s.releaseRepo, s.release1, s.release2)
	if err != nil {
		return err
	}
	d, err := differ.Diff()
	if err != nil {
		return err
	}
	s.printDiffResult(w, d)
	return nil
}
func (s *DiffCmd) Job(job string, w io.Writer) error {
	differ, err := diff.New(s.releaseRepo, s.release1, s.release2)
	if err != nil {
		return err
	}
	d, err := differ.DiffJob(job)
	if err != nil {
		return err
	}
	s.printDiffResult(w, d)
	return nil
}
func New(releaseRepo pull.Release, r1Path, r2Path string) (differ Differ, err error) {
	if filepath.Ext(r1Path) != filepath.Ext(r2Path) {
		err = fmt.Errorf("The specified releases didn't have matching file extensions, " +
			"assuming different release types.")
		return
	}
	if filepath.Ext(r1Path) == ".pivotal" {
		var r1, r2 *release.PivnetRelease
		if r1, err = release.LoadPivnetRelease(releaseRepo, r1Path); err == nil {
			if r2, err = release.LoadPivnetRelease(releaseRepo, r2Path); err == nil {
				differ = pivnetReleaseDiffer{
					release1: r1,
					release2: r2,
				}
			}
		}
	} else {
		var r1, r2 *release.BoshRelease
		if r1, err = release.LoadBoshRelease(releaseRepo, r1Path); err == nil {
			if r2, err = release.LoadBoshRelease(releaseRepo, r2Path); err == nil {
				differ = boshReleaseDiffer{
					release1: r1,
					release2: r2,
				}
			}
		}
	}
	return
}
func (v *Record) StructName(i int, packagename string, properties []string) (structname string) {
	if i > 0 {
		currentNode := v.Slice[i-1]
		structname = FormatName(currentNode)
		if i > 1 {
			parentNames := v.FindAllParentsOfSameNamedElement(currentNode, properties)
			if len(parentNames) > 1 {
				structname = FormatName(v.Slice[i-2] + "_" + currentNode)
			}
		}
	} else {
		structname = FormatName(packagename + "_job")
	}
	return
}
func (v *Record) TypeName(i int, properties []string) (typename string) {
	if i+1 < v.Length {
		currentNode := v.Slice[i]
		typename = "*" + FormatName(currentNode)
		if i >= 1 {
			parentNames := v.FindAllParentsOfSameNamedElement(currentNode, properties)
			if len(parentNames) > 1 {
				typename = "*" + FormatName(v.Slice[i-1]+"_"+currentNode)
			}
		}
	} else {
		typename = "interface{}"
	}
	return
}
func NewTgzWalker(pkgReader io.Reader) Walker {
	return tgzWalker{
		pkgReader: pkgReader,
		callbacks: make(map[*regexp.Regexp]WalkFunc),
	}
}
func NewBackoff(strategy BackoffStrategy, start time.Duration, limit time.Duration) *Backoff {
	backoff := Backoff{strategy: strategy, start: start, limit: limit}
	backoff.Reset()
	return &backoff
}
func (b *Backoff) Reset() {
	b.count = 0
	b.LastDuration = 0
	b.NextDuration = b.getNextDuration()
}
func NewExponential(start time.Duration, limit time.Duration) *Backoff {
	return NewBackoff(exponential{}, start, limit)
}
func NewExponentialFullJitter(start time.Duration, limit time.Duration) *Backoff {
	return NewBackoff(exponentialFullJitter{limit: limit}, start, limit)
}
func NewLinear(start time.Duration, limit time.Duration) *Backoff {
	return NewBackoff(linear{}, start, limit)
}
func (s *Sapin) GetLineSize(floor, line int) int {
	return 1 + line*2 + floor*4 + int(floor/2*2)*int((floor+1)/2)
}
func (s *Sapin) GetMaxSize() int {
	return s.GetLineSize(s.Size-1, s.Size+3)
}
func (s *Sapin) compute() {
	if s.output != "" {
		return
	}
	// size of the last line of the last floor
	maxSize := s.GetMaxSize()

	// each floor in the floors
	for floor := 0; floor < s.Size; floor++ {

		// each line in the lines of the floor
		for line := 0; line < floor+4; line++ {

			// size of the current line
			lineSize := s.GetLineSize(floor, line)

			// pad left with spaces
			for i := (maxSize-lineSize)/2 - 1; i > 0; i-- {
				s.putchar(" ")
			}

			// draw the body
			for i := 0; i < lineSize; i++ {
				s.putchar("*")
			}

			// new line
			s.putchar("\n")
		}
	}

	// the trunc
	for i := 0; i < s.Size; i++ {
		lineSize := s.Size + (s.Size+1)%2

		// pad left with spaces
		for i := (maxSize-lineSize)/2 - 1; i > 0; i-- {
			s.putchar(" ")
		}

		// draw the body
		for i := 0; i < lineSize; i++ {
			s.putchar("|")
		}

		// new line
		s.putchar("\n")
	}
}
func (o *PostAppsParams) WithTimeout(timeout time.Duration) *PostAppsParams {
	o.SetTimeout(timeout)
	return o
}
func (o *PostAppsParams) WithContext(ctx context.Context) *PostAppsParams {
	o.SetContext(ctx)
	return o
}
func (o *PostAppsParams) WithHTTPClient(client *http.Client) *PostAppsParams {
	o.SetHTTPClient(client)
	return o
}
func (o *PostAppsParams) WithBody(body *models.AppWrapper) *PostAppsParams {
	o.SetBody(body)
	return o
}
func (o *GetAppsAppParams) WithTimeout(timeout time.Duration) *GetAppsAppParams {
	o.SetTimeout(timeout)
	return o
}
func (o *GetAppsAppParams) WithContext(ctx context.Context) *GetAppsAppParams {
	o.SetContext(ctx)
	return o
}
func (o *GetAppsAppParams) WithHTTPClient(client *http.Client) *GetAppsAppParams {
	o.SetHTTPClient(client)
	return o
}
func (o *GetAppsAppParams) WithApp(app string) *GetAppsAppParams {
	o.SetApp(app)
	return o
}
func (m *RouteWrapper) UnmarshalBinary(b []byte) error {
	var res RouteWrapper
	if err := swag.ReadJSON(b, &res); err != nil {
		return err
	}
	*m = res
	return nil
}
func (o *GetAppsParams) WithTimeout(timeout time.Duration) *GetAppsParams {
	o.SetTimeout(timeout)
	return o
}
func (o *GetAppsParams) WithContext(ctx context.Context) *GetAppsParams {
	o.SetContext(ctx)
	return o
}
func (o *GetAppsParams) WithHTTPClient(client *http.Client) *GetAppsParams {
	o.SetHTTPClient(client)
	return o
}
func (o *DeleteAppsAppParams) WithTimeout(timeout time.Duration) *DeleteAppsAppParams {
	o.SetTimeout(timeout)
	return o
}
func (o *DeleteAppsAppParams) WithContext(ctx context.Context) *DeleteAppsAppParams {
	o.SetContext(ctx)
	return o
}
func (o *DeleteAppsAppParams) WithHTTPClient(client *http.Client) *DeleteAppsAppParams {
	o.SetHTTPClient(client)
	return o
}
func (o *DeleteAppsAppParams) WithApp(app string) *DeleteAppsAppParams {
	o.SetApp(app)
	return o
}
func (o *PatchAppsAppRoutesRouteParams) WithTimeout(timeout time.Duration) *PatchAppsAppRoutesRouteParams {
	o.SetTimeout(timeout)
	return o
}
func (o *PatchAppsAppRoutesRouteParams) WithContext(ctx context.Context) *PatchAppsAppRoutesRouteParams {
	o.SetContext(ctx)
	return o
}
func (o *PatchAppsAppRoutesRouteParams) WithHTTPClient(client *http.Client) *PatchAppsAppRoutesRouteParams {
	o.SetHTTPClient(client)
	return o
}
func (o *PatchAppsAppRoutesRouteParams) WithApp(app string) *PatchAppsAppRoutesRouteParams {
	o.SetApp(app)
	return o
}
func (o *PatchAppsAppRoutesRouteParams) WithBody(body *models.RouteWrapper) *PatchAppsAppRoutesRouteParams {
	o.SetBody(body)
	return o
}
func (o *PatchAppsAppRoutesRouteParams) WithRoute(route string) *PatchAppsAppRoutesRouteParams {
	o.SetRoute(route)
	return o
}
func (o *PostAppsAppRoutesParams) WithTimeout(timeout time.Duration) *PostAppsAppRoutesParams {
	o.SetTimeout(timeout)
	return o
}
func (o *PostAppsAppRoutesParams) WithContext(ctx context.Context) *PostAppsAppRoutesParams {
	o.SetContext(ctx)
	return o
}
func (o *PostAppsAppRoutesParams) WithHTTPClient(client *http.Client) *PostAppsAppRoutesParams {
	o.SetHTTPClient(client)
	return o
}
func (o *PostAppsAppRoutesParams) WithApp(app string) *PostAppsAppRoutesParams {
	o.SetApp(app)
	return o
}
func (o *PostAppsAppRoutesParams) WithBody(body *models.RouteWrapper) *PostAppsAppRoutesParams {
	o.SetBody(body)
	return o
}
func (o *PutAppsAppRoutesRouteParams) WithTimeout(timeout time.Duration) *PutAppsAppRoutesRouteParams {
	o.SetTimeout(timeout)
	return o
}
func (o *PutAppsAppRoutesRouteParams) WithContext(ctx context.Context) *PutAppsAppRoutesRouteParams {
	o.SetContext(ctx)
	return o
}
func (o *PutAppsAppRoutesRouteParams) WithApp(app string) *PutAppsAppRoutesRouteParams {
	o.SetApp(app)
	return o
}
func (o *PutAppsAppRoutesRouteParams) WithBody(body *models.RouteWrapper) *PutAppsAppRoutesRouteParams {
	o.SetBody(body)
	return o
}
func (o *PutAppsAppRoutesRouteParams) WithRoute(route string) *PutAppsAppRoutesRouteParams {
	o.SetRoute(route)
	return o
}
func (o *GetAppsAppRoutesParams) WithTimeout(timeout time.Duration) *GetAppsAppRoutesParams {
	o.SetTimeout(timeout)
	return o
}
func (o *GetAppsAppRoutesParams) WithContext(ctx context.Context) *GetAppsAppRoutesParams {
	o.SetContext(ctx)
	return o
}
func (o *GetAppsAppRoutesParams) WithHTTPClient(client *http.Client) *GetAppsAppRoutesParams {
	o.SetHTTPClient(client)
	return o
}
func (o *GetAppsAppRoutesParams) WithApp(app string) *GetAppsAppRoutesParams {
	o.SetApp(app)
	return o
}
func (o *PatchAppsAppParams) WithTimeout(timeout time.Duration) *PatchAppsAppParams {
	o.SetTimeout(timeout)
	return o
}
func (o *PatchAppsAppParams) WithContext(ctx context.Context) *PatchAppsAppParams {
	o.SetContext(ctx)
	return o
}
func (o *PatchAppsAppParams) WithHTTPClient(client *http.Client) *PatchAppsAppParams {
	o.SetHTTPClient(client)
	return o
}
func (o *PatchAppsAppParams) WithApp(app string) *PatchAppsAppParams {
	o.SetApp(app)
	return o
}
func (o *PatchAppsAppParams) WithBody(body *models.AppWrapper) *PatchAppsAppParams {
	o.SetBody(body)
	return o
}
func (o *PutAppsAppParams) WithTimeout(timeout time.Duration) *PutAppsAppParams {
	o.SetTimeout(timeout)
	return o
}
func (o *PutAppsAppParams) WithContext(ctx context.Context) *PutAppsAppParams {
	o.SetContext(ctx)
	return o
}
func (o *PutAppsAppParams) WithApp(app string) *PutAppsAppParams {
	o.SetApp(app)
	return o
}
func (o *PutAppsAppParams) WithBody(body *models.AppWrapper) *PutAppsAppParams {
	o.SetBody(body)
	return o
}
func (m *Task) UnmarshalJSON(raw []byte) error {

	var aO0 NewTask
	if err := swag.ReadJSON(raw, &aO0); err != nil {
		return err
	}
	m.NewTask = aO0

	var aO1 TaskAllOf1
	if err := swag.ReadJSON(raw, &aO1); err != nil {
		return err
	}
	m.TaskAllOf1 = aO1

	return nil
}
func (m Task) MarshalJSON() ([]byte, error) {
	var _parts [][]byte

	aO0, err := swag.WriteJSON(m.NewTask)
	if err != nil {
		return nil, err
	}
	_parts = append(_parts, aO0)

	aO1, err := swag.WriteJSON(m.TaskAllOf1)
	if err != nil {
		return nil, err
	}
	_parts = append(_parts, aO1)

	return swag.ConcatJSON(_parts...), nil
}
func (m *Task) Validate(formats strfmt.Registry) error {
	var res []error

	if err := m.NewTask.Validate(formats); err != nil {
		res = append(res, err)
	}

	if err := m.TaskAllOf1.Validate(formats); err != nil {
		res = append(res, err)
	}

	if len(res) > 0 {
		return errors.CompositeValidationError(res...)
	}
	return nil
}
func GetRanges(ips []string, ip4_cidr string, ip6_cidr string) ([]net.IPNet, error) {
	net_out := make([]net.IPNet, 0)

	for _, ip := range ips {
		cidr := ""
		if strings.Contains(ip, ":") {
			// IPv6
			cidr = ip6_cidr
			if cidr == "" {
				cidr = "128"
			}
			if c, err := strconv.ParseInt(cidr, 10, 16); err != nil || c < 0 || c > 128 {
				return nil, &PermError{"Invalid IPv6 CIDR length: " + cidr}
			}

		} else {
			// IPv4
			cidr = ip4_cidr
			if cidr == "" {
				cidr = "32"
			}
			if c, err := strconv.ParseInt(cidr, 10, 16); err != nil || c < 0 || c > 32 {
				return nil, &PermError{"Invalid IPv4 CIDR length: " + cidr}
			}
		}
		ip += "/" + cidr

		_, ipnet, err := net.ParseCIDR(ip)
		if err != nil {
			return nil, err
		}
		net_out = append(net_out, *ipnet)

	}

	return net_out, nil
}
func (o *GetAppsAppRoutesRouteParams) WithTimeout(timeout time.Duration) *GetAppsAppRoutesRouteParams {
	o.SetTimeout(timeout)
	return o
}
func (o *GetAppsAppRoutesRouteParams) WithContext(ctx context.Context) *GetAppsAppRoutesRouteParams {
	o.SetContext(ctx)
	return o
}
func (o *GetAppsAppRoutesRouteParams) WithHTTPClient(client *http.Client) *GetAppsAppRoutesRouteParams {
	o.SetHTTPClient(client)
	return o
}
func (o *GetAppsAppRoutesRouteParams) WithApp(app string) *GetAppsAppRoutesRouteParams {
	o.SetApp(app)
	return o
}
func (o *GetAppsAppRoutesRouteParams) WithRoute(route string) *GetAppsAppRoutesRouteParams {
	o.SetRoute(route)
	return o
}
func New(transport runtime.ClientTransport, formats strfmt.Registry) *Functions {
	cli := new(Functions)
	cli.Transport = transport

	cli.Apps = apps.New(transport, formats)

	cli.Routes = routes.New(transport, formats)

	cli.Tasks = tasks.New(transport, formats)

	cli.Version = version.New(transport, formats)

	return cli
}
func (c *Functions) SetTransport(transport runtime.ClientTransport) {
	c.Transport = transport

	c.Apps.SetTransport(transport)

	c.Routes.SetTransport(transport)

	c.Tasks.SetTransport(transport)

	c.Version.SetTransport(transport)

}
func (o *DeleteAppsAppRoutesRouteParams) WithTimeout(timeout time.Duration) *DeleteAppsAppRoutesRouteParams {
	o.SetTimeout(timeout)
	return o
}
func (o *DeleteAppsAppRoutesRouteParams) WithContext(ctx context.Context) *DeleteAppsAppRoutesRouteParams {
	o.SetContext(ctx)
	return o
}
func (o *DeleteAppsAppRoutesRouteParams) WithHTTPClient(client *http.Client) *DeleteAppsAppRoutesRouteParams {
	o.SetHTTPClient(client)
	return o
}
func (o *DeleteAppsAppRoutesRouteParams) WithApp(app string) *DeleteAppsAppRoutesRouteParams {
	o.SetApp(app)
	return o
}
func (o *DeleteAppsAppRoutesRouteParams) WithRoute(route string) *DeleteAppsAppRoutesRouteParams {
	o.SetRoute(route)
	return o
}
func (o *GetTasksParams) WithTimeout(timeout time.Duration) *GetTasksParams {
	o.SetTimeout(timeout)
	return o
}
func (o *GetTasksParams) WithContext(ctx context.Context) *GetTasksParams {
	o.SetContext(ctx)
	return o
}
func (o *GetTasksParams) WithHTTPClient(client *http.Client) *GetTasksParams {
	o.SetHTTPClient(client)
	return o
}
func NewValueStore(addr string, concurrency int, ftlsConfig *ftls.Config, opts ...grpc.DialOption) (store.ValueStore, error) {
	stor := &valueStore{
		addr:             addr,
		ftlsc:            ftlsConfig,
		opts:             opts,
		handlersDoneChan: make(chan struct{}),
	}

	stor.pendingLookupReqChan = make(chan *asyncValueLookupRequest, concurrency)
	stor.freeLookupReqChan = make(chan *asyncValueLookupRequest, concurrency)
	stor.freeLookupResChan = make(chan *asyncValueLookupResponse, concurrency)
	for i := 0; i < cap(stor.freeLookupReqChan); i++ {
		stor.freeLookupReqChan <- &asyncValueLookupRequest{resChan: make(chan *asyncValueLookupResponse, 1)}
	}
	for i := 0; i < cap(stor.freeLookupResChan); i++ {
		stor.freeLookupResChan <- &asyncValueLookupResponse{}
	}
	go stor.handleLookupStream()

	stor.pendingReadReqChan = make(chan *asyncValueReadRequest, concurrency)
	stor.freeReadReqChan = make(chan *asyncValueReadRequest, concurrency)
	stor.freeReadResChan = make(chan *asyncValueReadResponse, concurrency)
	for i := 0; i < cap(stor.freeReadReqChan); i++ {
		stor.freeReadReqChan <- &asyncValueReadRequest{resChan: make(chan *asyncValueReadResponse, 1)}
	}
	for i := 0; i < cap(stor.freeReadResChan); i++ {
		stor.freeReadResChan <- &asyncValueReadResponse{}
	}
	go stor.handleReadStream()

	stor.pendingWriteReqChan = make(chan *asyncValueWriteRequest, concurrency)
	stor.freeWriteReqChan = make(chan *asyncValueWriteRequest, concurrency)
	stor.freeWriteResChan = make(chan *asyncValueWriteResponse, concurrency)
	for i := 0; i < cap(stor.freeWriteReqChan); i++ {
		stor.freeWriteReqChan <- &asyncValueWriteRequest{resChan: make(chan *asyncValueWriteResponse, 1)}
	}
	for i := 0; i < cap(stor.freeWriteResChan); i++ {
		stor.freeWriteResChan <- &asyncValueWriteResponse{}
	}
	go stor.handleWriteStream()

	stor.pendingDeleteReqChan = make(chan *asyncValueDeleteRequest, concurrency)
	stor.freeDeleteReqChan = make(chan *asyncValueDeleteRequest, concurrency)
	stor.freeDeleteResChan = make(chan *asyncValueDeleteResponse, concurrency)
	for i := 0; i < cap(stor.freeDeleteReqChan); i++ {
		stor.freeDeleteReqChan <- &asyncValueDeleteRequest{resChan: make(chan *asyncValueDeleteResponse, 1)}
	}
	for i := 0; i < cap(stor.freeDeleteResChan); i++ {
		stor.freeDeleteResChan <- &asyncValueDeleteResponse{}
	}
	go stor.handleDeleteStream()

	return stor, nil
}
func (stor *valueStore) Close() {
	stor.lock.Lock()
	stor.shutdown()
	close(stor.handlersDoneChan)
	stor.lock.Unlock()
}
func (o *Server) SetBackend(backend OortService) {
	o.Lock()
	o.backend = backend
	o.Unlock()
}
func (o *Server) Ring() ring.Ring {
	o.RLock()
	defer o.RUnlock()
	return o.ring
}
func (o *Server) GetLocalID() uint64 {
	o.RLock()
	defer o.RUnlock()
	return o.localID
}
func (o *Server) GetListenAddr() string {
	o.RLock()
	defer o.RUnlock()
	return o.ring.LocalNode().Address(2)
}
func (rs *ReplGroupStore) Startup(ctx context.Context) error {
	rs.ringLock.Lock()
	if rs.ringServerExitChan == nil {
		rs.ringServerExitChan = make(chan struct{})
		go rs.ringServerConnector(rs.ringServerExitChan)
	}
	rs.ringLock.Unlock()
	return nil
}
func (o *Server) shutdownFinished() {
	time.Sleep(10 * time.Millisecond)
	close(o.ShutdownComplete)
}
func (o *Server) Stop() error {
	o.cmdCtrlLock.Lock()
	defer o.cmdCtrlLock.Unlock()
	if o.stopped {
		return fmt.Errorf("Service already stopped")
	}
	close(o.ch)
	o.backend.StopListenAndServe()
	o.backend.Wait()
	o.backend.Stop()
	o.stopped = true
	return nil
}
func (o *Server) Exit() error {
	o.cmdCtrlLock.Lock()
	defer o.cmdCtrlLock.Unlock()
	if o.stopped {
		o.backend.Stop()
		defer o.shutdownFinished()
		return nil
	}
	close(o.ch)
	o.backend.StopListenAndServe()
	o.backend.Wait()
	o.backend.Stop()
	o.stopped = true
	defer o.shutdownFinished()
	return nil
}
func (o *Server) SelfUpgrade(version string, bindiff, checksum []byte) (bool, string) {
	o.cmdCtrlLock.Lock()
	defer o.cmdCtrlLock.Unlock()
	err := o.binaryUpgrade.Upgrade(version)
	if err != nil {
		return false, err.Error()
	}
	return true, ""
}
func (o *Server) SoftwareVersion() string {
	o.cmdCtrlLock.RLock()
	defer o.cmdCtrlLock.RUnlock()
	return o.binaryUpgrade.GetCurrentVersion()
}
func (stor *groupStore) Shutdown(ctx context.Context) error {
	stor.lock.Lock()
	err := stor.shutdown()
	stor.lock.Unlock()
	return err
}
func (c *Client) Rant(rantId int) (RantModel, []CommentModel, error) {
	url := fmt.Sprintf(RANT_PATH, API, rantId, APP_VERSION)
	res, err := http.Get(url)
	if err != nil {
		return RantModel{}, nil, err
	}
	var data RantResponse
	json.NewDecoder(res.Body).Decode(&data)
	if !data.Success && data.Error != "" {
		return RantModel{}, nil, errors.New(data.Error)
	}
	return data.Rant, data.Comments, nil
}
func (c *Client) Profile(username string) (UserModel, ContentModel, error) {
	userId, err := getUserId(username)
	if err != nil {
		return UserModel{}, ContentModel{}, err
	}
	url := fmt.Sprintf(USER_PATH, API, userId, APP_VERSION)
	res, err := http.Get(url)
	if err != nil {
		return UserModel{}, ContentModel{}, err
	}
	var data UserResponse
	json.NewDecoder(res.Body).Decode(&data)
	if !data.Success && data.Error != "" {
		return UserModel{}, ContentModel{}, errors.New(data.Error)
	}
	return data.Profile, data.Profile.Content.Content, nil
}
func (c *Client) Search(term string) ([]RantModel, error) {
	url := fmt.Sprintf(SEARCH_PATH, API, term, APP_VERSION)
	res, err := http.Get(url)
	if err != nil {
		return nil, err
	}
	var data SearchResponse
	json.NewDecoder(res.Body).Decode(&data)
	if !data.Success && data.Error != "" {
		return nil, errors.New(data.Error)
	}
	return data.Rants, nil
}
func (c *Client) Surprise() (RantModel, error) {
	url := fmt.Sprintf(SURPRISE_PATH, API, APP_VERSION)
	res, err := http.Get(url)
	if err != nil {
		return RantModel{}, err
	}
	var data RantResponse
	json.NewDecoder(res.Body).Decode(&data)
	if !data.Success && data.Error != "" {
		return RantModel{}, errors.New(data.Error)
	}
	return data.Rant, nil
}
func (c *Client) WeeklyRants() ([]RantModel, error) {
	url := fmt.Sprintf(WEEKLY_PATH, API, APP_VERSION)
	res, err := http.Get(url)
	if err != nil {
		return nil, err
	}
	var data RantsResponse
	json.NewDecoder(res.Body).Decode(&data)
	if !data.Success && data.Error != "" {
		return nil, errors.New(data.Error)
	}
	return data.Rants, nil
}
func getUserId(username string) (int, error) {
	url := fmt.Sprintf(USER_ID_PATH, API, username, APP_VERSION)
	res, err := http.Get(url)
	if err != nil {
		return 0, err
	}
	var data GetUserIdResponse
	json.NewDecoder(res.Body).Decode(&data)
	if !data.Success && data.Error != "" {
		return 0, errors.New(data.Error)
	}
	return data.UserId, nil
}
func (rs *ReplValueStore) Shutdown(ctx context.Context) error {
	rs.ringLock.Lock()
	if rs.ringServerExitChan != nil {
		close(rs.ringServerExitChan)
		rs.ringServerExitChan = nil
	}
	rs.storesLock.Lock()
	for addr, stc := range rs.stores {
		if err := stc.store.Shutdown(ctx); err != nil {
			rs.logDebug("replValueStore: error during shutdown of store %s: %s", addr, err)
		}
		delete(rs.stores, addr)
		select {
		case <-ctx.Done():
			rs.storesLock.Unlock()
			return ctx.Err()
		default:
		}
	}
	rs.storesLock.Unlock()
	rs.ringLock.Unlock()
	return nil
}
func FExists(name string) bool {
	if _, err := os.Stat(name); os.IsNotExist(err) {
		return false
	}
	return true
}
func (m *MessageStream) outbound() {
	for {
		select {
		case <-m.Shutdown:
			log.Infof("Closing OpenFlow message stream.")
			m.conn.Close()
			return
		case msg := <-m.Outbound:
			// Forward outbound messages to conn
			data, _ := msg.MarshalBinary()
			if _, err := m.conn.Write(data); err != nil {
				log.Warnln("OutboundError:", err)
				m.Error <- err
				m.Shutdown <- true
			}

			log.Debugf("Sent(%d): %v", len(data), data)
		}
	}
}
func (m *MessageStream) parse() {
	for {
		b := <-m.pool.Full
		log.Debugf("Rcvd: %v", b.Bytes())
		msg, err := m.parser.Parse(b.Bytes())
		// Log all message parsing errors.
		if err != nil {
			log.Print(err)
		}

		m.Inbound <- msg
		b.Reset()
		m.pool.Empty <- b
	}
}
func (dom *Domain) Group(name string, members []*Account) *Group {
	return &Group{Domain: dom, Name: name, Members: members}
}
func (dom *Domain) Groups() ([]*Group, error) {
	var vl valueList
	err := dom.cgp.request(listGroups{Domain: dom.Name}, &vl)
	if err != nil {
		return []*Group{}, err
	}
	vals := vl.compact()
	grps := make([]*Group, len(vals))
	for i, v := range vals {
		g, err := dom.GetGroup(v)
		if err != nil {
			return grps, err
		}
		grps[i] = g
	}
	return grps, nil
}
func (dom *Domain) GetGroup(name string) (*Group, error) {
	var d dictionary
	err := dom.cgp.request(getGroup{Name: fmt.Sprintf("%s@%s", name, dom.Name)}, &d)
	if err != nil {
		return &Group{}, err
	}
	memStr := d.toMap()["Members"]
	var mems []*Account
	dec := xml.NewDecoder(bytes.NewBufferString(memStr))
	for {
		var a string
		err := dec.Decode(&a)
		if err == io.EOF {
			break
		}
		if err != nil {
			return dom.Group(name, mems), err
		}
		if a == "" {
			continue
		}
		mems = append(mems, dom.Account(a))
	}
	return dom.Group(name, mems), nil
}
func Reticence(str string, length int) string {
	if length > len(str) {
		return str
	}
	var i int
F:
	for i = len(str) - 1; i >= 0; i-- {
		switch str[i] {
		case ' ', ',', '?', ';', ':', '\'', '"', '!':
			if i <= length {
				break F
			}
		case '.':
			if i-2 >= 0 {
				s := str[i-2 : i]
				if s == ".." {
					i = i - 2
					if i <= length {
						break F
					}
				}
			}
			if i <= length {
				break F
			}
		}
	}
	if i-1 > 0 {
		switch str[i-1] {
		case ' ', ',', '?', ';', ':', '\'', '"', '!':
			i--
		case '.':
			if i-2 > 0 && str[i-2:i] == ".." {
				i -= 3
			}
		}
	}
	if i >= 2 {
		if i+3 >= len(str) {
			return str
		}
		return str[:i] + "..."
	}
	if length >= 2 && length < len(str) {
		if length+3 >= len(str) {
			return str
		}
		return str[:length] + "..."
	}
	return str
}
func CheckPassword(pass string, min, max int) error {
	if len(pass) < min || len(pass) > max {
		return e.New(ErrInvalidPassLength)
	}
	for _, r := range pass {
		if !unicode.IsGraphic(r) {
			return e.New(ErrInvalidPassChar)
		}
	}
	return nil
}
func CleanUrl(rawurl string, min, max int) (string, error) {
	err := CheckUrl(rawurl, min, max)
	if err != nil {
		return "", e.Forward(err)
	}
	u, err := url.Parse(rawurl)
	if err != nil {
		return "", e.Push(e.New(ErrInvUrl), err)
	}
	if u.Scheme == "" {
		return u.String(), e.New(ErrNoScheme)
	}
	return u.String(), nil
}
func NewParameter(name string, value reflect.Value) *Parameter {
	parameter := Parameter{
		Name:  name,
		Value: value,
	}
	return &parameter
}
func MakeParams(fieldCount int) *Params {
	return &Params{
		make(map[string]*Parameter),
		make([]*Parameter, 0, fieldCount),
	}
}
func (p *Params) Parse(pvalue *reflect.Value) error {
	vtype := pvalue.Type().Elem()

	for idx := 0; idx < vtype.NumField(); idx++ {
		field := vtype.Field(idx)

		value := pvalue.Elem().Field(idx)

		if value.Kind() == reflect.Slice {
			value.Set(reflect.MakeSlice(value.Type(), 0, 0))
		}

		parameter := NewParameter(field.Name, value)
		if err := parameter.DiscoverProperties(field.Tag); err != nil {
			return err
		}

		if err := p.Set(parameter.Name, parameter); err != nil {
			return err
		}

		if parameter.Alias != "" {
			if err := p.Set(parameter.Alias, parameter); err != nil {
				return err
			}
		}
		p.Listing = append(p.Listing, parameter)
	}
	return nil
}
func (f Forwarder) Email() string {
	return fmt.Sprintf("%s@%s", f.Name, f.Domain.Name)
}
func (dom *Domain) Forwarder(name, to string) *Forwarder {
	return &Forwarder{Domain: dom, Name: name, To: to}
}
func (dom *Domain) Forwarders() ([]*Forwarder, error) {
	var vl valueList
	err := dom.cgp.request(listForwarders{Param: dom.Name}, &vl)
	if err != nil {
		return []*Forwarder{}, err
	}
	vals := vl.compact()
	fs := make([]*Forwarder, len(vals))
	for i, v := range vals {
		f, err := dom.GetForwarder(v)
		if err != nil {
			return fs, err
		}
		fs[i] = f
	}
	return fs, err
}
func (dom *Domain) GetForwarder(name string) (*Forwarder, error) {
	var f string
	err := dom.cgp.request(getForwarder{Param: fmt.Sprintf("%s@%s", name, dom.Name)}, &f)
	if err != nil {
		return &Forwarder{}, err
	}
	return &Forwarder{Domain: dom, Name: name, To: f}, nil
}
func New(url, user, pass string) *CGP {
	return &CGP{url: url, user: user, pass: pass}
}
func EscapeCommaSeparated(in ...string) string {
	var out string
	for i, str := range in {
		escaped := strings.Replace(url.QueryEscape(str), "%2F", "%252F", -1)
		escaped = strings.Replace(escaped, "\"", "%22", -1)
		escaped = strings.Replace(escaped, " ", "%20", -1)
		out += escaped
		if i < len(in)-1 {
			out += ","
		}
	}
	return out
}
func (acc *Account) Alias(name string) *Alias {
	return &Alias{account: acc, Name: name}
}
func (a Alias) Email() string {
	return fmt.Sprintf("%s@%s", a.Name, a.account.Domain.Name)
}
func (acc *Account) Aliases() ([]*Alias, error) {
	var vl valueList
	err := acc.Domain.cgp.request(listAliases{Param: fmt.Sprintf("%s@%s", acc.Name, acc.Domain.Name)}, &vl)
	if err != nil {
		return []*Alias{}, err
	}
	vals := vl.compact()
	as := make([]*Alias, len(vals))
	for i, v := range vals {
		as[i] = acc.Alias(v)
	}
	return as, nil
}
func (a Account) RealName() (string, error) {
	var d dictionary
	err := a.Domain.cgp.request(getAccountSettings{Account: a.Email()}, &d)
	if err != nil {
		return "", err
	}
	return d.toMap()["RealName"], nil
}
func (a Account) Email() string {
	return fmt.Sprintf("%s@%s", a.Name, a.Domain.Name)
}
func (dom *Domain) Account(name string) *Account {
	return &Account{Domain: dom, Name: name}
}
func (dom *Domain) Accounts() ([]*Account, error) {
	var al accountList
	err := dom.cgp.request(listAccounts{Domain: dom.Name}, &al)
	if err != nil {
		return []*Account{}, err
	}
	keys := al.SubKeys
	as := make([]*Account, len(keys))
	for i, k := range keys {
		as[i] = dom.Account(k.Name)
	}
	return as, nil
}
func (dom Domain) Exists() (bool, error) {
	var d dictionary
	err := dom.cgp.request(getDomainSettings{Domain: dom.Name}, &d)
	if _, ok := err.(SOAPNotFoundError); ok {
		return false, nil
	}
	if err != nil {
		return false, err
	}
	return true, nil
}
func (dom Domain) Aliases() ([]string, error) {
	var vl valueList
	err := dom.cgp.request(getDomainAliases{Domain: dom.Name}, &vl)
	if err != nil {
		return []string{}, err
	}
	return vl.compact(), nil
}
func (cgp *CGP) Domain(name string) *Domain {
	return &Domain{cgp: cgp, Name: name}
}
func (cgp *CGP) Domains() ([]*Domain, error) {
	var vl valueList
	err := cgp.request(listDomains{}, &vl)
	if err != nil {
		return []*Domain{}, err
	}
	vals := vl.SubValues
	ds := make([]*Domain, len(vals))
	for i, d := range vals {
		ds[i] = cgp.Domain(d)
	}
	return ds, nil
}
func New(info Info, publicMsg ...interface{}) Err {
	return newErr(debug.Stack(), nil, false, info, publicMsg)
}
func Wrap(wrapErr error, info Info, publicMsg ...interface{}) Err {
	if wrapErr == nil {
		return nil
	}
	if info == nil {
		info = Info{}
	}
	if errsErr, isErr := IsErr(wrapErr); isErr {
		if errStructErr, isErrsErr := errsErr.(*err); isErrsErr {
			errStructErr.mergeIn(info, publicMsg)
			return errStructErr
		}
		return errsErr
	}
	return newErr(debug.Stack(), wrapErr, false, info, publicMsg)
}
func (e *err) mergeIn(info Info, publicMsgParts []interface{}) {
	for key, val := range info {
		for e.info[key] != nil {
			key = key + "_duplicate"
		}
		e.info[key] = val
	}
	publicMsgPrefix := concatArgs(publicMsgParts...)
	if publicMsgPrefix == "" {
		// do nothing
	} else if e.publicMsg == "" {
		e.publicMsg = publicMsgPrefix
	} else {
		e.publicMsg = publicMsgPrefix + " - " + e.publicMsg
	}
}
func (e *err) wrappedErrStr() string {
	if e == nil {
		return ""
	}
	if e.wrappedErr == nil {
		return ""
	}
	return e.wrappedErr.Error()
}
func concatArgs(args ...interface{}) string {
	res := fmt.Sprintln(args...)
	return res[0 : len(res)-1] // Remove newline at the end
}
func (dom *Domain) MailingList(name string) *MailingList {
	return &MailingList{Domain: dom, Name: name}
}
func (ml *MailingList) Subscriber(email, name string) *Subscriber {
	return &Subscriber{MailingList: ml, Email: email, RealName: name}
}
func (ml *MailingList) Subscribers() ([]*Subscriber, error) {
	var res readSubscribersResponse
	err := ml.Domain.cgp.request(readSubscribers{Name: fmt.Sprintf("%s@%s", ml.Name, ml.Domain.Name)}, &res)
	if err != nil {
		return []*Subscriber{}, err
	}
	ds := res.SubValues[1].SubValues
	subs := make([]*Subscriber, len(ds))
	for i, d := range ds {
		m := d.toMap()
		subs[i] = ml.Subscriber(m["Sub"], m["RealName"])
	}
	return subs, nil
}
func (dom *Domain) MailingLists() ([]*MailingList, error) {
	var vl valueList
	err := dom.cgp.request(listLists{Domain: dom.Name}, &vl)
	if err != nil {
		return []*MailingList{}, err
	}
	vals := vl.compact()
	mls := make([]*MailingList, len(vals))
	for i, v := range vals {
		mls[i] = dom.MailingList(v)
	}
	return mls, nil
}
func NewByteKeyItem(k []byte) unsafe.Pointer {
	itm := byteKeyItem(k)
	return unsafe.Pointer(&itm)
}
func CompareBytes(this, that unsafe.Pointer) int {
	thisItem := (*byteKeyItem)(this)
	thatItem := (*byteKeyItem)(that)
	return bytes.Compare([]byte(*thisItem), []byte(*thatItem))
}
func CompareInt(this, that unsafe.Pointer) int {
	thisItem := (*intKeyItem)(this)
	thatItem := (*intKeyItem)(that)
	return int(*thisItem - *thatItem)
}
func Malloc(l int) unsafe.Pointer {
	if Debug {
		atomic.AddUint64(&stats.allocs, 1)
	}
	return C.mm_malloc(C.size_t(l))
}
func Free(p unsafe.Pointer) {
	if Debug {
		atomic.AddUint64(&stats.frees, 1)
	}
	C.mm_free(p)
}
func Stats() string {
	mu.Lock()
	defer mu.Unlock()

	buf := C.mm_stats()
	s := "==== Stats ====\n"
	if Debug {
		s += fmt.Sprintf("Mallocs = %d\n"+
			"Frees   = %d\n", stats.allocs, stats.frees)
	}

	if buf != nil {
		s += C.GoString(buf)
		C.free(unsafe.Pointer(buf))
	}

	return s
}
func FreeOSMemory() error {
	errCode := int(C.mm_free2os())
	if errCode != 0 {
		return fmt.Errorf("status: %d", errCode)
	}

	return nil
}
func (s *Segment) Add(itm unsafe.Pointer) {
	itemLevel := s.builder.store.NewLevel(s.rand.Float32)
	x := s.builder.store.newNode(itm, itemLevel)
	s.sts.AddInt64(&s.sts.nodeAllocs, 1)
	s.sts.AddInt64(&s.sts.levelNodesCount[itemLevel], 1)
	s.sts.AddInt64(&s.sts.usedBytes, int64(s.builder.store.Size(x)))

	for l := 0; l <= itemLevel; l++ {
		if s.tail[l] != nil {
			s.tail[l].setNext(l, x, false)
		} else {
			s.head[l] = x
		}
		s.tail[l] = x
	}

	if s.callb != nil {
		s.callb(x)
	}
}
func (b *Builder) NewSegment() *Segment {
	seg := &Segment{tail: make([]*Node, MaxLevel+1),
		head: make([]*Node, MaxLevel+1), builder: b,
		rand: rand.New(rand.NewSource(int64(rand.Int()))),
	}

	seg.sts.IsLocal(true)
	return seg
}
func (b *Builder) Assemble(segments ...*Segment) *Skiplist {
	tail := make([]*Node, MaxLevel+1)
	head := make([]*Node, MaxLevel+1)

	for _, seg := range segments {
		for l := 0; l <= MaxLevel; l++ {
			if tail[l] != nil && seg.head[l] != nil {
				tail[l].setNext(l, seg.head[l], false)
			} else if head[l] == nil && seg.head[l] != nil {
				head[l] = seg.head[l]
			}

			if seg.tail[l] != nil {
				tail[l] = seg.tail[l]
			}
		}
	}

	for l := 0; l <= MaxLevel; l++ {
		if head[l] != nil {
			b.store.head.setNext(l, head[l], false)
		}
		if tail[l] != nil {
			tail[l].setNext(l, b.store.tail, false)
		}
	}

	for _, seg := range segments {
		b.store.Stats.Merge(&seg.sts)
	}

	return b.store

}
func CompareNodeTable(a, b unsafe.Pointer) int {
	return int(uintptr(a)) - int(uintptr(b))
}
func New(hfn HashFn, kfn EqualKeyFn) *NodeTable {
	nt := &NodeTable{
		fastHT:   make(map[uint32]uint64),
		slowHT:   make(map[uint32][]uint64),
		hash:     hfn,
		keyEqual: kfn,
	}

	buf := dbInstances.MakeBuf()
	defer dbInstances.FreeBuf(buf)
	dbInstances.Insert(unsafe.Pointer(nt), CompareNodeTable, buf, &dbInstances.Stats)

	return nt
}
func (nt *NodeTable) Stats() string {
	return fmt.Sprintf("\nFastHTCount = %d\n"+
		"SlowHTCount = %d\n"+
		"Conflicts   = %d\n"+
		"MemoryInUse = %d\n",
		nt.fastHTCount, nt.slowHTCount, nt.conflicts, nt.MemoryInUse())
}
func (nt *NodeTable) MemoryInUse() int64 {
	return int64(approxItemSize * (nt.fastHTCount + nt.slowHTCount))
}
func (nt *NodeTable) Get(key []byte) unsafe.Pointer {
	res := nt.find(key)
	if res.status&ntFoundMask == ntFoundMask {
		if res.status == ntFoundInFast {
			return decodePointer(res.fastHTValue)
		}
		return decodePointer(res.slowHTValues[res.slowHTPos])
	}

	return nil
}
func (nt *NodeTable) Update(key []byte, nptr unsafe.Pointer) (updated bool, oldPtr unsafe.Pointer) {
	res := nt.find(key)
	if res.status&ntFoundMask == ntFoundMask {
		// Found key, replace old pointer value with new one
		updated = true
		if res.status == ntFoundInFast {
			oldPtr = decodePointer(res.fastHTValue)
			nt.fastHT[res.hash] = encodePointer(nptr, res.hasConflict)
		} else {
			oldPtr = decodePointer(res.slowHTValues[res.slowHTPos])
			res.slowHTValues[res.slowHTPos] = encodePointer(nptr, true)
		}
	} else {
		// Insert new key
		updated = false
		newSlowValue := res.fastHTHasEntry && !res.hasConflict
		// Key needs to be inserted into slowHT
		if res.hasConflict || newSlowValue {
			slowHTValues := nt.slowHT[res.hash]
			slowHTValues = append(slowHTValues, encodePointer(nptr, false))
			nt.slowHT[res.hash] = slowHTValues
			// There is an entry already in the fastHT for same crc32 hash
			// We have inserted first entry into the slowHT. Now mark conflict bit.
			if newSlowValue {
				nt.fastHT[res.hash] = encodePointer(decodePointer(nt.fastHT[res.hash]), true)
				nt.conflicts++
			}
			nt.slowHTCount++
		} else {
			// Insert new item into fastHT
			nt.fastHT[res.hash] = encodePointer(nptr, false)
			nt.fastHTCount++
		}
	}

	return
}
func (nt *NodeTable) Remove(key []byte) (success bool, nptr unsafe.Pointer) {
	res := nt.find(key)
	if res.status&ntFoundMask == ntFoundMask {
		success = true
		if res.status == ntFoundInFast {
			nptr = decodePointer(res.fastHTValue)
			// Key needs to be removed from fastHT. For that we need to move
			// an item present in slowHT and overwrite fastHT entry.
			if res.hasConflict {
				slowHTValues := nt.slowHT[res.hash]
				v := slowHTValues[0] // New fastHT candidate
				slowHTValues = append([]uint64(nil), slowHTValues[1:]...)
				nt.slowHTCount--

				var conflict bool
				if len(slowHTValues) == 0 {
					delete(nt.slowHT, res.hash)
					nt.conflicts--
				} else {
					conflict = true
					nt.slowHT[res.hash] = slowHTValues
				}

				nt.fastHT[res.hash] = encodePointer(decodePointer(v), conflict)
			} else {
				delete(nt.fastHT, res.hash)
				nt.fastHTCount--
			}
		} else {
			nptr = decodePointer(res.slowHTValues[res.slowHTPos])
			// Remove key from slowHT
			newSlowValue := append([]uint64(nil), res.slowHTValues[:res.slowHTPos]...)
			if res.slowHTPos+1 != len(res.slowHTValues) {
				newSlowValue = append(newSlowValue, res.slowHTValues[res.slowHTPos+1:]...)
			}
			nt.slowHTCount--

			if len(newSlowValue) == 0 {
				delete(nt.slowHT, res.hash)
				nt.fastHT[res.hash] = encodePointer(decodePointer(nt.fastHT[res.hash]), false)
				nt.conflicts--
			} else {
				nt.slowHT[res.hash] = newSlowValue
			}
		}
	}
	return
}
func (nt *NodeTable) Close() {
	nt.fastHTCount = 0
	nt.slowHTCount = 0
	nt.conflicts = 0
	nt.fastHT = make(map[uint32]uint64)
	nt.slowHT = make(map[uint32][]uint64)

	buf := dbInstances.MakeBuf()
	defer dbInstances.FreeBuf(buf)
	dbInstances.Delete(unsafe.Pointer(nt), CompareNodeTable, buf, &dbInstances.Stats)
}
func MemoryInUse() (sz int64) {
	buf := dbInstances.MakeBuf()
	defer dbInstances.FreeBuf(buf)
	iter := dbInstances.NewIterator(CompareNodeTable, buf)
	for iter.SeekFirst(); iter.Valid(); iter.Next() {
		db := (*NodeTable)(iter.Get())
		sz += db.MemoryInUse()
	}

	return
}
func debugMarkFree(n *Node) {
	var block []byte
	l := int(nodeTypes[n.level].Size())
	sh := (*reflect.SliceHeader)(unsafe.Pointer(&block))
	sh.Data = uintptr(unsafe.Pointer(n))
	sh.Len = l
	sh.Cap = l

	copy(block, freeBlockContent)
}
func (it *Iterator) Seek(bs []byte) {
	itm := it.snap.db.newItem(bs, false)
	it.iter.Seek(unsafe.Pointer(itm))
	it.skipUnwanted()
}
func (it *Iterator) Next() {
	it.iter.Next()
	it.count++
	it.skipUnwanted()
	if it.refreshRate > 0 && it.count > it.refreshRate {
		it.Refresh()
		it.count = 0
	}
}
func (it *Iterator) Refresh() {
	if it.Valid() {
		itm := it.snap.db.ptrToItem(it.GetNode().Item())
		it.iter.Close()
		it.iter = it.snap.db.store.NewIterator(it.snap.db.iterCmp, it.buf)
		it.iter.Seek(unsafe.Pointer(itm))
	}
}
func (it *Iterator) Close() {
	it.snap.Close()
	it.snap.db.store.FreeBuf(it.buf)
	it.iter.Close()
}
func (m *Nitro) NewIterator(snap *Snapshot) *Iterator {
	if !snap.Open() {
		return nil
	}
	buf := snap.db.store.MakeBuf()
	return &Iterator{
		snap: snap,
		iter: m.store.NewIterator(m.iterCmp, buf),
		buf:  buf,
	}
}
func (mit *MergeIterator) SeekFirst() {
	for _, it := range mit.iters {
		it.SeekFirst()
		if it.Valid() {
			n := it.GetNode()
			mit.h = append(mit.h, heapItem{iter: it, n: n})
		}
	}

	heap.Init(&mit.h)
	mit.Next()
}
func (mit *MergeIterator) Next() {
	mit.curr = nil
	if mit.h.Len() == 0 {
		return
	}

	o := heap.Pop(&mit.h)
	hi := o.(heapItem)
	mit.curr = hi.n
	hi.iter.Next()
	if hi.iter.Valid() {
		hi.n = hi.iter.GetNode()
		heap.Push(&mit.h, hi)
	}
}
func (mit *MergeIterator) Seek(itm unsafe.Pointer) bool {
	var found bool
	for _, it := range mit.iters {
		if it.Seek(itm) {
			found = true
		}
		if it.Valid() {
			n := it.GetNode()
			mit.h = append(mit.h, heapItem{iter: it, n: n})
		}
	}

	heap.Init(&mit.h)
	mit.Next()

	return found
}
func (l *NodeList) Keys() (keys [][]byte) {
	node := l.head
	for node != nil {
		key := (*Item)(node.Item()).Bytes()
		keys = append(keys, key)
		node = node.GetLink()
	}

	return
}
func (l *NodeList) Remove(key []byte) *skiplist.Node {
	var prev *skiplist.Node
	node := l.head
	for node != nil {
		nodeKey := (*Item)(node.Item()).Bytes()
		if bytes.Equal(nodeKey, key) {
			if prev == nil {
				l.head = node.GetLink()
				return node
			}

			prev.SetLink(node.GetLink())
			return node
		}
		prev = node
		node = node.GetLink()
	}

	return nil
}
func (l *NodeList) Add(node *skiplist.Node) {
	node.SetLink(l.head)
	l.head = node
}
func NewWithConfig(cfg Config) *Skiplist {
	if runtime.GOARCH != "amd64" {
		cfg.UseMemoryMgmt = false
	}

	s := &Skiplist{
		Config:  cfg,
		barrier: newAccessBarrier(cfg.UseMemoryMgmt, cfg.BarrierDestructor),
	}

	s.newNode = func(itm unsafe.Pointer, level int) *Node {
		return allocNode(itm, level, cfg.Malloc)
	}

	if cfg.UseMemoryMgmt {
		s.freeNode = func(n *Node) {
			if Debug {
				debugMarkFree(n)
			}
			cfg.Free(unsafe.Pointer(n))
		}
	} else {
		s.freeNode = func(*Node) {}
	}

	head := allocNode(minItem, MaxLevel, nil)
	tail := allocNode(maxItem, MaxLevel, nil)

	for i := 0; i <= MaxLevel; i++ {
		head.setNext(i, tail, false)
	}

	s.head = head
	s.tail = tail

	return s
}
func (s *Skiplist) FreeNode(n *Node, sts *Stats) {
	s.freeNode(n)
	sts.AddInt64(&sts.nodeFrees, 1)
}
func (s *Skiplist) MakeBuf() *ActionBuffer {
	return &ActionBuffer{
		preds: make([]*Node, MaxLevel+1),
		succs: make([]*Node, MaxLevel+1),
	}
}
func (s *Skiplist) Size(n *Node) int {
	return s.ItemSize(n.Item()) + n.Size()
}
func (s *Skiplist) NewLevel(randFn func() float32) int {
	var nextLevel int

	for ; randFn() < p; nextLevel++ {
	}

	if nextLevel > MaxLevel {
		nextLevel = MaxLevel
	}

	level := int(atomic.LoadInt32(&s.level))
	if nextLevel > level {
		if atomic.CompareAndSwapInt32(&s.level, int32(level), int32(level+1)) {
			nextLevel = level + 1
		} else {
			nextLevel = level
		}
	}

	return nextLevel
}
func (s *Skiplist) Insert(itm unsafe.Pointer, cmp CompareFn,
	buf *ActionBuffer, sts *Stats) (success bool) {
	_, success = s.Insert2(itm, cmp, nil, buf, rand.Float32, sts)
	return
}
func (s *Skiplist) Insert2(itm unsafe.Pointer, inscmp CompareFn, eqCmp CompareFn,
	buf *ActionBuffer, randFn func() float32, sts *Stats) (*Node, bool) {
	itemLevel := s.NewLevel(randFn)
	return s.Insert3(itm, inscmp, eqCmp, buf, itemLevel, false, sts)
}
func (s *Skiplist) Insert3(itm unsafe.Pointer, insCmp CompareFn, eqCmp CompareFn,
	buf *ActionBuffer, itemLevel int, skipFindPath bool, sts *Stats) (*Node, bool) {

	token := s.barrier.Acquire()
	defer s.barrier.Release(token)

	x := s.newNode(itm, itemLevel)

retry:
	if skipFindPath {
		skipFindPath = false
	} else {
		if s.findPath(itm, insCmp, buf, sts) != nil ||
			eqCmp != nil && compare(eqCmp, itm, buf.preds[0].Item()) == 0 {

			s.freeNode(x)
			return nil, false
		}
	}

	// Set all next links for the node non-atomically
	for i := 0; i <= int(itemLevel); i++ {
		x.setNext(i, buf.succs[i], false)
	}

	// Now node is part of the skiplist
	if !buf.preds[0].dcasNext(0, buf.succs[0], x, false, false) {
		sts.AddUint64(&sts.insertConflicts, 1)
		goto retry
	}

	// Add to index levels
	for i := 1; i <= int(itemLevel); i++ {
	fixThisLevel:
		for {
			nodeNext, deleted := x.getNext(i)
			next := buf.succs[i]

			// Update the node's next pointer at current level if required.
			// This is the only thread which can modify next pointer at this level
			// The dcas operation can fail only if another thread marked delete
			if deleted || (nodeNext != next && !x.dcasNext(i, nodeNext, next, false, false)) {
				goto finished
			}

			if buf.preds[i].dcasNext(i, next, x, false, false) {
				break fixThisLevel
			}

			s.findPath(itm, insCmp, buf, sts)
		}
	}

finished:
	sts.AddInt64(&sts.nodeAllocs, 1)
	sts.AddInt64(&sts.levelNodesCount[itemLevel], 1)
	sts.AddInt64(&sts.usedBytes, int64(s.Size(x)))
	return x, true
}
func (s *Skiplist) Delete(itm unsafe.Pointer, cmp CompareFn,
	buf *ActionBuffer, sts *Stats) bool {
	token := s.barrier.Acquire()
	defer s.barrier.Release(token)

	found := s.findPath(itm, cmp, buf, sts) != nil
	if !found {
		return false
	}

	delNode := buf.succs[0]
	return s.deleteNode(delNode, cmp, buf, sts)
}
func (s *Skiplist) DeleteNode(n *Node, cmp CompareFn,
	buf *ActionBuffer, sts *Stats) bool {
	token := s.barrier.Acquire()
	defer s.barrier.Release(token)

	return s.deleteNode(n, cmp, buf, sts)
}
func (s *Skiplist) GetRangeSplitItems(nways int) []unsafe.Pointer {
	var deleted bool
repeat:
	var itms []unsafe.Pointer
	var finished bool

	l := int(atomic.LoadInt32(&s.level))
	for ; l >= 0; l-- {
		c := int(atomic.LoadInt64(&s.Stats.levelNodesCount[l]) + 1)
		if c >= nways {
			perSplit := c / nways
			node := s.head
			for j := 0; node != s.tail && !finished; j++ {
				if j == perSplit {
					j = -1
					itms = append(itms, node.Item())
					finished = len(itms) == nways-1
				}

				node, deleted = node.getNext(l)
				if deleted {
					goto repeat
				}
			}

			break
		}
	}

	return itms
}
func (itm *Item) Bytes() (bs []byte) {
	l := itm.dataLen
	dataOffset := uintptr(unsafe.Pointer(itm)) + itemHeaderSize

	hdr := (*reflect.SliceHeader)(unsafe.Pointer(&bs))
	hdr.Data = dataOffset
	hdr.Len = int(l)
	hdr.Cap = hdr.Len
	return
}
func ItemSize(p unsafe.Pointer) int {
	itm := (*Item)(p)
	return int(itemHeaderSize + uintptr(itm.dataLen))
}
func KVFromBytes(bs []byte) (k, v []byte) {
	klen := int(binary.LittleEndian.Uint16(bs[0:2]))
	return bs[2 : 2+klen], bs[2+klen:]
}
func CompareKV(a []byte, b []byte) int {
	la := int(binary.LittleEndian.Uint16(a[0:2]))
	lb := int(binary.LittleEndian.Uint16(b[0:2]))

	return bytes.Compare(a[2:2+la], b[2:2+lb])
}
func (e *EventController) Emit(b EventBody) (int, error) {
	// int used to count the number of Handlers fired.
	var i int
	// We build an event struct to contain the Body and generate a Header.
	event := Event{Header: generateHeader(), Body: b}

	// Fire a gorountine for each handler.
	// By design the is no waiting for any Handlers to complete
	// before firing another. Therefore there is also no guarantee
	// that any Handler will predictably fire before another one.
	//
	// Any synchronizing needs to be within the Handler.
	for _, h := range e.Handlers {
		i++
		go h.HandleGomitEvent(event)
	}

	return i, nil
}
func (e *EventController) UnregisterHandler(n string) error {

	e.handlerMutex.Lock()
	delete(e.Handlers, n)
	e.handlerMutex.Unlock()

	return nil
}
func (e *EventController) IsHandlerRegistered(n string) bool {
	_, x := e.Handlers[n]
	return x
}
func CompareNitro(this unsafe.Pointer, that unsafe.Pointer) int {
	thisItem := (*Nitro)(this)
	thatItem := (*Nitro)(that)

	return int(thisItem.id - thatItem.id)
}
func DefaultConfig() Config {
	var cfg Config
	cfg.SetKeyComparator(defaultKeyCmp)
	cfg.fileType = RawdbFile
	cfg.useMemoryMgmt = false
	cfg.refreshRate = defaultRefreshRate
	return cfg
}
func (w *Writer) Delete(bs []byte) (success bool) {
	_, success = w.Delete2(bs)
	return
}
func (w *Writer) GetNode(bs []byte) *skiplist.Node {
	iter := w.store.NewIterator(w.iterCmp, w.buf)
	defer iter.Close()

	x := w.newItem(bs, false)
	x.bornSn = w.getCurrSn()

	if found := iter.SeekWithCmp(unsafe.Pointer(x), w.insCmp, w.existCmp); found {
		return iter.GetNode()
	}

	return nil
}
func (cfg *Config) SetKeyComparator(cmp KeyCompare) {
	cfg.keyCmp = cmp
	cfg.insCmp = newInsertCompare(cmp)
	cfg.iterCmp = newIterCompare(cmp)
	cfg.existCmp = newExistCompare(cmp)
}
func (cfg *Config) UseMemoryMgmt(malloc skiplist.MallocFn, free skiplist.FreeFn) {
	if runtime.GOARCH == "amd64" {
		cfg.useMemoryMgmt = true
		cfg.mallocFun = malloc
		cfg.freeFun = free
	}
}
func NewWithConfig(cfg Config) *Nitro {
	m := &Nitro{
		snapshots:   skiplist.New(),
		gcsnapshots: skiplist.New(),
		currSn:      1,
		Config:      cfg,
		gcchan:      make(chan *skiplist.Node, gcchanBufSize),
		id:          int(atomic.AddInt64(&dbInstancesCount, 1)),
	}

	m.freechan = make(chan *skiplist.Node, gcchanBufSize)
	m.store = skiplist.NewWithConfig(m.newStoreConfig())
	m.initSizeFuns()

	buf := dbInstances.MakeBuf()
	defer dbInstances.FreeBuf(buf)
	dbInstances.Insert(unsafe.Pointer(m), CompareNitro, buf, &dbInstances.Stats)

	return m

}
func (m *Nitro) MemoryInUse() int64 {
	storeStats := m.aggrStoreStats()
	return storeStats.Memory + m.snapshots.MemoryInUse() + m.gcsnapshots.MemoryInUse()
}
func (m *Nitro) Close() {
	// Wait until all snapshot iterators have finished
	for s := m.snapshots.GetStats(); int(s.NodeCount) != 0; s = m.snapshots.GetStats() {
		time.Sleep(time.Millisecond)
	}

	m.hasShutdown = true

	// Acquire gc chan ownership
	// This will make sure that no other goroutine will write to gcchan
	for !atomic.CompareAndSwapInt32(&m.isGCRunning, 0, 1) {
		time.Sleep(time.Millisecond)
	}
	close(m.gcchan)

	buf := dbInstances.MakeBuf()
	defer dbInstances.FreeBuf(buf)
	dbInstances.Delete(unsafe.Pointer(m), CompareNitro, buf, &dbInstances.Stats)

	if m.useMemoryMgmt {
		buf := m.snapshots.MakeBuf()
		defer m.snapshots.FreeBuf(buf)

		m.shutdownWg1.Wait()
		close(m.freechan)
		m.shutdownWg2.Wait()

		// Manually free up all nodes
		iter := m.store.NewIterator(m.iterCmp, buf)
		defer iter.Close()
		var lastNode *skiplist.Node

		iter.SeekFirst()
		if iter.Valid() {
			lastNode = iter.GetNode()
			iter.Next()
		}

		for lastNode != nil {
			m.freeItem((*Item)(lastNode.Item()))
			m.store.FreeNode(lastNode, &m.store.Stats)
			lastNode = nil

			if iter.Valid() {
				lastNode = iter.GetNode()
				iter.Next()
			}
		}
	}
}
func (m *Nitro) NewWriter() *Writer {
	w := m.newWriter()
	w.next = m.wlist
	m.wlist = w
	w.dwrCtx.Init()

	m.shutdownWg1.Add(1)
	go m.collectionWorker(w)
	if m.useMemoryMgmt {
		m.shutdownWg2.Add(1)
		go m.freeWorker(w)
	}

	return w
}
func SnapshotSize(p unsafe.Pointer) int {
	s := (*Snapshot)(p)
	return int(unsafe.Sizeof(s.sn) + unsafe.Sizeof(s.refCount) + unsafe.Sizeof(s.db) +
		unsafe.Sizeof(s.count) + unsafe.Sizeof(s.gclist))
}
func (s *Snapshot) Encode(buf []byte, w io.Writer) error {
	l := 4
	if len(buf) < l {
		return errNotEnoughSpace
	}

	binary.BigEndian.PutUint32(buf[0:4], s.sn)
	if _, err := w.Write(buf[0:4]); err != nil {
		return err
	}

	return nil

}
func (s *Snapshot) Decode(buf []byte, r io.Reader) error {
	if _, err := io.ReadFull(r, buf[0:4]); err != nil {
		return err
	}
	s.sn = binary.BigEndian.Uint32(buf[0:4])
	return nil
}
func (s *Snapshot) Open() bool {
	if atomic.LoadInt32(&s.refCount) == 0 {
		return false
	}
	atomic.AddInt32(&s.refCount, 1)
	return true
}
func CompareSnapshot(this, that unsafe.Pointer) int {
	thisItem := (*Snapshot)(this)
	thatItem := (*Snapshot)(that)

	return int(thisItem.sn) - int(thatItem.sn)
}
func (m *Nitro) GC() {
	if atomic.CompareAndSwapInt32(&m.isGCRunning, 0, 1) {
		m.collectDead()
		atomic.CompareAndSwapInt32(&m.isGCRunning, 1, 0)
	}
}
func (m *Nitro) GetSnapshots() []*Snapshot {
	var snaps []*Snapshot
	buf := m.snapshots.MakeBuf()
	defer m.snapshots.FreeBuf(buf)
	iter := m.snapshots.NewIterator(CompareSnapshot, buf)
	iter.SeekFirst()
	for ; iter.Valid(); iter.Next() {
		snaps = append(snaps, (*Snapshot)(iter.Get()))
	}

	return snaps
}
func MemoryInUse() (sz int64) {
	buf := dbInstances.MakeBuf()
	defer dbInstances.FreeBuf(buf)
	iter := dbInstances.NewIterator(CompareNitro, buf)
	for iter.SeekFirst(); iter.Valid(); iter.Next() {
		db := (*Nitro)(iter.Get())
		sz += db.MemoryInUse()
	}

	return
}
func CompareBS(this, that unsafe.Pointer) int {
	thisItm := (*BarrierSession)(this)
	thatItm := (*BarrierSession)(that)

	return int(thisItm.seqno) - int(thatItm.seqno)
}
func (ab *AccessBarrier) Acquire() *BarrierSession {
	if ab.active {
	retry:
		bs := (*BarrierSession)(atomic.LoadPointer(&ab.session))
		liveCount := atomic.AddInt32(bs.liveCount, 1)
		if liveCount > barrierFlushOffset {
			ab.Release(bs)
			goto retry
		}

		return bs
	}

	return nil
}
func (ab *AccessBarrier) Release(bs *BarrierSession) {
	if ab.active {
		liveCount := atomic.AddInt32(bs.liveCount, -1)
		if liveCount == barrierFlushOffset {
			buf := ab.freeq.MakeBuf()
			defer ab.freeq.FreeBuf(buf)

			// Accessors which entered a closed barrier session steps down automatically
			// But, they may try to close an already closed session.
			if atomic.AddInt32(&bs.closed, 1) == 1 {
				ab.freeq.Insert(unsafe.Pointer(bs), CompareBS, buf, &ab.freeq.Stats)
				if atomic.CompareAndSwapInt32(&ab.isDestructorRunning, 0, 1) {
					ab.doCleanup()
					atomic.CompareAndSwapInt32(&ab.isDestructorRunning, 1, 0)
				}
			}
		}
	}
}
func (ab *AccessBarrier) FlushSession(ref unsafe.Pointer) {
	if ab.active {
		ab.Lock()
		defer ab.Unlock()

		bsPtr := atomic.LoadPointer(&ab.session)
		newBsPtr := unsafe.Pointer(newBarrierSession())
		atomic.CompareAndSwapPointer(&ab.session, bsPtr, newBsPtr)
		bs := (*BarrierSession)(bsPtr)
		bs.objectRef = ref
		ab.activeSeqno++
		bs.seqno = ab.activeSeqno

		atomic.AddInt32(bs.liveCount, barrierFlushOffset+1)
		ab.Release(bs)
	}
}
func (report *StatsReport) Apply(s *Stats) {
	var totalNextPtrs int
	var totalNodes int

	report.ReadConflicts += s.readConflicts
	report.InsertConflicts += s.insertConflicts

	for i, c := range s.levelNodesCount {
		report.NodeDistribution[i] += c
		nodesAtlevel := report.NodeDistribution[i]
		totalNodes += int(nodesAtlevel)
		totalNextPtrs += (i + 1) * int(nodesAtlevel)
	}

	report.SoftDeletes += s.softDeletes
	report.NodeCount = totalNodes
	report.NextPointersPerNode = float64(totalNextPtrs) / float64(totalNodes)
	report.NodeAllocs += s.nodeAllocs
	report.NodeFrees += s.nodeFrees
	report.Memory += s.usedBytes
}
func (s *Stats) AddInt64(src *int64, val int64) {
	if s.isLocal {
		*src += val
	} else {
		atomic.AddInt64(src, val)
	}
}
func (s *Stats) AddUint64(src *uint64, val uint64) {
	if s.isLocal {
		*src += val
	} else {
		atomic.AddUint64(src, val)
	}
}
func (s *Stats) Merge(sts *Stats) {
	atomic.AddUint64(&s.insertConflicts, sts.insertConflicts)
	sts.insertConflicts = 0
	atomic.AddUint64(&s.readConflicts, sts.readConflicts)
	sts.readConflicts = 0
	atomic.AddInt64(&s.softDeletes, sts.softDeletes)
	sts.softDeletes = 0
	atomic.AddInt64(&s.nodeAllocs, sts.nodeAllocs)
	sts.nodeAllocs = 0
	atomic.AddInt64(&s.nodeFrees, sts.nodeFrees)
	sts.nodeFrees = 0
	atomic.AddInt64(&s.usedBytes, sts.usedBytes)
	sts.usedBytes = 0

	for i, val := range sts.levelNodesCount {
		if val != 0 {
			atomic.AddInt64(&s.levelNodesCount[i], val)
			sts.levelNodesCount[i] = 0
		}
	}
}
func (s *Skiplist) GetStats() StatsReport {
	var report StatsReport
	report.Apply(&s.Stats)
	return report
}
func (s *Skiplist) NewIterator(cmp CompareFn,
	buf *ActionBuffer) *Iterator {

	return &Iterator{
		cmp: cmp,
		s:   s,
		buf: buf,
		bs:  s.barrier.Acquire(),
	}
}
func (it *Iterator) SeekFirst() {
	it.prev = it.s.head
	it.curr, _ = it.s.head.getNext(0)
	it.valid = true
}
func (it *Iterator) SeekWithCmp(itm unsafe.Pointer, cmp CompareFn, eqCmp CompareFn) bool {
	var found bool
	if found = it.s.findPath(itm, cmp, it.buf, &it.s.Stats) != nil; found {
		it.prev = it.buf.preds[0]
		it.curr = it.buf.succs[0]
	} else {
		if found = eqCmp != nil && compare(eqCmp, itm, it.buf.preds[0].Item()) == 0; found {
			it.prev = nil
			it.curr = it.buf.preds[0]
		}
	}
	return found
}
func (it *Iterator) Seek(itm unsafe.Pointer) bool {
	it.valid = true
	found := it.s.findPath(itm, it.cmp, it.buf, &it.s.Stats) != nil
	it.prev = it.buf.preds[0]
	it.curr = it.buf.succs[0]
	return found
}
func (it *Iterator) Valid() bool {
	if it.valid && it.curr == it.s.tail {
		it.valid = false
	}

	return it.valid
}
func (it *Iterator) Delete() {
	it.s.softDelete(it.curr, &it.s.Stats)
	// It will observe that current item is deleted
	// Run delete helper and move to the next possible item
	it.Next()
	it.deleted = true
}
func (it *Iterator) Next() {
	if it.deleted {
		it.deleted = false
		return
	}

retry:
	it.valid = true
	next, deleted := it.curr.getNext(0)
	if deleted {
		// Current node is deleted. Unlink current node from the level
		// and make next node as current node.
		// If it fails, refresh the path buffer and obtain new current node.
		if it.s.helpDelete(0, it.prev, it.curr, next, &it.s.Stats) {
			it.curr = next
		} else {
			atomic.AddUint64(&it.s.Stats.readConflicts, 1)
			found := it.s.findPath(it.curr.Item(), it.cmp, it.buf, &it.s.Stats) != nil
			last := it.curr
			it.prev = it.buf.preds[0]
			it.curr = it.buf.succs[0]
			if found && last == it.curr {
				goto retry
			}
		}
	} else {
		it.prev = it.curr
		it.curr = next
	}
}
func Init() {
	s := new(SkuM1SmallBuilder)
	s.Client, _ = new(SkuM1Small).GetInnkeeperClient()
	skurepo.Register(SkuName, s)
}
func FromURL(url string) string {
	result := url
	for _, replace := range replaces {
		result = strings.Replace(result, replace.a, replace.b, -1)
	}
	return result
}
func ToURL(folder string) string {
	result := folder
	for _, replace := range replaces {
		result = strings.Replace(result, replace.b, replace.a, -1)
	}
	return result
}
func (t *TupleHeader) Size() int {
	return VersionOneTupleHeaderSize + int(t.FieldSize)*int(t.FieldCount)
}
func (t *TupleHeader) WriteTo(w io.Writer) (int64, error) {

	if len(t.Offsets) != int(t.FieldCount) {
		return 0, errors.New("Invalid Header: Field count does not equal number of field offsets")
	}

	// Encode Header
	dst := make([]byte, t.Size())
	dst[0] = byte(t.TupleVersion)
	binary.LittleEndian.PutUint32(dst[1:], t.NamespaceHash)
	binary.LittleEndian.PutUint32(dst[5:], t.Hash)
	binary.LittleEndian.PutUint32(dst[9:], t.FieldCount)

	pos := int64(13)
	switch t.FieldSize {
	case 1:

		// Write field offsets
		for _, offset := range t.Offsets {
			dst[pos] = byte(offset)
			pos++
		}
	case 2:
		// Set size enum
		dst[0] |= 64

		// Write field offsets
		for _, offset := range t.Offsets {
			binary.LittleEndian.PutUint16(dst[pos:], uint16(offset))
			pos += 2
		}
	case 4:
		// Set size enum
		dst[0] |= 128

		// Write field offsets
		for _, offset := range t.Offsets {
			binary.LittleEndian.PutUint32(dst[pos:], uint32(offset))
			pos += 4
		}
	case 8:
		// Set size enum
		dst[0] |= 192

		// Write field offsets
		for _, offset := range t.Offsets {
			binary.LittleEndian.PutUint64(dst[pos:], offset)
			pos += 8
		}
	default:
		return pos, errors.New("Invalid Header: Field size must be 1,2,4 or 8 bytes")
	}

	n, err := w.Write(dst)
	return int64(n), err
}
func Static(directory string, staticOpt ...StaticOptions) Handler {
	if !path.IsAbs(directory) {
		directory = path.Join(Root, directory)
	}
	dir := http.Dir(directory)
	opt := prepareStaticOptions(staticOpt)

	return func(res http.ResponseWriter, req *http.Request, log *log.Logger) {
		if req.Method != "GET" && req.Method != "HEAD" {
			return
		}
		file := req.URL.Path
		// if we have a prefix, filter requests by stripping the prefix
		if opt.Prefix != "" {
			if !strings.HasPrefix(file, opt.Prefix) {
				return
			}
			file = file[len(opt.Prefix):]
			if file != "" && file[0] != '/' {
				return
			}
		}
		f, err := dir.Open(file)
		if err != nil {
			// discard the error?
			return
		}
		defer f.Close()

		fi, err := f.Stat()
		if err != nil {
			return
		}

		// try to serve index file
		if fi.IsDir() {
			// redirect if missing trailing slash
			if !strings.HasSuffix(req.URL.Path, "/") {
				http.Redirect(res, req, req.URL.Path+"/", http.StatusFound)
				return
			}

			file = path.Join(file, opt.IndexFile)
			f, err = dir.Open(file)
			if err != nil {
				return
			}
			defer f.Close()

			fi, err = f.Stat()
			if err != nil || fi.IsDir() {
				return
			}
		}

		if !opt.SkipLogging {
			log.Println("[Static] Serving " + file)
		}

		// Add an Expires header to the static content
		if opt.Expires != nil {
			res.Header().Set("Expires", opt.Expires())
		}

		http.ServeContent(res, req, file, fi.ModTime(), f)
	}
}
func (c *Config) Read() error {
	in, err := os.Open(c.filename)
	if err != nil {
		return err
	}
	defer in.Close()
	scanner := bufio.NewScanner(in)
	line := ""
	section := ""
	for scanner.Scan() {
		if scanner.Text() == "" {
			continue
		}
		if line == "" {
			sec, ok := checkSection(scanner.Text())
			if ok {
				section = sec
				continue
			}
		}
		if checkComment(scanner.Text()) {
			continue
		}
		line += scanner.Text()
		if strings.HasSuffix(line, "\\") {
			line = line[:len(line)-1]
			continue
		}
		key, value, ok := checkLine(line)
		if !ok {
			return errors.New("WRONG: " + line)
		}
		c.Set(section, key, value)
		line = ""
	}
	return nil
}
func (c *Config) Del(section string, key string) {
	_, ok := c.config[section]
	if ok {
		delete(c.config[section], key)
		if len(c.config[section]) == 0 {
			delete(c.config, section)
		}
	}
}
func (c *Config) WriteTo(filename string) error {
	content := ""
	for k, v := range c.config {
		format := "%v = %v\n"
		if k != "" {
			content += fmt.Sprintf("[%v]\n", k)
			format = "\t" + format
		}
		for key, value := range v {
			content += fmt.Sprintf(format, key, value)
		}
	}
	return ioutil.WriteFile(filename, []byte(content), 0644)
}
func checkSection(line string) (string, bool) {
	line = strings.TrimSpace(line)
	lineLen := len(line)
	if lineLen < 2 {
		return "", false
	}
	if line[0] == '[' && line[lineLen-1] == ']' {
		return line[1 : lineLen-1], true
	}
	return "", false
}
func checkLine(line string) (string, string, bool) {
	key := ""
	value := ""
	sp := strings.SplitN(line, "=", 2)
	if len(sp) != 2 {
		return key, value, false
	}
	key = strings.TrimSpace(sp[0])
	value = strings.TrimSpace(sp[1])
	return key, value, true
}
func checkComment(line string) bool {
	line = strings.TrimSpace(line)
	for p := range commentPrefix {
		if strings.HasPrefix(line, commentPrefix[p]) {
			return true
		}
	}
	return false
}
func NewResponseWrapper(w http.ResponseWriter) *ResponseWrapper {
	return &ResponseWrapper{ResponseRecorder: httptest.NewRecorder(), writer: w}
}
func (w *ResponseWrapper) Hijack() (net.Conn, *bufio.ReadWriter, error) {
	if hijacker, ok := w.writer.(http.Hijacker); ok {
		c, rw, err := hijacker.Hijack()

		if err == nil {
			w.Hijacked = true
		}

		return c, rw, err
	}

	return nil, nil, errors.New("Wrapped ResponseWriter is not a Hijacker")
}
func (w *ResponseWrapper) CloseNotify() <-chan bool {
	if notifier, ok := w.writer.(http.CloseNotifier); ok {
		c := notifier.CloseNotify()
		return c
	}

	return make(chan bool)
}
func DateFormat(f string) Option {
	return Option{func(o *options) {
		o.dateFormat = f
	}}
}
func Logger(next http.Handler) http.HandlerFunc {
	stdlogger := log.New(os.Stdout, "", 0)
	//errlogger := log.New(os.Stderr, "", 0)

	return func(w http.ResponseWriter, r *http.Request) {
		// Start timer
		start := time.Now()

		// Process request
		writer := statusWriter{w, 0}
		next.ServeHTTP(&writer, r)

		// Stop timer
		end := time.Now()
		latency := end.Sub(start)

		clientIP := r.RemoteAddr
		method := r.Method
		statusCode := writer.status
		statusColor := colorForStatus(statusCode)
		methodColor := colorForMethod(method)

		stdlogger.Printf("[HTTP] %v |%s %3d %s| %12v | %s |%s  %s %-7s %s\n",
			end.Format("2006/01/02 - 15:04:05"),
			statusColor, statusCode, reset,
			latency,
			clientIP,
			methodColor, reset, method,
			r.URL.Path,
		)
	}
}
func GetAvailableInventory(taskCollection integrations.Collection) (inventory map[string]skurepo.SkuBuilder) {
	inventory = skurepo.GetRegistry()

	onceLoadInventoryPoller.Do(func() {
		startTaskPollingForRegisteredSkus(taskCollection)
	})
	return
}
func Expiration(e time.Duration) TokenOpt {
	return TokenOpt{func(o *options) {
		o.expiration = e
	}}
}
func Claimer(c func(claims *jwt.StandardClaims) jwt.Claims) TokenOpt {
	return TokenOpt{func(o *options) {
		o.claimer = c
	}}
}
func Issuer(issuer string) TokenOpt {
	return TokenOpt{func(o *options) {
		o.issuer = issuer
	}}
}
func User(user string) TokenOpt {
	return TokenOpt{func(o *options) {
		o.user = user
	}}
}
func Password(password string) TokenOpt {
	return TokenOpt{func(o *options) {
		o.password = password
	}}
}
func Extractor(e request.Extractor) TokenOpt {
	return TokenOpt{func(o *options) {
		o.extractor = e
	}}
}
func TokenGenerator(h http.Handler, auth Authenticator, secret []byte, opts ...TokenOpt) http.Handler {
	o := options{
		logger:     handler.NopLogger(),
		claimer:    func(c *jwt.StandardClaims) jwt.Claims { return c },
		expiration: time.Hour * 24 * 15,
		user:       "user",
		password:   "password",
	}

	o.apply(opts)

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		var err error
		if strings.HasPrefix(r.Header.Get("Content-Type"), "multipart/form-data") {
			err = r.ParseMultipartForm(0)
		} else {
			err = r.ParseForm()
		}
		if err != nil {
			o.logger.Print("Invalid request form: ", err)

			http.Error(w, err.Error(), http.StatusBadRequest)
			return
		}

		user := r.FormValue(o.user)
		password := r.FormValue(o.password)
		if user == "" || password == "" || !auth.Authenticate(user, password) {
			w.WriteHeader(http.StatusUnauthorized)
			return
		}

		expiration := time.Now().Add(o.expiration)
		t := jwt.NewWithClaims(jwt.SigningMethodHS512, o.claimer(&jwt.StandardClaims{
			Subject:   user,
			ExpiresAt: expiration.Unix(),
			Issuer:    o.issuer,
		}))

		if token, err := t.SignedString(secret); err == nil {
			if h == nil {
				w.Header().Add("Authorization", "Bearer "+token)
				w.Write([]byte(token))

				return
			}

			r = r.WithContext(context.WithValue(r.Context(), TokenKey, token))

			h.ServeHTTP(w, r)
		} else {
			o.logger.Print("Error authenticating user:", err)

			w.WriteHeader(http.StatusInternalServerError)
			return
		}
	})
}
func Token(r *http.Request) string {
	if token, ok := r.Context().Value(TokenKey).(string); ok {
		return token
	}

	return ""
}
func Claims(r *http.Request) jwt.Claims {
	if claims, ok := r.Context().Value(ClaimsKey).(jwt.Claims); ok {
		return claims
	}

	return nil
}
func (t Token) String() string {
	switch t.Type {
	case TokenEOF:
		return "EOF"
	case TokenError:
		return t.Value
	}
	if len(t.Value) > 10 {
		return fmt.Sprintf("%.25q...", t.Value)
	}
	return fmt.Sprintf("%q", t.Value)
}
func NewLexer(name, input string, h Handler) *Lexer {
	return &Lexer{
		Name:    name,
		input:   input + "\n",
		state:   lexText,
		handler: h,
	}
}
func (l *Lexer) run() {
	for state := lexText; state != nil; {
		state = state(l)
	}
}
func (l *Lexer) emit(t TokenType) {

	// if the position is the same as the start, do not emit a token
	if l.Pos == l.Start {
		return
	}

	tok := Token{t, l.input[l.Start:l.Pos]}
	l.handler(tok)
	l.Start = l.Pos
}
func (l *Lexer) skipWhitespace() {
	for unicode.Is(unicode.White_Space, l.next()) {
	}
	l.backup()
	l.ignore()
}
func (l *Lexer) next() (r rune) {
	if l.Pos >= len(l.input) {
		l.Width = 0
		return eof
	}
	r, l.Width = utf8.DecodeRuneInString(l.remaining())
	l.advance(l.Width)
	return
}
func (l *Lexer) LineNum() int {
	return strings.Count(l.input[:l.Pos], "\n")
}
func (l *Lexer) Offset() int {

	// find last line break
	lineoffset := strings.LastIndex(l.input[:l.Pos], "\n")
	if lineoffset != -1 {

		// calculate current offset from last line break
		return l.Pos - lineoffset
	}

	// first line
	return l.Pos
}
func (l *Lexer) errorf(format string, args ...interface{}) stateFn {
	l.handler(Token{TokenError, fmt.Sprintf(fmt.Sprintf("%s[%d:%d] ", l.Name, l.LineNum(), l.Offset())+format, args...)})
	return nil
}
func lexText(l *Lexer) stateFn {
OUTER:
	for {
		l.skipWhitespace()
		remaining := l.remaining()

		if strings.HasPrefix(remaining, comment) { // Start comment
			// state function which lexes a comment
			return lexComment
		} else if strings.HasPrefix(remaining, pkg) { // Start package decl
			// state function which lexes a package decl
			return lexPackage
		} else if strings.HasPrefix(remaining, from) { // Start from decl
			// state function which lexes a from decl
			return lexFrom
		} else if strings.HasPrefix(remaining, typeDef) { // Start type def
			// state function which lexes a type
			return lexTypeDef
		} else if strings.HasPrefix(remaining, version) { // Start version
			// state function which lexes a version
			return lexVersion
		} else if strings.HasPrefix(remaining, required) { // Start required field
			// state function which lexes a field
			l.Pos += len(required)
			l.emit(TokenRequired)
			l.skipWhitespace()

			return lexType
		} else if strings.HasPrefix(remaining, optional) { // Start optional field
			// state function which lexes a field
			l.Pos += len(optional)
			l.emit(TokenOptional)
			l.skipWhitespace()
			return lexType
		} else if strings.HasPrefix(remaining, openScope) { // Open scope
			l.Pos += len(openScope)
			l.emit(TokenOpenCurlyBracket)
		} else if strings.HasPrefix(remaining, closeScope) { // Close scope
			l.Pos += len(closeScope)
			l.emit(TokenCloseCurlyBracket)
		} else {
			switch r := l.next(); {

			case r == eof: // reached EOF?
				l.emit(TokenEOF)
				break OUTER
			default:
				l.errorf("unknown token: %#v", string(r))
			}
		}
	}

	// Stops the run loop
	return nil
}
func lexComment(l *Lexer) stateFn {
	l.skipWhitespace()

	// if strings.HasPrefix(l.remaining(), comment) {
	// skip comment //
	l.Pos += len(comment)

	// find next new line and add location to pos which
	// advances the scanner
	if index := strings.Index(l.remaining(), "\n"); index > 0 {
		l.Pos += index
	} else {
		l.Pos += len(l.remaining())
		// l.emit(TokenComment)
		// break
	}

	// emit the comment string
	l.emit(TokenComment)

	l.skipWhitespace()
	// }

	// continue on scanner
	return lexText
}
func New(namespace string, name string) (t TupleType) {
	hash := syncHash.Hash([]byte(name))
	ns_hash := syncHash.Hash([]byte(namespace))
	t = TupleType{namespace, name, ns_hash, hash, make([][]Field, 0), make(map[string]int)}
	return
}
func (t *TupleType) AddVersion(fields ...Field) {
	t.versions = append(t.versions, fields)
	for _, field := range fields {
		t.fields[field.Name] = len(t.fields)
	}
}
func (t *TupleType) Contains(field string) bool {
	_, exists := t.fields[field]
	return exists
}
func (t *TupleType) Offset(field string) (offset int, exists bool) {
	offset, exists = t.fields[field]
	return
}
func (t *TupleType) Versions() (vers []Version) {
	vers = make([]Version, t.NumVersions())
	for i := 0; i < t.NumVersions(); i++ {
		vers[i] = Version{uint8(i + 1), t.versions[i]}
	}
	return
}
func (s *Task) SetPrivateMeta(name string, value interface{}) {
	if s.PrivateMetaData == nil {
		s.PrivateMetaData = make(map[string]interface{})
	}

	s.PrivateMetaData[name] = value
}
func (s *Task) SetPublicMeta(name string, value interface{}) {
	if s.MetaData == nil {
		s.MetaData = make(map[string]interface{})
	}
	s.MetaData[name] = value
}
func (s *Task) GetRedactedVersion() RedactedTask {
	s.mutex.RLock()
	rt := RedactedTask{
		ID:         s.ID,
		Timestamp:  s.Timestamp,
		Expires:    s.Expires,
		Status:     s.Status,
		Profile:    s.Profile,
		CallerName: s.CallerName,
		MetaData:   s.MetaData,
	}
	s.mutex.RUnlock()
	return rt
}
func (s Task) Equal(b Task) bool {
	return (s.ID == b.ID &&
		s.Timestamp == b.Timestamp &&
		s.Expires == b.Expires &&
		s.Status == b.Status &&
		s.Profile == b.Profile &&
		s.CallerName == b.CallerName)
}
func HTTP(h http.Handler, verb Verb, verbs ...Verb) http.Handler {
	verbSet := map[Verb]struct{}{verb: struct{}{}}
	for _, v := range verbs {
		verbSet[v] = struct{}{}
	}

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if _, ok := verbSet[Verb(r.Method)]; ok {
			h.ServeHTTP(w, r)
		} else {
			w.WriteHeader(http.StatusBadRequest)
		}
	})
}
func (b *TupleBuilder) PutUint8(field string, value uint8) (wrote uint64, err error) {

	// field type should be a Uint8Field
	if err = b.typeCheck(field, Uint8Field); err != nil {
		return 0, err
	}

	// minimum bytes is 2 (type code + value)
	if b.available() < 2 {
		return 0, xbinary.ErrOutOfRange
	}

	// write type code
	b.buffer[b.pos] = byte(UnsignedInt8Code.OpCode)

	// write value
	b.buffer[b.pos+1] = byte(value)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 2

	return 2, nil
}
func (b *TupleBuilder) PutInt8(field string, value int8) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Int8Field); err != nil {
		return 0, err
	}

	// minimum bytes is 2 (type code + value)
	if b.available() < 2 {
		return 0, xbinary.ErrOutOfRange
	}

	// write type code
	b.buffer[b.pos] = byte(Int8Code.OpCode)

	// write value
	b.buffer[b.pos+1] = byte(value)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 2

	return 2, nil
}
func (b *TupleBuilder) PutUint16(field string, value uint16) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Uint16Field); err != nil {
		return 0, err
	}

	if value < math.MaxUint8 {

		// minimum bytes is 2 (type code + value)
		if b.available() < 2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(UnsignedShort8Code.OpCode)

		// write value
		b.buffer[b.pos+1] = byte(value)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 2

		return 2, nil
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}

	// write type code
	b.buffer[b.pos] = byte(UnsignedShort16Code.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 3

	// wrote 3 bytes
	return 3, nil

}
func (b *TupleBuilder) PutInt16(field string, value int16) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Int16Field); err != nil {
		return 0, err
	}

	if uint16(value) < math.MaxUint8 {

		// minimum bytes is 2 (type code + value)
		if b.available() < 2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Short8Code.OpCode)

		// write value
		b.buffer[b.pos+1] = byte(value)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 2

		return 2, nil
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutInt16(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}

	// write type code
	b.buffer[b.pos] = byte(Short16Code.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 3

	// wrote 3 bytes
	return 3, nil

}
func (b *TupleBuilder) PutUint32(field string, value uint32) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Uint32Field); err != nil {
		return 0, err
	}

	if value < math.MaxUint8 {

		// minimum bytes is 2 (type code + value)
		if b.available() < 2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(UnsignedInt8Code.OpCode)

		// write value
		b.buffer[b.pos+1] = byte(value)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 2

		return 2, nil
	} else if value < math.MaxUint16 {

		// write value
		// length check performed by xbinary
		wrote, err = xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, uint16(value))
		if err != nil {
			return 0, err
		}
		// write type code
		b.buffer[b.pos] = byte(UnsignedInt16Code.OpCode)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 3

		// wrote 3 bytes
		return 3, nil
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutUint32(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}

	// write type code
	b.buffer[b.pos] = byte(UnsignedInt32Code.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 5

	// wrote 5 bytes
	return 5, nil
}
func (b *TupleBuilder) PutInt32(field string, value int32) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Int32Field); err != nil {
		return 0, err
	}

	unsigned := uint32(value)
	if unsigned < math.MaxUint8 {

		// minimum bytes is 2 (type code + value)
		if b.available() < 2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Int8Code.OpCode)

		// write value
		b.buffer[b.pos+1] = byte(value)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 2

		return 2, nil
	} else if unsigned < math.MaxUint16 {

		// write value
		// length check performed by xbinary
		wrote, err = xbinary.LittleEndian.PutInt16(b.buffer, b.pos+1, int16(value))
		if err != nil {
			return 0, err
		}

		// write type code
		b.buffer[b.pos] = byte(Int16Code.OpCode)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 3

		// wrote 3 bytes
		return 3, nil
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutInt32(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}

	// write type code
	b.buffer[b.pos] = byte(Int32Code.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 5

	// wrote 5 bytes
	return 5, nil
}
func (b *TupleBuilder) PutUint64(field string, value uint64) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Uint64Field); err != nil {
		return 0, err
	}

	if value < math.MaxUint8 {

		// minimum bytes is 2 (type code + value)
		if b.available() < 2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(UnsignedLong8Code.OpCode)

		// write value
		b.buffer[b.pos+1] = byte(value)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 2

		return 2, nil
	} else if value < math.MaxUint16 {

		// write value
		// length check performed by xbinary
		wrote, err = xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, uint16(value))
		if err != nil {
			return 0, err
		}

		// write type code
		b.buffer[b.pos] = byte(UnsignedLong16Code.OpCode)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 3

		// wrote 3 bytes
		return 3, nil
	} else if value < math.MaxUint32 {

		// write value
		// length check performed by xbinary
		wrote, err = xbinary.LittleEndian.PutUint32(b.buffer, b.pos+1, uint32(value))
		if err != nil {
			return 0, err
		}

		// write type code
		b.buffer[b.pos] = byte(UnsignedLong32Code.OpCode)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 5

		// wrote 5 bytes
		return 5, nil
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutUint64(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}
	// write type code
	b.buffer[b.pos] = byte(UnsignedLong64Code.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 9

	// wrote 9 bytes
	return 9, nil

}
func (b *TupleBuilder) PutInt64(field string, value int64) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Int64Field); err != nil {
		return 0, err
	}

	unsigned := uint64(value)
	if unsigned < math.MaxUint8 {

		// minimum bytes is 2 (type code + value)
		if b.available() < 2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Long8Code.OpCode)

		// write value
		b.buffer[b.pos+1] = byte(value)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 2

		return 2, nil
	} else if unsigned < math.MaxUint16 {

		// write value
		// length check performed by xbinary
		wrote, err = xbinary.LittleEndian.PutInt16(b.buffer, b.pos+1, int16(value))
		if err != nil {
			return 0, err
		}

		// write type code
		b.buffer[b.pos] = byte(Long16Code.OpCode)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 3

		// wrote 3 bytes
		return 3, nil
	} else if unsigned < math.MaxUint32 {

		// write value
		// length check performed by xbinary
		wrote, err = xbinary.LittleEndian.PutInt32(b.buffer, b.pos+1, int32(value))
		if err != nil {
			return 0, err
		}

		// write type code
		b.buffer[b.pos] = byte(Long32Code.OpCode)

		// set field offset
		b.offsets[field] = b.pos

		// incr pos
		b.pos += 5

		// wrote 5 bytes
		return 5, nil
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutInt64(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}

	// write type code
	b.buffer[b.pos] = byte(Long64Code.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 9

	// wrote 9 bytes
	return 9, nil
}
func NewPackageList() PackageList {
    var lock sync.Mutex
    return &packageList{make(map[string]Package), lock}
}
func (b *TupleBuilder) PutFloat32(field string, value float32) (wrote uint64, err error) {

	// field type should be
	if err = b.typeCheck(field, Float32Field); err != nil {
		return 0, err
	}

	// write value
	// length check performed by xbinary
	wrote, err = xbinary.LittleEndian.PutFloat32(b.buffer, b.pos+1, value)
	if err != nil {
		return 0, err
	}

	// write type code
	b.buffer[b.pos] = byte(FloatCode.OpCode)

	// set field offset
	b.offsets[field] = b.pos

	// incr pos
	b.pos += 5

	return 5, nil
}
func Classic() *ClassicMartini {
	r := NewRouter()
	m := New()
	m.Use(Logger())
	m.Use(Recovery())
	m.Use(Static("static"))
	m.Use(ContextRender("", RenderOptions{
		Extensions: []string{".html", ".tmpl", "tpl"},
	}))
	m.MapTo(r, (*Routes)(nil))
	m.Action(r.Handle)
	return &ClassicMartini{m, r}
}
func Languages(tags []xlang.Tag) Option {
	return Option{func(o *options) {
		o.languages = tags
	}}
}
func Session(s handler.Session) Option {
	return Option{func(o *options) {
		o.session = s
	}}
}
func Data(r *http.Request) ContextValue {
	if v, ok := r.Context().Value(ContextKey).(ContextValue); ok {
		return v
	}

	return ContextValue{}
}
func URL(url, prefix string, data ContextValue) string {
	if data.Current.IsRoot() {
		return url
	}

	if prefix == "" {
		prefix = "/"
	} else if prefix[len(prefix)-1] != '/' {
		prefix += "/"
	}

	if url == "" {
		url = "/"
	}

	if url[0] != '/' {
		url = "/" + url
	}

	return prefix + data.Current.String() + url
}
func Size() (w, h int, err error) {
	if !IsInit {
		err = errors.New("termsize not yet iniitialied")
		return
	}

	return get_size()
}
func GetRequestIDFromTaskResponse(taskResponse TaskResponse) (requestID string, err error) {
	var provisionHostInfoBytes []byte
	firstRecordIndex := 0
	meta := taskResponse.MetaData
	provisionHostInfo := ProvisionHostInfo{}
	lo.G.Debug("taskResponse: ", taskResponse)
	lo.G.Debug("metadata: ", meta)

	if provisionHostInfoBytes, err = json.Marshal(meta[ProvisionHostInformationFieldname]); err == nil {

		if err = json.Unmarshal(provisionHostInfoBytes, &provisionHostInfo); err == nil {

			if len(provisionHostInfo.Data) > firstRecordIndex {
				requestID = provisionHostInfo.Data[firstRecordIndex].RequestID

			} else {
				lo.G.Error("no request id found in: ", provisionHostInfo)
			}

		} else {
			lo.G.Error("error unmarshalling: ", err, meta)
			lo.G.Error("metadata: ", meta)
		}

	} else {
		lo.G.Error("error marshalling: ", err)
	}
	return
}
func (b *TupleBuilder) PutString(field string, value string) (wrote int, err error) {

	// field type should be
	if err = b.typeCheck(field, StringField); err != nil {
		return 0, err
	}

	size := len(value)
	if size < math.MaxUint8 {

		if b.available() < size+2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutString(b.buffer, b.pos+2, value)

		// write type code
		b.buffer[b.pos] = byte(String8Code.OpCode)

		// write length
		b.buffer[b.pos+1] = byte(size)

		wrote += size + 2
	} else if size < math.MaxUint16 {

		if b.available() < size+3 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, uint16(size))

		// write value
		xbinary.LittleEndian.PutString(b.buffer, b.pos+3, value)

		// write type code
		b.buffer[b.pos] = byte(String16Code.OpCode)

		wrote += 3 + size
	} else if size < math.MaxUint32 {

		if b.available() < size+5 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint32(b.buffer, b.pos+1, uint32(size))

		// write value
		xbinary.LittleEndian.PutString(b.buffer, b.pos+5, value)

		// write type code
		b.buffer[b.pos] = byte(String32Code.OpCode)

		wrote += 5 + size
	} else {

		if b.available() < size+9 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint64(b.buffer, b.pos+1, uint64(size))

		// write value
		xbinary.LittleEndian.PutString(b.buffer, b.pos+9, value)

		// write type code
		b.buffer[b.pos] = byte(String64Code.OpCode)

		wrote += 9 + size
	}

	b.offsets[field] = b.pos
	b.pos += wrote
	return
}
func LoadDirectory(dir string, parser Parser) (err error) {

    // Open dir for reading
    d, err := os.Open(dir)
    if err != nil {
        return
    }

    // Iterate over all the files in the directory.
    for {

        // Only read 128 files at a time.
        if fis, err := d.Readdir(128); err == nil {

            // Read each entry
            for _, fi := range fis {
                // fmt.Println("%#v", fi)

                // If the FileInfo is a directory, read the directory.
                // Otherwise, read the file.
                switch fi.IsDir() {
                case true:

                    // return error if there is one
                    if err := LoadDirectory(fi.Name(), parser); err != nil {
                        return err
                    }
                case false:

                    // All schema files should end with .nt
                    if !strings.HasSuffix(fi.Name(), ".ent") {
                        break
                    }

                    // Read the file
                    if _, err := LoadFile(filepath.Join(dir, fi.Name()), parser); err != nil {
                        return err
                    }
                }
            }
        } else if err == io.EOF {
            // If there are no more files in the directory, break.
            break
        } else {
            // If there is any other error, return it.
            return err
        }
    }

    // If you have reached this far, you are done.
    return nil
}
func LoadFile(filename string, parser Parser) (Package, error) {
    file, err := os.Open(filename)
    if err != nil {
        return Package{}, err
    }
    defer file.Close()

    // read file
    bytes, err := ioutil.ReadAll(file)
    if err != nil {
        return Package{}, err
    }

    // convert to string and load
    return parser.Parse(file.Name(), string(bytes))
}
func LoadPackage(parser Parser, name, text string) (Package, error) {
    return parser.Parse(name, text)
}
func NewDecoder(reg Registry, r io.Reader) Decoder {
	var buf []byte
	return decoder{reg, DefaultMaxSize, bytes.NewBuffer(buf), bufio.NewReader(r)}
}
func NewDecoderSize(reg Registry, maxSize uint64, r io.Reader) Decoder {
	var buf []byte
	return decoder{reg, maxSize, bytes.NewBuffer(buf), bufio.NewReader(r)}
}
func Panic(h http.Handler, opts ...Option) http.Handler {
	o := options{logger: handler.ErrLogger(), dateFormat: PanicDateFormat}
	o.apply(opts)

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		defer func() {
			if rec := recover(); rec != nil {
				stack := debug.Stack()
				timestamp := time.Now().Format(o.dateFormat)
				message := fmt.Sprintf("%s - %s\n%s\n", timestamp, rec, stack)

				o.logger.Print(message)

				w.WriteHeader(http.StatusInternalServerError)

				if !o.showStack {
					message = "Internal Server Error"
				}

				w.Write([]byte(message))
			}
		}()

		h.ServeHTTP(w, r)
	})
}
func (s *FakeVCDClient) DeployVApp(templateName, templateHref, vcdHref string) (vapp *vcloudclient.VApp, err error) {
	return s.FakeVApp, s.ErrUnDeployFake
}
func (s *FakeVCDClient) UnDeployVApp(vappID string) (task *vcloudclient.TaskElem, err error) {
	return &s.FakeVApp.Tasks.Task, s.ErrDeployFake
}
func (s *FakeVCDClient) Auth(username, password string) (err error) {
	return s.ErrAuthFake
}
func (s *FakeVCDClient) QueryTemplate(templateName string) (vappTemplate *vcloudclient.VAppTemplateRecord, err error) {
	return s.FakeVAppTemplateRecord, s.ErrDeployFake
}
func NewEncoder(w io.Writer) Encoder {
	return versionOneEncoder{w, make([]byte, 9), bytes.NewBuffer(make([]byte, 0, 4096))}
}
func Getter(g NonceGetter) Option {
	return Option{func(o *options) {
		o.getter = g
	}}
}
func Setter(s NonceSetter) Option {
	return Option{func(o *options) {
		o.setter = s
	}}
}
func Age(age time.Duration) Option {
	return Option{func(o *options) {
		o.age = age
	}}
}
func Nonce(h http.Handler, opts ...Option) http.Handler {
	headerStorage := nonceHeaderStorage{}
	o := options{
		logger:    handler.OutLogger(),
		generator: timeRandomGenerator,
		getter:    headerStorage,
		setter:    headerStorage,
		age:       45 * time.Second,
	}
	o.apply(opts)

	store := nonceStore{}
	opChan := make(chan func(nonceStore))

	go func() {
		for op := range opChan {
			op(store)
		}
	}()

	go func() {
		for {
			select {
			case <-time.After(5 * time.Minute):
				cleanup(o.age, opChan)
			}
		}
	}()

	setter := func(w http.ResponseWriter, r *http.Request) error {
		nonce, err := generateAndStore(o.age, o.generator, opChan)
		if err != nil {
			return err
		}

		return o.setter.SetNonce(nonce, w, r)
	}

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		ctx := r.Context()

		nonce := o.getter.GetNonce(r)
		if nonce != "" {
			if validateAndRemoveNonce(nonce, o.age, opChan) {
				ctx = context.WithValue(ctx, nonceValueKey, NonceStatus{NonceValid})
			} else {
				ctx = context.WithValue(ctx, nonceValueKey, NonceStatus{NonceInvalid})
			}
		}

		h.ServeHTTP(w, r.WithContext(context.WithValue(ctx, nonceSetterKey, setter)))
	})
}
func NonceValueFromRequest(r *http.Request) NonceStatus {
	if c := r.Context().Value(nonceValueKey); c != nil {
		if v, ok := c.(NonceStatus); ok {
			return v
		}
	}

	return NonceStatus{NonceNotRequested}
}
func StoreNonce(w http.ResponseWriter, r *http.Request) (err error) {
	if c := r.Context().Value(nonceSetterKey); c != nil {
		if setter, ok := c.(func(http.ResponseWriter, *http.Request) error); ok {
			err = setter(w, r)
		}
	}

	return err
}
func (b *TupleBuilder) PutFloat32Array(field string, value []float32) (wrote int, err error) {

	// field type should be
	if err = b.typeCheck(field, Float32ArrayField); err != nil {
		return 0, err
	}

	size := len(value)
	if size < math.MaxUint8 {

		if b.available() < size*4+2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutFloat32Array(b.buffer, b.pos+2, value)

		// write type code
		b.buffer[b.pos] = byte(FloatArray8Code.OpCode)

		// write length
		b.buffer[b.pos+1] = byte(size)

		wrote += size + 2
	} else if size < math.MaxUint16 {

		if b.available() < size*4+3 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, uint16(size))

		// write value
		xbinary.LittleEndian.PutFloat32Array(b.buffer, b.pos+3, value)

		// write type code
		b.buffer[b.pos] = byte(FloatArray16Code.OpCode)

		wrote += 3 + size
	} else if size < math.MaxUint32 {

		if b.available() < size*4+5 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint32(b.buffer, b.pos+1, uint32(size))

		// write value
		xbinary.LittleEndian.PutFloat32Array(b.buffer, b.pos+5, value)

		// write type code
		b.buffer[b.pos] = byte(FloatArray32Code.OpCode)

		wrote += 5 + size
	} else {

		if b.available() < size*4+9 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint64(b.buffer, b.pos+1, uint64(size))

		// write value
		xbinary.LittleEndian.PutFloat32Array(b.buffer, b.pos+9, value)

		// write type code
		b.buffer[b.pos] = byte(FloatArray64Code.OpCode)

		wrote += 9 + size
	}

	b.offsets[field] = b.pos
	b.pos += wrote
	return
}
func (b *TupleBuilder) PutFloat64Array(field string, value []float64) (wrote int, err error) {

	// field type should be
	if err = b.typeCheck(field, Float64ArrayField); err != nil {
		return 0, err
	}

	size := len(value)
	if size < math.MaxUint8 {

		if b.available() < size*8+2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutFloat64Array(b.buffer, b.pos+2, value)

		// write type code
		b.buffer[b.pos] = byte(DoubleArray8Code.OpCode)

		// write length
		b.buffer[b.pos+1] = byte(size)

		wrote += size + 2
	} else if size < math.MaxUint16 {

		if b.available() < size*8+3 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, uint16(size))

		// write value
		xbinary.LittleEndian.PutFloat64Array(b.buffer, b.pos+3, value)

		// write type code
		b.buffer[b.pos] = byte(DoubleArray16Code.OpCode)

		wrote += 3 + size
	} else if size < math.MaxUint32 {

		if b.available() < size*8+5 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint32(b.buffer, b.pos+1, uint32(size))

		// write value
		xbinary.LittleEndian.PutFloat64Array(b.buffer, b.pos+5, value)

		// write type code
		b.buffer[b.pos] = byte(DoubleArray32Code.OpCode)

		wrote += 5 + size
	} else {

		if b.available() < size*8+9 {
			return 0, xbinary.ErrOutOfRange
		}

		// write length
		xbinary.LittleEndian.PutUint64(b.buffer, b.pos+1, uint64(size))

		// write value
		xbinary.LittleEndian.PutFloat64Array(b.buffer, b.pos+9, value)

		// write type code
		b.buffer[b.pos] = byte(DoubleArray64Code.OpCode)

		wrote += 9 + size
	}

	b.offsets[field] = b.pos
	b.pos += wrote
	return
}
func (s *ClientDoer) Do(req *http.Request) (resp *http.Response, err error) {
	s.SpyRequest = *req
	return s.Response, s.Error
}
func NewLease(taskCollection integrations.Collection, availableSkus map[string]skurepo.SkuBuilder) *Lease {

	return &Lease{
		taskCollection: taskCollection,
		taskManager:    taskmanager.NewTaskManager(taskCollection),
		availableSkus:  availableSkus,
		Task:           taskmanager.RedactedTask{},
	}
}
func (s *Lease) Delete(logger *log.Logger, req *http.Request) (statusCode int, response interface{}) {
	var (
		err error
	)
	statusCode = http.StatusNotFound
	s.taskCollection.Wake()

	if err = s.InitFromHTTPRequest(req); err == nil {
		logger.Println("restocking inventory...")
		s.ReStock()
		statusCode = http.StatusAccepted
		response = s.Task

	} else {
		response = map[string]string{"error": err.Error()}
	}
	return
}
func (s *Lease) Post(logger *log.Logger, req *http.Request) (statusCode int, response interface{}) {
	var (
		err error
	)
	statusCode = http.StatusNotFound
	s.taskCollection.Wake()
	logger.Println("collection dialed successfully")

	if err = s.InitFromHTTPRequest(req); err == nil {
		logger.Println("obtaining lease...", s)
		s.Procurement()
		statusCode = http.StatusCreated
		response = s.Task

	} else {
		response = map[string]string{"error": err.Error()}
	}
	return
}
func (s *Lease) ReStock() (skuTask *taskmanager.Task) {

	if skuConstructor, ok := s.availableSkus[s.Sku]; ok {
		leaseMap := structs.Map(s)
		sku := skuConstructor.New(s.taskManager, leaseMap)
		skuTask = sku.ReStock()
		s.Task = skuTask.GetRedactedVersion()

	} else {
		s.Task.Status = TaskStatusUnavailable
	}
	return
}
func (s *Lease) Procurement() (skuTask *taskmanager.Task) {

	if skuConstructor, ok := s.availableSkus[s.Sku]; ok {
		leaseMap := structs.Map(s)
		sku := skuConstructor.New(s.taskManager, leaseMap)
		GLogger.Println("here is my sku: ", sku)
		skuTask = sku.Procurement()
		tt := skuTask.Read(func(t *taskmanager.Task) interface{} {
			tt := *t
			return tt
		})
		GLogger.Println("here is my task after procurement: ", tt)
		s.Task = skuTask.GetRedactedVersion()

	} else {
		GLogger.Println("No Sku Match: ", s.Sku, s.availableSkus)
		s.Task.Status = TaskStatusUnavailable
	}
	return
}
func (s *Lease) InitFromHTTPRequest(req *http.Request) (err error) {

	if req.Body != nil {

		if body, err := ioutil.ReadAll(req.Body); err == nil {

			if err = json.Unmarshal(body, s); err != nil {
				GLogger.Println(err)
			}
		}
	} else {
		err = ErrEmptyBody
	}

	if s.ProcurementMeta == nil {
		s.ProcurementMeta = make(map[string]interface{})
	}
	return
}
func (t *Tuple) Is(tupleType TupleType) bool {
	return t.Header.Hash == tupleType.Hash && t.Header.NamespaceHash == tupleType.NamespaceHash
}
func (t *Tuple) Offset(field string) (int, error) {
	index, exists := t.Header.Type.Offset(field)
	if !exists {
		return 0, ErrFieldDoesNotExist
	}

	// Tuple type and tuple header do not agree on fields
	if index < 0 || index >= int(t.Header.FieldCount) {
		return 0, ErrInvalidFieldIndex
	}
	return int(t.Header.Offsets[index]), nil
}
func (t Tuple) WriteTo(w io.Writer) (n int, err error) {
	// write header
	wrote, err := t.Header.WriteTo(w)
	if err != nil {
		return int(wrote), nil
	}

	n, err = w.Write(t.data)
	if err != nil {
		return int(n), err
	}
	return int(wrote) + n, nil
}
func (b *TupleBuilder) PutTuple(field string, value Tuple) (wrote int, err error) {

	// field type should be
	if err = b.typeCheck(field, TupleField); err != nil {
		return 0, err
	}

	size := value.Size() + value.Header.Size()
	if size < math.MaxUint8 {

		// check length
		if b.available() < size+2 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Tuple8Code.OpCode)

		// write length
		b.buffer[b.pos+1] = byte(size)
		wrote += 2

		// Write tuple
		n, err := b.writeTuple(value, b.pos+wrote, size)
		wrote += int(n)

		// Return err
		if err != nil {
			return 0, err
		}

	} else if size < math.MaxUint16 {

		// check length
		if b.available() < size+3 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Tuple16Code.OpCode)

		// write length
		xbinary.LittleEndian.PutUint16(b.buffer, b.pos+1, uint16(size))
		wrote += 3

		// write tuple
		n, err := b.writeTuple(value, b.pos+wrote, size)
		// n, err := value.WriteAt(&b.buffer, int64(b.pos+3))
		wrote += int(n)

		// Return err
		if err != nil {
			return 0, err
		}

	} else if size < math.MaxUint32 {

		// check length
		if b.available() < size+5 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Tuple32Code.OpCode)

		// write length
		xbinary.LittleEndian.PutUint32(b.buffer, b.pos+1, uint32(size))
		wrote += 5

		// write tuple
		n, err := b.writeTuple(value, b.pos+wrote, size)
		// n, err := value.WriteAt(&b.buffer, int64(b.pos+5))
		wrote += int(n)

		// Return err
		if err != nil {
			return 0, err
		}

	} else {

		// check length
		if b.available() < size+9 {
			return 0, xbinary.ErrOutOfRange
		}

		// write type code
		b.buffer[b.pos] = byte(Tuple64Code.OpCode)

		// write length
		xbinary.LittleEndian.PutUint64(b.buffer, b.pos+1, uint64(size))
		wrote += 9

		// write tuple
		n, err := b.writeTuple(value, b.pos+wrote, size)
		// n, err := value.WriteAt(&b.buffer, int64(b.pos+9))
		wrote += int(n)

		// Return err
		if err != nil {
			return 0, err
		}
	}

	// store offset and increment position
	b.offsets[field] = b.pos
	b.pos += wrote
	return
}
func (ctx *Cotex) WriteString(content string) {
	ctx.ResponseWriter.Write([]byte(content))
}
func (ctx *Cotex) NotFound(message string) {
	ctx.ResponseWriter.WriteHeader(404)
	ctx.ResponseWriter.Write([]byte(message))
}
func (s *Agent) Run(process func(*Agent) error) {
	s.task.Update(func(t *Task) interface{} {
		t.taskManager = s.taskManager
		t.Status = AgentTaskStatusRunning
		return t
	})
	s.statusEmitter <- AgentTaskStatusRunning
	go s.startTaskPoller()
	go s.listenForPoll()

	go func(agent Agent) {
		s := &agent
		s.processExitHanderlDecorate(process)
		<-s.processComplete
	}(*s)
}
func NewWaitGroup(throttle int) *WaitGroup {
	return &WaitGroup{
		outstanding: 0,
		throttle:    throttle,
		completed:   make(chan bool, throttle),
	}
}
func (w *WaitGroup) Add() {
	w.outstanding++
	if w.outstanding > w.throttle {
		select {
		case <-w.completed:
			w.outstanding--
			return
		}
	}
}
func (w *WaitGroup) Wait() {
	if w.outstanding == 0 {
		return
	}
	for w.outstanding > 0 {
		select {
		case <-w.completed:
			w.outstanding--
		}
	}
}
func Gzip(h http.Handler, opts ...Option) http.Handler {
	o := options{logger: handler.OutLogger()}
	o.apply(opts)

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if !strings.Contains(r.Header.Get("Accept-Encoding"), "gzip") {
			h.ServeHTTP(w, r)
			return
		}

		wrapper := handler.NewResponseWrapper(w)

		h.ServeHTTP(wrapper, r)

		for k, v := range wrapper.Header() {
			w.Header()[k] = v
		}
		w.Header().Set("Vary", "Accept-Encoding")
		w.Header().Set("Content-Encoding", "gzip")

		if w.Header().Get("Content-Type") == "" {
			w.Header().Set("Content-Type", http.DetectContentType(wrapper.Body.Bytes()))
		}
		w.Header().Del("Content-Length")

		if wrapper.Code != http.StatusOK {
			w.WriteHeader(wrapper.Code)
		}

		gz := gzip.NewWriter(w)
		gz.Flush()

		if _, err := gz.Write(wrapper.Body.Bytes()); err != nil {
			o.logger.Print("gzip handler: " + err.Error())
			http.Error(w, "Internal Server Error", http.StatusInternalServerError)
			return
		}

		gz.Close()
	})
}
func New(uri string, user string, password string) InnkeeperClient {
	return &IkClient{
		URI:      uri,
		User:     user,
		Password: password,
	}
}
func (r *Render) Error(status int, message ...string) {
	r.WriteHeader(status)
	if len(message) > 0 {
		r.Write([]byte(message[0]))
	}
}
func Renderer(options ...RenderOptions) Handler {
	opt := prepareRenderOptions(options)
	cs := prepareCharset(opt.Charset)
	t := compile(opt)
	return func(res http.ResponseWriter, req *http.Request, c Context) {
		var tc *template.Template
		if Env == Dev {
			// recompile for easy development
			tc = compile(opt)
		} else {
			// use a clone of the initial template
			tc, _ = t.Clone()
		}
		//c.MapTo(&Render{res, req, tc, opt, cs, Data}, (*Render)(nil))
		c.Map(&Render{res, req, tc, opt, cs, Data})
	}
}
func NewClient(apiKey string, url string, client clientDoer) *PDClient {
	return &PDClient{
		APIKey: apiKey,
		client: client,
		URL:    url,
	}
}
func (s *PDClient) GetTask(taskID string) (task TaskResponse, res *http.Response, err error) {
	req, _ := s.createRequest("GET", fmt.Sprintf("%s/v1/task/%s", s.URL, taskID), bytes.NewBufferString(``))

	if res, err = s.client.Do(req); err == nil && res.StatusCode == http.StatusOK {
		resBodyBytes, _ := ioutil.ReadAll(res.Body)
		json.Unmarshal(resBodyBytes, &task)

	} else {
		lo.G.Error("client Do Error: ", err)
		lo.G.Error("client Res: ", res)
		err = ErrInvalidDispenserResponse
	}
	return
}
func WriteImageToHTTP(w http.ResponseWriter, img image.Image) error {
	buffer := new(bytes.Buffer)
	if err := png.Encode(buffer, img); err != nil {
		return err
	}

	w.Header().Set("Content-Type", "image/png")
	w.Header().Set("Content-Length", strconv.Itoa(len(buffer.Bytes())))
	if _, err := w.Write(buffer.Bytes()); err != nil {
		return err
	}
	return nil
}
